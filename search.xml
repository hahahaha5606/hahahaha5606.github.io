<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>PPT 转 PDF 小程序</title>
      <link href="/2024/04/22/%E5%88%B6%E4%BD%9C%20PPT%20%E8%BD%AC%20PDF%20%E5%B0%8F%E7%A8%8B%E5%BA%8F/"/>
      <url>/2024/04/22/%E5%88%B6%E4%BD%9C%20PPT%20%E8%BD%AC%20PDF%20%E5%B0%8F%E7%A8%8B%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[<blockquote><p>官方网址：<a href="https://pypi.org/project/Spire.Presentation/">Spire.Presentation · PyPI</a></p><p>参考说明：<a href="https://blog.csdn.net/Eiceblue/article/details/135415462">Python实现PowerPoint（PPT/PPTX）到PDF的批量转换_python ppt转pdf-CSDN博客</a></p><p>参考打包教程：<a href="https://blog.csdn.net/libaineu2004/article/details/112612421">Python脚本打包成exe，看这一篇就够了！_python 打包-CSDN博客</a></p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>3D Gaussian Splatting for Real-Time Radiance Field Rendering 论文笔记</title>
      <link href="/2024/04/20/3DGS%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>/2024/04/20/3DGS%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>论文名称：3D Gaussian Splatting for Real-Time Radiance Field Rendering (实时辐射场渲染的 3D 高斯泼溅)</strong></p><p>论文代码：<a href="https://github.com/graphdeco-inria/gaussian-splatting">GitHub - graphdeco-inria/gaussian-splatting: Original reference implementation of "3D Gaussian Splatting for Real-Time Radiance Field Rendering"</a></p><p>论文主页：</p></blockquote><h1 id="论文基本信息">论文基本信息</h1><p>3D Gaussian Splatting for Real-Time Radiance Field Rendering<sup>[1]</sup>是由来自法国蔚蓝海岸大学的 Bernhard 等人发表在 <strong>2023 SIGGRAPH</strong> 会议上的SCI论文。</p><p>3DGS 之所以如此大受欢迎，原因主要在于以下几个亮点：</p><ul><li><u>SOTA级别的视觉质量</u>：优于 Mip-NeRF360[Barron et al. 2022]<sup>[2]</sup></li><li>以1080p的分辨率进行<u>实时渲染</u>(<span class="math inline">\(\ge30fps\)</span>)</li><li>competitive 的训练时长：比 InstantNGP[Müller et al. 2022]<sup>[3]</sup>、 Plenoxels[Fridovich-Keil and Yu et al. 2022]<sup>[5]</sup>快</li></ul><p><strong>3D Gaussian Splatting 主要工作</strong>：</p><ol type="1"><li><font color=#ef042a>使用 3D Gaussian 球进行场景表示，并通过可微的 tile-based Splatting 技术以1080p的质量实时渲染出来。</font></li><li>3DGS 证明了可以使用非连续的表示来训练快速、高质量的辐射场。</li><li>除了80%的 python 代码外，使用了 InstantNGP 中的优化 CUDA kernels 来加速<u>光栅化 rasterization / 新视角合成</u>。</li></ol><p><strong>数据集</strong>：Mip-NeRF360 数据集、Deep Blending 数据集[Hedman et al. 2018]<sup>[4]</sup>、Tanks&amp;Temples 数据集、Synthetic NeRF 数据集（在这个数据集下，甚至在随机初始化的情况下也达到了高质量）</p><p><strong>sparkling</strong>：3DGS 是重建辐射场的，可以考虑把它扩展到重建三维表面 mesh reconstruction？</p><h1 id="预备知识">预备知识</h1><h2 id="original-splatting">original splatting</h2><blockquote><p><a href="https://zhuanlan.zhihu.com/p/660512916">Splatting 抛雪球法简介 - 知乎 (zhihu.com)</a></p><p><a href="https://www3.cs.stonybrook.edu/~qin/courses/visualization/visualization-splatting.pdf">Slide 1 (stonybrook.edu)</a></p></blockquote><h2 id="dgs-中的-splatting">3DGS 中的 splatting</h2><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240411203920419.png" alt="image-20240411203920419" /><figcaption aria-hidden="true">image-20240411203920419</figcaption></figure><p>之所以用“splatting”即“泼溅”来形容抛雪球算法，是因为它将椭球体的 3D 高斯投影投影到图像平面成为椭圆的过程，就像水珠溅到二维平面上留下印子，是一个从立体三维到平面二维的过程。</p><h2 id="dgs-标识形状">3DGS 标识形状</h2><h2 id="球谐函数标识颜色">球谐函数标识颜色</h2><p>3DGS 中的 SH following standard practice [Fridovich-Keil and Yu et al. 2022; Müller et al. 2022].</p><h1 id="算法全貌">算法全貌</h1><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240413093014740.png" alt="image-20240413093014740" /><figcaption aria-hidden="true">image-20240413093014740</figcaption></figure><p><strong>第一部分：用 3D Gaussian 作为场景表示</strong></p><p>我们模型的输入是一组静态场景的图片，然后使用SfM方法 [Snavely et al. 2006]<sup>[7]</sup>，<u>通过匹配这组场景图片的关键点得到该场景的稀疏点云，同时也估计了相机位姿</u>。用 SfM 过程产生的稀疏点云初始化一组 3D Gaussians。（<span style="background:#eef0f4;">注意其他方法往往还需要多视图立体化 MVS 进行几何初始化，以此作为 SfM 的辅助。因此它们不可避免地继承了来自 MVS 的 artifacts ,在 featureless/shiny 的区域或者薄结构处出现 over 或 under 重建</span>）。</p><p><strong>第二部分：optimization 过程</strong></p><p>在这里将优化 3D 位置、不透明度 <span class="math inline">\(\alpha\)</span> 、各向异性协方差/三维协方差矩阵、颜色/球谐系数等3D Gaussians 属性。优化属性的步骤与自适应的密度控制步骤相交织，在这个过程中我们将添加和偶尔删除 3D Gaussians。Optimization 过程最后将产生所有被测试场景的 1-5 million 高斯的高质量表示（<span style="background:#eef0f4;">in part because highly anisotropic volumetric splats can be used to represent fine structures compactly</span>）。</p><p><strong>第三部分：使用快速GPU排序算法和 tile-based 的 Splatting 技术进行实时渲染</strong></p><p>这一部分是受启发于最近的一个<font color=#985fff> tile-based 的光栅化工作</font>[Lassner and Zollhofer 2021]<sup>[8]</sup>。这里作者的用法有所不同：However, thanks to our 3D Gaussian representation, we can perform anisotropic splatting that respects visibility ordering – <font color=#985fff>thanks to sorting and 𝛼-blending</font> – and enable a fast and accurate backward pass by tracking the traversal of as many sorted splats as required.</p><p>也就是说，作者在这一部分主要使用 <font color=#985fff>GPU 排序算法+tile-based 光栅化+<span class="math inline">\(\alpha\)</span>-blending</font> 来快速渲染高质量的新视图。且该过程是 visibility-aware 的。对应关系分别是：</p><ul><li>tile-based rasterizer——𝛼-blending of anisotropic splats</li><li>fast sorting——respecting visibility</li></ul><h2 id="引言">引言</h2><p><strong>动机</strong>：目前，最有效的辐射场方案一般建模在通过插值<u>体素[Fridovich-Keil and Yu et al. 2022]<sup>[5]</sup>、哈希网格[Müller et al. 2022]<sup>[3]</sup>和点[Xu et al. 2022]<sup>[6]</sup></u>得到的连续表示，虽然这些模型的连续性有助于优化(Plenoxels 和 InstantNGP快，MipNeRF360 质量高)，然而渲染时需要的 stochastic sampling 是 costly 且可能导致噪声的。——具体见“3DGS 与 NeRF 的区别与联系”</p><p><strong>目标</strong>：给定场景的多张图片，能实时渲染出场景，且训练时间应该达到 SOTA 级别。</p><p><strong>初始化 3D 高斯点</strong>：使用SfM方法，即通过匹配一组场景图片的关键点从而给出该场景的点云，同时也估计了相机位姿。SfM方法可以直接拿来用！</p><p><strong>对点密度的控制</strong>：反向传播时，梯度下降会改变 3D 高斯点的值，但是并不改变其数目。因此，3DGS 还对点的密度做了控制，主要是在训练周期执行以下操作：</p><ul><li>删除几乎透明的高斯</li><li>将高斯设置为完全透明，在之后训练时，有用的高斯被重现，无用的则一直保持透明直到被删除</li><li>over-reconstruction 时，迭代分裂范围大的高斯</li><li>under-reconstruction 时，迭代进行高斯克隆</li></ul><h1 id="dgs-与-nerf-的区别与联系">3DGS 与 NeRF 的区别与联系</h1><h2 id="从-ray-marching-的体渲染到-point-based-alpha-blending-渲染">从 ray-marching 的体渲染到 point-based <span class="math inline">\(\alpha\)</span>-blending 渲染</h2><p>NeRF 中计算<strong>每个像素点</strong>颜色的体渲染公式： <span class="math display">\[\hat{C}_{c}(\mathbf{r})=\sum_{i=1}^{N_{c}} w_{i} c_{i}, \quad w_{i}=T_{i}\left(1-\exp \left(-\sigma_{i} \delta_{i}\right)\right) \tag{1} \\T_{i}=\exp \left(-\sum_{j=1}^{i-1} \sigma_{j} \delta_{j}\right)\]</span> 我们也可以将 NeRF 中这个 ray-marhing 的体渲染写成另一种形式，即： <span class="math display">\[\begin{align}&amp;C=\sum^N_{i=1}{T_i \alpha_i \mathbf{c}_i}, \tag{2} \\&amp;where \ \ \alpha_i=(1-exp(-\sigma_i\delta_i))\\&amp;\ \ \ \ \ \ \ \ \ \ \  \ \ T_i=\prod^{i-1}_{j=1}{(1-\alpha_i)}\end{align}\]</span> 通过上式，可计算得射线穿过的那个像素的颜色值。而一种经典的基于点的 neural 方法[Kopanas et al. 2022,2021]通过融合 <span class="math inline">\(\mathcal{N}\)</span> 个 ordered points 来覆盖该像素，从而得到该像素的颜色，计算公式如下： <span class="math display">\[C=\sum_{i \in \mathcal{N}}{c_i\alpha_i \prod^{i-1}_{j=1}{(1-\alpha_i})}，\tag{3}\]</span> 这里 <span class="math inline">\(\mathcal{c}_i\)</span> 是每个 point 的颜色，而不是每个采样点的颜色。<span class="math inline">\(\alpha_i\)</span> 是通过<u>用协方差 <span class="math inline">\(\varSigma\)</span> [Yifan et al. 2019]乘以学习到的 per-point 不透明度</u>来估计的 2D Gaussian 给出。</p><p><span style="background:#fbd4d0;">可以看出上面两种模型成像公式是一样的，但是这两种渲染算法是很不同的。</span></p><ol type="1"><li>NeRF 是连续表示，且通过隐式表达体现占据空间；而 points 是非结构化的离散表示，能更为灵活地构筑几何；</li><li>NeRF 需要 expensive 的随机抽样来采样 (2) 式中的样本点，由此产生噪声和很大的计算成本，因此无法达到 real-time。</li></ol><h2 id="nerf-vs.-3dgs"><strong>NeRF vs. 3DGS</strong></h2><p>在保持 <span class="math inline">\(\alpha\)</span>-blending 表示的体渲染优势的同时，结合 Pulsar[Lassner and Zollhofer 2021]<sup>[8]</sup> 的球体光栅化，3DGS 创造了一种 tile-based 排序渲染器。这种渲染器和 NeRF 中的渲染有着许多差别：训练时，NeRF 以<strong>每个像素</strong>为样本单位，而 3DGS 则以<strong>每张图片</strong>为样本单位，在训练完一张图片的颜色后才反向传播梯度，更新网络的参数。</p><p>此外，3DGS 还创新性地开创了用非连续的表达来进行快速高质量辐射场训练的先河：</p><ul><li>NeRF 的整体模型是连续可微、隐式表达的；</li><li>传统的 mesh 模型是纯离散（通过插值得到不同位置的值）、显式表达的；</li><li>3DGS 介于离散和连续之间：在高斯球内部连续可微、每个高斯球之间是离散的。</li></ul><h1 id="实验">实验</h1><h1 id="bib-tex">Bib Tex</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@Article&#123;kerbl3Dgaussians,</span><br><span class="line">      author       = &#123;Kerbl, Bernhard and Kopanas, Georgios and Leimk&#123;<span class="keyword">\&quot;</span>u&#125;hler, Thomas and Drettakis, George&#125;,</span><br><span class="line">      title        = &#123;3D Gaussian Splatting for Real-Time Radiance Field Rendering&#125;,</span><br><span class="line">      journal      = &#123;ACM Transactions on Graphics&#125;,</span><br><span class="line">      number       = &#123;4&#125;,</span><br><span class="line">      volume       = &#123;42&#125;,</span><br><span class="line">      month        = &#123;July&#125;,</span><br><span class="line">      year         = &#123;2023&#125;,</span><br><span class="line">      url          = &#123;https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="参考文献">参考文献</h1><blockquote><ul><li>适合小白阅读：<a href="https://zhuanlan.zhihu.com/p/688636817">NeRF与3DGS速通 - 知乎 (zhihu.com)</a></li><li>有很多基础知识：<a href="https://zhuanlan.zhihu.com/p/681085846">3d Gaussian splatting笔记 - 知乎 (zhihu.com)</a></li></ul></blockquote><p>[1] Kerbl B, Kopanas G, Leimkühler T, et al. 3d gaussian splatting for real-time radiance field rendering[J]. ACM Transactions on Graphics, 2023, 42(4): 1-14.</p><p>[2] Barron J T, Mildenhall B, Verbin D, et al. Mip-nerf 360: Unbounded anti-aliased neural radiance fields[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 5470-5479.</p><p>[3] Müller T, Evans A, Schied C, et al. Instant neural graphics primitives with a multiresolution hash encoding[J]. ACM transactions on graphics (TOG), 2022, 41(4): 1-15.</p><p>[4] Hedman P, Philip J, Price T, et al. Deep blending for free-viewpoint image-based rendering[J]. ACM Transactions on Graphics (ToG), 2018, 37(6): 1-15.</p><p>[5] Fridovich-Keil S, Yu A, Tancik M, et al. Plenoxels: Radiance fields without neural networks[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 5501-5510.</p><p>[6] Xu Q, Xu Z, Philip J, et al. Point-nerf: Point-based neural radiance fields[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 5438-5448.</p><p>[7] Snavely N, Seitz S M, Szeliski R. Photo tourism: exploring photo collections in 3D[M]//ACM siggraph 2006 papers. 2006: 835-846.</p><p>[8] Lassner C, Zollhofer M. Pulsar: Efficient sphere-based neural rendering[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 1440-1449.</p><p>[9]</p>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SIGGRAPH 2023 </tag>
            
            <tag> 辐射场 </tag>
            
            <tag> NeRF </tag>
            
            <tag> splatting </tag>
            
            <tag> CUDA kernels 加速 </tag>
            
            <tag> InstantNGP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows 复现代码指南</title>
      <link href="/2024/04/20/Windows%20%E5%A4%8D%E7%8E%B0%E4%BB%A3%E7%A0%81%E6%8C%87%E5%8D%97/"/>
      <url>/2024/04/20/Windows%20%E5%A4%8D%E7%8E%B0%E4%BB%A3%E7%A0%81%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<h1 id="windows-复现代码指南">Windows 复现代码指南</h1><p><span style="background:#fbd4d0;">安装环境时需要注意<strong>Python版本</strong>、<strong>Pytorch版本</strong>和<strong>CUDA版本</strong>是否兼容！</span></p><h2 id="python修改pip源">Python修改pip源</h2><h3 id="一步到位版">一步到位版</h3><blockquote><p>参考网址：<a href="https://www.jianshu.com/p/b2412f7fc93f">pip换源一行命令直接搞定 - 简书 (jianshu.com)</a></p></blockquote><h4 id="使用官方源">使用官方源</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip config <span class="built_in">set</span> <span class="keyword">global</span>.index-url  https://pypi.org/simple  </span><br><span class="line">pip install xx -i https://pypi.org/simple</span><br></pre></td></tr></table></figure><h4 id="使用国内源">使用国内源：</h4><p>打开cmd,输入：<code>pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</code></p><p>或者如果临时使用的话，可以使用：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install +库名 -i +源</span><br><span class="line">eg:    pip install numpy -i http://mirrors.aliyun.com/pypi/simple/</span><br></pre></td></tr></table></figure><h3 id="几个国内源">几个国内源</h3><p>阿里云 <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fmirrors.aliyun.com%2Fpypi%2Fsimple%2F">http://mirrors.aliyun.com/pypi/simple/</a> 中国科技大学 <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fpypi.mirrors.ustc.edu.cn%2Fsimple%2F">https://pypi.mirrors.ustc.edu.cn/simple/</a> 豆瓣(douban) <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fpypi.douban.com%2Fsimple%2F">http://pypi.douban.com/simple/</a> 清华大学 <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fpypi.tuna.tsinghua.edu.cn%2Fsimple%2F">https://pypi.tuna.tsinghua.edu.cn/simple/</a> 中国科学技术大学 <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fpypi.mirrors.ustc.edu.cn%2Fsimple%2F">http://pypi.mirrors.ustc.edu.cn/simple/</a></p><h2 id="离线安装pytorch">离线安装Pytorch</h2><p>参考网址：<a href="https://blog.csdn.net/weixin_47142735/article/details/113684365">离线安装Pytorch 最简单 高效的方法_pytorch离线安装_正在学习的浅语的博客-CSDN博客</a></p><h3 id="在线安装指令离线下载地址">在线安装指令&amp;离线下载地址</h3><blockquote><p><strong>在线安装</strong>的一些指令例子：</p><ul><li>安装 torch 的官方指令网址：<a href="https://pytorch.org/get-started/previous-versions/#conda">Previous PyTorch Versions | PyTorch</a></li><li>安装单个 torch 包: pip --default-timeout=1000 install torch==1.10.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html</li><li>同时安装多个包：pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116</li></ul></blockquote><p>在线下载安装包，特别是比较大的安装包，很容易因为网络原因失败:</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231202203943.png" alt="image-20231202203938666" /><figcaption aria-hidden="true">image-20231202203938666</figcaption></figure><p>所以我们考虑先将需要的包下载后，再离线安装。下载地址：<span style="background:#fbd4d0;"><a href="http://download.pytorch.org/whl/torch_stable.html">download.pytorch.org/whl/torch_stable.html</a></span></p><blockquote><p><strong>附上pytorch其他相关包的下载地址：</strong></p><ul><li><p>下载 torch_scatter：<a href="https://pytorch-geometric.com/whl/">pytorch-geometric.com/whl/</a> （点击链接选择对应的 torch 与 cuda，具体可参考 <a href="https://blog.csdn.net/weixin_42421914/article/details/132875571">torch、torch-scatter版本依赖和下载安装教程_torch_scatter下载-CSDN博客</a>）</p></li><li><p><a href="https://mmcv.readthedocs.io/zh-cn/latest/get_started/installation.html">安装 MMCV — mmcv 2.1.0 文档</a></p></li></ul></blockquote><h3 id="下载离线安装包">下载离线安装包</h3><p>点进去后有pytorch安装包、torchaudio安装包和torchvision安装包等，可以通过<span style="background:#eef0f4;">Ctrl + F</span>寻找需要下载的包。主要有两种pytorch安装包（<span style="background:#f9eda6;">一般只使用gpu版本的安装包</span>）：</p><ol type="1"><li><p>cpu版本pytorch,<font color=red>开头为cpu</font>;</p></li><li><p>gpu版本pytorch,<font color=red>开头为cu</font>，如cu111表示gpu版本pytorch，且该pytorch的cuda版本为11.1;</p></li></ol><p>cp表示python版本，linux/window 表示系统版本。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231202204318.png" alt="cpu" /><figcaption aria-hidden="true">cpu</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231202204355.png" alt="gpu" /><figcaption aria-hidden="true">gpu</figcaption></figure><p><strong>注意，我们要根据自己cuda的版本和系统版本来下载安装包，且一定要使Python版本、Pytorch版本和CUDA版本三者兼容！！！</strong>下面附一张版本对应关系表（也可以去问chatgpt）： <img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231202212206.png" alt="cuda_pytorch" /></p><blockquote><ol type="1"><li>torch_scatter 与 pytorch版本对应关系：见<a href="https://pytorch-geometric.com/whl/">pytorch-geometric.com/whl/</a></li><li>torchvision 与 pytorch 版本 linux 系统对应关系（如果直接安装torchvision，可能会自动安装最新的版本，同时也把torch升级到最新，因此安装时可以参考联合安装的方法）</li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240118150625277.png" alt="image-20240118150625277" /><figcaption aria-hidden="true">image-20240118150625277</figcaption></figure></blockquote><h3 id="离线安装以pycharm安装为例">离线安装（以Pycharm安装为例）</h3><p>打开Pycharm中的终端，切换到需要安装包的环境，先使用<span style="background:#dad5e9;">cd指令</span>跳转到下载文件夹；然后<span style="background:#d4e9d5;">pip install 安装包名.后缀</span>进行安装；最后可使用<span style="background:#f9eda6;">conda list 或 python - import torch - torch.cuda.is_available()（需要完成cuda+cudnn+torch配套安装）</span>进行查看。<span style="background:#fbd4d0;">大功告成！</span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231202212635162.png" alt="image-20231203191239667" /><figcaption aria-hidden="true">image-20231203191239667</figcaption></figure><p><strong>值得注意的是，一定要使用纯英文的路径，并且安装包的名字不饿能有任何变动，这些问题都会导致安装的失败。</strong>如果在终端中直接安装，别忘了先用<span style="background:#eef0f4;">conda activate</span>先激活环境。</p><h2 id="安装cuda">安装cuda</h2><h3 id="安装过程">安装过程</h3><p>cuda官网下载地址：<span style="background:#fbd4d0;"><a href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA Toolkit Archive | NVIDIA Developer</a></span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231203184824.png" alt="image-20231203184822148" /><figcaption aria-hidden="true">image-20231203184822148</figcaption></figure><p>按照图中白色位置选择下载包，然后双击选择自定义安装。</p><p>在文件资源管理器中创建一个名为cuxx的文件夹，用来存放临时的cuda安装包；然后设置安装路径（安装包在安装后可直接删除）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203190515923.png" alt="image-20231203190515923" /><figcaption aria-hidden="true">image-20231203190515923</figcaption></figure><p>检查完系统兼容性后，点击<span style="background:#d4e9d5;">同意并继续</span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203190837475.png" alt="image-20231202212635162" /><figcaption aria-hidden="true">image-20231202212635162</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203190858663.png" alt="image-20231203190858663" /><figcaption aria-hidden="true">image-20231203190858663</figcaption></figure><p>勾选<span style="background:#d4e9d5;">自定义</span>，点击<span style="background:#d4e9d5;">下一步</span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203191209483.png" alt="image-20231203191209483" /><figcaption aria-hidden="true">image-20231203191209483</figcaption></figure><p>如果是<strong>第一次安装</strong>需要把组件都勾选上，但是<span style="background:#f9eda6;">浅黄色背景一般情况下只勾选第一个 cuda </span>！如需更新显卡驱动的可以勾选另外两个选项，但是建议不怎么做。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203191239667.png" alt="image-20231203190837475" /><figcaption aria-hidden="true">image-20231203190837475</figcaption></figure><p><span style="background:#fbd4d0;"><strong>选择安装位置，并记住安装路径：</strong></span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203191504491.png" alt="image-20231203192352716" /><figcaption aria-hidden="true">image-20231203192352716</figcaption></figure><p>等待安装完成</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203191624776.png" alt="image-20231203193341199" /><figcaption aria-hidden="true">image-20231203193341199</figcaption></figure><h3 id="验证是否安装成功">验证是否安装成功</h3><p>打开<span style="background:#dad5e9;">设置 - 系统 - 系统信息 - 高级系统设置 - 高级 - 环境变量</span>，出现橙色框中的两个环境变量（也可在cmd中输入<code>set cuda</code>查看cuda设置的环境变量）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203192352716.png" alt="image-20231203191624776" /><figcaption aria-hidden="true">image-20231203191624776</figcaption></figure><p><span style="background:#f9eda6;">重启电脑！！（否则nvcc -V会找不到指令）</span>进入cmd界面，输入 <code>nvcc -V / nvcc --version</code> 查看版本号，出现如下界面，说明 cuda 安装成功啦！</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203193341199.png" alt="image-20231203191504491" /><figcaption aria-hidden="true">image-20231203191504491</figcaption></figure><h2 id="安装cudnn">安装cudnn</h2><p>注：cudnn是用于配置深度学习使用，相当于cuda的一个专为深度学习运算进行优化的补丁</p><h3 id="安装过程-1">安装过程</h3><p>cudnn 官方下载地址：<span style="background:#d4e9d5;"><a href="https://developer.nvidia.com/rdp/cudnn-download">cuDNN Download | NVIDIA Developer</a></span></p><p>进入cudnn下载时需要注册或登录账号，然后选择需要对应版本下载安装包。这里下载的是压缩后的安装包。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203193624549.png" alt="image-20231203193813587" /><figcaption aria-hidden="true">image-20231203193813587</figcaption></figure><p>解压后出现三个文件夹和License</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203193813587.png" alt="image-20231203193624549" /><figcaption aria-hidden="true">image-20231203193624549</figcaption></figure><p><span style="background:#fbd4d0;">找到cuda的安装路径，如：<strong>C:FilesGPU Computing Toolkit1.1</strong></span>。将cudnn三个文件夹里的内容分别替换到对应的文件夹里。</p><p>打开<span style="background:#dad5e9;">设置 - 系统 - 系统信息 - 高级系统设置 - 高级 - 环境变量</span>，再找到<span style="background:#d4e9d5;">系统变量 - path</span>，将以下三个变量添加进去，完成安装(<font color=red>注意修改对应变量名喔</font>)</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\bin</span><br><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\include</span><br><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\lib</span><br><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\libnvvp</span><br></pre></td></tr></table></figure><h3 id="验证是否安装成功-1">验证是否安装成功</h3><p>在cmd中使用<span style="background:#d4e9d5;">cd命令</span>进入cuda的安装路径找到测试工具，如：<span style="background:#f9eda6;">C:FilesGPU Computing Toolkit1.1_suite</span>，执行如下两个.exe文件</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203195334390.png" alt="image-20231203195334390" /><figcaption aria-hidden="true">image-20231203195334390</figcaption></figure><p>执行：<code>deviceQuery.exe</code>查询本机的GPU设备和<code>bandwidthTest.exe</code>测试带宽，如果结果都为PASS，说明运行正常</p><h2 id="安装多个版本的cuda">安装多个版本的cuda</h2><blockquote><p>详细步骤参考：<a href="https://blog.csdn.net/m0_46579864/article/details/122887343">详细讲解如何在win10系统上安装多个版本的CUDA_如何同时安装cuda11.8 和 cuda12.0-CSDN博客</a></p><p>下面假设新安装的 cuda 版本为 11.7。</p></blockquote><h3 id="安装步骤">安装步骤</h3><p>1.首先老规矩，在命令行 <code>cmd</code> 输入 <code>nvidia-smi</code> 查看电脑驱动支持的最高版本 cuda (作者这里查看了，是 cuda 12.3)。</p><p>2.新建文件夹<code>v11.7</code>存放cuda ,这一步选择默认位置 <span style="background:#fbd4d0;">C:GPU Computing Toolkit</span> 新建即可，方便之后切换环境时更改地址里的版本号。</p><p>3.在官网上下载好 <code>CUDA Toolkit</code> 包进行离线安装（网速快的话在线安装也可）。<span style="background:#fbd4d0;">这里需要注意的是，安装选项选择<strong>自定义</strong>，且自定义安装选项里只勾选<strong>CUDA 组件</strong>即可</span>。</p><p>4.为新安装的cuda配置cudnn :将下载的<strong>cuDNN</strong>压缩包解压，并把<strong>cuda</strong>下的“bin”、“include”、“lib”这三个文件拷贝到新CUDA的安装位置上。</p><p>5.添加新cuda到环境变量：在地址栏搜索“环境变量”直接进入，在环境变量中直接更改之前cuda环境变量的版本号就OK！</p><p>6.重启电脑，通过在终端输入指令<code>nvcc -V</code> 与 <code>set cuda</code> 查看cuda是否安装成功。</p><h3 id="切换不同版本的cuda">切换不同版本的cuda</h3><p>搜索“环境变量”进入，在<code>PATH</code>中将想要切换的cuda对应环境变量移动到最上面。</p><h2 id="拆卸某版本的cuda">拆卸某版本的cuda</h2><blockquote><p>详情参考：<a href="https://segmentfault.com/a/1190000044598668">Windows 下 CUDA, cudnn, pytoch 卸载、更新、安装 - 个人文章 - SegmentFault 思否</a></p><p>这里以拆卸 cuda 11.2 为例</p></blockquote><p>1.打开<code>控制面板</code>或<code>设置-应用</code>，拆卸含"cuda 11.2"字样的程序。</p><p>2.搜索<code>环境变量</code>进入，删除cuda11.2的环境变量。</p><p>3.进入cuda目录，一般为<code>C:\ProgramData\NVIDIA GPU Computing Toolkit\CUDA</code>和<code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA</code>，删除残留文件。</p><h1 id="问题汇总">问题汇总：</h1><h2 id="cuda-driver-version-is-insufficient-for-cuda-runtime-version"><strong>CUDA driver version is insufficient for CUDA runtime version</strong></h2><p>cuda驱动程序版本和cuda运行时版本不匹配：问题可大可小，可能是程序装错了，也可能你的电脑根本配不了cuda环境（如amd显卡）</p><h2 id="failed-to-initialize-nvml-unknown-error"><strong>Failed to initialize NVML: Unknown Error</strong></h2><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216143546376.png" alt="image-20231216143546376" /><figcaption aria-hidden="true">image-20231216143546376</figcaption></figure><p>在复现代码时，配置完cuda后，想验证<code>torch.cuda.is_available()</code>，结果过了很久也没有输出；然后又输入<code>nvidia-smi</code>查看显卡配置，报错！</p><p>在网上查找后发现了几个方法：</p><h3 id="win11下运行nvidia-smi报错failed-to-initialize-nvml-unknown-error-csdn博客"><strong><a href="https://blog.csdn.net/qq_39499680/article/details/134855395">WIN11下运行nvidia-smi报错Failed to initialize NVML: Unknown Error-CSDN博客</a></strong></h3><p>使用Everything(或电脑自带的文件搜索)去查找nvidia-smi的位置，然后进入文件目录（如：<span style="background:#d4e9d5;">C:_dispig.inf_amd64_49aadc39d4f73881</span>），点击运行 <span style="background:#f9eda6;color:red">setup.exe</span> 重新安装即可。</p><p>尝试方法一，在点击 setup.exe 后报错：<span style="background:#fb6172d0;color:white">所需文件丢失</span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216144502672.png" alt="image-20231216144502672" /><figcaption aria-hidden="true">image-20231216144502672</figcaption></figure><h3 id="考虑重新安装nvidia驱动"><strong>考虑重新安装nvidia驱动</strong></h3><h4 id="卸载驱动">2.1 卸载驱动</h4><p>打开<font color=purple>设备管理器</font>，点击<span style="background:#dad5e9;">显卡适配器-驱动程序-卸载设备</span>，然后重启电脑</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216151900323.png" alt="image-20231216151900323" /><figcaption aria-hidden="true">image-20231216151900323</figcaption></figure><p>发现这时显卡已经变为<font color=purple>Microsoft基本显示适配器</font>：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216152946150.png" alt="image-20231216152946150" /><figcaption aria-hidden="true">image-20231216152946150</figcaption></figure><h4 id="重新安装驱动">2.2 重新安装驱动</h4><p>打开nvidia驱动下载地址：<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn#">官方驱动 | NVIDIA</a>，按自己的电脑型号选择合适的驱动（笔记本产品系列是<font color=red>notebooks</font>），开始下载</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216153411364.png" alt="image-20231216153411364" /><figcaption aria-hidden="true">image-20231216153411364</figcaption></figure><p>完成下载后双击，在解压提示框中点击OK，等待进度完成（此处默认安装路径是：<font color=orange>C:\546.33_Win10-DCH_64</font>）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154244543.png" alt="image-20231216154244543" /><figcaption aria-hidden="true">image-20231216154244543</figcaption></figure><p>等待检查系统兼容性完成，后续工作见图</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154626870.png" alt="image-20231216154626870" /><figcaption aria-hidden="true">image-20231216154626870</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154713103.png" alt="image-20231216154713103" /><figcaption aria-hidden="true">image-20231216154713103</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154731465.png" alt="image-20231216154731465" /><figcaption aria-hidden="true">image-20231216154731465</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154826092.png" alt="image-20231216154826092" /><figcaption aria-hidden="true">image-20231216154826092</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154918798.png" alt="image-20231216154918798" /><figcaption aria-hidden="true">image-20231216154918798</figcaption></figure><blockquote><p>安装快完成时会提示<strong>是否立即重启计算机完成安装</strong>，点击确定前记得保存未保存的文件</p></blockquote><p>耐心等待安装完成后，再次打开<strong>设备管理器</strong>，即可发现安装成功！</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216155635451.png" alt="image-20231216155635451" /><figcaption aria-hidden="true">image-20231216155635451</figcaption></figure><p>我们打开终端，输入<code>nvidia-smi</code>,成功输出显卡驱动信息</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216155757435.png" alt="image-20231216155757435" /><figcaption aria-hidden="true">image-20231216155757435</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 环境搭建 </tag>
            
            <tag> cuda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/04/19/%E7%82%B9%E4%BA%91%E4%B8%8E%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"/>
      <url>/2024/04/19/%E7%82%B9%E4%BA%91%E4%B8%8E%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<p><a href="https://aitechtogether.com/article/46747.html">浅谈点云与三维重建 | AI技术聚合 (aitechtogether.com)</a></p><p><a href="https://blog.csdn.net/Yong_Qi2015/article/details/118123852">3D点云补全算法汇总及最新进展-CSDN博客</a></p><p><a href="https://zhuanlan.zhihu.com/p/370997659">CVPR 2021 | RfD-Net: 从点云中重建三维物体实例 - 知乎 (zhihu.com)</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Diffusion Model（二）</title>
      <link href="/2024/04/14/Diffusion%20Model%20%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2024/04/14/Diffusion%20Model%20%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="引入">引入</h1><p><strong>模型网站：</strong></p><p>DALL·E2： <a href="https://openai.com/dall-e-2">DALL·E 2 (openai.com)</a></p><p>DALL·E3：<a href="https://openai.com/dall-e-3">DALL·E 3 (openai.com)</a></p><p>Sora: <a href="https://openai.com/sora?ref=aihub.cn">Sora (openai.com)</a></p><h2 id="发展简介">发展简介</h2><blockquote><p><a href="https://zhuanlan.zhihu.com/p/593896912">李沐论文精读系列——由DALL·E 2看图像生成模型 - 知乎 (zhihu.com)</a></p><p><a href="https://swarma.org/?p=37227">AI艺术的背后：详解文本生成图像模型 | 集智俱乐部 (swarma.org)</a></p></blockquote><p>近几年图像生成模型的工作：</p><figure><img src="https://pic1.zhimg.com/80/v2-6350528949be8a46b618e7fb84c28e04_720w.webp" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="基于vq-vae">基于VQ-VAE</h2><p>顺序：<code>AE</code>(20世纪80年代)——<code>VAE</code>（Kingma et al, 2014<sup>[2]</sup>）——<code>VQ-VAE</code>(Van et al, 2017<sup>[3]</sup>)——<code>DALL·E</code>（Ramesh et al, 2021[4]）</p><h3 id="ae">AE</h3><p>自编码器（Auto Encoder）主要由编码器 encoder 和解码器 decoder 组成，目标是重建出输入图像。首先，输入图像 x 通过编码器被压缩，提取出高维特征 latent feature，然后这些高维特征再经过解码器重建出图像 x' 。AE 希望重建出的图像和输入图像越接近越好，因此损失函数为 <span class="math inline">\(loss=||x-x&#39;||^2\)</span> 。通常 latent feature 的维度比输入、输出的维度小，因此称之为 bottleneck。</p><figure><img src="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-d690623495c81ce7adb97e3eab397475.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>由于 AE 是一个自重建的过程，因此被称为自编码器，实际常用于降维，去噪，异常检测或者神经风格迁移中。</p><h3 id="vae">VAE</h3><p>VAE（Variational Auto Encoder）仍然由一个编码器和一个解码器构成，并且目标仍然是重建原始输入。与 AE 不同的是，VAE 的中间表征是通过采样高斯分布得到的 <span class="math inline">\(z\)</span> 。首先，VAE 将原图 <span class="math inline">\(x\)</span> 通过编码器网络映射为高斯分布 $( _x,_x ) $ 。然后通过重参数技巧在该分布中采样中间表征 $z=_x+_x$ ，其中 <span class="math inline">\(\epsilon\thicksim \mathcal{N}\left( 0,\mathbf{I} \right)\)</span> 。最后，解码器解码 <span class="math inline">\(z\)</span>​ 重建出图像。其损失函数定义为： <span class="math display">\[loss=||x-\hat{x}||^2+D_{KL}\left[ \mathcal{N}\left( \mu _x,\sigma _x \right) ,\mathcal{N}\left( 0,\mathbf{I} \right) \right]\]</span> 在 VAE 的损失函数中，第二项是之所以让编码器输出的分布尽可能接近标准正态分布，是因为这样在生成时可以直接从正态分布中采样，然后通过解码器重建图像（怪怪的）。</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/574959469">事实上，编码器拟合的是后验概率 <span class="math inline">\(p(z|x)\)</span> ，解码器拟合的是似然函数 <span class="math inline">\(p(x|z)\)</span>。</a>似然函数理解为，在模型参数 <span class="math inline">\(z\)</span> 下，使得模型输出为 <span class="math inline">\(x\)</span> 的概率最大化的函数。</p></blockquote><figure><img src="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-249c281b4571a5b27ac531512e7c1cc0.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>VAE 提高了生成结果的多样性（引入了符合高斯分布的随机变量）。对于同一个输入，由于 latent vector 不再是固定的，而是采样得到，我们可以得到不同的但类似的任意多个输出。</p><h3 id="vq-vae">VQ-VAE</h3><p>VAE 具有一个最大的问题就是使用了固定的先验（正态分布），其次是使用了连续的中间表征，这样会导致图片生成的多样性并不是很好以及模型的可控性差。为了解决这个问题，VQ-VAE（ Vector Quantized Variational Auto Encoder） 选择使用<strong>离散的中间表征</strong>，同时，通常会使用一个自回归模型来学习先验（例如 PixelCNN 或者 Transformer），在训练完成后，直接用来采样得到 <span class="math inline">\(z_q\)</span>​ ，然后通过匹配 Codebook, 使用解码器进行图片生成。</p><blockquote><p>VAE的目的是训练完成后, 丢掉 encoder, 在 prior 上直接采样, 加上 decoder 就能生成. 如果我们现在独立地采 <span class="math inline">\(H×W\)</span> 个 <span class="math inline">\(z\)</span> 组成 <span class="math inline">\(z_e(x)\)</span> , 然后查表得到维度为 <span class="math inline">\(H×W×D\)</span> 的 <span class="math inline">\(z_q(x)\)</span>，那么生成的图片在空间上的每块区域之间几乎就是独立的。<strong>因此我们需要让各个 <span class="math inline">\(z\)</span> 之间有关系</strong>。用 PixelCNN, 对这些 <span class="math inline">\(z\)</span> 建立一个自回归模型： <span class="math inline">\(p(z_1,z_2,z_3,...)=p(z_1)p(z_2|z_1)p(z_3|z_1,z_2)...\)</span> 这样就可以进行 <a href="https://hyper.ai/wiki/2986">ancestral sampling</a> 生成 <span class="math inline">\(x\)</span>, 得到一个互相之间有关联的 <span class="math inline">\(H×W\)</span> 的整数矩阵。 <span class="math inline">\(p(z_1,z_2,z_3,...)\)</span> 这个联合概率即为我们想要的 prior。——<a href="https://zhuanlan.zhihu.com/p/91434658">Elijha：VQ-VAE解读</a></p><p>The prior distribution over the discrete latents <span class="math inline">\(p(z)\)</span> is a categorical distribution, and can be made autoregressive by depending on other <span class="math inline">\(z\)</span> in the feature map. Whilst training the VQ-VAE, the prior is kept constant and uniform. After training, we fit an autoregressive distribution over <span class="math inline">\(z\)</span>, <span class="math inline">\(p(z)\)</span>, so that we can generate <span class="math inline">\(x\)</span> via ancestral sampling. We use a PixelCNN over the discrete latents for images, and a WaveNet for raw audio. Training the prior and the VQ-VAE jointly, which could strengthen our results, is left as future research. ——来自<a href="https://arxiv.org/pdf/1711.00937.pdf">原论文</a></p></blockquote><figure><img src="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-b761643dc19adceb1734df82210eda76.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p><strong>VQ-VAE 的算法流程为：</strong></p><ol type="1"><li><span style="background:#dad5e9;">（Embedding Space）</span> 首先设置 <span class="math inline">\(K\)</span> 个 <span class="math inline">\(D\)</span> 维向量 <span class="math inline">\(e_1,e_2...e_k\)</span> 作为可查询的 Codebook。</li><li><span style="background:#daf5e9;">（绿方块）</span> 输入图片通过编码器 CNN 来得到中间表征 <span class="math inline">\(z_e(x)\)</span>，<span class="math inline">\(z_e(x)\)</span> 是 <span class="math inline">\(H×W\)</span> 个 <span class="math inline">\(D\)</span> 维向量。</li><li><span style="background:#dcffff;">（蓝方块）</span> 通过最邻近算法，在 Codebook 中查询与 <span class="math inline">\(z_e(x)\)</span> 中每个 <span class="math inline">\(D\)</span> 维向量最相似的向量 <span class="math inline">\(e_i^{(1)},e_i^{(2)}...e_i^{(H×W)}\)</span> ，用 index 表示，就得到了 <span class="math inline">\(q(z|x)\)</span> 。</li><li><span style="background:#dad5e9;">（紫方块）</span>根据 <span class="math inline">\(q(z|x)\)</span> 将 Codebook 中查询的相似向量放到对应 <span class="math inline">\(z_e(x)\)</span> 的位置上，得到 <span class="math inline">\(z_q(x)\)</span>​ 。</li><li>解码器通过得到的中间表征 <span class="math inline">\(z_q(x)\)</span> 重建图片。</li></ol><p>一般 <span class="math inline">\(K=8192\)</span>，<span class="math inline">\(D=512\)</span> or <span class="math inline">\(768\)</span>。</p><p>VQ-VAE 最核心的部分就是 <strong>Codebook 查询操作</strong>，通过使用具有高度一致性的 Codebook 来代替混乱的中间表征，可以有效的提高图像生成的可控性。初代的DALL·E 模型就是基于 VQ-VAE 的架构实现的。</p><h3 id="delle">DELL·E</h3><blockquote><p>Project page: <a href="https://openai.com/research/dall-e">DALL·E: Creating images from text (openai.com)</a></p></blockquote><p>文本生成图像模型 DALL·E 由 OpenAI 开发，其第一代版本使用的是在 VQ-VAE 自回归生成的基础上加上文本条件，实现了 <a href="https://zhuanlan.zhihu.com/p/624793654">zero-shot</a> 的 text-to-image 生成。</p><p>在 DELL·E 中使用了 Transformer 来作为自回归模型。在生成过程中，输入文本通过 Transformer 预测中间表征 <span class="math inline">\(z\)</span> ,然后匹配 <span class="math inline">\(K=8192\)</span> 的 Codebook得到 <span class="math inline">\(z_q\)</span> ，最后通过 Decoder 模块将 <span class="math inline">\(z_q\)</span>​ 解码为图像。 具体过程见<a href="https://zhuanlan.zhihu.com/p/683882116">DALL-E 系列 (1-3) - 知乎 (zhihu.com)</a>。在 DALL-E 的论文中，作者还提出了很多技术上的细节，例如，在最后挑选图片的时候，可以使用 CLIP 模型来选择与文本相似度最高的模型，以及分布式训练，混合精度训练等，具体细节可以查看<a href="https://arxiv.org/pdf/2102.12092.pdf">原论文</a>。</p><blockquote><p>CLIP (Contrastive Language-Image Pre-training，对比图文预训练) 是一种 zero-shot 的视觉分类模型，它是和 DALL·E 一起被发布的。</p><p>CLIP 是一种神经网络，为输入的图像返回最佳的标题。它所做的事情与 DALL-E 所做的相反 —— 它是将图像转换为文本，而 DALL-E 是将文本转换为图像。引入 CLIP 的目的是为了学习物体的视觉和文字表示之间的联系。</p></blockquote><figure><img src="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-5bb80123f487ae06090f48943cfa8322.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="基于-gan">基于 GAN</h2><p>顺序： <code>GAN</code>（Creswell et al, 2014<sup>[5]</sup>）——<code>VQGAN</code>（Esser et al, 2021<sup>[6]</sup>）——<code>VQGAN-CLIP</code>（Crowson et al, 2022<sup>[7]</sup>）——<code>DELL·E Mini</code>——<code>Parti</code>（Google, 2022 在 Imagen 之后一个月推出, AI 绘图）——<code>NUWA-Infinity</code>（微软亚洲研究院， 2022， 无限创作）</p><h3 id="gan">GAN</h3><p><strong>生成对抗网络</strong>（<strong>GAN</strong>，Generative Adversarial Networks）由两个主要的模块构成：生成器和判别器。生成器负责生成一张图片，而判别器则负责判断这张图片质量，也就是判断是真实样本还是生成的虚假样本，通过逐步的迭代，左右互博，最终生成器可以生成越来越逼真的图像，而判别器则可以更加精准的判断图片的真假。GAN 的最大优势是其<strong>不依赖于先验假设，而是通过迭代的方式逐渐学到数据的分布</strong> [6]。</p><p>其训练流程如下：</p><ol type="1"><li>初始化一个生成器 <span class="math inline">\(G\)</span> 和一个判别器 <span class="math inline">\(D\)</span> ;</li><li>固定生成器 <span class="math inline">\(G\)</span> 的参数， 只更新判别器 <span class="math inline">\(D\)</span> 的参数。具体过程为：选择一部分真实样本，以及从生成器 <span class="math inline">\(G\)</span> 得到一些生成的样本，送入到判别器 <span class="math inline">\(D\)</span> 中，判别器需要判断哪些样本为真实的，哪些样本为生成的，通过与真实结果的误差来优化判别器;</li><li>固定判别器 <span class="math inline">\(D\)</span> 的参数, 只更新生成器 <span class="math inline">\(G\)</span> 的参数。具体过程为：使用生成器生成一部分样本， 将生成的样本喂入到判别器 <span class="math inline">\(D\)</span> 中，判别器会对其进行判断，优化生成器 <span class="math inline">\(G\)</span> 的参数，使得判别器将其判断为更加偏向于真实样本。</li></ol><h3 id="vqgan">VQGAN</h3><p>VQGAN （ Vector Quantized Generative Adversarial Networks ） 是一种 GAN 的变种，一种视觉生成模型,来自德国海德堡大学IWR研究团队受到 VQ-VAE 的启发，<strong>使用了 Codebook 来进行离散表征</strong>。</p><p>具体来说，预先定义 <span class="math inline">\(K\)</span> 个向量作为离散的特征查询表 Codebook。当一张图片被送入到 CNN Encoder 中后，会得到 <span class="math inline">\(h×w×n_z\)</span> 维的中间表征 <span class="math inline">\(\hat{z}\)</span> ,之后与 VQ-VAE 类似地查询 Codebook 得到 <span class="math inline">\(z_q\)</span> 。最后，CNN Decoder 也就是生成器 <span class="math inline">\(G\)</span> 根据得到的表征 <span class="math inline">\(z_q\)</span> 重建出图像。其中，通过 <span class="math inline">\(\hat{z}\)</span> 得到 <span class="math inline">\(z_q\)</span> 的公式为： <span class="math display">\[z_q=q(\hat{z}):=\left( \underset{z_k\in \mathcal{Z}}{arg\ \min}||\hat{z}_{ij}-z_k|| \right) \in \mathbb{R}^{h×w×n_z}\]</span> 同样地，这里的对于生成器部分的优化，需要使用与 VQ-VAE 一样的方法去进行：</p><p>在训练好 VQGAN 之后，在生成的时候，可以直接初始化一个 <span class="math inline">\(z_q\)</span> 去生成图像，然而，为了能够得到稳定的 <span class="math inline">\(z_q\)</span> ，需要使用一个模型对先验进行学习。这里使用了 Transformer 模型来学习 <span class="math inline">\(z_q\)</span> 中离散表征的序列，可以简单的将其建模为自回归模型 <span class="math inline">\(p(s)=\prod_i{p(s_i|s_{&lt;i})}\)</span> ，这样，我们只需要给定一个初始的随机向量，就可以通过 Transfomer 模型生成完整的 <span class="math inline">\(z_q\)</span> ，从而可以通过 CNN Decoder 模块生成最终的图像。</p><figure><img src="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-dfb34aff6b13b8f38a0ed05e4c0b0de1.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><blockquote><p>这里的判别器不是对每张整体图片进行判断，而是对图片的每一小块进行判断。</p></blockquote><h2 id="基于-diffusion">基于 Diffusion</h2><p>顺序: Diffusion（Sohl-Dickstein et al, 2015<sup>[12]</sup>）——Diffusion DDPM（ Ho et al, 2020<sup>[1]</sup>）——GLIDE（Nichol et al, 2021.12<sup>[8]</sup>）——DALL·E2（Ramesh, 2022.4<sup>[9]</sup>）——Imagen（Saharia, 2022.5<sup>[10]</sup>）——Stable Diffusion(Rombach et al, 2022<sup>[11]</sup>)——ChatGPT（2022.11）——ChatGPT-4（2023.3）——DALL·E3（2023.10）——Sora（2024.2）</p><h3 id="diffusion">Diffusion</h3><p>回忆上文提到的 VQ-VAE 以及 VQ-GAN，都是先通过编码器将图像映射到中间潜变量，然后解码器在通过中间潜变量进行还原。实际上，扩散模型做的事情本质上是一样的，不同的是，扩散模型完全使用了全新的思路来实现这个目标。</p><blockquote><p><strong>为什么叫“扩散”模型呢？</strong> 扩散这个名词来自于热力学，某个区域物质密度很高，那么它就会向低密度的区域扩散，最终达到平衡；就好比香水的味道会扩散到整个房间。这里的平衡指的是标准高斯分布的噪声。图片由原来的有序，逐渐变为无序的噪声，可以认为是“无序度”的扩散。</p></blockquote><p>在传统的扩散模型中，主要有两个过程组成，<strong>前向扩散过程，反向去噪过程</strong>，前向扩散过程主要是在一张图片上随机添加高斯噪声，而逆向去噪过程则是将一张随机噪音的图片还原为一张完整的图片。</p><p>最初的 Diffusion 在2015 年甚至更早就被提出了，当时有人基于非平衡热力学提出了一个纯数学的生成模型（Sohl-Dickstein et al, 2015<sup>[12]</sup>）。这种生成模型通过迭代正向扩散破坏数据分布结构，并通过学习反向扩散过程恢复数据结构。最初的模型并没有用代码实现。同时由于 forward 和 backward 训练采样和推理都需要很长时间，所以扩散模型一直不如 GAN 受关注，直到2020年 DDPM 的出现。DDPM 主要有两个贡献：</p><ol type="1"><li>不需要推测 <span class="math inline">\(x_t\)</span> ，只需要推测 <span class="math inline">\(\epsilon_\theta\)</span> 就可以了。这样大大降低了推理的难度。这里计算梯度的网络是 U-net。</li><li>由于每步都是高斯噪声，只需要预测它的均值和方差即可。并且作者提出，固定方差 <span class="math inline">\(\sigma\)</span>​ ，仅仅预测噪声的均值就可以达到很好的效果。</li></ol><p>DDPM 中 Diffusion 的损失函数如下（具体推导见下文）： <span class="math display">\[L=\lVert \boldsymbol{\epsilon }_t-\boldsymbol{\epsilon }_{\theta}\left( \sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon }_t,t \right) \rVert ^2\]</span> 其算法流程为：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240320200738823.png" alt="image-20240320200738823" /><figcaption aria-hidden="true">image-20240320200738823</figcaption></figure><p>在完成训练之后，只需要通过重参数化技巧，进行采样操作即可，具体流程如上边右图所示，通过不断的「减去」模型预测的噪音，可以逐渐生成一张完整的图片。</p><p><strong>Diffusion 的初步改进： classifier guided diffusion</strong></p><p>DDPM的成功引起了大家的兴趣。2020年底， <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2102.09672.pdf">improved DDPM</a> 就出炉了。它在DDPM的基础上，不仅仅预测均值，而且预测方差；并且在更大模型上尝试了DDPM，发现大模型会带来很大的效果提升。于是紧接着有一篇 <a href="https://arxiv.org/pdf/2105.05233.pdf">Diffusion Models Beat GANs on Image Synthesis</a>（2021.5, OpenAI），进一步把模型做大、做复杂，取得了很好的效果；并且，作者在这篇论文里使用了 <code>classifier guidance</code> 的方法，引导模型做采样和生成。这不仅让生成的图像更逼真，而且加速了反向采样的速度，只需要25次采样就能从噪声还原出高质量的图片。</p><p>简单来说，<code>classifier guided diffusion</code>使用一个额外训练好的图像分类器 classifier ，每次判别 <span class="math inline">\(x_t\)</span> 的类别，得到交叉熵损失函数，进一步可以得到梯度 gradient ，用这个梯度去引导 <span class="math inline">\(f_\theta\)</span> （这里的 <span class="math inline">\(f_\theta\)</span>​ 是如上文的 U-net）做预测。这里的梯度大概暗含了当前生成的图像有没有某个物体，以及生成的物体真不真实。通过引导，扩散模型的逼真度提升了很多，击败了一些GAN模型。</p><figure><img src="https://pic3.zhimg.com/80/v2-9d9e7d2039c35f9f82056b3c180cd4c2_720w.webp" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>这和 conditional GAN 的思路差不多，提供更多的 condition，也就是引导，辅助它完成任务。这里的引导不一定非得是一个分类器，可以是文本，也可以是CLIP模型。</p><p><strong>Diffusion 的再次改进： Classifier-Free Guidance Diffusion</strong></p><p>后续又有一些改进操作，这些改进操作使得扩散模型被广泛的应用于文本生成图像任务中。其中，最常用的改进版本为 Classifier-Free Guidance Diffusion。OpenAI 后续的 <strong>GLIDE</strong> 模型和 <strong>DALL·E 2</strong> ，以及谷歌的 <strong>Imagen</strong>，在推理过程中抛弃了分类器的引导——Classifier free guidance。</p><p>这种扩散模型在每个时间步 <span class="math inline">\(t\)</span>， 除了原有的无引导情况下的输出 <span class="math inline">\(\epsilon_\theta (x_t)\)</span> 外，还增加了在有引导情况下的输出 <span class="math inline">\(\epsilon_\theta (x_t,y)\)</span> ，由此 Classifier-Free Guidance Diffusion 结合了条件和无条件噪声估计模型，定义为： <span class="math display">\[\hat{\epsilon}_\theta(x_t|y)=\epsilon_\theta(x_t)+s·(\epsilon_\theta(x_t,y)-\epsilon_\theta(x_t))\]</span> 通过有引导时的输出与无引导时的输出作差，就得到了一个方向，告诉网络如何从无引导的输出到达有引导的输出。这样在训练完成后，就能达到抛开引导的目的。这种改进优点是<strong>训练过程非常稳定，且摆脱了分类器的限制</strong>（实际上等价于学习了一个隐含的分类器），缺点是，训练成本比较高，相当于每次要生成两个输出，尽管如此，后面的大部份知名文本生成图像模型，都是基于这个方法进行的。</p><p>在使用了很多技巧之后，基于扩散模型的 <strong>GLIDE</strong> 用了35亿参数，效果就直逼基于VQ-VAE 的、用了120亿参数的<strong>DALL·E</strong> 模型。OpenAI 看到扩散模型确实靠谱，所以顺着 GLIDE 的思路，孕育出了现在的 <strong>DALL·E 2</strong>。</p><h3 id="glide">GLIDE</h3><blockquote><p>代码已开源</p></blockquote><p>GLIDE 使用了文本作为 condition 来引导扩散模型，主要用了两种策略: Classifier-Free Guidance Diffusion 和 CLIP 来作为条件监督。GLIDE 的扩散模型中的 <span class="math inline">\(\hat{\epsilon}_\theta(x_t|y)\)</span> 中的 <span class="math inline">\(y\)</span> 是一段文本描述。</p><p>GLIDE的生成效果。下图是GLIDE基于不同的文本提示生成的16个图像集，例如“使用计算器的刺猬”、“戴着红色领带和紫色帽子的柯基”等等，如图所示，生成的图像基本符合文本描述。</p><figure><img src="https://img-blog.csdnimg.cn/img_convert/d2e902a630e984757eb922d0b5662bdc.png" alt="d2e902a630e984757eb922d0b5662bdc.png" /><figcaption aria-hidden="true">d2e902a630e984757eb922d0b5662bdc.png</figcaption></figure><p>除了图文转换，该论文还包括一个交互式系统的原型，支持通过 <strong>选取区域+文本Prompt</strong> 来对图像进行编辑操作，用于逐步细化图像的选定部分。使用过程中，只需要将遮蔽区域进行 mask，以及剩下的图片一起送入到网络中，即可产生补全之后的图片。</p><figure><img src="https://img-blog.csdnimg.cn/img_convert/da853db5c9f392a658cf03922989c6c4.png" alt="da853db5c9f392a658cf03922989c6c4.png" /><figcaption aria-hidden="true">da853db5c9f392a658cf03922989c6c4.png</figcaption></figure><p>此外，<strong>GLIDE 的语义理解能力并不是很强，在一些少见的文本描述下（如八条腿的猫），很难产生合乎逻辑的图像，而 DALL-E2 在这方面的能力上，要远超 GLIDE</strong></p><figure><img src="https://img-blog.csdnimg.cn/img_convert/65b3e3049c10e65525e3e5e92f57b60b.png" alt="65b3e3049c10e65525e3e5e92f57b60b.png" /><figcaption aria-hidden="true">65b3e3049c10e65525e3e5e92f57b60b.png</figcaption></figure><h3 id="dalle2">DALL·E2</h3><p>DALL-E2 是 OpenAI 2022年4月推出的 AI 生成图像模型，其最大的特色是<strong>模型具有惊人的理解力和创造力</strong>，它可以根据给定的概念、特性以及风格来生成原创性的图片。除此之外，DALL·E 2 还能根据描述，对已有的图片进行二次编辑，比如移除或添加某个物体，并且把阴影、反射、纹理考虑在内。还有，就算不给定语言描述，DALL·E 2 也能根据已有的图片，生成一系列风格相似的新的图片。 其参数大约 3.5B , 相对于上一代版本，DALL-E2 可以生成4倍分倍率的图片，且非常贴合语义信息。作者使用了人工评测方法，让志愿者看1000张图，71.7% 的人认为其更加匹配文本描述 ，88.8% 认为画的图相对于上一代版本更加好看。</p><figure><img src="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-0544da18e16e0b011f1d4d85d5b1cf07.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p><strong>DALL-E2 由三个模块组成</strong>（相当于两阶段生成，第一阶段用 prior 模型从文本生成图像特征；第二阶段用 decoder 生成图像）：</p><ul><li>CLIP模型，对齐图片文本表征</li><li>先验模型，接收文本信息，将其转换成 CLIP 图像表征</li><li>扩散模型，接受图像表征，解码来生成完整图像</li></ul><figure><img src="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-0fce8bc8d73882ce798bb7e4b18763df.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p><strong>DALL-E2 的训练过程</strong>为：</p><ul><li>训练一个 CLIP 模型，使其能够对齐文本和图片特征。</li><li>训练一个先验模型，由自回归模型或者一个扩散先验模型（实验证明，扩散先验模型表现更好。这里的扩散模型是用 sequence (text feature) 预测 sequence (image feature)，没有要求前后尺寸不变。作者这里没有用 U-net，而是用了 Transformer——将CLIP的文本编码、加入噪声的CLIP的图像编码、扩散时间步的time embedding等输入，去预测未加噪声的CLIP图像编码。），其功能是将文本表征映射为图片表征。</li><li>训练一个扩散解码模型，其目标是根据图片表征，还原原始图片。</li></ul><p>在训练完成之后，推理过程就比较直接了，首先使用CLIP 文本编码器，获得文本编码，之后使用先验模型将文本编码映射为图片编码，最后使用扩散解码器用图片编码生成完整图片。注意这里扩散解码模型使用的是经过修改的 GLIDE 扩散模型，其生成的图像尺寸为 64×64，然后使用两个上采样扩散模型将其上采样至 256×256，以及 1024×1024。</p><p>DALL·E2 原论文中也提到了其许多不足，例如容易将物体和属性混淆，无法精确的将文本放置到图像中，以及社会伦理道德等方面的问题。</p><p>谷歌的 Imagen 模型是继 DALL·E2 的后续工作，它没有用两阶段的生成，直接用了一个 U-net 搞定，更简单，效果也好。另外谷歌2022年6月份最新的基于 GAN 的 <a href="https://sites.research.google/parti/">Pathways Autoregressive Text-to-Image Model</a>（Parti）模型，用 200亿参数的 Pathways 模型做自回归的图像生成，效果直接超越了 DALL·E 2 和 Imagen。</p><h3 id="imagen">Imagen</h3><p>在 DALL-E2 提出没多久，Google 就提出了一个新的文本生成图像模型 Imagen，论文中提到，<strong>其生成的图片相对于 DALL-E2 真实感和语言理解能力都更加强大</strong>（使用一种新的评测方法 DrawBench）。</p><figure><img src="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-67fa2ff5f289e376304e905a8c87eff7.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>Imagen 的模型结构与 DALL·E2 十分类似，首先将文本进行编码表征，然后使用扩散模型将表征解码为完整图像，最后使用两个 text-conditional super-resolution 的扩散模型将其上采样至 256×256，以及 1024×1024。不同的是，<strong>Imagen 使用了 T5-XXL 模型直接编码文本信息，然后使用条件扩散模型，直接用文本编码生成图像。因此，在 Imagen 中，无需学习先验模型。</strong></p><figure><img src="https://pic2.zhimg.com/80/v2-be42b33c6d55d5fece5b8ae9866e42a9_720w.webp" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>由于直接使用 T5-XXL 模型，其语义知识相对于 CLIP 要丰富很多（图文匹配数据集数量要远远少于纯文本数据集数量），因此 Imagen 相对于 DALL-E2 在语义保真度上做的更好。同时，作者也发现，增大语言模型，可以有效的提高样本的语义保真度。</p><h3 id="stable-diffusion">Stable Diffusion</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/593896912">李沐论文精读系列——由DALL·E 2看图像生成模型 - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/560645985">图像生成：VQGAN，CLIP，DALLE，DIFFUSION - 知乎 (zhihu.com)</a></p><p><a href="https://swarma.org/?p=37227">AI艺术的背后：详解文本生成图像模型 | 集智俱乐部 (swarma.org)</a></p><p><a href="https://zhuanlan.zhihu.com/p/640545463">一文读懂Stable Diffusion 论文原理+代码超详细解读 - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/imwaters/article/details/127269368">【论文简介】Stable Diffusion的基础论文:2112.High-Resolution Image Synthesis with Latent Diffusion Models_stable diffusion论文-CSDN博客</a></p><p><a href="https://blog.csdn.net/sinat_31711799/article/details/130874306">【AI绘图】一、stable diffusion的发展史_stable diffusion研究现状-CSDN博客</a></p></blockquote><p>DALL·E3</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/660733999#:~:text=DALL-E%203%20将于%2010,月初向%20ChatGPT%20Plus%20和企业客户推出。">OpenAI发布DALL-E 3 | 原理简介 - 知乎 (zhihu.com)</a></p></blockquote><h3 id="sora">Sora</h3><p>Sora 是由 OpenAI 于 2024.2.15 发布的文生视频大模型。</p><h2 id="各生成模型对比">各生成模型对比</h2><p>翻译自 Lil' Log:</p><p>GAN、VAE 和流动模型等生成模型在生成高质量 samples 方面取得了巨大成功，但它们都有其自身的局限性。GAN模型因对抗性训练性质而具有潜在的不稳定训练和较少的生成多样性（需要精心选择的超参数和正则化器）。VAE依赖于替代损失 surrogate loss 。流动模型必须使用专门的架构来构建可逆转换。</p><p>扩散模型的灵感来自非平衡热力学。他们定义了一个扩散步骤的马尔可夫链，以缓慢地将随机噪声添加到数据中，然后学习反向扩散过程以从噪声中构建所需的数据样本。与VAE或流动模型不同，扩散模型是通过固定程序学习的，并且潜在变量 latent code（z）具有高维数（与原图同尺寸大小）。</p><figure><img src="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png" alt="Edge Image Viewer (lilianweng.github.io)" /><figcaption aria-hidden="true">Edge Image Viewer (lilianweng.github.io)</figcaption></figure><blockquote><p>各种生成模型的对比图，来自<a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models? | Lil'Log (lilianweng.github.io)</a></p></blockquote><h1 id="ddpm">DDPM</h1><details><summary>参考链接</summary><p><a href="https://zhuanlan.zhihu.com/p/525106459">由浅入深了解Diffusion Model - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/576475987">扩散模型 (Diffusion Model) 简要介绍与源码分析 - 知乎 (zhihu.com)</a></p><p><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models? | Lil'Log (lilianweng.github.io)</a></p><p><a href="https://zhuanlan.zhihu.com/p/601641045">Diffusion Model学习笔记(1)——DDPM - 知乎 (zhihu.com)</a></p></details><p>本文主要基于 <strong>DDPM：Denoising Diffusion Probabilistic Models</strong><sup>[1]</sup> [Ho et al. 2020] 展开 Diffusion 模型的原理推导。</p><h1 id="forward-与-backward-过程">forward 与 backward 过程</h1><p>如下，存在一系列高斯噪声（ <span class="math inline">\(T\)</span> 轮），将输入图片 <span class="math inline">\(x_0\)</span> 变为纯高斯噪声 <span class="math inline">\(x_T\)</span> 。而我们的模型则负责将 <span class="math inline">\(x_T\)</span> 复原回图片 <span class="math inline">\(x_0\)</span> 。这样一来其实 diffusion model 和 GAN 很像，都是给定噪声 <span class="math inline">\(x_t\)</span> 生成图片 <span class="math inline">\(x_0\)</span> ，但是要强调的是，这里噪声 <span class="math inline">\(x_T\)</span> 与图片 <span class="math inline">\(x_0\)</span> 是<strong>同维度</strong>的。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240315_145856_0.jpg" alt="20240315_145856_0" /><figcaption aria-hidden="true">20240315_145856_0</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240315_145951_1.jpg" alt="20240315_145951_1" /><figcaption aria-hidden="true">20240315_145951_1</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/diff222.jpg" alt="20240315_150015_2" /><figcaption aria-hidden="true">20240315_150015_2</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240315_150043_3.jpg" alt="20240315_150043_3" /><figcaption aria-hidden="true">20240315_150043_3</figcaption></figure><h1 id="模型训练">模型训练</h1><h2 id="预备知识">预备知识</h2><blockquote><p>关于变分下界与KL散度参考： <a href="https://bluefisher.github.io/2020/02/06/理解-Variational-Lower-Bound/">理解 Variational Lower Bound | Fisher's Blog (bluefisher.github.io)</a></p></blockquote><p><strong>变分贝叶斯</strong>：变分贝叶斯（Variational Bayesian, VB）是一类非常受欢迎的统计类机器学习方法。VB 非常有用的一个特性是推断优化的二元性：<u>我们可以将统计推断问题（从一个随机变量的值推断出另一种随机变量的值）作为优化问题（找到参变量的值来最小化某些目标函数）</u>。另外，<strong>variational lower bound</strong> ，也被称作 <strong>evidence lower bound</strong> (ELBO)，在 VB 的推导中起了非常重要的作用。</p><p><strong>变分下界</strong>：变分下界（variational lower bound 或 evidence lower bound, ELBO）是一种用于估计概率分布参数的方法。它通过最大化似然函数的下界来逼近真实的后验概率分布。这种方法可以用于在无法直接计算后验概率分布的情况下，近似地推断参数的取值。</p><p>已知变分贝叶斯（VB）将统计推断问题，即从一种随机变量推断出另一种随机变量的值，作为优化问题。</p><p>这里，将问题设置成从观测值（observed variable） <span class="math inline">\(X\)</span> 推断隐变量（hidden/latent variable） <span class="math inline">\(Z\)</span>​ ,由于两者之间存在关系：</p><blockquote><p>隐变量 <span class="math inline">\(Z\)</span> 可能包含参数 <span class="math inline">\(\theta\)</span> 。</p></blockquote><figure><img src="https://bluefisher.github.io/images/2020-02-06-%E7%90%86%E8%A7%A3-Variational-Lower-Bound/image-20200206195028885.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>因此，<span class="math inline">\(X \rightarrow Z\)</span> 可视为求隐变量的后验概率： <span class="math display">\[P\left( Z|X \right) =\frac{P\left( X|Z \right) P\left( Z \right)}{P\left( X \right)}=\frac{P\left( X|Z \right) P\left( Z \right)}{\int_Z{p\left( X,Z \right)}}\]</span> 其中，大写的 <span class="math inline">\(P(X)\)</span> 表示某个变量的概率分布，小写的 <span class="math inline">\(p(X)\)</span> 表示 <span class="math inline">\(X\)</span> 分布的概率密度函数。</p><h3 id="推导变分下界-l">推导变分下界 <span class="math inline">\(L\)</span></h3><p>从观测值 <span class="math inline">\(X\)</span> 的边缘分布出发，设 <span class="math inline">\(q(Z)\)</span> 为VB中后验概率 <span class="math inline">\(p(Z|X)\)</span> 的估计概率，则： <span class="math display">\[\begin{align}\log P(X) &amp;=\log \int_{Z} p(X, Z) \tag{1}\\&amp;=\log \int_{Z} p(X, Z) \frac{q(Z)}{q(Z)} \tag{2}\\&amp;=\log \left(\mathbb{E}_{q}\left[\frac{p(X, Z)}{q(Z)}\right]\right) \tag{3}\\&amp; \geq \mathbb{E}_{q}\left[\log \frac{p(X, Z)}{q(Z)}\right] \tag{4}\\&amp;=\mathbb{E}_{q}[\log p(X, Z)]+H[Z] \tag{5}\\&amp;=L\ \ --\ variational\ lower\ bound\end{align}\]</span> 从公式（2）到公式（3）运用到期望函数的定义： <span class="math inline">\(E\left( f\left( x \right) \right) =\int{xf\left( x \right) dx}\)</span> 。公式（4）对凸函数 log 运用了<strong>琴生不等式</strong>： <span class="math inline">\(f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)]\)</span> 。公式（5）中 <span class="math inline">\(H[Z]=-\mathbb{E}_{q}[\log q(Z)]\)</span> 是香农熵。</p><p>我们做标记： <span class="math display">\[L=\mathbb{E}_{q}[\log p(X, Z)]+H[Z]\]</span> 很明显 <span class="math inline">\(L\)</span> 就是观测变量的 log 概率的一个 lower bound。<strong>也就是说，如果我们想要去最大化边缘分布，我们可以转而最大化它的 variational lower bound <span class="math inline">\(L\)</span></strong> 。</p><h3 id="推导-kl-散度相对熵">推导 KL 散度/相对熵</h3><p>在很多情况下，后验概率 <span class="math inline">\(P(Z|X)\)</span> 的计算是十分困难的，比如我们可能需要对所有的隐变量做积分（求和）来计算分母。</p><p>变分方法的主要思想就是找一个估计的概率分布 <span class="math inline">\(q(Z)\)</span> 来尽可能地接近后验概率 <span class="math inline">\(p(Z|X)\)</span> 。这些估计的概率分布可以有它们独有的<em>变分参数（variational parameters）</em>：<span class="math inline">\(q(Z|\theta)\)</span> ，所以我们想要去寻找这些参数来使 <span class="math inline">\(q(Z)\)</span> 尽可能接近后验概率。当然 <span class="math inline">\(q(Z)\)</span> 的分布肯定要在推断中相对来说更加简单好求一些。</p><p>为了衡量两个概率分布 <span class="math inline">\(q(Z)\)</span> 和 <span class="math inline">\(p(Z|X)\)</span> 的相似程度，一个常用的标准就是就是 <strong>Kullback-Leibler (KL) 散度</strong>。其计算如下：</p><blockquote><p>来自百度百科：https://baike.baidu.com/item/%E7%9B%B8%E5%AF%B9%E7%86%B5/4233536</p><p><strong>相对熵（relative entropy）</strong>，又被称为KL散度（Kullback-Leibler divergence）或信息散度（information divergence），是两个概率分布间差异的非对称性度量。在信息理论中，<strong>相对熵等价于两个概率分布的信息熵（Shannon entropy）的差值</strong>。</p><p>相对熵是一些优化算法，例如最大期望算法（Expectation-Maximization algorithm, EM）的损失函数。此时参与计算的一个概率分布为真实分布，另一个为理论（拟合）分布，相对熵表示使用理论分布拟合真实分布时产生的信息损耗。</p><p>设 <span class="math inline">\(P(x),Q(x)\)</span> 是随机变量 <span class="math inline">\(X\)</span> 上的两个概率分布，则在离散和连续随机变量的情形下，相对熵的定义分别为: <span class="math display">\[KL\left( P||Q \right) =\sum{P\left( x \right) \log \frac{P\left( x \right)}{Q\left( x \right)}} \\KL\left( P||Q \right) =\int{P\left( x \right) \log \frac{P\left( x \right)}{Q\left( x \right)}}dx\]</span></p><p><strong>相对熵具有非负性</strong>，根据<a href="https://baike.baidu.com/item/%E5%90%89%E5%B8%83%E6%96%AF%E4%B8%8D%E7%AD%89%E5%BC%8F/22780937">吉布斯不等式</a>推导离散时的情况如下（事实上吉布斯不等式可以直接得到<span class="math inline">\(0 \geq \sum_{i=1}^{n} p_{i} \log q_{i}-\sum_{i=1}^{n} p_{i} \log p_{i}=\sum_{i=1}^{n} p_{i} \log \left(q_{i} / p_{i}\right)=-D_{\mathrm{KL}}(P \| Q)\)</span> ，百度百科这里大等于号后还用到了琴生不等式，这是由于通过琴生不等式可以证明吉布斯不等式）。由于 -log<span class="math inline">\(x\)</span> 是凸函数，因此根据相对熵的定义有： <span class="math display">\[\mathrm{KL}(P \| Q)=\sum_{x \in X} P(x) \log \frac{P(x)}{Q(x)}=-E\left[\log \frac{Q(x)}{P(x)}\right] \geq-\log \left[\sum_{x \in X} P(x) \frac{Q(x)}{P(x)}\right]=-\log \left[\sum_{x \in X} Q(x)\right]=0\]</span> 由上可知，相对熵是恒大于等于0的。当且仅当两分布相同时，相对熵等于0。</p></blockquote><p><span class="math display">\[\begin{align} K L[q(Z) \| p(Z | X)] &amp;=\int_{Z} q(Z) \log \frac{q(Z)}{p(Z | X)} \tag{6}\\ &amp;=-\int_{Z} q(Z) \log \frac{p(Z | X)}{q(Z)} \tag{7}\\ &amp;=-\left(\int_{Z} q(Z) \log \frac{p(X, Z)}{q(Z)}-\int_{Z} q(Z) \log p(X)\right) \tag{8}\\ &amp;=-\int_{Z} q(Z) \log \frac{p(X, Z)}{q(Z)}+\log p(X) \int_{Z} q(Z) \tag{9}\\ &amp;=-L+\log p(X) \tag{10}\end{align}\]</span></p><p>这里 <span class="math inline">\(L\)</span> 是变分下界 variational lower bound 。公式 (10) 是归一化常量 $ _{Z} q(Z) =1$ 而推导得来。整理可得： <span class="math display">\[L=\log p(X)-K L[q(Z) \| p(Z | X)]\]</span> 因为 KL 散度永远是 ≥0 的，所以，再一次，我们得到了 <span class="math inline">\(L \le log\ p(X)\)</span> 是观测变量的分布的一个 lower bound。同时我们也知道了它们之间的区别就在于估计分布和真实分布之间的 KL 散度。换句话说，如果估计分布与真实后验分布完美接近，那么 lower bound <span class="math inline">\(L\)</span>​ 就等于 log 概率。</p><h3 id="推导交叉熵">推导交叉熵</h3><blockquote><p>参考自：<a href="https://blog.csdn.net/tsyccnh/article/details/79163834">一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉-CSDN博客</a></p><p>简洁版介绍看：<a href="https://zhuanlan.zhihu.com/p/115277553">损失函数：交叉熵详解 - 知乎 (zhihu.com)</a></p></blockquote><p>在此前推导相对熵时，注解里给出了离散情况下 <span class="math inline">\(P(x)、Q(x)\)</span> 两种概率分布的相对熵计算公式： <span class="math display">\[D_{KL}\left( p||q \right) =\sum_{i=1}^n{p\left( x_i \right) \log \frac{p\left( x_i \right)}{q\left( x_i \right)}}\]</span> 对该式变形可得： <span class="math display">\[\begin{aligned}    D_{KL}\left( p|q \right) &amp;=\sum_{i=1}^n{p}\left( x_i \right) \log \left( p\left( x_i \right) \right) -\sum_{i=1}^n{p}\left( x_i \right) \log \left( q\left( x_i \right) \right)\\    &amp;=-H\left( p\left( x \right) \right) +\left[ -\sum_{i=1}^n{p}\left( x_i \right) \log \left( q\left( x_i \right) \right) \right]\\    &amp;=-H\left( p\left( x \right) \right) +E_p\left[ -\log \left( q\left( x_i \right) \right) \right]\\    &amp;=-H\left( p\left( x \right) \right) +H\left( p,q \right)\\\end{aligned}\]</span> 即 KL 散度等式的前一部分恰巧就是 <span class="math inline">\(p\)</span> 的熵，后一部分正是交叉熵 <span class="math inline">\(H(p,q)\)</span> （<u>交叉熵 <span class="math inline">\(H(p,q)\)</span> 可理解为把来自一个分布 <span class="math inline">\(q\)</span> 的消息使用另一个分布 <span class="math inline">\(p\)</span> 的最佳代码传达的平均消息长度</u>）： <span class="math display">\[H\left( p,q \right) =E_p\left[ -\log \left( q\left( x_i \right) \right) \right] =-\sum_{i=1}^n{p}\left( x_i \right) \log \left( q\left( x_i \right) \right)\]</span> 在机器学习中，如果 <span class="math inline">\(P(x)\)</span> 用来表示样本的真实分布，而 <span class="math inline">\(Q(x)\)</span> 用来表示模型所预测的分布。如果我们需要评估 label 和 predicts 之间的差距，可以使用 KL 散度，即 $D_{KL}( p|q ) $ 。<strong>由于 KL 散度中的前一部分 <span class="math inline">\(-H(p)\)</span> 只与真值有关，是不变的，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做loss，评估模型</strong>。</p><h2 id="损失函数">损失函数</h2><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240320_194222_0.jpg" alt="20240320_194222_0" /><figcaption aria-hidden="true">20240320_194222_0</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/diff22111.jpg" alt="20240320_194348_1" /><figcaption aria-hidden="true">20240320_194348_1</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240320_194446_2.jpg" alt="20240320_194446_2" /><figcaption aria-hidden="true">20240320_194446_2</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/diff22333.jpg" alt="20240320_194530_3" /><figcaption aria-hidden="true">20240320_194530_3</figcaption></figure><p><strong>注3</strong>： 上述&lt;8&gt;式去掉权重系数是因为 DDPM 发现这样能使得 diffusion 模型工作得更好。</p><p>最后的简化形式： <span class="math display">\[L_{simple}=L^{simple}_t+C\]</span> 其中， <span class="math inline">\(C\)</span> 是与模型参数 <span class="math inline">\(\theta\)</span> 无关的常量。</p><h1 id="最终算法流程">最终算法流程</h1><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240320200738823.png" alt="image-20240320200738823" /><figcaption aria-hidden="true">image-20240320200738823</figcaption></figure><blockquote><p>The training and sampling algorithms in DDPM (Image source: <a href="https://arxiv.org/abs/2006.11239">Ho et al. 2020</a>)</p></blockquote><p><strong>训练阶段</strong>重复如下步骤:</p><ul><li>从数据集中采样 <span class="math inline">\(x_0\)</span></li><li>从 <span class="math inline">\(1...T\)</span> 随机采样 time step <span class="math inline">\(t\)</span> ：对该 batch 中的每个图像随机取 <span class="math inline">\(t\)</span> 进行训练</li><li>从标准高斯分布采样噪声 $_t( 0, ) $</li><li>调用模型预估 $_{}( _0+_t,t ) $：这里使用的模型主要是 U-net 模型</li><li>计算最小化噪声之间的 MSE Loss: $<em>t-</em>{}( _0+_t,t ) ^2 $, 并利用反向传播算法训练模型.</li></ul><p><strong>逆向阶段</strong>采用如下步骤进行采样:</p><ul><li>从高斯分布采样 <span class="math inline">\(x_T\)</span></li><li>按照 <span class="math inline">\(T,...,1\)</span> 的顺序进行迭代:<ul><li>如果 <span class="math inline">\(t=1\)</span> , 令 <span class="math inline">\(\mathbf{z}=0\)</span> ; 如果 <span class="math inline">\(t&gt;1\)</span> , 从高斯分布中采样 $( 0, ) $ <span style="background:#FFCC99;">（最后一步不加噪音）</span></li><li>利用式 &lt;6&gt; 学习出均值 $_{}( _t,t ) =( <em>t-</em>{}( _t,t ) ) $ , 并计算均方差 <span class="math inline">\(\sigma _t=\sqrt{\tilde{\beta}_t}=\sqrt{\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\cdot \beta _t}\)</span></li><li>通过重参数技巧采样 <span class="math inline">\(x_{t-1}=\mu _{\theta}\left( x_t,t \right) +\sigma _t\mathbf{z}\)</span></li></ul></li><li>经过以上过程的迭代, 最终恢复 <span class="math inline">\(x_0\)</span> .</li></ul><h1 id="参考文献">参考文献</h1><p>[1] <a href="https://arxiv.org/pdf/2006.11239.pdf">Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.</a></p><p>[2] <a href="https://arxiv.org/abs/1312.6114">Kingma D P, Welling M. Auto-encoding variational bayes[J]. arxiv preprint arxiv:1312.6114, 2013.</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/684456268">Van Den Oord A, Vinyals O. Neural discrete representation learning[J]. Advances in neural information processing systems, 2017, 30.</a></p><p>[4] <a href="https://arxiv.org/pdf/2102.12092.pdf">Ramesh A, Pavlov M, Goh G, et al. Zero-shot text-to-image generation[C]International conference on machine learning. Pmlr, 2021: 8821-8831.</a></p><p>[5] <a href="https://arxiv.org/pdf/1406.2661.pdf">Creswell A, White T, Dumoulin V, et al. Generative adversarial networks: An overview[J]. IEEE signal processing magazine, 2018, 35(1): 53-65.</a></p><p>[6] <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html">Esser P, Rombach R, Ommer B. Taming transformers for high-resolution image synthesis[C]Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 12873-12883.</a></p><p>[7] <a href="https://arxiv.org/abs/2204.08583">Crowson K, Biderman S, Kornis D, et al. Vqgan-clip: Open domain image generation and editing with natural language guidance[C]European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022: 88-105.</a></p><p>[8] <a href="https://arxiv.org/pdf/2112.10741.pdf">Nichol A, Dhariwal P, Ramesh A, et al. Glide: Towards photorealistic image generation and editing with text-guided diffusion models[J]. arixv preprint arixv:2112.10741, 2021.</a></p><p>[9] <a href="https://arxiv.org/pdf/2204.06125.pdf">Ramesh A, Dhariwal P, Nichol A, et al. Hierarchical text-conditional image generation with clip latents[J]. arixv preprint arixv:2204.06125, 2022, 1(2): 3.</a></p><p>[10] <a href="https://arxiv.org/pdf/2205.11487.pdf">Saharia C, Chan W, Saxena S, et al. Photorealistic text-to-image diffusion models with deep language understanding[J]. Advances in neural information processing systems, 2022, 35: 36479-36494.</a></p><p>[11] <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf">Rombach R, Blattmann A, Lorenz D, et al. High-resolution image synthesis with latent diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 10684-10695.</a></p><p>[12] <a href="https://arxiv.org/abs/1503.03585">Sohl-Dickstein J, Weiss E, Maheswaranathan N, et al. Deep unsupervised learning using nonequilibrium thermodynamics[C]//International conference on machine learning. PMLR, 2015: 2256-2265.</a></p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>最优化问题概述</title>
      <link href="/2024/04/11/%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E6%A6%82%E8%BF%B0/"/>
      <url>/2024/04/11/%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="预备知识">预备知识</h1><h1 id="基本模型">基本模型</h1><p>最优化问题的一般数学模型为： <span class="math display">\[\begin{align}\min&amp;\text{\ }f\left( x \right) \tag{1.1}\\s.t.\ h_i\left( x \right) =&amp;0,\ i=1,...,m;\tag{1.2}\\g_j\left( x \right) \ge &amp;0,\ j=1,...,p\text{。}\tag{1.3}\end{align}\]</span></p><blockquote><p><span class="math inline">\(s.t.\)</span> 为英文“subject to"缩写，表示受限制于。</p></blockquote><p>其中，<span class="math inline">\(x=(x_1,x_2,..,x_n)^T\)</span> 即 <strong><span class="math inline">\(x\)</span> 是n维向量</strong>。在实际问题中，<span class="math inline">\(x_1,x_2,...,x_n\)</span> 常被称为<strong>决策变量</strong>。求极小值的函数<span class="math inline">\(f(x)\)</span> 称为<strong>目标函数</strong>。<span class="math inline">\(h_i(x)=0\)</span> 称为<strong>等式约束</strong>。<span class="math inline">\(g_j(x)\ge 0\)</span> 称为<strong>不等式约束</strong>。对于求目标函数极大值的问题，可以通过 <span class="math inline">\(max\text{\ }f(x)=min \text{\ }[-f(x)]\)</span> 来化为求解极小值问题。</p><p>满足等式约束和不等式约束的解 <span class="math inline">\(x\)</span> 称为<u>可行解/可行点/容许解</u>。全体可行解构成的集合称为<u>可行域/容许集</u>，记为 <span class="math inline">\(D\)</span> ： <span class="math display">\[D=\left\{ x|h_i\left( x \right) =0,i=1,···,m;g_j\left( x \right) \ge 0,j=1,···,p,x∈R^n \right\}\]</span> 若 <span class="math inline">\(h_i(x)\)</span> 和 <span class="math inline">\(g_j(x)\)</span> 是连续函数，则 <span class="math inline">\(D\)</span> 是闭集。</p><h2 id="整体最优解与局部最优解">整体最优解与局部最优解</h2><p>若 <span class="math inline">\(x^*\in D\)</span> ，对于一切 <span class="math inline">\(x \in D\)</span> 恒有 <span class="math inline">\(f(x^*)\le f(x)\)</span> ，则称 <span class="math inline">\(x^*\)</span> 为最优化问题的<u>整体最优解</u>。 若 <span class="math inline">\(x^*\in D\)</span> ，对于一切 <span class="math inline">\(x \in D\)</span> 且 <span class="math inline">\(x\ne x^*\)</span> 恒有 <span class="math inline">\(f(x^*)&lt; f(x)\)</span> ，则称 <span class="math inline">\(x^*\)</span> 为最优化问题的<u>严格整体最优解</u>。</p><p>若 <span class="math inline">\(x^* \in D\)</span> ，存在 <span class="math inline">\(x^*\)</span> 的某邻域 <span class="math inline">\(N_{\epsilon}(x^*)=\{x| \text{\ }\lVert x-x^* \rVert &lt;\epsilon,\epsilon&gt;0 \}\)</span> ，使得对一切 <span class="math inline">\(x\in D\bigcap N_{\epsilon}(x^*)\)</span> ，有 <span class="math inline">\(f(x^*)\le f(x)\)</span> ，则称 <span class="math inline">\(x^*\)</span> 为最优化问题的<u>局部最优解</u>。 若 <span class="math inline">\(x^* \in D\)</span> ，存在 <span class="math inline">\(x^*\)</span> 的某邻域 <span class="math inline">\(N_{\epsilon}(x^*)=\{x| \text{\ }\lVert x-x^* \rVert &lt;\epsilon,\epsilon&gt;0 \}\)</span> ，使得对一切 <span class="math inline">\(x\in D\bigcap N_{\epsilon}(x^*)\)</span> 且 <span class="math inline">\(x\ne x^*\)</span> ，有 <span class="math inline">\(f(x^*) &lt; f(x)\)</span> ，则称 <span class="math inline">\(x^*\)</span> 为最优化问题的<u>严格局部最优解</u>。</p><p>显然，整体最优解一定是局部最优解，反之不然。一般情况下，很难求出整体最优解，往往只能求出局部最优解。<span style="background:#daf5e9;">最优解 <span class="math inline">\(x^*\)</span> 对应的目标函数值 <span class="math inline">\(f(x^*)\)</span> 称为最优值，常用 $ f^*$ 表示。</span></p><h2 id="模型分类">模型分类</h2><p><strong>线性规划</strong>： 约束函数都是线性函数。</p><p><strong>非线性规划</strong>：约束函数至少有一个是非线性函数。</p><p><strong>二次规划</strong>：目标函数是二次函数，约束函数都是线性函数。</p><p><strong>多目标规划</strong>：目标函数不是数量函数，而是向量函数。</p><p><strong>动态规划</strong>：用于解决多阶段决策问题。</p><h1 id="一般算法">一般算法</h1><p>最优化问题的算法一般有一下迭代格式：</p><blockquote><p>[!IMPORTANT]</p><p>给定初始点 <span class="math inline">\(x_0\)</span> ，令 <span class="math inline">\(k=0\)</span> 。</p><p>1.确定 <span class="math inline">\(x_k\)</span> 处的<strong>可行下降方向</strong> <span class="math inline">\(\boldsymbol{p}_{k}\)</span> ，使得 <span class="math inline">\(f(x_k)\)</span> 沿着该方向移动时函数值有所下降且始终位于可行域内;</p><p>2.以 <span class="math inline">\(x_k\)</span> 为出发点，确定步长 <span class="math inline">\(\alpha_k&gt;0\)</span> ，使得以 <span class="math inline">\(\boldsymbol{p}_{k}\)</span> 为方向作射线 <span class="math inline">\(x_k+\alpha_k\boldsymbol{p}_{k}\)</span> 时有 <span class="math inline">\(f(x_k+\alpha_k\boldsymbol{p}_{k})&lt;f(x_k)\)</span> ; <span style="background:#fbd4d0;">（该步骤也称为“一维搜索”）</span></p><p>3.令 <span class="math inline">\(x_{k+1}=x_k+\alpha_k\boldsymbol{p}_{k}\)</span>;</p><p>4.若 <span class="math inline">\(x_{k+1}\)</span> 满足某种终止准则，或达到最大迭代次数，则停止迭代且以 <span class="math inline">\(x_{k+1}\)</span> 为近似最优解。否则令 <span class="math inline">\(k=k+1\)</span> ，转第1步。</p></blockquote><p><strong>根据不同的原则选择不同的搜索方向 <span class="math inline">\(p_k\)</span> ，就可以得到不同的算法。</strong>（该过程类似于神经网络中的求解损失函数极小值的梯度下降过程。换个思路说，求解损失函数极小值的过程，也是一种最优化问题。）</p><p>求解步长的过程，即步骤2，也称为<u>一维搜索</u>或<u>线性搜索</u>。当搜索方向确定后，一维搜索的优劣便成为求解最优化问题的关键。</p><h2 id="算法的收敛性">算法的收敛性</h2><p>我们称一个最优化算法是收敛的当且仅当<strong>该算法构造的点列 <span class="math inline">\(\{x_k\}\)</span> 能在有限步内得到最优化问题的最优解 <span class="math inline">\(x^*\)</span></strong> 。换言之，点列 <span class="math inline">\(\{x_k\}\)</span> 有极限点，且极限点是最优解 <span class="math inline">\(x^*\)</span> ，则算法收敛。</p><h3 id="局部收敛与全局收敛">局部收敛与全局收敛</h3><p>一个算法是否收敛，往往与初始点 <span class="math inline">\(x_0\)</span>​ 的选取有关：</p><ul><li>如果只有当 <span class="math inline">\(x_0\)</span> 充分接近最优解 <span class="math inline">\(x^*\)</span> 时，由算法产生的点列才收敛于 <span class="math inline">\(x^*\)</span> ，则该算法为具有<span style="background:#f9eda6;"><strong>局部收敛性</strong></span>的算法。</li><li>如果对于任意的初始点 <span class="math inline">\(x_0 \in D\)</span> ，由算法产生的点列都收敛于最优解 <span class="math inline">\(x^*\)</span>​ ，则这个算法是具有<span style="background:#f9eda6;"><strong>全局收敛性</strong></span>的算法。</li></ul><p>由于最优解一般未知，因此具有全局收敛性的算法更有实用意义。</p><p>此外，算法的收敛快慢对算法的优劣评价也至关重要。下面给出有关收敛速度的概念。</p><h3 id="线性收敛超线性收敛与-p-阶收敛">线性收敛、超线性收敛与 <span class="math inline">\(p\)</span>​​ 阶收敛</h3><blockquote><p>[!NOTE]</p><p>牛顿法是平方收敛的，弦截法是1.618阶收敛的，梯度下降法是线性收敛的，随机梯度下降法是次线性收敛的。</p></blockquote><p>设序列 <span class="math inline">\(\{x_k\}\)</span> 收敛于 <span class="math inline">\(x^*\)</span> ,且 <span class="math display">\[\underset{k\rightarrow \infty}{\lim}\frac{\epsilon _{k+1}}{\epsilon _k}=\underset{k\rightarrow \infty}{\lim}\frac{\lVert x_{k+1}-x^* \rVert}{\lVert x_k-x^* \rVert}=\beta，\]</span> 若 <span class="math inline">\(\beta=1\)</span> ，则称序列 <span class="math inline">\(\{x_k\}\)</span> 是<strong>次线性收敛</strong>(sublinear convergence)的； 若 <span class="math inline">\(0&lt; \beta &lt;1\)</span> ，则称序列 <span class="math inline">\(\{x_k\}\)</span> 是<strong>线性收敛</strong>（linear convergence），称 <span class="math inline">\(\beta\)</span> 为收敛比； 若 <span class="math inline">\(\beta=0\)</span> ，则称序列 <span class="math inline">\(\{x_k\}\)</span> 是<strong>超线性收敛</strong>(superlinear convergence)的。</p><p>再次，设序列 <span class="math inline">\(\{x_k\}\)</span> 收敛于 <span class="math inline">\(x^*\)</span> ，且对于某个实数 <span class="math inline">\(p \ge 1\)</span> 有 <span class="math display">\[\underset{k\rightarrow \infty}{\lim}\frac{\epsilon _{k+1}}{\epsilon _k}=\underset{k\rightarrow \infty}{\lim}\frac{\lVert x_{k+1}-x^* \rVert}{\lVert x_k-x^* \rVert ^p}=\beta ,\ 0&lt;\beta &lt;+\infty ,\]</span> 则称序列 <span class="math inline">\(\{x_k\}\)</span> 是 <strong><span class="math inline">\(p\)</span>​ 阶收敛</strong>的。当 <span class="math inline">\(p=2\)</span>​ 时，二阶收敛也称平方收敛。</p><p><span style="background:#daf5e9;">如果我们说一个算法是线性收敛等的，是指算法产生的序列是线性收敛等的。</span></p><blockquote><p>[!CAUTION]</p><p>此处若差值写为误差 <span class="math inline">\(e_k\)</span>​ 形式，可类比于现代数值计算中的迭代绝对误差等内容：<a href="https://baike.baidu.com/item/二阶收敛/2838248">二阶收敛_百度百科 (baidu.com)</a></p><p>另一种收敛定义：<a href="https://baike.baidu.com/item/二阶收敛/2838248">二阶收敛_百度百科 (baidu.com)</a></p></blockquote><p>我们也可以将收敛速度量化为以下形式：</p><ul><li><strong>次线性收敛</strong>： $O(  ) , O(  ) , O(  ) $ 等</li><li><strong>线性收敛</strong>： <span class="math inline">\(O(log\frac{1}{\epsilon})\)</span></li><li><strong>超线性收敛</strong>： <span class="math inline">\(O(log(log \frac{1}{\epsilon}))\)</span></li></ul><p>为了更好地理解上述收敛速度的概念，这里举一个栗子（<a href="https://zhuanlan.zhihu.com/p/278151142">收敛速度的三种形式 - 知乎 (zhihu.com)，附画图代码</a>）：</p><p>令：</p><ul><li><span class="math inline">\(a_k=1/k\)</span> ，这里 <span class="math inline">\(a_k\)</span> 是次线性收敛的</li><li><span class="math inline">\(b_k=1/k^2\)</span> ，这里 <span class="math inline">\(b_k\)</span> 也是次线性收敛的</li><li><span class="math inline">\(c_k=1/2^k\)</span> ，这里 <span class="math inline">\(c_k\)</span>​ 是线性收敛的</li><li><span class="math inline">\(d_k=1/2^{2^k}\)</span> ，这里 <span class="math inline">\(d_k\)</span> 是平方收敛的</li></ul><p>这四个序列都以0为极限，可以感受到， 哪怕是<strong>普通的线性收敛都已经达到指数级别的收敛速度了</strong>（这也是我们为什么把线性收敛也叫指数收敛的原因！）超线性收敛的效果更是难以想象！</p><p>画出四个数列的对数图：</p><figure><img src="C:\Users\21218\AppData\Roaming\Typora\typora-user-images\image-20240227123343023.png" alt="image-20240227123343023" /><figcaption aria-hidden="true">image-20240227123343023</figcaption></figure><p>线性收敛在对数坐标图上已经是一条直线了，平方收敛就更是一条急速向下的曲线，这也就是为什么牛顿法能够在短短四五步迭代就达到我们期望的误差限的原因。至于次线性收敛，则一般<strong>是以多项式的速度逼近</strong>，但即便都是次线性收敛，其实快慢也是可以有区别的（具体区别方法参考栗子链接）。</p><h2 id="迭代终止准则">迭代终止准则</h2><p>对于一种算法，我们还要给出某种终止准则。当某次迭代满足终止准则时，就停止迭代，而以这次迭代得到的 <span class="math inline">\(x_k\)</span> 或 <span class="math inline">\(x_{k+1}\)</span> 作为最优解 <span class="math inline">\(x^*\)</span> 的近似解。常用的终止准则有以下四种：</p><ul><li>$x_{k+1}-x_k &lt;$ 或 $&lt;$ ；</li><li>$| f( x_{k+1} ) -f( x_k ) |&lt;$ 或 $&lt;$ ；</li><li>$f( x_k ) =_k &lt;$ ； #梯度足够小</li><li>上述三种终止准则的组合；</li></ul><p>其中 <span class="math inline">\(\varepsilon &gt; 0\)</span> 是预先给定的足够小的实数。</p><blockquote><p>当 <span class="math inline">\(f(x)\)</span> 具有连续的一阶偏导数时，记 <span class="math inline">\(f(x)\)</span> 在 <span class="math inline">\(x_k\)</span> 处的梯度为 $f( x_k )=_k $​ 。由泰勒公式， <span class="math display">\[f\left( x_k+\alpha p_k \right) =f\left( x_k \right) +\alpha \boldsymbol{g}_{k}^{T}\boldsymbol{p}_{k}+o\left( \alpha \right)\]</span> 其中， <span class="math inline">\(\boldsymbol{p}_{k}\)</span> 为函数在点 <span class="math inline">\(x_k\)</span> 处的下降方向（ <span class="math inline">\(\boldsymbol{g}_{k}^{T}\boldsymbol{p}_{k}&lt;0\)</span> ），<span class="math inline">\(\alpha\)</span> 为步长。</p></blockquote><h1 id="二维最优化问题的几何解释">二维最优化问题的几何解释</h1><p>这里举一个栗子：</p><figure><img src="C:\Users\21218\AppData\Roaming\Typora\typora-user-images\image-20240227141939293.png" alt="image-20240227141939293" /><figcaption aria-hidden="true">image-20240227141939293</figcaption></figure><figure><img src="C:\Users\21218\AppData\Roaming\Typora\typora-user-images\image-20240227142006937.png" alt="image-20240227142006937" /><figcaption aria-hidden="true">image-20240227142006937</figcaption></figure><h1 id="一维搜索">一维搜索</h1><p>即使目标函数 <span class="math inline">\(f(x)\)</span> 是一元函数， 求最小值点也经常需要使用数值迭代方法。 另外，在多元目标函数优化中， 一般每次迭代从上一步的 <span class="math inline">\(\boldsymbol x^{(t)}\)</span> 先确定一个下降方向 <span class="math inline">\(\boldsymbol p^{(t)}\)</span> , 然后对派生出的一元函数 <span class="math inline">\(h(\alpha) = f(\boldsymbol x^{(t)} + \alpha \boldsymbol p^{(t)}),\alpha \ge 0\)</span>, 求最小值点 $min f(x^{(t)} + p^{(t)})=min  h() $ 得到下降的步长 <span class="math inline">\(\alpha_t\)</span> ， 并令 <span class="math inline">\(\boldsymbol x^{(t+1)} = \boldsymbol x^{(t)} + \alpha_t \boldsymbol p^{(t)}\)</span> ， 求步长的过程称为<strong>一维搜索/一维最优化问题</strong>。 所以一维搜索就是求解一元函数 <span class="math inline">\(h(\alpha)\)</span> 的最优化问题。</p><p>搜索可以是求一元函数 <span class="math inline">\(h(\alpha)\)</span> 的精确最小值点， 也可以求一个使得目标函数下降足够多的 <span class="math inline">\(\alpha\)</span> 作为步长。</p><p>有几种求解一维最优化问题的方法：</p><ul><li><strong>Fibonacci 法/分数法</strong><ul><li><span style="background:#daf5e9;">线性收敛</span>，收敛比为 <span class="math inline">\(\frac{\sqrt{5}-1}{2}\)</span>​</li><li>要求函数在初始区间上是下单峰函数，且函数值可求。</li></ul></li><li><strong>黄金分割法/0.618法</strong><ul><li><span style="background:#daf5e9;">线性收敛</span>，收敛比为 <span class="math inline">\(\frac{\sqrt{5}-1}{2}\)</span>​​</li><li>要求函数在初始区间上是下单峰函数，且函数值可求</li><li>事实上，黄金分割法是 Fibonacci 法的极限形式。</li></ul></li><li><strong>平分法</strong><ul><li><span style="background:#daf5e9;">线性收敛</span>，收敛比为 <span class="math inline">\(\frac{1}{2}\)</span></li><li>要求函数在初始区间上是下单峰函数，且具有连续的一阶导数</li></ul></li><li><strong>抛物线法/二次插值法</strong><ul><li><span style="background:#daf5e9;">超线性收敛</span></li><li>用二次多项式逼近函数。利用若干点处的函数值构造一个多项式，用这个多项式的极小点来作为原函数极小点的近似值。</li></ul></li><li><strong>非精确一维搜索 Wolfe 算法</strong><ul><li>在求解非线性规划问题时，前几种精确一维搜索很难达到真正的精确值，因此我们想只计算少量的几个函数值，就可以得到一个满足 <span class="math inline">\(f(x_{k+1})&lt;f(x_k)\)</span> 的近似点 <span class="math inline">\(x_{k+1}=x_k+\alpha_k\boldsymbol{p}_{k}\)</span></li><li>要求产生的点列 <span class="math inline">\(\{x_k\}\)</span> 具有某种收敛性质。所以除了对下降方向 <span class="math inline">\(\boldsymbol{p}_{k}\)</span> 有要求外，对步长 <span class="math inline">\(\alpha_k\)</span> 也有要求，即目标函数 <span class="math inline">\(f(x)\)</span> 要"充分地下降"。</li></ul></li></ul><blockquote><p>[!NOTE]</p><p>1.在 Fibonacci 法和黄金分割法中，都要求 <span class="math inline">\(f(x)\)</span>​ 在初始区间上是下单峰函数。因此我们可以预先用“进退法”求符合该要求的初始区间。</p><p>2.<u>非精确一维搜索</u>，就是找到一个步长 <span class="math inline">\(\alpha\)</span> 满足一定条件即可，好比Wolfe-Powell,Armijo条件； <u>精确一维搜索</u>，就是找到一个参数 <span class="math inline">\(\alpha\)</span> ，使得 <span class="math inline">\(min \text{ \ } f(x+\alpha\boldsymbol{p}_{k})\)</span>​ 。</p></blockquote><h2 id="fibonacci-法分数法">Fibonacci 法/分数法</h2><h3 id="原理讨论">原理讨论</h3><p>设 <span class="math inline">\(f(x)\)</span> 在区间 <span class="math inline">\([a,b]\)</span> 上是下单峰函数，即 <span class="math inline">\(f(x)\)</span> 在区间内有唯一极小值点 <span class="math inline">\(x^*\)</span> 。我们想求最小值点，即求该极小值点 <span class="math inline">\(x^*\)</span> 。 <span class="math inline">\(f(x)\)</span> 在 <span class="math inline">\(x^*\)</span> 左边单调递减，右边单调递增。那么我们该如何求该极小值呢？</p><p>思路很简单，只需指定步长，从区间 <span class="math inline">\([a,b]\)</span> 左端点搜索至右端点，首先在区间内取两个点 <span class="math inline">\(x_1\)</span> , <span class="math inline">\(x_2\)</span> 且 <span class="math inline">\(x_1 &lt; x_2\)</span> 。计算这两点处的函数值 <span class="math inline">\(f(x_1)\)</span> , <span class="math inline">\(f(x_2)\)</span>:</p><ul><li>若 <span class="math inline">\(f(x_1)&lt;f(x_2)\)</span> ,说明 <span class="math inline">\(x_2\)</span> 位于 <span class="math inline">\(x^*\)</span> 右侧，则 <span class="math inline">\(x^* \in [a,x_2]\)</span> ；</li><li>若 <span class="math inline">\(f(x_1) \ge f(x_2)\)</span> ,说明 <span class="math inline">\(x_1\)</span> 位于 <span class="math inline">\(x^*\)</span> 左侧，则 <span class="math inline">\(x^* \in [x_1,b]\)</span>​ 。</li></ul><p>由此，将原区间 <span class="math inline">\([a,b]\)</span> 缩短为 <span class="math inline">\([a,x_2]\)</span> 或 <span class="math inline">\([x_1,b]\)</span>​ 。因为新区间内包含一个已经计算过函数值的点，因此只需根据步长再新取一试点，就可将该新区间又缩短一次。不断重复该过程，直至最终的区间长度满足给定精确度为止。</p><blockquote><p>这里的“新区间内包含一个已经计算过函数值的点”是指：由于 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span> 处的函数值已计算，若缩短后区间为 <span class="math inline">\([a,x_2]\)</span> ，则该区间包含 <span class="math inline">\(x_1\)</span> ，只需再取一点 <span class="math inline">\(x_3\)</span> ,比较 <span class="math inline">\(f(x_1)\)</span> 和 <span class="math inline">\(f(x_3)\)</span> 的值即可。整个过程需要新计算的函数值只有 <span class="math inline">\(f(x_3)\)</span> 。缩短后区间为 <span class="math inline">\([x_1, b]\)</span> 的情况同理。</p></blockquote><figure><img src="C:\Users\21218\AppData\Roaming\Typora\typora-user-images\image-20240228184442917.png" alt="image-20240228184442917" /><figcaption aria-hidden="true">image-20240228184442917</figcaption></figure><p>这种遍历法虽然简单，但是需要迭代的次数多，要不断计算函数值 <span class="math inline">\(f(x)\)</span> ，浪费计算资源严重。<strong>我们想要在初始区间 <span class="math inline">\([a, b]\)</span> 长度一定，且迭代次数固定为 <span class="math inline">\(n\)</span> 的条件下，根据步长新取点来缩短区间后，得到最终的区间最短。</strong><span style="background:#daf5e9;">换言之，问按什么方式取点，求 <span class="math inline">\(n\)</span> 次函数值后可最多将多长的原始区间缩短为最终区间长度为 1 ？</span></p><p>设 <span class="math inline">\(L_n\)</span> 表示试点个数为 <span class="math inline">\(n\)</span> 、最终区间长度为 1 时的原始区间 <span class="math inline">\([a,b]\)</span> 的最大可能长度，即 <span class="math inline">\(L_n = b-a\)</span> 。现在需找出 <span class="math inline">\(L_n\)</span> 的一个上界。设最初的两个试点为 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span> ，且 <span class="math inline">\(x_1&lt;x_2\)</span> 。</p><ul><li>如果极小值点位于 <span class="math inline">\([a,x_1]\)</span> 内，除去 <span class="math inline">\(x_2\)</span> ，则我们至多还有 <span class="math inline">\(n-2\)</span> 个试点，因此 <span class="math inline">\(x_1-a \le L_{n-2}\)</span> ；</li><li>如果极小值点位于 <span class="math inline">\([x_1,b]\)</span> 内，包括 <span class="math inline">\(x_2\)</span> ，则我们至多还有 <span class="math inline">\(n-1\)</span> 个试点，因此 $ b-x_1 L_{n-1}$ 。</li></ul><p>因为 <span class="math inline">\(L_n = b-a = (b-x_1)+(x_1-a) \le L_{n-1} + L_{n-2}\)</span> ，故 <span class="math inline">\(L_n \le L_{n-1}+L_{n-2}\)</span>。</p><p>又显然不计算函数值或只计算一个函数值无法缩短区间，故 <span class="math inline">\(L_0=L_1=1\)</span>。</p>将两个关系式合并，可知若原始区间长度满足递推关系： <span class="math display">\[\begin{align}L_n &amp;=L_{n-1}+L_{n-2},\ n\ge 2, \\L_0 &amp;=L_1=1,\end{align}\]</span> 则 <span class="math inline">\(L_n\)</span> 为最大原始区间的长度。观察发现，该递推关系恰与 Fibonacci 数列 <span class="math inline">\(\{ F_n \}\)</span> 的递推关系 ${<span class="math display">\[\begin{array}{l}    F_n=F_{n-1}+F_{n-2},\,\,n\ge 2,\\    F_0=F_1=1,\\\end{array}\]</span><p>. $ 相同，其中 <span class="math inline">\(F_n\)</span> 表示第 <span class="math inline">\(n+1\)</span> 个 Fibonacci 数。不妨以 <span class="math inline">\(F_n\)</span> 代 <span class="math inline">\(L_n\)</span> 表示表示试点个数为 <span class="math inline">\(n\)</span> 、最终区间长度为 1 时的原始区间 <span class="math inline">\([a,b]\)</span>​ 的最大可能长度。</p><blockquote><p>由以上讨论可知，若原区间长度为1，计算n次函数值所能获得的最大缩短率(缩短后的区间长度与原区间长度之比)为1/<span class="math inline">\(F_n\)</span>。例如<span class="math inline">\(F_{20}\)</span>=10946，所以计算20个函数值即可把原长度为L的区间缩短为 <span class="math inline">\(L/10946\)</span> 长度的区间。</p></blockquote><p><span style="background:#daf5e9;">故答：由于求 <span class="math inline">\(n\)</span> 次函数值后最终区间长度为 1 ，则原始区间长度最长为 <span class="math inline">\(F_n\)</span> 。</span></p><p>若原始区间为 <span class="math inline">\([a,b]\)</span> ,要求最终的区间长度小等于<u>绝对精度</u> <span class="math inline">\(\varepsilon(\varepsilon &gt; 0)\)</span>​ ，则有 <span class="math display">\[\varepsilon \ge \frac{b-a}{F_n}\Rightarrow F_n\ge \frac{b-a}{\varepsilon}。 \tag{1}\]</span> <strong>由（1）式可确定试点个数 <span class="math inline">\(n\)</span>​ 。</strong></p><blockquote><p>若换一种说法，已知原区间为 <span class="math inline">\([a_0,b_0]\)</span> ，要将其长度缩短为原来的 <span class="math inline">\(\delta\)</span> 倍，则如何确定试点个数 <span class="math inline">\(n\)</span> ?</p><p>设缩短后区间为 <span class="math inline">\([a_{n-1},b_{n-1}]\)</span> ，则缩短后区间长度满足: <span class="math inline">\(b_{n-1}-a_{n-1} \le (b_0-a_0)\delta\)</span> ，即: <span class="math display">\[F_n\ge \frac{b_0-a_0}{b_{n-1}-a_{n-1} }\ge \frac{1}{\delta}\]</span> 式中 <span class="math inline">\(\delta\)</span> 为一个正小数，称为区间缩短的<u>相对精度</u>。通过式子 <span class="math inline">\(F_n \ge \frac{1}{\delta}\)</span> 即可确定满足条件的最小的 <span class="math inline">\(n\)</span> 。有时给出区间缩短的绝对精度 <span class="math inline">\(\varepsilon\)</span> ，即要求 <span class="math display">\[b_{n-1}-a_{n-1} \le \varepsilon\]</span> 显然，相对精度与绝对精度有关系式： <span class="math inline">\(\varepsilon = (b_0-a_0)\delta\)</span> 。</p></blockquote><p>试点个数 <span class="math inline">\(n\)</span> 确定后，由于 Fibonacci 数列 <span class="math inline">\(\{F_n\}=\{1,1,2,3,...,F_{n-1},F_n \}\)</span> 中首项 1 表示最终区间长度，尾项 <span class="math inline">\(F_n\)</span> 表示原始区间长度，因此我们应该倒过来从右往左看。即试点个数确定后，区间缩短率依次为 <span class="math display">\[\frac{F_{n-1}}{F_n},\ \frac{F_{n-2}}{F_{n-1}},\ ...\ ,\frac{3}{5},\ \frac{2}{3},\ \frac{1}{2}\]</span> 因此<strong>最初的两个试点 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span> 应该满足以下关系式</strong>： <span class="math display">\[\begin{align}\frac{x_1-a}{b-a} &amp;=1-\frac{F_{n-1}}{F_n}=\frac{F_{n-2}}{F_n},\\\frac{x_2-a}{b-a} &amp;=\frac{F_{n-1}}{F_n},\\end{align}\]</span> 即取试点时， <span class="math display">\[\begin{align}x_1&amp;=a+\frac{F_{n-2}}{F_n}\left( b-a \right)=b-\frac{F_{n-1}}{F_n}\left( b-a \right)\\x_2&amp;=a+\frac{F_{n-1}}{F_n}\left( b-a \right)\end{align}\]</span> 显然 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span> 关于区间 <span class="math inline">\([a,b]\)</span> 对称，即有 <span class="math inline">\(x_1-a=b-x_2\)</span>​ 。</p><p>具体过程可参考下图：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240305195155122.png" alt="image-20240305195155122" /><figcaption aria-hidden="true">image-20240305195155122</figcaption></figure><p>通过计算<span class="math inline">\(f(x_1)\)</span> , <span class="math inline">\(f(x_2)\)</span> 并比较其大小就得到一个新的区间。新区间不妨仍然记为区间 <span class="math inline">\([a,b]\)</span> 。这就完成了一次迭代。现在假设已经迭代了 <span class="math inline">\(i-1\)</span> 次,<strong>在第 <span class="math inline">\(i\)</span> 次迭代开始时</strong>,我们还有 <span class="math inline">\(n-i+1\)</span> 个试点,其中包括已经计算过函数值的一个试点。这时令 <span class="math display">\[\begin{array}{l}x_{1}=a+\frac{F_{n-i-1}}{F_{n-i+1}}(b-a), \\x_{2}=a+\frac{F_{n-i}}{F_{n-i+1}}(b-a),\end{array}\]</span> <span class="math inline">\(x_1\)</span> , <span class="math inline">\(x_2\)</span> 中已有一个已经计算过函数值,只需再计算另一点的函数值并进行比较,便可完成第 <span class="math inline">\(i\)</span> 次迭代。<strong>当 <span class="math inline">\(i=n-1\)</span> 时,即进行最后一次迭代时</strong>,由于 <span class="math inline">\(F_0=F_1=1\)</span> , <span class="math inline">\(x_1\)</span> 与 <span class="math inline">\(x_2\)</span> 重合,且已计算过函数值,因此第 <span class="math inline">\(n\)</span> 个试点应选在离该点距离为一个充分小的正数 <span class="math inline">\(\varepsilon\)</span> 处。</p><p>归纳以上讨论,就得到一个求解问题 <span class="math display">\[\underset{a\le x\le b}{\min}\ f\left( x \right)\]</span> 的方法,这个方法叫 Fibonacci 法。</p><h3 id="迭代步骤">迭代步骤</h3><p>根据上述讨论，可将 Fibonacci 法的迭代步骤简述如下：</p><p>需要指出的是,在使用 Fibonacci 法之前必须事先计算出计算函数值的次数 <span class="math inline">\(n\)</span> 。<span style="background:#daf5e9;">除了第一次迭代需要计算两个函数值之外,其余每次迭代只需计算一个函数值</span>。可以证明,在借助于计算 <span class="math inline">\(n\)</span> 个函数值的所有非随机搜索方法中, Fibonacci 法可使原始区间与最终区间长度之比达到最大值。这是它的优点。而 Fibonacci 法的主要缺点是区间缩短率不固定，选取试点的公式不是固定的，这样就增加了计算量。</p><h3 id="代码实现">代码实现</h3><h2 id="黄金分割法">黄金分割法</h2><p>在最优化方法这个数学分支中，可以证明斐波拉契法是压缩比（在同样迭代次数下开始区间和最终区间的比值）最高的方法，黄金分割比相比它压缩比小一些，但是算法会比较简单，占用计算资源也会略少。</p><h1 id="参考链接">参考链接</h1><ul><li><a href="https://www.math.pku.edu.cn/teachers/lidf/docs/statcomp/html/_statcompbook/opt-1d.html#opt-1d-bins">36 一维搜索与求根 | 统计计算 (pku.edu.cn)</a></li><li><a href="https://blog.csdn.net/hei653779919/article/details/106387340">最优化问题——一维搜索(一)_精确一维搜索步长公式-CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/78507430">【优化】一维搜索方法 - 知乎 (zhihu.com)</a></li><li><a href="https://blog.csdn.net/weixin_44044411/article/details/88091024">【python】最优化方法之一维搜索（黄金分割法+斐波那契法）_斐波那契法一维搜索-CSDN博客</a></li><li>《最优化方法》解可新、韩立兴等，天津大学出版社</li></ul>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 最优化方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TV 正则化</title>
      <link href="/2024/04/10/TV%20%E6%AD%A3%E5%88%99%E5%8C%96/"/>
      <url>/2024/04/10/TV%20%E6%AD%A3%E5%88%99%E5%8C%96/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>TensoRF</title>
      <link href="/2024/04/10/TensoRF%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>/2024/04/10/TensoRF%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<hr /><blockquote><p><strong>论文名称： TensoRF: Tensorial Radiance Fields （TensoRF: 张量辐射场）</strong></p><p>代码地址： [GitHub - apchenstu/TensoRF: <a href="https://github.com/apchenstu/TensoRF">ECCV 2022] Tensorial Radiance Fields, a novel approach to model and reconstruct radiance fields</a></p><p>论文主页： https://apchenstu.github.io/TensoRF/</p></blockquote><h1 id="论文基本信息">论文基本信息</h1><p>TensoRF: Tensorial Radiance Fields<sup>[1]</sup>是由来自上海科技大学的 Chen Anpei 等人发表在 <strong>2022 ECCV</strong> 会议上的SCI论文。</p><p>主要亮点在使用了张量分解技术（tensor decomposition techniques），即<strong>以分解后的低秩张量分量来建模辐射场</strong>。文中提出了一种新的分解技术——VM分解，来克服传统CP分解的局限性。</p><ul><li>高效：在少于 30 min 的时间内重建每个场景的辐射场（NeRF[3] 需要20+ hours）</li><li>质量高：比 NeRF好</li><li>内存占用小：仅需75MB</li></ul><p><strong>sparkling</strong>：如果不用CP分解和VM分解，用Tucker分解与BTD分解来压缩模型参数或节约计算成本加速如何呢</p><h1 id="预备知识">预备知识</h1><h2 id="模型压缩之参数矩阵近似">模型压缩之参数矩阵近似</h2><blockquote><p>之前在<a href="https://hahahaha5606.github.io/2024/03/17/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B9%8B%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/">模型压缩之知识蒸馏</a>中提过，模型压缩技术大致可分为五种：</p><ul><li><strong>模型剪枝</strong>：即移除对结果作用较小的组件，如减少 head 的数量和去除作用较少的层，共享参数等，ALBERT属于这种；</li><li><strong>量化</strong>：比如将 float32 降到 float8；</li><li><strong>知识蒸馏</strong>：将 teacher 的能力蒸馏到 student上，一般 student 会比 teacher 小。我们可以把一个大而深的网络蒸馏到一个小的网络，也可以把集成的网络蒸馏到一个小的网络上；</li><li><strong>参数共享</strong>：通过共享参数，达到减少网络参数的目的，如 ALBERT 共享了 Transformer 层；</li><li><strong>参数矩阵近似</strong>：通过矩阵的低秩分解或其他方法达到降低矩阵参数的目的。</li></ul></blockquote><p>在 TensoRF 中使用到的恰好是这五种模型压缩技术中的<strong>参数矩阵近似</strong>下的<strong>矩阵的低秩分解</strong>。这里的矩阵分解技术主要分为两种： CP 分解和 VM 分解。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240406143709598.png" alt="image-20240406143709598" /><figcaption aria-hidden="true">image-20240406143709598</figcaption></figure><h3 id="cp-分解">CP 分解</h3><p>CP 分解是一种较为经典的矩阵分解技术。我们知道，向量是一维，矩阵是二维，张量就是三维及以上。<strong>从这个角度来看，张量就是高维的矩阵、高高维的向量，因此如果能用向量或矩阵来表示张量。就能达到降维的目的。</strong>以一个三维的张量 <span class="math inline">\(\tau \in \mathbb{R}^{I×J×K}\)</span> 为例，CP分解将其因式分解<code>factorize</code>为一系列<code>秩1张量元</code>(rank-one tensor component)的和： <span class="math display">\[\tau =\sum_{r=1}^R{\text{v}_{r}^{1}\circ}\text{v}_{r}^{2}\circ \text{v}_{r}^{3},\tag{1}\]</span> 这里的 <span class="math inline">\(1、2、3\)</span> 表示区分三个不同维度/mode的 <span class="math inline">\(\mathbb{R}^I、\mathbb{R}^J、\mathbb{R}^K\)</span> 的向量 <span class="math inline">\(\mathbb{v}_r\)</span> ，这三个向量的外积 <span class="math inline">\({\text{v}_{r}^{1}\circ}\text{v}_{r}^{2}\circ \text{v}_{r}^{3}\)</span> 是一个秩1张量元 ( rank-one tensor component )。CP 分解将 <span class="math inline">\(\tau\)</span> 分解为 <span class="math inline">\(R\)</span> 个秩1张量元之和，<span class="math inline">\(R\)</span> 是CP秩。根据外积定义，每个张量元 <span class="math inline">\(\tau_{ijk}\)</span> 可以写成如下的标量积的和（sum of scalar products）的形式： <span class="math display">\[\tau_{ijk}= \sum^R_{r=1} \mathbf{v}^1_{r,i} \mathbf{v}^2_{r,j} \mathbf{v}^3_{r,k}\tag{2}\]</span> 这里，<span class="math inline">\(i、j、k\)</span>​​ 表示三个 mode 的索引（<span class="math inline">\(i=1,...,I; j=1,...,J; k=1,...,K\)</span>）。</p><p><strong>CP 分解的优点</strong>在于它可以直接被用于我们的张量辐射场中，并重建出高质量的结果如表1。</p><p><strong>但是它的局限性在于</strong>：由于高度压缩（multiple compact rank-one components）(<span style="background:#daf5e9;">这些秩1张量因子又被分解为多个向量因子的外积，相当于 CP 分解用了大量的 pure vector factors</span>)，CP 分解需要很多 components 来建模复杂场景，进而导致高计算成本，拖慢了速度。</p><h3 id="vm-分解">VM 分解</h3><p><strong>VM 分解（vector-matrix decomposition，VM）</strong>是一种新的向量矩阵分解技术。它是作者受到分项分解（block term decomposition, BTD）的启发提出的，可以更 efficient 地重建辐射场。</p><p>VM 分解中不仅用了一维向量因子 vector factor 来降低张量维度，还用到了二维矩阵因子 matrix factor ： <span class="math display">\[\tau =\sum_{r=1}^{R_1}{\mathbf{v}_{r}^{1}}\circ \mathbf{M}_{r}^{2,3}+\sum_{r=1}^{R_2}{\mathbf{v}_{r}^{2}}\circ \mathbf{M}_{r}^{1,3}+\sum_{r=1}^{R_3}{\mathbf{v}_{r}^{3}}\circ \mathbf{M}_{r}^{1,2}\tag{3}\]</span> 这里的矩阵因子 <span class="math inline">\(\mathbf{M}_{r}^{2,3}、\mathbf{M}_{r}^{1,3}、\mathbf{M}_{r}^{1,2}\)</span> 分别表示3-mode中的三种不同切片，即 <span class="math inline">\(\mathbb{R}^{J×K}、\mathbb{R}^{I×K}、\mathbb{R}^{I×J}\)</span>。对于每个 component，我们放宽它的两个mode的秩可以为任意大 arbitrarily large，用矩阵表示；只限制一个mode是秩1的，用向量表示。这样，<span style="background:#daf5e9;">相比于CP中使用三个分离的向量因子表示张量元，我们用一个矩阵因子和一个向量因子表示张量元，放宽了对秩的限制，且 <span class="math inline">\(R_1、R_2、R_3\)</span>​ 可以根据每个mode的复杂程度设置为不同的值</span>，可以进一步节约计算成本，使速度加快（见表2、表3），从而弥补了 CP 分解的不足。 作者还提到，VM 分解可被视为 BTD 分解的特例。</p><blockquote><p>ppdop：<span style="background:#eef0f4;">这里对秩的理解是这样，向量可以视为一阶张量，且作为张量可以用向量做外积表示，因此根据秩1张量的定义，“向量”张量的秩是1。而矩阵是二阶张量，且未必能用多个向量做外积表示，因此"矩阵"张量的秩是不固定的，可以是 arbitrarily large 的。</span></p></blockquote><p>注意 <strong>VM 分解的每个张量元都比 CP 分解中的参数数量更多</strong>（一个矩阵的参数 vs. 两个向量的参数），因此一方面导致了更低的压缩 lower compactness 的缺点（见表1），另一方面也可视为表示更加复杂的高维数据特征的优点，即在建模相同复杂度的功能时需要的 component 的数量更少。 <span style="background:#eef0f4;">嗯，提出缺点后立即找补，好办法~</span></p><p>虽然 VM 分解在 compactness 上无法达到 CP 分解那么高，但是相比于稠密网格表示<code>dense grid representation</code> 来说仍具有很大优势，将内存复杂度从 <span class="math inline">\(O(N^3)\)</span> 降低至 <span class="math inline">\(O(N^2)\)</span> 。</p><h2 id="用张量来表示重建场景">用张量来表示重建场景</h2><p><strong>作者直接用三维空间的 <span class="math inline">\(\mathbf{XYZ}\)</span> 坐标对应到三维张量的三个维度，且 <span class="math inline">\(\mathbf{XYZ}\)</span> 轴每个方向的特征网格分辨率（数量）为 <span class="math inline">\(IJK\)</span></strong> 。同时虽然前面在讲 VM 分解时提到三个维度的 分解秩可以是不同的，但是作者在文章中认为场景的三个坐标轴方向的复杂度可以是一样的，因此把它们设定为一样的，即 <span class="math inline">\(R_1=R_2=R_3=R\)</span> 。那么 VM 分解的公式就可以重新写成下式（4.1），并进一步简化为（4.2）： <span class="math display">\[\begin{align}\tau &amp;=\sum_{r=1}^{R}{\mathbf{v}_{r}^{X}}\circ \mathbf{M}_{r}^{Y,Z}+{\mathbf{v}_{r}^{Y}}\circ \mathbf{M}_{r}^{X,Z}+{\mathbf{v}_{r}^{Z}}\circ \mathbf{M}_{r}^{X,Y}\tag{4.1}\\&amp;=\sum_{r=1}^R{\mathcal{A}_{r}^{X}+}\mathcal{A}_{r}^{Y}+\mathcal{A}_{r}^{Z}\tag{4.2}\end{align}\]</span> 这样，每个张量元素就可写成： <span class="math display">\[\begin{align}\tau_{ijk}&amp;= \sum_{r=1}^R\text{v}^X_{r,i}\text{M}^{Y,Z}_{r,jk} + \text{v}^Y_{r,j}\text{M}^{X,Z}_{r,ik} + \text{v}^Z_{r,k}\text{M}^{X,Y}_{r,ij} \tag{5.1} \\&amp;=\sum^R_{r=1} \sum_m \mathcal{A}^m_{r,ijk} \ , \  m\in XYZ  \tag{5.2}\end{align}\]</span> 类似，可以简化 CP 分解的表达式为： <span class="math display">\[\tau=\sum_{r=1}^R{\text{v}_{r}^{X}\circ}\text{v}_{r}^{Y}\circ \text{v}_{r}^{Z}=\sum_{r=1}^R \mathcal{A}^{\gamma}\]</span> 此时 CP 分解后每个张量元素表达为： <span class="math display">\[\tau_{ijk}= \sum^R_{r=1} \mathbf{v}^X_{r,i} \mathbf{v}^Y_{r,j} \mathbf{v}^Z_{r,k}=\sum^R_{r=1} \mathcal{A}_{r,ijk}^m  \ ,  \ m=\gamma\]</span></p><h1 id="算法框架">算法框架</h1><h2 id="总览">总览</h2><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240409144231888.png" alt="image-20240409144231888" /><figcaption aria-hidden="true">image-20240409144231888</figcaption></figure><blockquote><p>上图是 TensoRF（VM）的重建和渲染框架图。有两条管线，上管线计算 view-dependent 颜色，下管线计算体密度。</p><p>TensoRF (VM) reconstruction and rendering. We model radiance fields as tensors using a set of vectors (v) and matrices (M), which describe the scene along their corresponding (XYZ) axes and are used for computing volume density σ and view-dependent color c in differentiable ray marching. For each shading location x = (x, y, z), we use linearly/bilinearly sampled values from the vector/matrix factors to efficiently compute the corresponding trilinearly interpolated values (A(x)) of the tensor components. The density component values ( Aσ(x)) are summed to get the volume density (σ) directly. The appearance values (Ac(x)) are concatenated into a vector (⊕[Amc (x)]m) that is then multiplied by an appearance matrix B and sent to the decoding function S for RGB color (c) regression.</p></blockquote><p>论文的总框架公式与 NeRF 中一样，都是由<u>三维位置和视角方向</u>映射到<u>体密度和 view-dependent 的颜色</u>的函数： <span class="math display">\[f:(\mathbf{x},d) \rightarrow (\sigma,c)\]</span> 且该函数支持体渲染中可微的 ray marching 。和 DVGO (2022, Sun et al.)[2] 类似地，本文使用常规的三维体素特征网格 <span class="math inline">\(\mathcal{G}\)</span> 来建模函数 <span class="math inline">\(f\)</span> ，我们在 feature channel 把网格分为<font color=#4eb434>几何网格 <span class="math inline">\(\mathcal{G}_{\sigma}\)</span></font> 和<font color=#df8400>外观网格 <span class="math inline">\(\mathcal{G}_c\)</span>​ </font>，分别用来得到体密度和 view-dependent 颜色。其中，只有外观网格 <span class="math inline">\(\mathcal{G}_c\)</span> 会用到视角 <span class="math inline">\(d\)</span>​ 。</p><p><strong>总的来说，基于网格的连续辐射场可以被表示为：</strong> <span class="math display">\[\sigma,c = \mathcal{G}_{\sigma}(\mathbf{x}),S(\mathcal{G}_c(\mathbf{x}),d),\tag{6}\]</span> 这里，<span class="math inline">\(\mathcal{G}_{\sigma}(\mathbf{x}),\mathcal{G}_c(\mathbf{x})\)</span> 分别表示<u>对位置 <span class="math inline">\(\mathbf{x}\)</span>​ 的两个网格进行特征的三线性插值</u>。</p><blockquote><p>外观网格和几何网格进行 VM 分解后，共有 <span class="math inline">\(3R_{\sigma}+3R_c\)</span> 个矩阵，<span class="math inline">\(3R_{\sigma}+6R_c\)</span> 个向量。且 <span class="math inline">\(R_{\sigma}\)</span> 和 <span class="math inline">\(R_c\)</span> 都远远小于 <span class="math inline">\(XYZ\)</span> 轴方向上的网格数 <span class="math inline">\(IJK\)</span>，因此 VM 分解能达到很高的 compact representation。</p></blockquote><h2 id="外观网格">外观网格</h2><p><font color=#df8400>在外观网格 <span class="math inline">\(\mathcal{G}_c\)</span> 中</font>，我们使用预先选定的函数 <span class="math inline">\(S\)</span> 来实现从<u>外观特征向量 <span class="math inline">\(\mathcal{G}_c(x)\)</span> 和视角方向 <span class="math inline">\(d\)</span></u> 到<u>所求颜色 <span class="math inline">\(c\)</span></u> 的映射。例如，这个函数 <span class="math inline">\(S\)</span> 可以是一个小的 MLP 或 SH 函数，那么 <span class="math inline">\(\mathcal{G}_c\)</span> 中包含的分别就是 neural features 或 SH 系数（这两种函数都在我们的模型中工作良好，见表1）。因此 <span class="math inline">\(\mathcal{G}_c \in \mathbb{R}^{I×J×K×P}\)</span> 储存四维张量，其中 <span class="math inline">\(IJK\)</span> 是<span class="math inline">\(\mathbf{XYZ}\)</span> 轴每个方向的特征网格分辨率（数量），而 <span class="math inline">\(P\)</span> 则是外观特征 mode 的输出通道数。</p><p><strong>注意到外观特征 mode 一般比另外三个 mode 更低维，也就是 lower rank，因此我们仅仅用向量因子 <span class="math inline">\(\mathbf{b}_r\)</span> 表示它</strong>（共有 <span class="math inline">\(3R_c\)</span> 个 <span class="math inline">\(\mathbf{b}_r\)</span>）；而另外三个 mode 则作为三维张量的形式被 CP/VM 分解。特别地，张量 <span class="math inline">\(\mathcal{G}_c\)</span> 的 VM 因式分解为： <span class="math display">\[\begin{align}\mathcal{G}_c &amp;=\sum_{r=1}^{R_c}{\mathbf{v}_{c,r}^{X}}\circ \mathbf{M}_{c,r}^{Y,Z}\circ \mathbf{b}_{3r-2} +{\mathbf{v}_{c,r}^{Y}}\circ \mathbf{M}_{c,r}^{X,Z}\circ \mathbf{b}_{3r-1}+{\mathbf{v}_{c,r}^{Z}}\circ \mathbf{M}_{c,r}^{X,Y}\circ \mathbf{b}_{3r}\tag{8.1}\\&amp;=\sum_{r=1}^{R_c}{\mathcal{A}_{c,r}^{X}\circ \mathbf{b}_{3r-2}+}\mathcal{A}_{c,r}^{Y}\circ \mathbf{b}_{3r-1}+\mathcal{A}_{c,r}^{Z}\circ \mathbf{b}_{3r} \tag{8.2}\end{align}\]</span> 将所有表示外观特征mode的向量因子 <span class="math inline">\(\mathbf{b}_r\)</span> 按列 stack together，我们就得到了一个大小为 <span class="math inline">\(P×3R_c\)</span> 的二维矩阵 <span class="math inline">\(\mathbf{B}\)</span>​​ ，该矩阵包含了全局外观特征。</p><p>注意！<strong>在直接计算索引 <span class="math inline">\(ijk\)</span> 处的单个体素的外观特征向量时</strong>，外观 mode 仍然用的是完整的 <span class="math inline">\(P\)</span>-channel 特征向量 <span class="math inline">\(\mathbf{b}_r\)</span> ，表示为： <span class="math display">\[\mathcal{G}_{c,ijk} = \sum_{r=1}^{R_c}{\mathcal{A}_{c,r,ijk}^{X}\mathbf{b}_{3r-2}+}\mathcal{A}_{c,r,ijk}^{Y}\mathbf{b}_{3r-1}+\mathcal{A}_{c,r,ijk}^{Z}\mathbf{b}_{3r} \tag{10}\]</span> 为了进一步简化计算，我们将所有的 <span class="math inline">\(\mathcal{A}_{c,r,ijk}^{m}\)</span> （<span class="math inline">\(m\in \{XYZ\},r=1,...,R_c\)</span>）值堆叠成一个 <span class="math inline">\(3R_c\)</span> 的向量，记作 <span class="math inline">\(\oplus[\mathcal{A}^m_{c,ijk}]_{m,r}\)</span> 。符号“<span class="math inline">\(\oplus\)</span>”表示串联 concatenation。那么（10）式就可以被简化为： <span class="math display">\[\mathcal{G}_{c,ijk} = \mathbf{B} (\oplus[\mathcal{A}^m_{c,ijk}]_{m,r}) \tag{11}\]</span> 这样，当同时计算大量体素时，我们只需先将所有 <span class="math inline">\(\mathcal{A}_{c,r,ijk}^{m}\)</span> 串联成列向量，然后一次性与 shared 矩阵 <span class="math inline">\(\mathbf{B}\)</span> 相乘即可。而不用像(10)式那样反复相乘再相加。</p><h2 id="几何网格">几何网格</h2><p><font color=#4eb434>而在几何网格 <span class="math inline">\(\mathcal{G}_{\sigma}\)</span> 中</font>，我们只考虑输出单通道——网格值只表示体密度 <span class="math inline">\(\sigma\)</span> 的情况，而不需要额外的转换函数。则 <span class="math inline">\(\mathcal{G}_{\sigma} \in \mathbb{R}^{I×J×K}\)</span> 储存表示位置的三维张量。</p><p>为了 compact components，我们将张量进行 VM 分解或 CP 分解。特别地，在 VM 分解中，三维几何张量 <span class="math inline">\(\mathcal{G}_{\sigma}\)</span> 被因式分解为： <span class="math display">\[\begin{align}\mathcal{G}_{\sigma} &amp;=\sum_{r=1}^{R_{\sigma}}{\mathbf{v}_{\sigma,r}^{X}}\circ \mathbf{M}_{\sigma,r}^{Y,Z}+{\mathbf{v}_{\sigma,r}^{Y}}\circ \mathbf{M}_{\sigma,r}^{X,Z}+{\mathbf{v}_{\sigma,r}^{Z}}\circ \mathbf{M}_{\sigma,r}^{X,Y}\tag{7.1}\\&amp;=\sum_{r=1}^{R_{\sigma}} \sum_{m \in XYZ}{\mathcal{A}_{\sigma,r}^{m}}\tag{7.2}\end{align}\]</span> 同时，<strong>在索引 <span class="math inline">\(ijk\)</span> 处的单个体素的密度值可以直接表示为</strong>： <span class="math display">\[\mathcal{G}_{\sigma,ijk}=\sum_{r=1}^{R_{\sigma}} \sum_{m \in XYZ}{\mathcal{A}_{\sigma,r,ijk}^{m}}\tag{9}\]</span> 这样，计算时只需要计算索引处的相应向量因子和矩阵因子的乘积即可。</p><h2 id="改进的三线性插值">改进的三线性插值</h2><blockquote><p>作者认为，“Na¨ıvely achieving trilinear interpolation is costly, as it requires evaluation of 8 tensor values and interpolating them, increasing computation by a factor of 8 compared to computing a single tensor element” 简单地实现三线性插值的成本很高，因为它需要评估 8 个张量值并对它们进行插值，与计算单个张量元素相比，计算量增加了 8 倍！</p><p>对此作者的解决方案是,"We avoid recovering 8 individual tensor el- ements for trilinear interpolation and instead directly recover the interpolated value, leading to low computation and memory costs at run time."我们避免为三线性插值恢复 8 个单独的张量元素，而是直接恢复插值，从而降低运行时的计算和内存成本。</p></blockquote><p>改进后，三线性插值两个网格的公式为： <span class="math display">\[\begin{align}\mathcal{G}_{\sigma}(\mathbf{x}) &amp;= \sum_r\sum_m \mathcal{A}^m_{\sigma,r}(\mathbf{x})   \tag{13}\\\mathcal{G}_c(\mathbf{x}) &amp;=\mathbf{B}(\oplus [\mathcal{A}^m_{c,r} (\mathbf{x})]_{m,r})  \tag{14}\end{align}\]</span> <strong>将上式代入(6)式，那么因式分解后辐射场就可表示为：</strong> <span class="math display">\[\sigma,c = \sum_r\sum_m \mathcal{A}^m_{\sigma,r}(\mathbf{x}),S(\mathbf{B}(\oplus [\mathcal{A}^m_{c,r} (\mathbf{x})]_{m,r}) ,d),\tag{15}\]</span> 这样，只要给定任意三维位置与视觉方向，我们就行计算出连续的体密度与 view-dependent 颜色。</p><h2 id="体渲染">体渲染</h2><p>渲染图像时，我们仍然使用 NeRF 中的可微体渲染。 <img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240409202309869.png" alt="image-20240409202309869" style="zoom:80%;" /></p><h2 id="重建张量辐射场">重建张量辐射场</h2><p>给定一组具有已知相机姿势的多视图输入图像，我们的张量辐射场通过梯度下降针对每个场景进行优化，最大限度地减少 L2 渲染损失，仅使用地面真实像素颜色作为监督。我们的辐射场通过张量分解来解释，并通过一组全局向量和矩阵进行建模，作为在优化中关联和正则整个场的基本因子。<strong>然而，这有时会导致梯度下降中的过度拟合和局部最小值问题，从而导致观测值较少的区域出现异常值或噪声。</strong></p><p><strong>我们利用压缩感知<code>compressive sensing</code>中常用的标准正则化术语，包括向量和矩阵因子上的 L1 norm 损失和 TV（total variation）损失，从而有效地解决这些问题。</strong>我们发现仅应用 L1 sparsity 损失对于大多数数据集就足够了。然而，<u>对于输入图像很少（如 LLFF[36]）或不完美的捕获条件（如具有不同曝光度和不一致掩模的 Tanks and Temples [26,31]）的真实数据集，<strong>TV 损失比 L1 norm 损失更有效。</strong></u></p><p>为了进一步提高质量并避免局部最小值，我们应用从粗到细的重建。与之前需要对其稀疏选择的体素集进行独特细分的从粗到细的技术不同，我们的从粗到细的重建是通过对 XYZ-mode 向量和矩阵因子进行线性和双线性 upsampling 上采样来简单地实现的。</p><h2 id="实现细节">实现细节</h2><p>见：</p><p><a href="https://zhuanlan.zhihu.com/p/596250930">【论文阅读】TensoRF: Tensorial Radiance Fields - 知乎 (zhihu.com)</a></p><p><a href="https://apchenstu.github.io/TensoRF/">TensoRF: Tensorial Radiance Fields (apchenstu.github.io)</a></p><h1 id="实验">实验</h1><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240408192850614.png" alt="image-20240408192850614" style="zoom:80%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240409204044354.png" alt="image-20240409204044354" style="zoom:80%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240409112153321.png" alt="image-20240409112153321" style="zoom:80%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240409204110423.png" alt="image-20240409204110423" style="zoom:80%;" /></p><h1 id="bib-citation">Bib Citation</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@INPROCEEDINGS&#123;Chen2022ECCV,</span><br><span class="line">  author = &#123;Anpei Chen and Zexiang Xu and Andreas Geiger and Jingyi Yu and Hao Su&#125;,</span><br><span class="line">  title = &#123;TensoRF: Tensorial Radiance Fields&#125;,</span><br><span class="line">  booktitle = &#123;European Conference on Computer Vision (ECCV)&#125;,</span><br><span class="line">  year = &#123;2022&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="参考文献">参考文献</h1><p>[1] Chen A, Xu Z, Geiger A, et al. Tensorf: Tensorial radiance fields[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022: 333-350.</p><p>[2] Sun C, Sun M, Chen H T. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction[C], Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 5459-5469.</p><p>[3] Mildenhall B, Srinivasan P P, Tancik M, et al. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis[C], European Conference on Computer Vision. Cham: Springer International Publishing, 2020: 405-421.</p><ul><li><a href="https://zhuanlan.zhihu.com/p/596250930">【论文阅读】TensoRF: Tensorial Radiance Fields - 知乎 (zhihu.com)</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 辐射场 </tag>
            
            <tag> 加速算法 </tag>
            
            <tag> ECCV2022 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>球谐函数</title>
      <link href="/2024/04/10/%E7%90%83%E8%B0%90%E5%87%BD%E6%95%B0/"/>
      <url>/2024/04/10/%E7%90%83%E8%B0%90%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>一种模型压缩技术：参数矩阵近似</title>
      <link href="/2024/04/10/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B9%8B%E5%8F%82%E6%95%B0%E7%9F%A9%E9%98%B5%E8%BF%91%E4%BC%BC/"/>
      <url>/2024/04/10/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B9%8B%E5%8F%82%E6%95%B0%E7%9F%A9%E9%98%B5%E8%BF%91%E4%BC%BC/</url>
      
        <content type="html"><![CDATA[<blockquote><p>之前在<a href="https://hahahaha5606.github.io/2024/03/17/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B9%8B%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/">模型压缩之知识蒸馏</a>中提过，模型压缩技术大致可分为五种：</p><ul><li><strong>模型剪枝</strong>：即移除对结果作用较小的组件，如减少 head 的数量和去除作用较少的层，共享参数等，ALBERT属于这种；</li><li><strong>量化</strong>：比如将 float32 降到 float8；</li><li><strong>知识蒸馏</strong>：将 teacher 的能力蒸馏到 student上，一般 student 会比 teacher 小。我们可以把一个大而深的网络蒸馏到一个小的网络，也可以把集成的网络蒸馏到一个小的网络上；</li><li><strong>参数共享</strong>：通过共享参数，达到减少网络参数的目的，如 ALBERT 共享了 Transformer 层；</li><li><strong>参数矩阵近似</strong>：通过矩阵的低秩分解或其他方法达到降低矩阵参数的目的。</li></ul><p>这篇文章应理解 TensoRF 的需要诞生，罗列了有关<strong>参数矩阵近似中的矩阵低秩分解</strong>的相关知识。</p></blockquote><h1 id="张量的基本概念">张量的基本概念</h1><p>首先，什么是张量？<span style="background:#FFCC99;"> 其实从维度的角度看很简单：向量是一维，矩阵是二维，张量就是三维及以上。</span> 或者从计算机里阶数的角度看：向量是一阶张量[]，矩阵是二阶张量[[],[]]，“n张平面”构成一个"立方体"是三阶张量[[[],[],...,[]],[[],[],...,[]]]]，...，将 k 个 n-1 阶张量组合成一个数组，就是 n 阶张量。</p><p><strong>常见的张量实例有：</strong></p><ul><li>向量数据：2D 张量，形状为 (samples, features) 。</li><li>时间序列数据或序列数据：3D 张量，形状为 (samples, timesteps, features) 。</li><li>图像：4D张量，形状为 (samples, height, width, channels) 或 (samples, channels, height, width) 。</li><li>视频：5D张量，形状为 (samples, frames, height, width, channels) 或 (samples,frames, channels, height, width)。</li></ul><h1 id="纤维-fiber切片-slice">纤维 fiber，切片 slice</h1><blockquote><p>图解请看<a href="https://blog.csdn.net/weixin_49883619/article/details/109902127">这个链接</a>。</p></blockquote><p><strong>纤维( fiber )</strong>是指<strong>从张量中抽取一维向量的操作</strong>。在矩阵中固定其中一个维度，可以得到行或者列。类似于矩阵操作，保留一个维度变化，固定其它维度，可以得到有纤维的概念。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240409090724264.png" alt="image-20240409090724264" style="zoom: 80%;" /></p><blockquote><p>图来自 Kolda 的论文[1]（2009，Kolda et al.）：<a href="https://www.cs.cornell.edu/courses/cs6241/2020sp/readings/Kolda-Bader-2009-survey.pdf">Tensor Decompositions and Applications | SIAM Review | Vol. 51, No. 3 | Society for Industrial and Applied Mathematics (cornell.edu)</a></p></blockquote><p>以三维的张量为例，向量是一条线，矩阵是一个面，三维张量就是矩阵（面）的高阶形式，是一个长方体。我们把长方体的三个方向/mode（上下列、左右行、前后管）记作 <span class="math inline">\(i,j,k\)</span> ，那么如果保留管<span class="math inline">\(k\)</span>变化，固定住行<span class="math inline">\(j\)</span>、列<span class="math inline">\(i\)</span>，就得到管纤维 <span class="math inline">\(X_{ij}\)</span>​。</p><p><strong>切片（slice）</strong>是指<strong>在张量中抽取二维矩阵的操作。</strong>在张量中如果保留两个维度变化，固定其它维度，可以得到一个矩阵，这个矩阵即为张量的切片。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240409090751040.png" alt="image-20240409090751040" style="zoom:80%;" /></p><p>同样以三维的张量为例，如果我们保留行<span class="math inline">\(j\)</span>、列<span class="math inline">\(i\)</span> 变化，固定住管<span class="math inline">\(k\)</span> ，那么我们就能得到一个正面方向的切片 <span class="math inline">\(X_{::k}\)</span>。</p><h1 id="回顾奇异值分解svd">回顾奇异值分解（SVD）</h1><blockquote><p><a href="https://zhuanlan.zhihu.com/p/29846048">奇异值分解（SVD） - 知乎 (zhihu.com)</a></p><p><a href="https://www.zhihu.com/question/506931984">(99+ 封私信 / 81 条消息) 知道特征值和特征向量 怎么求原矩阵? - 知乎 (zhihu.com)</a></p></blockquote><p><strong>奇异值分解(Singular Value Decomposition，SVD)</strong>是在机器学习领域广泛应用的算法，它不光可以用于降维算法(如PCA 降维)中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。</p><p>我们在高等代数中学过关于特征值与特征向量的定义： <span class="math display">\[A\alpha =\lambda \alpha\]</span> 其中，<span class="math inline">\(A\)</span> 是 <span class="math inline">\(n\)</span> 维的原方阵，<span class="math inline">\(\lambda\)</span> 是矩阵 <span class="math inline">\(A\)</span> 的一个<code>特征值</code>， <span class="math inline">\(\alpha\)</span> 是对应于<code>特征值</code> <span class="math inline">\(\lambda\)</span> 的 <span class="math inline">\(n\)</span> 维的<code>特征向量</code>。</p><p><strong>若矩阵 <span class="math inline">\(A\)</span> 满秩</strong>，则它有 <span class="math inline">\(n\)</span> 个特征值 <span class="math inline">\(\lambda_1,\lambda_2,...,\lambda_n\)</span> ，且 <span class="math inline">\(\alpha_1,\alpha_2,...,\alpha_n\)</span> 是分别对应于这些特征值的其中一个向量。以 <span class="math inline">\(n=3\)</span>​ 时的情况为例，即 <span class="math display">\[A\alpha _1=\lambda_1 \alpha _1 \\A\alpha _2=\lambda_2 \alpha _2 \\A\alpha _3=\lambda_3 \alpha _3\]</span> 那么我们有以下推导： <span class="math display">\[A\left( \alpha _1\ \alpha _2\ \alpha _3 \right) =\left( \lambda _1\alpha _1\ \lambda _2\alpha _2\ \lambda _3\alpha _3 \right) =\left( \alpha _1\ \alpha _2\ \alpha _3 \right) \left( \begin{matrix}    \lambda _1&amp;     &amp;       \\    &amp;       \lambda _2&amp;     \\    &amp;       &amp;       \lambda _3\\\end{matrix} \right)\]</span> 令 <span class="math inline">\(( \alpha _1\ \alpha _2\ \alpha _3)=P\)</span> ，那么 <span class="math display">\[\begin{align}AP&amp;=P\left( \begin{matrix}    \lambda _1&amp;     &amp;       \\    &amp;       \lambda _2&amp;     \\    &amp;       &amp;       \lambda _3\\\end{matrix} \right)\\A&amp;=P\left( \begin{matrix}    \lambda _1&amp;     &amp;       \\    &amp;       \lambda _2&amp;     \\    &amp;       &amp;       \lambda _3\\\end{matrix} \right) P^{-1}\end{align}\]</span> 也就是说，如果矩阵 <span class="math inline">\(A\)</span> 的特征值和特征向量都已知，那么就能以特征值为主元构造对角阵 <span class="math inline">\(P\)</span> ，并根据 <span class="math inline">\(A=PBP^{-1}\)</span> 求出原矩阵 <span class="math inline">\(A\)</span> 。</p><p>一般在计算时，我们还会将已知的这些特征向量 <span class="math inline">\(\alpha_1,\alpha_2,...,\alpha_n\)</span> 标准化，即满足 <span class="math inline">\(||\alpha_i||_2=1\)</span> 或 <span class="math inline">\(\alpha_i^T \alpha_i=1\)</span> 。标准化后得到的特征向量是一组标准正交基，满足 <span class="math inline">\(P^TP=I\)</span> ，即 <span class="math inline">\(P^{-1}=P^T\)</span>，此时的 <span class="math inline">\(P\)</span> 也称为酉矩阵/正交矩阵。这样我们就得到 <span class="math inline">\(A\)</span> 的新特征分解表达式： <span class="math display">\[A=P\left( \begin{matrix}    \lambda _1&amp;     &amp;       \\    &amp;       \lambda _2&amp;     \\    &amp;       &amp;       \lambda _3\\\end{matrix} \right) P^T=\sum_{i=1}^3{\lambda _i\alpha _i\alpha _{i}^{T}}\]</span> <strong>特征分解要求 <span class="math inline">\(A\)</span> 必须为方阵，那么如果 <span class="math inline">\(A\)</span> 不是方阵，怎么进行对其进行分解呢？这时，我们的 SVD 登场了！</strong></p><h2 id="svd-定义">SVD 定义</h2><p>SVD 能对非方阵的矩阵进行分解，比特征分解更有普适性。</p><p>假设原矩阵 <span class="math inline">\(A\)</span> 是一个 <span class="math inline">\(m×n\)</span> 的矩阵，与特征分解类似地，<strong>我们定义它的奇异值分解为：</strong> <span class="math display">\[A=U\varLambda V^T\]</span> 其中，<span class="math inline">\(U\)</span> 是 <span class="math inline">\(m×m\)</span> 的酉矩阵，<span class="math inline">\(\varLambda\)</span> 是由奇异值组成的 <span class="math inline">\(m×n\)</span> 的对角阵，<span class="math inline">\(V\)</span> 是 <span class="math inline">\(n×n\)</span> 的酉矩阵。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240406184708508.png" alt="image-20240406184708508" /><figcaption aria-hidden="true">image-20240406184708508</figcaption></figure><p>接下来，我们只需设法求出这三个矩阵即可。</p><h3 id="右奇异矩阵">右奇异矩阵</h3><p>想法就是，原矩阵 <span class="math inline">\(A\)</span> 是 <span class="math inline">\(m×n\)</span> 的非方阵，我们只需做一个小小的变换，将其变为 <span class="math inline">\(n×n\)</span> 方阵 <span class="math inline">\(A^TA\)</span> 就可代入特征分解法中进行计算。</p><p>若 <span class="math inline">\(A\)</span> 列满秩，则 <span class="math inline">\(A^TA\)</span> 有特征值 <span class="math inline">\(\lambda_1,\lambda_2,...,\lambda_n\)</span> 和其对应的特征向量 <span class="math inline">\(\alpha_1,\alpha_2,...,\alpha_n\)</span> ，满足： <span class="math display">\[\left( A^TA \right) \alpha _i=\lambda _i\alpha _i\]</span> 将这些特征向量标准化，得到一组由标准正交基 <span class="math inline">\(v_1,v_2,...,v_n\)</span> 组成的正交矩阵 <span class="math inline">\(V\in R^{n×n}\)</span> ，满足（这里同样以 <span class="math inline">\(n=3\)</span> 为例）： <span class="math display">\[\begin{align}\left( A^TA \right) V&amp;=V\left( \begin{matrix}    \lambda _1&amp;     &amp;       \\    &amp;       \lambda _2&amp;     \\    &amp;       &amp;       \lambda _3\\\end{matrix} \right)\\A^TA&amp;=V\left( \begin{matrix}    \lambda _1&amp;     &amp;       \\    &amp;       \lambda _2&amp;     \\    &amp;       &amp;       \lambda _3\\\end{matrix} \right) V^T\end{align}\]</span> 这里的正交矩阵 <span class="math inline">\(V\)</span> 就是我们在SVD定义中的矩阵 <span class="math inline">\(V\)</span> 。一般地，我们将 <span class="math inline">\(V\)</span> 中的每个特征向量叫做原矩阵 <span class="math inline">\(A\)</span> 的<strong>右奇异向量</strong>。</p><h3 id="左奇异矩阵">左奇异矩阵</h3><p>同理地，我们将代入矩阵分解中的式子替换为 <span class="math inline">\(m×m\)</span> 的方阵 <span class="math inline">\(AA^T\)</span> ，就可以得到左奇异的形式。</p><p>若 <span class="math inline">\(A\)</span> 行满秩，则 <span class="math inline">\(AA^T\)</span> 有特征值 <span class="math inline">\(\lambda_1,\lambda_2,...,\lambda_m\)</span> 和其对应的特征向量 <span class="math inline">\(\beta_1,\beta_2,...,\beta_m\)</span> ，满足： <span class="math display">\[\left( AA^T \right) \beta _i=\lambda _i\beta _i\]</span> 将这些特征向量标准化，得到一组由标准正交基 <span class="math inline">\(u_1,u_2,...,u_m\)</span> 组成的正交矩阵 <span class="math inline">\(U\in R^{m×m}\)</span> 。该正交矩阵 <span class="math inline">\(U\)</span> 就是我们在SVD定义中的矩阵 <span class="math inline">\(U\)</span> 。一般地，我们将 <span class="math inline">\(U\)</span> 中的每个特征向量叫做原矩阵 <span class="math inline">\(A\)</span> 的<strong>左奇异向量</strong>。</p><h3 id="奇异值矩阵">奇异值矩阵</h3><p>由于 <span class="math inline">\(\varLambda\)</span> 除了对角线上是奇异值，<u>其他位置都是0</u>，那我们只需要求出每个奇异值<span class="math inline">\(\sigma_i\)</span>就可以了，有： <span class="math display">\[A=U\varLambda V^T\Rightarrow AV=U\varLambda \Rightarrow Av_i=\sigma _iu_i\Rightarrow \sigma _i=\frac{Av_i}{u_i}\]</span> 这样我们可以求出每个奇异值，进而求出奇异值矩阵 <span class="math inline">\(\varLambda\)</span> 。</p><blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240406193252469.png" alt="image-20240406193252469" /><figcaption aria-hidden="true">image-20240406193252469</figcaption></figure></blockquote><h2 id="svd-计算举例">SVD 计算举例</h2><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/ff2ba132bm9bbb93f1988a97f9088fef.jpg" alt="ff2ba132bm9bbb93f1988a97f9088fef" style="zoom: 67%;" /></p><blockquote><p><font color=#ef042a>这里有一个小错误</font>：在对第②步的 <span class="math inline">\(\lambda_2=1\)</span> 求特征向量时，由于使用的是基础解系法，应该令自由变量 <span class="math inline">\(x_2=1\)</span> ，进而特征向量 <span class="math inline">\(\alpha_2=(-1,1)^T\)</span>，再而 <span class="math inline">\(v_2=1/ \sqrt{2}(-1,1)^T\)</span> 。</p></blockquote><h2 id="svd-的特性">SVD 的特性</h2><p>对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。</p><p>也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵： <span class="math display">\[A_{m×n}=U_{m×m}\varLambda _{m×n}V_{n×n}^{T}\approx U_{m×k}\varLambda _{k×k}V_{k×n}^{T}\]</span> 其中，<span class="math inline">\(k\)</span> 比 <span class="math inline">\(n\)</span> 要小很多。如下图所示，现在我们的矩阵 <span class="math inline">\(A\)</span> 只需要绿色的部分的三个小矩阵就可以近似描述了。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240406201519266.png" alt="image-20240406201519266" /><figcaption aria-hidden="true">image-20240406201519266</figcaption></figure><p>由于这个重要的性质，SVD可以用于数据降维（PCA降维），来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引（LSI）。</p><h1 id="从-svd-到-秩1张量">从 SVD 到 秩1张量</h1><p>首先，来看看 SVD： <span class="math display">\[A=U\varLambda V^T=\sum_{i=1}^r{\sigma _iu_iv_{i}^{T}}\]</span> 第一个等号：在前面的回顾中我们可从 SVD 的定义知，一个秩为 <span class="math inline">\(r\)</span> 的矩阵可以分解为左右奇异向量矩阵以及一个奇异值矩阵的乘积形式。</p><p>第二个等号：我们现在不妨以另外一种角度来看待矩阵的SVD分解。<strong>我们如果拿出第一个左奇异向量 <span class="math inline">\(u_1\)</span> 以及第一个右奇异向量 <span class="math inline">\(v_1\)</span> ，这两个向量做外积 <span class="math inline">\(u_1v_1^T\)</span> ，我们就可以得到一个矩阵，同时这两个奇异向量对应同一个奇异值 <span class="math inline">\(\sigma_1\)</span> ，我们尝试将奇异值理解为这两个向量外积得到的这个矩阵的在原始矩阵中所占的权重，以此类推我们就可以得到所有奇异值对应的左右奇异向量外积的结果矩阵，然后将这些矩阵加起来就得到了原始矩阵 <span class="math inline">\(A\)</span> 。</strong>（这一想法是可验证的）</p><p>现在，我们知道了二维矩阵的SVD分解是<u>将二维矩阵分解为多组向量的外积-权重和</u>。然后，联系向量是一维，矩阵是二维，张量是三维及以上的概念，给出秩1张量的定义：<u>若张量 <span class="math inline">\(\chi \in R^{n_1×...×n_d}\)</span> 可以被写为 <span class="math inline">\(d\)</span> 个向量的外积时，则 <span class="math inline">\(rank(\chi)=1\)</span> ，即张量 <span class="math inline">\(\chi\)</span> 是秩1张量</u>。这里我们可以将秩1张量 <span class="math inline">\(\chi\)</span> 分解为以下形式： <span class="math display">\[\chi =U^{(1)}\circ U^{(2)}\circ ... \circ U^{(d)}\]</span> 其中， <span class="math inline">\(U^{(i)}\)</span> 是一维向量，符号 “<span class="math inline">\(\circ\)</span> ” 表示外积。关于张量的秩的更多内容请看<a href="https://zhuanlan.zhihu.com/p/382219345">这个链接</a>。</p><h1 id="cp-分解">CP 分解</h1><p><strong>CP分解是用低秩张量（秩1张量）的和来近似原始张量，然后将秩1张量表达为一系列向量的外积的形式，从而将高维的张量用低维的向量表示，以达到降低维度和提取张量中潜在结构的目的。</strong></p><p>之前我们在二维矩阵的 SVD 分解中讲到：矩阵的 SVD 分解= <span class="math inline">\(\sum\)</span> [对应奇异值权重·（左奇异向量 <span class="math inline">\(\circ\)</span> 右奇异向量）]。我们不妨将"[·]"中的带有权重的向量外积矩阵称为“因子矩阵”，那么<span style="background:#daf5e9;">矩阵的 SVD 分解就是将矩阵因式分解为许多因子矩阵之和</span>。</p><p>将这种思想由二维矩阵的 SVD 分解推广到三维以上的张量的 CP 分解中，即<span style="background:#daf5e9;"> CP 分解是将张量近似地因式分解为许多张量因子之和</span>。<strong>以使用最为广泛的三维张量为例</strong>，定义一个三维张量 <span class="math inline">\(\chi \in R^{I×J×K}\)</span> 的 CP 分解为： <span class="math display">\[\chi \approx \sum^R_{r=1} a_r \circ b_r \circ c_r ，\tag{1}\]</span> 这里，向量 <span class="math inline">\(a_r,b_r,c_r\)</span> 的维度分别是 <span class="math inline">\(R^I,R^J,R^K\)</span> ，它们作外积后得到一个秩1张量 <span class="math inline">\(a_r \circ b_r \circ c_r\)</span> 。 <span class="math inline">\(R\)</span> 个秩1张量之和就是原张量，<span class="math inline">\(R\)</span> 就是我们所说的 CP 秩。更直观的理解请看下图：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240408160633017.png" alt="image-20240408160633017" /><figcaption aria-hidden="true">image-20240408160633017</figcaption></figure><blockquote><p>且由外积的定义，对原张量 <span class="math inline">\(\chi\)</span> 中的每个元素都有： <span class="math display">\[x_{ijk} \approx \sum^R_{r=1} a_{ir}b_{jr}c_{kr} \ \ for\ i=1,...,I,j=1,...,J,k=1,...,K, \tag{2}\]</span></p></blockquote><p>我们称(1)式中那些通过外积组成秩1张量元的向量集合为“因子矩阵（factor matrices）”，如 <span class="math inline">\(A=[a_1,a_2,...,a_R]\)</span> ,类似地构造 <span class="math inline">\(B、C\)</span> 。这样，式(1)中的<strong>每个mode可写为如下矩阵形式</strong>： <span class="math display">\[\mathrm{X}_{(1)} \approx \mathrm{A}(\mathrm{C}\odot \mathrm{B})^\mathsf{T},\\\mathrm{X}_{(2)} \approx \mathrm{B}(\mathrm{C}\odot \mathrm{A})^\mathsf{T},\\\mathrm{X}_{(3)} \approx \mathrm{C}(\mathrm{B}\odot \mathrm{A})^\mathsf{T}.\]</span> 这里的符号“<span class="math inline">\(\odot\)</span>”表示 Khatri–Rao product。有时候我们也可以把这三个矩阵形式的方程写成 <span class="math inline">\(\chi\)</span> 的正面切片 frontal slices 形式（即用 <span class="math inline">\(c_{k::}\)</span>表示）： <span class="math display">\[\mathcal{X} \approx \mathrm{A}\mathrm{D}^{(k)}\mathrm{B}^\mathsf{T},\, \text{ where }\, \mathrm{D}^{(k)} \equiv \text{diag}(c_{k::}) \, \text{ for $\, k=1,...,K.$}\]</span> 当然啦，正面切片也可以换成侧面或水平的切片。不过这种切片形式的表达很难推广到超过三维的张量中（我们不知道四维以上的形状是怎样的）。因此我们想到了以下矩阵形式的表达。</p><p>利用 Kolda (2006, Kolda et al.)[2]的命名方式，我们可以进一步简化 CP 模型： <span class="math display">\[\chi \approx [\![A,B,C]\!]\equiv \sum_{r=1}^R a_r \circ b_r \circ c_r ，\]</span> 为了便利，我们通常假设 <span class="math inline">\(A,B,C\)</span> 的列向量是归一化的，且它们的权重被储存在向量 <span class="math inline">\(\lambda \in \mathbb{R}^R\)</span> 中，即： <span class="math display">\[\chi \approx [\![\lambda;A,B,C]\!] = \sum_{r=1}^R \lambda_r a_r \circ b_r \circ c_r,\]</span> <strong>如果将其推广到 <span class="math inline">\(N\)</span> 维张量（<span class="math inline">\(N\)</span>th-order tensor）的情形中</strong>， <span class="math inline">\(\chi \in \mathbb{R}^{I_1×I_2×…×I_N}\)</span> 的 CP 分解为： <span class="math display">\[\mathcal{X} \approx [\![\lambda \:; \mathrm{A}^{(1)}, \mathrm{A}^{(2)},\dots,\mathrm{A}^{(N)}]\!] \equiv \sum_{r=1}^R \lambda_r \: \mathrm{a}_r^{(1)} \circ \mathrm{a}_r^{(1)}\circ \dots \circ \mathrm{a}_r^{(N)},\]</span> 其中，<span class="math inline">\(\lambda \in \mathbb{R}^R\)</span> ， <span class="math inline">\(A^{(n)} \in \mathbb{R}^{I_n × R}\)</span> ， <span class="math inline">\(a_r^{(n)}\in \mathbb{R}^R\)</span> ，<span class="math inline">\(n=1,...,N\)</span> 。这种情况下，mode-<span class="math inline">\(n\)</span>​ 矩阵化的版本为： <span class="math display">\[\mathrm{X}_{(n)}\approx \mathrm{A}^{(n)}\Lambda(\mathrm{A}^{(N)}\circ \dots \circ \mathrm{A}^{(n+1)} \circ \mathrm{A}^{(n-1)} \cdot \dots \cdot \mathrm{A^{(1)})^\mathsf{T}},\]</span> 这里 <span class="math inline">\(\Lambda = diga(\lambda)\)</span> 。</p><h1 id="tuker-分解">Tuker 分解</h1><h1 id="btd-分解">BTD 分解</h1><h1 id="参考链接">参考链接</h1><p><a href="https://zhuanlan.zhihu.com/p/302453223">深入理解 | CP、Tucker分解 - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/FDS99999/article/details/131234000">张量分解（Cp、Tuker、Block-Term）_tucker分解-CSDN博客</a></p><p>[张量分解与应用-学习笔记<a href="https://www.cnblogs.com/lywangjapan/p/12068703.html">02] - LyWangJapan - 博客园 (cnblogs.com)</a> (主要是关于文献[1]的翻译理解)</p><p>[1] Kolda T G, Bader B W. Tensor decompositions and applications[J]. SIAM review, 2009, 51(3): 455-500.</p><p>[2] Kolda T G. Multilinear operators for higher-order decompositions[R]. Sandia National Laboratories (SNL), Albuquerque, NM, and Livermore, CA (United States), 2006.</p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 矩阵分解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>矩阵的各种&quot;积&quot;</title>
      <link href="/2024/04/10/%E7%9F%A9%E9%98%B5%E7%9A%84%E5%90%84%E7%A7%8D%E7%A7%AF/"/>
      <url>/2024/04/10/%E7%9F%A9%E9%98%B5%E7%9A%84%E5%90%84%E7%A7%8D%E7%A7%AF/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://blog.csdn.net/xuehuitanwan123/article/details/104291475">矩阵的Kronecker积、Khatri-Rao积、Hadamard积-CSDN博客</a></p><p><a href="https://blog.csdn.net/xq151750111/article/details/121049396">矩阵篇（三）-- 矩阵的普通乘积、Hadamard 积、Kronecker 积及其性质_哈达玛积-CSDN博客</a></p><p>张贤达《矩阵分析与应用》第二版</p></blockquote><h1 id="kronecker-积">Kronecker 积</h1><p><strong>克罗内克积</strong>（<strong>Kronecker product</strong> ,以德国数学家利奥波德·克罗内克命名）是两个任意大小矩阵之间的运算，也称“直积（direct product）”或“张量积(tensor product)”，符号为“<span class="math inline">\(\otimes\)</span>”。矩阵 <span class="math inline">\(A_{m×n}\)</span> 和矩阵 <span class="math inline">\(B_{p×q}\)</span> 的克罗内克积是一个 <span class="math inline">\(mp×nq\)</span> 的分块矩阵，其定义如下： <span class="math display">\[(A_{m×n}) \otimes (B_{p×q})= (A\otimes B)_{mp×nq}= [a_{ij}B]_{i=1,j=1}^{m,n} =\left[ \begin{matrix}    a_{11}B&amp;        \cdots&amp;     a_{1n}B\\    \vdots&amp;     \ddots&amp;     \vdots\\    a_{m1}B&amp;        \cdots&amp;     a_{mn}B\\\end{matrix} \right]\]</span> 具体写为： <span class="math display">\[\boldsymbol{A} \otimes \boldsymbol{B}=\left[\begin{array}{cccccccccc}a_{11} b_{11} &amp; a_{11} b_{12} &amp; \cdots &amp; a_{11} b_{1 q} &amp; \cdots &amp; \cdots &amp; a_{1 n} b_{11} &amp; a_{1 n} b_{12} &amp; \cdots &amp; a_{1 n} b_{1 q} \\a_{11} b_{21} &amp; a_{11} b_{22} &amp; \cdots &amp; a_{11} b_{2 q} &amp; \cdots &amp; \cdots &amp; a_{1 n} b_{21} &amp; a_{1 n} b_{22} &amp; \cdots &amp; a_{1 n} b_{2 q} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; &amp; &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\a_{11} b_{p 1} &amp; a_{11} b_{p 2} &amp; \cdots &amp; a_{11} b_{p q} &amp; \cdots &amp; \cdots &amp; a_{1 n} b_{p 1} &amp; a_{1 n} b_{p 2} &amp; \cdots &amp; a_{1 n} b_{p q} \\\vdots &amp; \vdots &amp; &amp; \vdots &amp; \ddots &amp; &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\\vdots &amp; \vdots &amp; &amp; \vdots &amp; &amp; \ddots &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\a_{m 1} b_{11} &amp; a_{m 1} b_{12} &amp; \cdots &amp; a_{m 1} b_{1 q} &amp; \cdots &amp; \cdots &amp; a_{m n} b_{11} &amp; a_{m n} b_{12} &amp; \cdots &amp; a_{m n} b_{1 q} \\a_{m 1} b_{21} &amp; a_{m 1} b_{22} &amp; \cdots &amp; a_{m 1} b_{2 q} &amp; \cdots &amp; \cdots &amp; a_{m n} b_{21} &amp; a_{m n} b_{22} &amp; \cdots &amp; a_{m n} b_{2 q} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; &amp; &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\a_{m 1} b_{p 1} &amp; a_{m 1} b_{p 2} &amp; \cdots &amp; a_{m 1} b_{p q} &amp; \cdots &amp; \cdots &amp; a_{m n} b_{p 1} &amp; a_{m n} b_{p 2} &amp; \cdots &amp; a_{m n} b_{p q}\end{array}\right]\]</span></p><h2 id="外积与-kronecker-积">外积与 Kronecker 积</h2><p>一个列向量与一个行向量做乘积称作向量的“外积”，外积可以视为一种特殊的克罗内克积： <span class="math display">\[\boldsymbol{x} \circ \boldsymbol{y}=\boldsymbol{x} \otimes \boldsymbol{y}^T=\boldsymbol{x}\boldsymbol{y}^T\]</span> 其结果是一个<u>矩阵</u>。</p><p>注意区别两个列向量直接做克罗内克积： <span class="math display">\[\boldsymbol{x} \otimes \boldsymbol{y}= vec(\boldsymbol{x} \circ \boldsymbol{y})=vec(\boldsymbol{x} \boldsymbol{y}^T)\]</span> 其结果是两个向量的外积按左右上下的顺序向量化 vectorization 的结果，仍然是一个<u>向量</u>。</p><h2 id="性质">性质</h2><h1 id="khatri-rao-积">Khatri-Rao 积</h1><p>Khatri-Rao 积是一种特殊的克罗内克积，符号为“<span class="math inline">\(\odot\)</span>”，定义为两个<u>具有相同列数</u>的矩阵 <span class="math inline">\(A_{I×K}\)</span> 与 <span class="math inline">\(B_{J×K}\)</span> 的对应列向量的克罗内克积的排列矩阵 <span class="math inline">\((A\odot B)_{IJ×K}\)</span> : <span class="math display">\[A\odot B=[a_1 \otimes b_1,a_2\otimes b_2,...,a_k\otimes b_k]\]</span> 为了更直观地理解，我们举个例子。在克罗内克积中我们讲过，两个列向量的克罗内克积就是他们做外积再向量化的结果，即若 <span class="math inline">\(A=[\boldsymbol{a}_1,\boldsymbol{a}_2]\)</span> , <span class="math inline">\(B=[\boldsymbol{b}_1,\boldsymbol{b}_2]\)</span> ，且： <span class="math display">\[\boldsymbol{a}_1\circ \boldsymbol{b}_1=\left[ \begin{array}{c}    1\\    3\\    5\\\end{array} \right] \times \left[ \begin{matrix}    2&amp;      3\\\end{matrix} \right] =\left[ \begin{matrix}    2&amp;      3\\    6&amp;      9\\    10&amp;     15\\\end{matrix} \right]\\\boldsymbol{a}_2\circ \boldsymbol{b}_2=\left[ \begin{array}{c}    2\\    4\\    6\\\end{array} \right] \times \left[ \begin{matrix}    6&amp;      4\\\end{matrix} \right] =\left[ \begin{matrix}    12&amp;     8\\    24&amp;     16\\    36&amp;     24\\\end{matrix} \right] \]</span> 则它们的 Khatri-Rao 积为： <span class="math display">\[A\odot B=\left[ \begin{matrix}    1&amp;      2\\    3&amp;      4\\    5&amp;      6\\\end{matrix} \right] \odot \left[ \begin{matrix}    2&amp;      6\\    3&amp;      4\\\end{matrix} \right] =\left[ \begin{matrix}    2&amp;      12\\    3&amp;      8\\    6&amp;      24\\    9&amp;      16\\    10&amp;     36\\    15&amp;     24\\\end{matrix} \right]\]</span></p><h2 id="性质-1">性质</h2><p><span class="math display">\[A \bigodot B \bigodot C=(A \bigodot B) \bigodot C=A \bigodot(B \bigodot C) ,\\\quad(A \bigodot B)^{T}(A \bigodot B)=A^{T} A * B^{T} B.\]</span></p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 矩阵分解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一种模型压缩技术：知识蒸馏</title>
      <link href="/2024/04/10/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B9%8B%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"/>
      <url>/2024/04/10/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B9%8B%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="模型压缩">模型压缩</h1><p><span style="background:#fbd4d0;"><strong>模型压缩技术大致可分为5种</strong>：</span></p><ul><li><strong>模型剪枝</strong>：即移除对结果作用较小的组件，如减少 head 的数量和去除作用较少的层，共享参数等，ALBERT属于这种；</li><li><strong>量化</strong>：比如将 float32 降到 float8；</li><li><strong>知识蒸馏</strong>：将 teacher 的能力蒸馏到 student上，一般 student 会比 teacher 小。我们可以把一个大而深的网络蒸馏到一个小的网络，也可以把集成的网络蒸馏到一个小的网络上；</li><li><strong>参数共享</strong>：通过共享参数，达到减少网络参数的目的，如 ALBERT 共享了 Transformer 层；</li><li><strong>参数矩阵近似</strong>：通过矩阵的低秩分解或其他方法达到降低矩阵参数的目的。</li></ul><blockquote><p>来自：<a href="https://blog.csdn.net/weixin_43694096/article/details/127505946">一文搞懂【知识蒸馏】【Knowledge Distillation】算法原理_知识蒸馏算法-CSDN博客</a></p><p><strong>轻量化网络的方式</strong></p><ol type="1"><li>压缩已训练好的模型：知识蒸馏、权值量化、权重剪枝、通道剪枝、注意力迁移</li><li>直接训练轻量化网络：SqueezeNet、MobileNetv1v2v3、MnasNet、SHhffleNet、Xception、EfficientNet、EfficieentDet</li><li>加速卷积运算：im2col+GEMM、Wiongrad、低秩分解</li><li>硬件部署：TensorRT、Jetson、TensorFlow-Slim、openvino、FPGA集成电路</li></ol></blockquote><h1 id="知识蒸馏">知识蒸馏</h1><p>在化学中，蒸馏（Distillation）是一种热力学分离工艺。它利用混合液体或液-固体系中各组分的沸点不同的原理，使不同沸点组分依次先蒸发冷凝，以达到分离组分的目的。例如，下图为实验室制取蒸馏水的示意图。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/imagezhengliu.png" alt="imagezhengliu" /><figcaption aria-hidden="true">imagezhengliu</figcaption></figure><p>类比地，知识蒸馏的混合物则是指原模型中含有大量的知识，我们要设法将原来的大模型中的知识”蒸馏“到小模型中（知识迁移）。大模型就像是一个全能的"集大成者"，而小模型则是各个领域的"专家"。换言之，知识蒸馏就是用相同的数据集训练不同的模型，然后 average 这些模型的 predictions 。</p><h2 id="动机与应用">动机与应用</h2><p>一般而言，大模型性能良好且泛化能力强，但是网络较为复杂。而相反地，小模型网络简单，参数量少，但是表达能力有限。因此如果我们能用<u>大模型学习到的知识去指导小模型，就能使得小模型在具备和大模型相当的性能的同时大大减少参数量</u>，进而实现模型压缩与加速！这就是知识蒸馏与迁移学习在模型优化中的应用。</p><p>在知识蒸馏中，我们将复杂的大模型称为<strong>教师模型（Teacher）</strong>，小模型称为<strong>学生模型（Student）</strong>。整体训练过程大致如下图：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240307182432025.png" alt="image-20240307182432025" /><figcaption aria-hidden="true">image-20240307182432025</figcaption></figure><blockquote><p>复杂笨重但是效果好的Teacher模型不上线，就单纯是个导师角色，真正部署上线进行预测任务的是灵活轻巧的Student小模型。</p></blockquote><p>通过知识蒸馏以实现压缩模型的好处如下：</p><p>1.<strong>提升模型精度</strong>：对模型A的精度不满意。训练一个高精度 Teacher 模型B，然后用模型B对需要提高精度的 Student 模型A进行知识蒸馏，最后得到一个更高精度的模型A。</p><p>2.<strong>降低模型时延：</strong>对模型A的时延不满意。先找一个时延低、参数量小但是精度可能低的模型B，然后训练一个高精度但是高时延的模型C来对模型B进行知识蒸馏，使得模型C在达到低时延的同时接近需要降低时延的模型A的精度。</p><p>3.<strong>标签之间的域集成</strong>：想要训练一个能识别猫、狗、香蕉和苹果的模型A。可以先用猫狗数据集训练一个能识别猫和狗的 Teacher 模型B，再用香蕉苹果数据集训练一个能识别香蕉和苹果的 Teacher 的模型C，最后用模型B和模型C蒸馏出能识别四种事物的模型A。这样便能将两个不同域的数据集进行集成。</p><p>3.<strong>标签之间的域迁移</strong>：可以先用猫狗数据集训练一个能识别猫和狗的 Teacher 模型B，再用香蕉苹果数据集训练一个能识别香蕉和苹果的 Teacher 的模型C，最后用模型C蒸馏出能识别四种事物的模型B。这样便能将两个不同域的数据集进行迁移。</p><blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240307181641653.png" alt="image-20240307181641653" /><figcaption aria-hidden="true">image-20240307181641653</figcaption></figure></blockquote><h2 id="发展">发展</h2><p>Hinton 等人最早在2015年的<a href="https://arxiv.org/pdf/1503.02531.pdf">《Distilling the knowledge in a neural network》</a>[1]中提出“知识蒸馏（Knowledge Distillation, KD）”的概念。知识蒸馏不仅是模型压缩的一种手段，也可以视为是将模型的能力进行迁移，根据迁移的方法不同可以分为<span style="background:#FFCC99;"><strong>基于目标蒸馏</strong>（ Soft-target 蒸馏 / Logits 方法蒸馏）</span>和<span style="background:#daf5e9;"><strong>基于特征蒸馏</strong></span>两个大方向。</p><h1 id="类1目标蒸馏">类1：目标蒸馏</h1><h2 id="算法全貌">算法全貌</h2><p>目标蒸馏方法中最经典的论文就是来自于2015年Hinton发表的一篇神作《Distilling the Knowledge in a Neural Network》[1]。在这篇论文中，Hinton以经典的数字数据集 MNIST 的<strong>分类问题</strong>为例展开。分类问题的共同点是模型最后会有一个 Softmax 层，其输出值对应了相应类别的概率值。这里我们将 Softmax 函数层的输出概率值称为 <strong>Soft-target</strong>。</p><blockquote><p><strong>关于 Softmax 函数详见：</strong></p><ul><li><a href="https://zhuanlan.zhihu.com/p/105722023">一文详解Softmax函数 - 知乎 (zhihu.com)</a></li><li><a href="https://blog.csdn.net/bitcarmanlee/article/details/82320853">入门级都能看懂的softmax详解-CSDN博客</a></li></ul><p>简言之，相对于二分类的逻辑回归， Softmax 函数能实现多分类。之所以认为它是"soft"的，是因为它的输出不是非黑即白的，而是<strong>事物属于各个类别的概率值（置信度）</strong>。Softmax 的输出和值为 1 。</p><p><u>Softmax 实现多分类的过程</u>大致是将各类别的最终得分 Logits 归一化，将其转化为符合概率分布的类别概率值。它会放大 Logits 之间的差异，使得得分高的类别概率值偏大，得分低的类别概率值偏小，实现两极分化。</p><p>在进行 k 分类问题的解决时，如果这些类别之间是互斥的（事物只能属于一种类别），则使用类别数 n=k 的 Softmax 回归；如果这些类别不是互斥的（事物可以属于多个类别），则使用 k 个二分类的逻辑回归分类器。</p></blockquote><p>在分类问题的传统解法中，模型损失函数的设置往往是为了使该模型的预测值尽快能接近真实值（Hard-target），这种过程是对真实值求极大似然。</p><p>而在知识蒸馏中，我们预先训练了一个泛化能力强的 Teacher 模型，其最后的 Softmax 层输出的概率值作为 Soft-target 指导 Student 模型。事实上，目标蒸馏迁移泛化能力的方法就是<strong>用 Teacher 模型 Softmax 层的输出概率 Soft-target 替换真实值 Hard-target 来设置 Student 模型的另一个新损失</strong>。新损失(Soft-target)和传统损失(Hard-target)<u>共同用于训练</u> Student 模型。</p><p>那么我们这么做有什么根据吗？</p><figure><img src="https://pic3.zhimg.com/v2-f5dcb7d95a9c78ec49923221091079ca_r.jpg" alt="img1" /><figcaption aria-hidden="true">img1</figcaption></figure><blockquote><p>分类一张“2”的图片：</p><ul><li>Hard-target：原始数据集标注的 one-hot 标签，除了真实标签为 1，其他标签都是 0。</li><li>Soft-target： Teacher 模型 Softmax 层输出的类别概率，每个类别都分配了概率，预测的真实标签概率最高。同时所有概率和为 1 。</li></ul><p>我们往往也称<u>真实标签</u>为<u>正标签</u>，<strong>错误标签</strong>为<strong>负标签</strong>。</p></blockquote><p>Softmax 层的输出，除了正标签之外，<strong>负标签也带有Teacher模型归纳推理的大量信息</strong>，<strong>比如某些负标签对应的概率远远大于其他负标签，则代表 Teacher模型在推理时认为该样本与该负标签有一定的相似性</strong>。而在传统的训练过程(Hard-target)中，所有负标签都被统一对待。也就是说，知识蒸馏的训练方式（Soft-target）使得每个样本给 Student 模型带来的信息量大于传统的训练方式!</p><p>此外， Soft-target 分布的熵相对高时，其 Soft-target 蕴含的知识就更丰富。同时，使用 Soft-target 训练时，梯度的方差会更小，训练时可以使用更大的学习率，所需要的样本也更少。这也解释了为什么通过蒸馏的方法训练出 Student 模型相比使用完全相同的模型结构和训练数据只使用Hard-target的训练方法得到的模型，拥有更好的泛化能力。</p><h2 id="具体过程">具体过程</h2><blockquote><p>Softmax 函数的介绍见上文</p></blockquote><p>对于一般的分类问题，在 DNN 到达最后的 Softmax 层之前，会得到各类别的最终得分 Logits （第 <span class="math inline">\(i\)</span> 个类别的得分为 <span class="math inline">\(z_i\)</span> ）。然后神经网络再使用 Softmax 层来实现 Logits 向 probabilities 的转换。令 <span class="math inline">\(q_i\)</span> 表示事物属于第 <span class="math inline">\(i\)</span> 种类别的概率，则原始的 Softmax 函数如下： <span class="math display">\[q_i=softmax(z_i)=\frac{\exp \left( z_i \right)}{\sum_j{\exp \left( z_j \right)}},\]</span> 但是直接使用 Softmax 层的输出值作为 Soft-target ，这又会带来一个问题: 当 Softmax 函数输出的概率分布熵相对较小时，负标签的值都很接近0，对损失函数的贡献非常小，小到可以忽略不计。因此"<strong>温度 </strong><span class="math inline">\(T\)</span> "这个变量就派上了用场。下面的公式是加了温度 <span class="math inline">\(T\)</span> 这个变量之后的 Softmax 函数: <span class="math display">\[q_i=softmax(z_i/T)=\frac{\exp \left( z_i/T \right)}{\sum_j{\exp \left( z_j/T \right)}},\]</span> 其中，当温度 <span class="math inline">\(T=1\)</span> 时，这就是标准的 Softmax 函数公式。<strong><span class="math inline">\(T\)</span>​ 越高，Softmax 函数的 output probability distribution 越趋于平滑，其分布的熵越大，负标签携带的信息会被相对地放大，模型训练将更加关注负标签</strong>。</p><blockquote><p><strong>！！！温度 <span class="math inline">\(T\)</span>​ 的调节：</strong></p><p>最开始提到过，化学中的蒸馏是利用混合物中各组分沸点不同的原理，通过逐步升温冷凝，将各组分分离开来。而在知识蒸馏中，通过逐步升温，我们可以分离出负标签携带的不同程度信息：Student 模型在训练时对负标签的关注程度（或者说负标签携带的信息量）则会随着温度 <span class="math inline">\(T\)</span> 的升高而增大，如下图：</p><figure><img src="https://pic2.zhimg.com/80/v2-41f8f48ad3c8a2b3097a73fec0ff0c9d_720w.webp" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>图中，中间 Softmax 函数输出在原始 <span class="math inline">\(T=1\)</span> 时的特例，左边为 <span class="math inline">\(T\rightarrow 0\)</span> 的情况，右边为 <span class="math inline">\(T \rightarrow \infty\)</span> 的情况。</p><p>可见，当 <span class="math inline">\(T\)</span> 逐渐变小时，概率分布会变得越来越“陡峭”，接近于 Hard-target （负标签携带的信息量为 0 ），Student 模型对负标签（尤其是显著低于平均值的负标签）的关注变小；当 <span class="math inline">\(T\)</span> 逐渐增大至无穷时，概率分布则愈发“平缓”，Softmax 输出概率值趋向平均分布，信息熵逐渐增大，模型相对更多地关注到负标签。</p><p>然而无论如何，Softmax 输出即 Soft-target 都有忽略较小的概率值 <span class="math inline">\(p_i\)</span>​ 携带的信息倾向，更多地关注概率值高于平均值的负标签（它携带的信息确实更多）。但由于 Teacher 模型的训练过程决定了负标签部分概率值都比较小，并且负标签的值越低，其信息就越不可靠。因此温度的选取需要进行实际实验的比较，本质上就是在下面两种情况之中取舍:</p><ul><li>当想从负标签中学到一些信息量的时候，温度 <span class="math inline">\(T\)</span> 应调高一些；</li><li>当想减少负标签的干扰，少学一些信息的时候，温度 <span class="math inline">\(T\)</span> 应调低一些；</li></ul><p>总的来说，T的选择和 Student 模型的大小有关， Student 模型参数量比较小的时候，相对比较低的温度就可以了。因为参数量小的模型不能学到所有 Teacher 模型的知识，所以可以适当忽略掉一些负标签的信息。</p><p>最后，在整个知识蒸馏过程中，我们先让温度 <span class="math inline">\(T\)</span> 升高，提取负标签中的信息；然后在 Hard-target 阶段恢复“低温“（ <span class="math inline">\(T=1\)</span> ），从而将原模型中的知识提取出来，因此将其称为是蒸馏，实在是妙啊。</p></blockquote><figure><img src="https://pic2.zhimg.com/80/v2-90c65f1d4dcec36f1ff7dd38a96c3cdd_720w.webp" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><blockquote><p>目标知识蒸馏的步骤图</p></blockquote><p>目标蒸馏的整体过程主要包括以下步骤：</p><ol type="1"><li>预训练 Teacher 模型；</li><li>在高温环境 <span class="math inline">\(T_{high}=t\)</span> 下 Teacher 模型产生 Soft-target；</li><li><span style="background:#daf5e9;">使用 {Soft-target, <span class="math inline">\(T_{high}=t\)</span> } 和 {Hard-target, <span class="math inline">\(T=1\)</span> } 同时训练 Student 模型</span>；</li><li>设置温度 <span class="math inline">\(T=1\)</span> 的环境下， Student 模型？？？</li></ol><p>其中第2、3步统称为：高温蒸馏的过程。高温蒸馏过程的损失函数 <span class="math inline">\(L\)</span> 由 Soft-target 对应的蒸馏损失 <code>distillation loss</code> <span class="math inline">\(L_{soft}\)</span> 和 Hard-target 对应的学生原损失 <code>student loss</code> <span class="math inline">\(L_{hard}\)</span> 加权求和得到： <span class="math display">\[L=\alpha L_{soft}+\beta L_{hard},\]</span></p><h3 id="蒸馏损失">蒸馏损失</h3><p>Teacher 模型和 Student 模型同时输入 transfer set (这里可以直接复用训练 Teacher 模型用到的 training set )，用 Teacher 模型在高温 <span class="math inline">\(t\)</span> 下产生的各类别概率 Softmax distribution 来作为 Soft-target <span class="math inline">\(q_i^t\)</span> 。 Student 模型在相同温度 <span class="math inline">\(t\)</span> 条件下的 Softmax 输出预测 <span class="math inline">\(p_i^t\)</span> 和 Teacher 模型产生的 Soft-target <span class="math inline">\(q_i^t\)</span> 的交叉熵 cross entropy 就是<strong>Loss函数的第一部分</strong> <span class="math inline">\(L_{soft}\)</span>​ ，具体形式如下所示： <span class="math display">\[\begin{align}L_{soft}&amp;=-\sum_i^N{p_{i}^{t}\log \left( q_{i}^{t} \right)}\\p_{i}^{t}&amp;=\frac{\exp \left( y_i/T \right)}{\sum_k^N{\exp \left( y_k/T \right)}}\\q_{i}^{t}&amp;=\frac{\exp \left( z_i/T \right)}{\sum_k^N{\exp \left( z_k/T \right)}}\end{align}\]</span> 其中， <span class="math inline">\(y_i\)</span> 表示 Teacher 模型的 Logits , <span class="math inline">\(z_i\)</span> 表示 Student 模型的 Logits , <span class="math inline">\(N\)</span> 表示总类别数。</p><h3 id="学生模型原损失">学生模型原损失</h3><p>在温度 <span class="math inline">\(T=1\)</span> 的条件下（不加温）， Student 模型产生的预测输出 <span class="math inline">\(q_i^1\)</span> 与 ground truth 真实值 <span class="math inline">\(c_i\)</span> 之间的 cross entropy 就是<strong>Loss函数的第二部分 <span class="math inline">\(L_{hard}\)</span> </strong>: <span class="math display">\[\begin{align}L_{hard}&amp;=-\sum_i^N{c_{i}\log \left( q_{i}^{1} \right)}\\q_{i}^{1}&amp;=\frac{\exp \left( z_i \right)}{\sum_k^N{\exp \left( z_k \right)}}\end{align}\]</span> 其中， <span class="math inline">\(c_i\)</span> 表示第 <span class="math inline">\(i\)</span> 个类别的真实值，且 <span class="math inline">\(c_i \in {0,1}\)</span> ，正标签取 1 ，负标签取 0 。</p><p>第二部分 <span class="math inline">\(L_{hard}\)</span> 的必要性其实很好理解：Teacher 模型也有一定的错误率，使用 ground truth 可以有效降低错误被传播给 Student 模型的可能性。打个比喻，老师虽然学识远远超过学生，但是他仍然有出错的可能，而这时候如果学生在老师的教授之外，可以同时参考到标准答案，就可以有效地降低被老师偶尔的错误“带偏”的可能性。</p><h3 id="损失权重">损失权重</h3><blockquote><p>理论推导见：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/102038521">【经典简读】知识蒸馏(Knowledge Distillation) 经典之作 - 知乎 (zhihu.com)</a></li><li><a href="https://baihaoran.xyz/2020/05/04/Knowledge-Distillation.html">baihaoran.xyz</a></li></ul></blockquote><p>实验发现，当 <span class="math inline">\(L_{hard}\)</span> 权重 <span class="math inline">\(\beta\)</span> 较小时，能产生最好的效果，这是一个经验性的结论。这里给出经过推导的结论：由于 <span class="math inline">\(L_{soft}\)</span> 贡献的梯度大约为 <span class="math inline">\(L_{hard}\)</span> 的 <span class="math inline">\(1/t^2\)</span>，因此在同时使用 Soft-target 和 Hard-target 的时候，需要在 <span class="math inline">\(L_{soft}\)</span> 的权重上乘以 <span class="math inline">\(t^2\)</span>​ 的系数，这样才能保证Soft-target和Hard-target贡献的梯度量基本一致。</p><h2 id="当trightarrow-infty时的特殊形式">当<span class="math inline">\(T\rightarrow \infty\)</span>时的特殊形式</h2><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240307211645510.png" alt="image-20240307211645510" /><figcaption aria-hidden="true">image-20240307211645510</figcaption></figure><h1 id="类2特征蒸馏">类2：特征蒸馏</h1><p>与目标蒸馏通过将 Teacher 模型的 Softmax 层输出概率值作为 Soft-target 来训练 Student 模型的损失函数 <span class="math inline">\(L_{soft}\)</span> 不同,特征蒸馏同时将使 Student 模型网络的中间层学习 Teacher 模型网络的中间层特征。最早提出这个想法的是论文《Fitnets: Hints for thin deep nets》[2]。接下来以这篇文章为主，介绍特征蒸馏方法的原理。</p><figure><img src="https://pic2.zhimg.com/80/v2-dba8b40b66d3c9170d984a6c4eeec3f5_720w.webp" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="何为-thin-deep-net-及主要-idea">何为 thin deep net 及主要 idea</h2><p>这篇论文首先提出一个案例，既宽又深的模型通常需要大量的乘法运算，从而导致对内存和计算的高需求。因此，即使网络在准确性方面是性能最高的模型，其在现实世界中的应用也受到限制。</p><p>为了解决这类问题，我们需要通过模型压缩（也称为知识蒸馏）将知识从复杂的模型转移到参数较少的简单模型。</p><p>到目前为止，知识蒸馏技术已经考虑了Student网络与Teacher网络有相同或更小的参数。<font color=#985fff>这里有一个洞察点是，深度是特征学习的基本层面，到目前为止尚未考虑到Student网络的深度。一个具有比Teacher网络更多的层但每层具有较少神经元数量的Student网络称为“<strong>thin deep network</strong>”。</font></p><p>因此，该篇论文主要针对Hinton提出的知识蒸馏法进行扩展，允许 Student 网络可以比 Teacher 网络更深更窄，并使用 Teacher 网络的输出和中间层的特征作为提示，改进训练过程和 Student 网络的性能。</p><h2 id="二阶段的模型结构">二阶段的模型结构</h2><blockquote><p>Student网络不仅仅通过目标蒸馏拟合Teacher网络的 Soft-target ，而且通过特征蒸馏拟合隐藏层的输出（Teacher网络抽取的特征）:</p><ul><li>第一阶段让Student网络去学习Teacher网络的隐藏层输出（特征蒸馏）；</li><li>第二阶段使用Soft-target来训练Student网络（目标蒸馏）。</li></ul></blockquote><p>论文将 Student 网络模型 FitNet 设置得比 Teacher 网络模型更深更窄。文中将“hints”定义为 Teacher 网络某一隐藏层 hint layer 的输出，且该输出将被用于指导 FitNet 网络中选定隐藏层 the guided layer 的训练过程，即使得两个 layer 的输出逐渐趋于一致。</p><p>那么如何选取 guided/hint layer 对呢？</p><p>注意，学习 hints是正则化的形式。因此，必须有选择地确定 hint/guided layer 对，以便学生网络不会过度正则化。我们设置的 guided 层越深，我们给网络的灵活性就越低，因此，FitNets 更有可能遭受过度正则化的影响。在我们的例子中，我们选择 hint 作为教师网络的中间层。同样，我们选择 guided 层作为学生网络的中间层。（Note that having hints is a form of regularization and thus, the pair hint/guided layer has to be chosen such that the student network is not over-regularized. The deeper we set the guided layer, the less flexibility we give to the network and, therefore, FitNets are more likely to suffer from over-regularization. In our case, we choose the hint to be the middle layer of the teacher network. Similarly, we choose the guided layer to be the middle layer of the student network.）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/liucheng.png" alt="liucheng" /><figcaption aria-hidden="true">liucheng</figcaption></figure><blockquote><p><strong>图解：</strong></p><p><font color=#4eb434>图（a）绿框</font>表示 <span class="math inline">\(W_{Hint}\)</span> ，即 Teacher 网络从第一层到中间层——前 <span class="math inline">\(h\)</span> 层（first layer-&gt;hint layer）的参数；<font color=#ef042a>图（a）红框</font>表示 <span class="math inline">\(W_{Guided}\)</span>​ ，即 Student 网络的第一层到中间层——前 <span class="math inline">\(g\)</span> 层（first layer-&gt;guided layer）的参数。</p><p><font color=#0091ff>图（b）蓝框</font>表示 <span class="math inline">\(W_{r}\)</span> ，即使更窄的 <span class="math inline">\(W_{Guided}\)</span> 的输出尺寸(guided layer)转换为更宽的 <span class="math inline">\(W_{hint}\)</span> 的尺寸（hint layer）的工具层 convolutional regressor 的参数。选择卷积型的 regressor 主要是为了缓解由全连接 regressor 带来的参数增多和内存消耗增大的影响，具体分析论文已给出。</p></blockquote><p><strong>把“宽”且“深”的网络蒸馏成“瘦”且“更深”的网络，需要进行两阶段的训练：</strong></p><p><strong>第一阶段 Hints Trianing/特征蒸馏：</strong></p><p>根据 Teacher 模型的中间层输出来指导预训练 Student 模型。在训练之初 Student 网络进行随机初始化，并需要学习一个映射函数 <span class="math inline">\(W_r\)</span> 使得 <span class="math inline">\(W_{Guided}\)</span> 的维度匹配 <span class="math inline">\(W_{Hint}\)</span>​ ，得到 Student 模型在下一阶段的参数初始化值，并最小化两者网络输出的 MSE 差异作为损失，如下： <span class="math display">\[\mathcal{L}_{H T}\left(\mathbf{W}_{\text {Guided }}, \mathbf{W}_{\mathbf{r}}\right)=\frac{1}{2}\left\|u_{h}\left(\mathbf{x} ; \mathbf{W}_{\text {Hint }}\right)-r\left(v_{g}\left(\mathbf{x} ; \mathbf{W}_{\text {Guided }}\right) ; \mathbf{W}_{\mathbf{r}}\right)\right\|^{2},\]</span> 这里，<span class="math inline">\(u_h\)</span> and <span class="math inline">\(v_g\)</span> are the teacher/student deep nested functions up to their respective hint/guided layers with parameters <span class="math inline">\(W_{Hint}\)</span> and <span class="math inline">\(W_{Guided}\)</span> 。</p><p><strong>第二阶段 Knowledge Distillation/目标蒸馏</strong>：</p><p>在训练好 guided layer之后，将当前的参数作为网络的初始参数，利用知识蒸馏的方式训练 Student 网络的所有层参数，使 Student 学习 Teacher 的输出。由于 Teacher 对于简单任务的预测非常准确，在分类任务中近乎 one-hot 输出，因此为了弱化预测输出，使所含信息更加丰富，作者使用 Hinton 等人论文《Distilling knowledge in a neural network》中提出的 Softmax 改造方法，即在 Softmax 前引入 <span class="math inline">\(\tau\)</span> 缩放因子，将 Teacher 和 Student 的 pre-softmax 输出 Logits 均除以 <span class="math inline">\(\tau\)</span>​ 。也就是上面我们讲的加了温度的 Softmax 。此时的损失函数为： <span class="math display">\[\mathcal{L}_{K D}\left(\mathbf{W}_{\mathbf{S}}\right)=\mathcal{H}\left(\mathbf{y}_{\text {true }}, \mathrm{P}_{\mathrm{S}}\right)+\lambda \mathcal{H}\left(\mathrm{P}_{\mathrm{T}}^{\tau}, \mathrm{P}_{\mathrm{S}}^{\tau}\right),\]</span> 其中， <span class="math inline">\(\mathcal{H}\)</span> 指交叉熵损失函数； <span class="math inline">\(\lambda\)</span> 是一个可调整参数，以平衡两个交叉熵；第一部分为 Student 的预测输出与 Ground-truth 的交叉熵损失；第二部分为在 <span class="math inline">\(\tau\)</span> 温度下 Student 与 Teacher 的 Softmax 输出的交叉熵损失。</p><h1 id="参考文献">参考文献</h1><p>[1] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. "Distilling the knowledge in a neural network." arixv preprint arixv:1503.02531 (2015).</p><p>[2] Adriana, Romero, Ballas Nicolas, K. Samira Ebrahimi, Chassang Antoine, Gatta Carlo, and Bengio Yoshua. "Fitnets: Hints for thin deep nets." <em>Proc. ICLR</em> 2, no. 3 (2015): 1.</p><h2 id="参考链接">参考链接</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/319880839">深度学习中的知识蒸馏技术 - 知乎 (zhihu.com)</a></li><li><a href="https://arxiv.org/pdf/1503.02531.pdf">1503.02531.pdf (arxiv.org)</a></li><li><a href="https://arxiv.org/pdf/1412.6550.pdf">1412.6550.pdf (arxiv.org)</a></li><li><a href="https://blog.csdn.net/weixin_43694096/article/details/127505946">一文搞懂【知识蒸馏】【Knowledge Distillation】算法原理_知识蒸馏算法-CSDN博客</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 蒸馏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/04/03/%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%953%20%E6%97%A0%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/"/>
      <url>/2024/04/03/%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%953%20%E6%97%A0%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.cnblogs.com/molinchn/p/13641437.html">【复习笔记】最优化方法 - 3. 无约束优化方法 - Molinchn - 博客园 (cnblogs.com)</a></p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DVGO</title>
      <link href="/2024/03/24/DVGO%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>/2024/03/24/DVGO%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文名称：<strong>Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction</strong> <strong>（DVGO： 超快速收敛的辐射场重建）</strong></p><p>代码地址： <a href="https://github.com/sunset1995/DirectVoxGO">GitHub - sunset1995/DirectVoxGO: Direct voxel grid optimization for fast radiance field reconstruction.</a></p><p>论文主页： <a href="https://sunset1995.github.io/dvgo/">DVGO (sunset1995.github.io)</a></p><p>参考网址：</p><ul><li>[<a href="https://zhuanlan.zhihu.com/p/584734270">NeRF-训练加速] DVGO - 知乎 (zhihu.com)</a></li><li><a href="https://blog.csdn.net/hy592070616/article/details/120623303">机器学习中的数学——激活函数（十）：Softplus函数-CSDN博客</a></li></ul></blockquote><h1 id="论文基本信息">论文基本信息</h1><p>Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction<sup>[1]</sup>是由来自清华大学的 Cheng Sun 等人发表在 <strong>2022 CVPR(Oral)</strong> 会议上的SCI论文。</p><p>DVGO 的主要目的是<u>加速 NeRF<sup>[2]</sup> 的训练</u>（从数个小时到十几分钟的逐个场景重建，且不需要预训练），采用的方式是将场景使用显式体素表达。DVGO 仍属于<strong>稠密重建</strong>。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240305143817326.png" alt="image-20240305143817326" /><figcaption aria-hidden="true">image-20240305143817326</figcaption></figure><h2 id="优点">优点</h2><ol type="1"><li><p><strong>在体素密度直接优化中，采用了两个先验算法来避免几何陷入局部最优解：</strong>直接优化密度体素网格会导致超快收敛，但容易出现次优解，所提出方法在自由空间分配"云"，并试图将光度损失与云拟合，而不是搜索具有更好多视图一致性的几何。对这个问题的解决方案简单而有效。<u>首先</u>，初始化密度体素网格，以产生非常接近于零的不透明度，以避免几何解决方案偏向于相机的附近平面。<u>其次</u>，给较少视图可见的体素一个较低的学习率，可以避免仅为解释少量视图的观察而分配的冗余体素。所提出的解决方案可以成功地避免次优几何，并在五个数据集上表现良好。</p></li><li><p><strong>提出了先插值后激活的体素网格插值，它可以在较低的网格分辨率下实现清晰的边界建模：</strong>之前的工作要么对激活的不透明度进行体素网格插值，要么使用最近邻插值，从而在每个网格单元中产生光滑的表面。从数学和经验上证明，所提出的后激活可以在单个网格单元内建模(超越)尖锐的线性表面。因此，可以使用更少的体素来实现更好的质量——具有160^3个密集体素的方法在大多数情况下已经优于NeRF。</p></li></ol><blockquote><p>[!IMPORTANT]</p><h3 id="features">Features</h3><ul><li><p><span style="background:#daf5e9;">Speedup NeRF by replacing the MLP with the voxel grid.</span></p></li><li><p><span style="background:#daf5e9;">Simple scene representation:</span></p></li><li><p><em>Volume densities</em>: dense voxel grid (3D).</p></li><li><p><em>View-dependent colors</em>: dense feature grid (4D) + shallow MLP.</p></li><li><p><span style="background:#daf5e9;">Pytorch cuda extention built just-in-time for another 2--3x speedup.</span></p></li><li><p><span style="background:#daf5e9;">O(N) realization for the distortion loss proposed by mip-nerf 360</span></p></li><li><p>The loss improves our training time and quality.</p></li><li><p>We have released a self-contained pytorch package: <a href="https://github.com/sunset1995/torch_efficient_distloss">torch_efficient_distloss</a>.</p></li><li><p>Consider a batch of 8192 rays X 256 points.</p><ul><li>GPU memory consumption: 6192MB =&gt; 96MB.</li><li>Run times for 100 iters: 20 sec =&gt; 0.2sec.</li></ul></li><li><p><span style="background:#daf5e9;"><strong>Supported datasets:</strong></span></p></li><li><p><strong><em>Bounded inward-facing</em>: <a href="https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1">NeRF</a>, <a href="https://dl.fbaipublicfiles.com/nsvf/dataset/Synthetic_NSVF.zip">NSVF</a>, <a href="https://dl.fbaipublicfiles.com/nsvf/dataset/BlendedMVS.zip">BlendedMVS</a>, <a href="https://dl.fbaipublicfiles.com/nsvf/dataset/TanksAndTemple.zip">T&amp;T (masked)</a>, <a href="https://drive.google.com/open?id=1ScsRlnzy9Bd_n-xw83SP-0t548v63mPH">DeepVoxels</a>.</strong></p></li><li><p><strong><em>Unbounded inward-facing</em>: <a href="https://drive.google.com/file/d/11KRfN91W1AxAW6lOFs4EeYDbeoQZCi87/view?usp=sharing">T&amp;T</a>, <a href="https://drive.google.com/file/d/1gsjDjkbTh4GAR9fFqlIDZ__qR9NYTURQ/view?usp=sharing">LF</a>, <a href="https://jonbarron.info/mipnerf360/">mip-NeRF360</a>.</strong></p></li><li><p><strong><em>Foward-facing</em>: <a href="https://drive.google.com/drive/folders/14boI-o5hGO9srnWaaogTU5_ji7wkX2S7">LLFF</a>.</strong></p></li></ul><details><summary>Directory structure for the datasets</summary><p><span style="font-size: 18pt;"><strong>data</strong></span><br /><span style="background-color: #bfedd2;">├── nerf_synthetic     # Link: https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1</span><br /><span style="background-color: #bfedd2;">│   └── [chair|drums|ficus|hotdog|lego|materials|mic|ship]</span><br /><span style="background-color: #bfedd2;">│       ├── [train|val|test]</span><br /><span style="background-color: #bfedd2;">│       │   └── r_*.png</span><br /><span style="background-color: #bfedd2;">│       └── transforms_[train|val|test].json</span><br />│<br /><span style="background-color: #fbeeb8;">├── Synthetic_NSVF     # Link: https://dl.fbaipublicfiles.com/nsvf/dataset/Synthetic_NSVF.zip</span><br /><span style="background-color: #fbeeb8;">│   └── [Bike|Lifestyle|Palace|Robot|Spaceship|Steamtrain|Toad|Wineholder]</span><br /><span style="background-color: #fbeeb8;">│       ├── intrinsics.txt</span><br /><span style="background-color: #fbeeb8;">│       ├── rgb</span><br /><span style="background-color: #fbeeb8;">│       │   └── [0_train|1_val|2_test]_*.png</span><br /><span style="background-color: #fbeeb8;">│       └── pose</span><br /><span style="background-color: #fbeeb8;">│           └── [0_train|1_val|2_test]_*.txt</span><br />│<br /><span style="background-color: #f8cac6;">├── BlendedMVS         # Link: https://dl.fbaipublicfiles.com/nsvf/dataset/BlendedMVS.zip</span><br /><span style="background-color: #f8cac6;">│   └── [Character|Fountain|Jade|Statues]</span><br /><span style="background-color: #f8cac6;">│       ├── intrinsics.txt</span><br /><span style="background-color: #f8cac6;">│       ├── rgb</span><br /><span style="background-color: #f8cac6;">│       │   └── [0|1|2]_*.png</span><br /><span style="background-color: #f8cac6;">│       └── pose</span><br /><span style="background-color: #f8cac6;">│           └── [0|1|2]_*.txt</span><br />│<br /><span style="background-color: #eccafa;">├── TanksAndTemple     # Link: https://dl.fbaipublicfiles.com/nsvf/dataset/TanksAndTemple.zip</span><br /><span style="background-color: #eccafa;">│   └── [Barn|Caterpillar|Family|Ignatius|Truck]</span><br /><span style="background-color: #eccafa;">│       ├── intrinsics.txt</span><br /><span style="background-color: #eccafa;">│       ├── rgb</span><br /><span style="background-color: #eccafa;">│       │   └── [0|1|2]_*.png</span><br /><span style="background-color: #eccafa;">│       └── pose</span><br /><span style="background-color: #eccafa;">│           └── [0|1|2]_<em>.txt</span><br />│<br /><span style="background-color: #c2e0f4;">├── deepvoxels         # Link: https://drive.google.com/drive/folders/1ScsRlnzy9Bd_n-xw83SP-0t548v63mPH</span><br /><span style="background-color: #c2e0f4;">│   └── [train|validation|test]</span><br /><span style="background-color: #c2e0f4;">│       └── [armchair|cube|greek|vase]</span><br /><span style="background-color: #c2e0f4;">│           ├── intrinsics.txt</span><br /><span style="background-color: #c2e0f4;">│           ├── rgb/</em>.png</span><br /><span style="background-color: #c2e0f4;">│           └── pose/<em>.txt</span><br />│<br /><span style="background-color: #bfedd2;">├── nerf_llff_data     # Link: https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1</span><br /><span style="background-color: #bfedd2;">│   └── [fern|flower|fortress|horns|leaves|orchids|room|trex]</span><br />│<br /><span style="background-color: #fbeeb8;">├── tanks_and_temples  # Link: https://drive.google.com/file/d/11KRfN91W1AxAW6lOFs4EeYDbeoQZCi87/view?usp=sharing</span><br /><span style="background-color: #fbeeb8;">│   └── [tat_intermediate_M60|tat_intermediate_Playground|tat_intermediate_Train|tat_training_Truck]</span><br /><span style="background-color: #fbeeb8;">│       └── [train|test]</span><br /><span style="background-color: #fbeeb8;">│           ├── intrinsics/</em>txt</span><br /><span style="background-color: #fbeeb8;">│           ├── pose/<em>txt</span><br /><span style="background-color: #fbeeb8;">│           └── rgb/</em>jpg</span><br />│<br /><span style="background-color: #f8cac6;">├── lf_data            # Link: https://drive.google.com/file/d/1gsjDjkbTh4GAR9fFqlIDZ__qR9NYTURQ/view?usp=sharing</span><br /><span style="background-color: #f8cac6;">│   └── [africa|basket|ship|statue|torch]</span><br /><span style="background-color: #f8cac6;">│       └── [train|test]</span><br /><span style="background-color: #f8cac6;">│           ├── intrinsics/*txt</span><br /><span style="background-color: #f8cac6;">│           ├── pose/<em>txt</span><br /><span style="background-color: #f8cac6;">│           └── rgb/</em>jpg</span><br />│<br /><span style="background-color: #eccafa;">├── 360_v2             # Link: https://jonbarron.info/mipnerf360/</span><br /><span style="background-color: #eccafa;">│   └── [bicycle|bonsai|counter|garden|kitchen|room|stump]</span><br /><span style="background-color: #eccafa;">│       ├── poses_bounds.npy</span><br /><span style="background-color: #eccafa;">│       └── [images_2|images_4]</span><br />│<br /><span style="background-color: #c2e0f4;">├── nerf_llff_data     # Link: https://drive.google.com/drive/folders/14boI-o5hGO9srnWaaogTU5_ji7wkX2S7</span><br /><span style="background-color: #c2e0f4;">│   └── [fern|flower|fortress|horns|leaves|orchids|room|trex]</span><br /><span style="background-color: #c2e0f4;">│       ├── poses_bounds.npy</span><br /><span style="background-color: #c2e0f4;">│       └── [images_2|images_4]</span><br />│<br /><span style="background-color: #bfedd2;">└── co3d               # Link: https://github.com/facebookresearch/co3d</span><br /><span style="background-color: #bfedd2;">    └── [donut|teddybear|umbrella|...]</span><br /><span style="background-color: #bfedd2;">        ├── frame_annotations.jgz</span><br /><span style="background-color: #bfedd2;">        ├── set_lists.json</span><br /><span style="background-color: #bfedd2;">        └── [129_14950_29917|189_20376_35616|...]</span><br /><span style="background-color: #bfedd2;">            ├── images</span><br /><span style="background-color: #bfedd2;">            │   └── frame<em>.jpg</span><br /><span style="background-color: #bfedd2;">            └── masks</span><br /><span style="background-color: #bfedd2;">                └── frame</em>.png</span></p><p> </p></blockquote><blockquote><p><strong>DVGO v2</strong></p><p>论文地址：<a href="https://arxiv.org/pdf/2206.05085.pdf">2206.05085.pdf (arxiv.org)</a></p><p>论文代码：同DVGO</p><p>摘要：In this technical report, we improve the DVGO framework (called DVGOv2), which is based on Pytorch and uses the simplest dense grid representation. First, we reimplement part of the Pytorch operations with cuda, achieving 2–3× speedup. The cuda extension is automatically compiled just in time. <span style="background:#FFCC99;">Second, we extend DVGO to support Forward-facing and Unbounded Inward-facing capturing.(该改进之处正是DVGO 1.0 版本在论文末尾提出的不足之处)</span> Third, we improve the space time complexity of the distortion loss proposed by mip-NeRF 360 from O(<span class="math inline">\(N^2\)</span>) to O(<span class="math inline">\(N\)</span>). The distortion loss improves our quality and training speed. Our efficient implementation could allow more future works to benefit from the loss.</p></blockquote><h2 id="缺陷">缺陷</h2><ul><li>稠密重建：需要输入百张图片，需要的样本量大</li><li>几何不太好（见 voxurf 的对比）</li></ul><h1 id="算法框架">算法框架</h1><h2 id="dvgo-全貌">DVGO 全貌</h2><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240227190531174.png" alt="image-20240227190531174" /><figcaption aria-hidden="true">image-20240227190531174</figcaption></figure><p>从上左图可以看到，DVGO 将整个场景表示为两个 voxel grid——密度体素网格和特征体素网格，并且渲染图像使用的依然是 NeRF 中的体渲染，对于从像素上发射的射线上的每一个点，首先在这两个 voxel grid 上 进行三线性插值，分别得到这个点的密度特征和颜色特征，再通过各自的解码过程最终得到该点的颜色和密度。<u>这样就把整个场景表示在 voxel grid 和对应的解码器里。</u></p><p>在训练的时候，DVGO 采用了 coarse to fine 的训练方式。在 coarse 阶段，使用先验信息和多视角图像训练两个粗粒度的voxel，然后使用其中的密度场确定场景中的空白区域（free space）。在 fine 阶段，利用 coarse 阶段确定的密度场可以得到更紧密的 bbox，可以将 grid 定义在这个 bbox 内，减少无关变量的训练。并且在体渲染的时候还可以通过粗的密度场提前得知射线上哪些点应当被跳过（空白点和被遮住的那些点）。</p><h2 id="类似-nerf-的体渲染">类似 NeRF 的体渲染</h2><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf9.png" /></p><p>NeRF 将场景用多层感知机 MLP 表示，输入相机位置坐标 <span class="math inline">\(\boldsymbol{x}\)</span> 并输出密度 <span class="math inline">\(\sigma\)</span> 和中间特征 <span class="math inline">\(e\)</span> 。 然后再次用这个中间特征 <span class="math inline">\(e\)</span> 和视角 <span class="math inline">\(\boldsymbol{d}\)</span> 作为输入，推测出这个点的颜色 <span class="math inline">\(\boldsymbol{c}\)</span>​ ，下面将这两个过程分开写，其实就是 NeRF 中的网络： <span class="math display">\[\begin{align}\left( \sigma ,e \right) &amp;=MLP^{\left( pos \right)}\left( x \right) ,\tag{1.a}\\c&amp;=MLP^{\left( rgb \right)}\left( e,d \right) , \tag{1.b}\end{align}\]</span> 然后通过体渲染得到像素点的值： <span class="math display">\[\begin{align}\hat{C}(\boldsymbol{r}) &amp; =\left(\sum_{i=1}^{K} T_{i} \alpha_{i} \boldsymbol{c}_{i}\right)+T_{K+1} \boldsymbol{c}_{\mathrm{bg}}, \tag{2.a}\\\alpha_{i} &amp; =\operatorname{alpha}\left(\sigma_{i}, \delta_{i}\right)=1-\exp \left(-\sigma_{i} \delta_{i}\right),  \tag{2.b}\\T_{i} &amp; =\prod_{j=1}^{i-1}\left(1-\alpha_{j}\right),\tag{2.c}\end{align}\]</span> 其中的符号和 NeRF 中的一致，就不详细介绍。不同的是这里加上了背景的考虑（加上背景的原因可参考<a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODA3MDkyMA==&amp;mid=2247485148&amp;idx=1&amp;sn=4e6acc1b358e624ac2301729d35cc89d&amp;chksm=cf81f0bbf8f679ad3f990f91d5c84fcc67cc6948c3cbc0c34c59874f7795e9263f2382979414&amp;token=400584181&amp;lang=zh_CN#rd">NeRF入门之体渲染 (Volume Rendering) (qq.com)</a>）：这里 <span class="math inline">\(\boldsymbol{c}_{bg}\)</span> 是一个实现定义好的背景像素值，透射比 <span class="math inline">\(T_{K+1}\)</span> 表示的是背景点被击中的概率。</p><blockquote><p>[!NOTE]</p><p>其实一条射线上只会采样 K 个点，这里假设最后还有一个虚拟的背景点，挂在每条射线的最后。当这条射线上所有采样点被击中的概率都很低时，就会呈现出背景的颜色。</p></blockquote><h2 id="求-alpha-后激活密度体素网格">求 <span class="math inline">\(\alpha\)</span> : 后激活密度体素网格</h2><blockquote><p>[!TIP]</p><p><strong>体素网格如何表示颜色/密度/特征？</strong></p><p>文中用以下式子概括： <span class="math display">\[interp\left( x,\boldsymbol{V} \right) :\left( \mathbb{R}^3,\mathbb{R}^{C\times N_x\times N_y\times N_z} \right) \rightarrow \mathbb{R}^C,\]</span> 其中 <span class="math inline">\(x\)</span> 为查询的三维点， <span class="math inline">\(\boldsymbol{V}\)</span> 表示 <span class="math inline">\(x, y, z\)</span> 方向上体素数量分别为 <span class="math inline">\(N_x, N_y, N_z\)</span> 的体素网格， <span class="math inline">\(C\)</span> 表示颜色/密度/特征的维度数。在本文中，运用的插值方法是三线性插值（trilinear interpolation）。</p></blockquote><p>前面介绍 DVGO 将场景表示为两种 voxel grid 和解码器的组合，这里想要探讨的问题是<u>如何从 density voxel grid 中得到一个点的密度信息</u>。</p><p>首先，density 体素网格上面定义了场景的密度信息，但不需要直接是真实的密度值。具体来说每个 voxel 上的值是一个待训练的参数实数（该实数的正负性不能保证），可理解为密度特征。<span style="background:#daf5e9;">由于密度是非负的，我们需要先通过激活函数 relu 或 softplus 将这个实数映射为一个大于零的数才能将输出与真实密度值建立关系。然后用每个点的密度值计算每个点的 <span class="math inline">\(\alpha\)</span> （见公式2.b）</span>。</p><p>因此我们想要进行后续体渲染的话，需要对一个点做下面三件事：<strong>插值、激活和求 <span class="math inline">\(\alpha\)</span>​</strong> 。</p><p>关于<strong>插值</strong>比较好理解，因为网格是稀疏的，而我们做体渲染的时候需要的是空间中任意点的信息，所以最简单直接的方式就是用空间点附近的 voxel 进行插值。</p><p>关于<strong>激活</strong>，这里采用的是 Mip-NeRF<sup>[3]</sup> 中的 shifted softplus，其中 <span class="math inline">\(\ddot{\sigma}\)</span> 就是 density grid 上的原始值，取值范围是 R，经过激活后得到非负的密度值 <span class="math inline">\(\sigma\)</span>。 <span class="math inline">\(b\)</span> 是一个超参数（hyperparameter） : <span class="math display">\[\sigma=\operatorname{softplus}(\ddot{\sigma})=\log (1+\exp (\ddot{\sigma}+b)),\]</span></p><blockquote><p>这里不用 relu 激活函数的原因是：Using softplus instead of ReLU is crucial to optimize voxel density directly, as it is irreparable when a voxel is falsely set to a negative value with ReLU as the density activation. Conversely, softplus allows us to explore density very close to 0.</p><p>意思就是，如果一个密度为正的地方的 <span class="math inline">\(\ddot{ \sigma}\)</span>​ 如果是负值会导致其经过 relu 之后是0，没有梯度能更新这个值让它能移动到正的部分。</p><p><strong>softplus 函数可以视为 relu 函数的平滑，随着 <span class="math inline">\(\ddot{ \sigma}\)</span> 的变小，函数值会无限接近于0但不会真正变为0</strong>。其函数表达式与图像如下： <span class="math display">\[softplus\left( x \right) =\log \left( 1+e^x \right)\]</span> <img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240101125119035.png" alt="image-20240101125119035" /></p></blockquote><p>关于<strong>求 <span class="math inline">\(\alpha\)</span></strong> ，则<span style="background:#f9eda6;">是 NeRF 中计算一个点的瞬时击中率的操作，即式（2.b）</span>。</p><p>除了激活 softplus 操作必须在求 <span class="math inline">\(\alpha\)</span> 前外，显然这三件事是可以交换的。在给定查询三维点 <span class="math inline">\(x\)</span> 和密度体素网格中的密度信息 <span class="math inline">\(\boldsymbol{V}^{(density)}\)</span> 后，我们考虑了三种不同的顺序来得到 <span class="math inline">\(\alpha \in [0,1]\)</span> 值：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301192035676.png" alt="image-20240301192035676" /><figcaption aria-hidden="true">image-20240301192035676</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301201349921.png" alt="image-20240301193624141" /><figcaption aria-hidden="true">image-20240301193624141</figcaption></figure><p>一般情况下，我们想到的是第三种情况：先插值、再激活，最后求 <span class="math inline">\(\alpha\)</span> 。 论文中根据插入三线性插值 interp 的顺序将其扩展为三种情况并展开对比发现，不同的顺序得到的结果还有所差异。<span style="background:#daf5e9;"><strong>所谓的“后激活”（Post-activated）指的是激活为 <span class="math inline">\(\alpha\)</span> 在插值之后(i.e. to activate density into alpha after interpolation)</strong></span>。作者证明，后激活能够用更少的网格单元生成尖锐的表面(决策边界)。作者在二维图像上证明了后激活的优越性：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301193624141.png" alt="image-20240301201349921" /><figcaption aria-hidden="true">image-20240301201349921</figcaption></figure><p>可以看到 ，后激活能够表示出更 sharp 的边缘，对应在 NeRF 中就是更 sharp 的几何。在二值图像上的实验也证明了同样的观点：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301211423996.png" alt="image-20240301193832857" /><figcaption aria-hidden="true">image-20240301193832857</figcaption></figure><blockquote><p>论文的补充材料中给出了后激活策略可以在单个网格单元中产生尖锐的线性表面（决策边界）的证明。论文先证明了一维网格和二维网格中的情况，然后利用这两种情况推导出三维网格中的情况。</p></blockquote><h2 id="由粗到细的训练过程">由粗到细的训练过程</h2><p>和 NeRF 的重要性采样类似，DVGO 也采用了两阶段的采样过程。</p><h3 id="coarse-to-fine-的好处">coarse to fine 的好处</h3><p>作者之所以选择 coarse to fine 的训练方式是因为：通常情况下，<u>场景由空闲空间(即未占用空间)主导</u>，即场景中的空白区占大部分，有物体的区域就一小部分。基于此，我们的目标是高效地找到感兴趣的粗三维区域，然后重构需要更多计算资源的精细细节和视图依赖效果。这样可以大大减少后期精细阶段每条射线上查询的点的数量。</p><p>在 NeRF 中也有类似的操作，主要目的还是增强采样点的针对性，尽量将采样点放在更有用的位置，不在无用位置进行采样。本文进行 coarse to fine 后，就可以快速有效地筛选出有物体存在的网格，进而在 fine 粒度的训练中跳过大量无关“空白点”，只关注有物体占据的网格，进一步加快了训练速度。</p><h3 id="体素分配方法">体素分配方法</h3><p>不论是 coarse 还是 fine 阶段，我们都需要知道应该在场景的什么位置创建 grid，才能包含所有要采样的点。作者采用的方式是找一个 BBox (bounding box) 将所有的摄像机截锥紧紧地包起来，像下面这样：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301201828110.png" alt="image-20240301201828110" /><figcaption aria-hidden="true">image-20240301201828110</figcaption></figure><p>寻找 BBox 的代码如下(.ipynb_checkpoints/run-checkpoint.py)：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_compute_bbox_by_cam_frustrm_bounded</span>(<span class="params">cfg, HW, Ks, poses, i_train, near, far</span>):</span><br><span class="line">    xyz_min = torch.Tensor([np.inf, np.inf, np.inf])</span><br><span class="line">    xyz_max = -xyz_min</span><br><span class="line">    <span class="keyword">for</span> (H, W), K, c2w <span class="keyword">in</span> <span class="built_in">zip</span>(HW[i_train], Ks[i_train], poses[i_train]):</span><br><span class="line">        rays_o, rays_d, viewdirs = dvgo.get_rays_of_a_view(</span><br><span class="line">                H=H, W=W, K=K, c2w=c2w,</span><br><span class="line">                ndc=cfg.data.ndc, inverse_y=cfg.data.inverse_y,</span><br><span class="line">                flip_x=cfg.data.flip_x, flip_y=cfg.data.flip_y)</span><br><span class="line">        <span class="keyword">if</span> cfg.data.ndc:</span><br><span class="line">            pts_nf = torch.stack([rays_o+rays_d*near, rays_o+rays_d*far])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pts_nf = torch.stack([rays_o+viewdirs*near, rays_o+viewdirs*far])</span><br><span class="line">        xyz_min = torch.minimum(xyz_min, pts_nf.amin((<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>)))</span><br><span class="line">        xyz_max = torch.maximum(xyz_max, pts_nf.amax((<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>)))</span><br><span class="line">    <span class="keyword">return</span> xyz_min, xyz_max</span><br></pre></td></tr></table></figure><blockquote><p>[!TIP]</p><p>这里的代码逻辑是，首先拿到训练集中所有的相机位置和视角，然后在生成射线，根据 near 和 far 确定射线上的采样点，把所有这些采样点放一块，求所有这些点的 x,y,z 的最值。通过这样可以保证 grid 能够包含所有要采样的点，并且不会包含过多的无关位置。</p></blockquote><p>令BBox的长宽高分别为 <span class="math inline">\(L_{x}^{\left( c \right)},L_{y}^{\left( c \right)},L_{z}^{\left( c \right)}\)</span> 。设粗粒度训练阶段的期望体素总数为超参数 <span class="math inline">\(M^{(c)}\)</span> ，则可计算每个体素的大小 size 为 <span class="math inline">\(s^{\left( c \right)}=\sqrt[3]{L_{x}^{\left( c \right)}\cdot L_{y}^{\left( c \right)}\cdot L_{z}^{\left( c \right)}/M^{\left( c \right)}}\)</span> ，进而可算得 BBox 长宽高方向上分别有 $N_{x}^{( c )},N_{y}^{( c )},N_{z}^{( c )}=L_{x}^{( c )}/s^{( c )} ,L_{y}^{( c )}/s^{( c )} ,L_{z}^{( c )}/s^{( c )} $ 个体素。</p><h3 id="粗粒度训练">粗粒度训练</h3><h4 id="几何与颜色表示">几何与颜色表示</h4><ul><li><strong>场景几何表示</strong>：通过粗粒度的密度体素网格 <span class="math inline">\(\boldsymbol{ V}^{(density)(c)}\)</span> 和后激活策略建模。这里 <span class="math inline">\(\boldsymbol{V}^{\left( density \right) \left( c \right)}\in \mathbb{R}^{1\times N_{x}^{\left( c \right)}\times N_{y}^{\left( c \right)}\times N_{z}^{\left( c \right)}}\)</span> 。</li><li><strong>颜色表示</strong>：通过在粗粒度阶段的颜色体素网格 <span class="math inline">\(\boldsymbol{ V}^{(rgb)(c)}\)</span> ，来建模视觉无关(view-invariant)的颜色 emissions 。这里 <span class="math inline">\(\boldsymbol{V}^{\left( rgb\right) \left( c \right)}\in \mathbb{R}^{3\times N_{x}^{\left( c \right)}\times N_{y}^{\left( c \right)}\times N_{z}^{\left( c \right)}}\)</span> 。</li></ul><p>任意三维点的密度和颜色信息可通过插值公式查询： <span class="math display">\[\begin{array}{l}\ddot{\sigma}^{\left( c \right)}=interp\left( x,\boldsymbol{V}^{\left( density \right) \left( c \right)} \right) ,\\c^{\left( c \right)}=interp\left( x,\boldsymbol{V}^{\left( rgb \right) \left( c \right)} \right) ,\end{array}\]</span> 这里 <span class="math inline">\(\ddot{\sigma}^{\left( c \right)}\)</span> 表示原始的可正可负的体密度信息。由于颜色和视角无关，所以不需要输入视角，颜色和密度一样直接插值就能得到。而且需要注意的是：<span style="background:#fbd4d0;"><strong>在 coarse 阶段没有使用任何 MLP</strong></span>。</p><p>粗粒度训练阶段的<strong>点采样公式</strong>如下： <span class="math display">\[\begin{array}{l}\boldsymbol{x}_{0}=\boldsymbol{o}+t^{(\text {near) }} \boldsymbol{d} ,\\\boldsymbol{x}_{i}=\boldsymbol{x}_{0}+i \cdot \delta^{(\mathrm{c})} \cdot \frac{\boldsymbol{d}}{\|\boldsymbol{d}\|^{2}},\end{array}\]</span> 显然， <span class="math inline">\(\boldsymbol{o}\)</span> 是相机中心， <span class="math inline">\(\boldsymbol{d}\)</span> 是射线投射方向。 $ t ^{ (near) } $ 是相机近平面 camera near bound ，<span class="math inline">\(\delta^{(\mathrm{c})}\)</span> 则是可根据体素大小 <span class="math inline">\(s^{(c)}\)</span> 自适应选择的超参数步长。查询索引 <span class="math inline">\(i\)</span> 取值从 1 到 $t^{( far )} <sup>2/</sup>{( c )} $ 。</p><h4 id="低密度初始化先验">低密度初始化先验</h4><blockquote><p>[!IMPORTANT]</p><p>作者发现，在训练开始时，远离相机的点的重要性由于累积的透过率项而降低。因此，粗密度体素网格可能会意外地被困在一个次优的“cloudy”几何，它在相机附近的平面密度更高。</p><p>导致这个问题的原因如果初始化做的不好 ，那么后面的采样点都被前面的遮住了，后面的采样点的被击中的概率 <span class="math inline">\(T_i\)</span> 就很低，那么这些位置就不会被训练到，网络就一直在纠结是不是前面那些点的颜色或者密度调的不够好，殊不知是重心在射线的后面。</p><p>作者解决这个问题的方法简单而有效，初始化密度网格的时候小心点，尽量让一开始的时候，射线上每个点都是可见的，也就是他们的透射比 <span class="math inline">\(T_i\)</span> 都比较高。</p></blockquote><p>由式(2.c)，</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301214544618.png" alt="image-20240301211423996" /><figcaption aria-hidden="true">image-20240301211423996</figcaption></figure><blockquote><p>中间的推导详见论文的补充材料。</p></blockquote><p>所以我们只需要让 <span class="math inline">\(b=log((1-\alpha^{(init)})^{-1/s}-1)\)</span> 即可。我们可以在代码中给超参数 <span class="math inline">\(\alpha^{(init)}\)</span> 一个很小的初值就可以了，代码中给的值一般是 <span class="math inline">\(10^{-6}\)</span> （coarse）和 <span class="math inline">\(10^{-6}\)</span> （fine）。这一段对应的代码块为：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.alpha_init = alpha_init</span><br><span class="line">self.register_buffer(<span class="string">&#x27;act_shift&#x27;</span>, torch.FloatTensor([np.log(<span class="number">1</span>/(<span class="number">1</span>-alpha_init) - <span class="number">1</span>)]))</span><br></pre></td></tr></table></figure><h4 id="基于视角的学习率先验">基于视角的学习率先验</h4><blockquote><p>[!IMPORTANT]</p><p>意思是 grid 中的不同 voxel 在训练集中的可见性是不同的，有一些 voxel 在大部分训练集视角下都可见，而有一些只在少部分视角下可见。比较有代表性的就是包含表面的那些 voxel 和表面内部的那些 voxel。</p><p>作者认为，可见度不同的 voxel 上的特征应该具有不同的学习率，可见性好的学习率更大，否则设置更小的学习率。</p><p>具体来说，用 <span class="math inline">\(n_j\)</span>表示第 <span class="math inline">\(j\)</span> 个 voxel 可以被看见的视角个数，用 <span class="math inline">\(n_{max}\)</span> 表示所有 <span class="math inline">\(n_j\)</span> 的最大值， <span class="math inline">\(n_j/n_{max}\)</span> 为每个 voxel 的学习率的缩放因子。</p></blockquote><p>对应的代码为：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">voxel_count_views</span>(<span class="params">self, rays_o_tr, rays_d_tr, imsz, near, far, stepsize, downrate=<span class="number">1</span>, irregular_shape=<span class="literal">False</span></span>):</span><br><span class="line">       <span class="built_in">print</span>(<span class="string">&#x27;dvgo: voxel_count_views start&#x27;</span>)</span><br><span class="line">       far = <span class="number">1e9</span>  <span class="comment"># the given far can be too small while rays stop when hitting scene bbox</span></span><br><span class="line">       eps_time = time.time()</span><br><span class="line">       N_samples = <span class="built_in">int</span>(np.linalg.norm(np.array(self.world_size.cpu())+<span class="number">1</span>) / stepsize) + <span class="number">1</span></span><br><span class="line">       rng = torch.arange(N_samples)[<span class="literal">None</span>].<span class="built_in">float</span>()</span><br><span class="line">       count = torch.zeros_like(self.density.get_dense_grid())</span><br><span class="line">       device = rng.device</span><br><span class="line">       <span class="keyword">for</span> rays_o_, rays_d_ <span class="keyword">in</span> <span class="built_in">zip</span>(rays_o_tr.split(imsz), rays_d_tr.split(imsz)):</span><br><span class="line">           ones = grid.DenseGrid(<span class="number">1</span>, self.world_size, self.xyz_min, self.xyz_max)</span><br><span class="line">           <span class="keyword">if</span> irregular_shape:</span><br><span class="line">               rays_o_ = rays_o_.split(<span class="number">10000</span>)</span><br><span class="line">               rays_d_ = rays_d_.split(<span class="number">10000</span>)</span><br><span class="line">           <span class="keyword">else</span>:</span><br><span class="line">               rays_o_ = rays_o_[::downrate, ::downrate].to(device).flatten(<span class="number">0</span>,-<span class="number">2</span>).split(<span class="number">10000</span>)</span><br><span class="line">               rays_d_ = rays_d_[::downrate, ::downrate].to(device).flatten(<span class="number">0</span>,-<span class="number">2</span>).split(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">           <span class="keyword">for</span> rays_o, rays_d <span class="keyword">in</span> <span class="built_in">zip</span>(rays_o_, rays_d_):</span><br><span class="line">               vec = torch.where(rays_d==<span class="number">0</span>, torch.full_like(rays_d, <span class="number">1e-6</span>), rays_d)</span><br><span class="line">               rate_a = (self.xyz_max - rays_o) / vec</span><br><span class="line">               rate_b = (self.xyz_min - rays_o) / vec</span><br><span class="line">               t_min = torch.minimum(rate_a, rate_b).amax(-<span class="number">1</span>).clamp(<span class="built_in">min</span>=near, <span class="built_in">max</span>=far)</span><br><span class="line">               t_max = torch.maximum(rate_a, rate_b).amin(-<span class="number">1</span>).clamp(<span class="built_in">min</span>=near, <span class="built_in">max</span>=far)</span><br><span class="line">               step = stepsize * self.voxel_size * rng</span><br><span class="line">               interpx = (t_min[...,<span class="literal">None</span>] + step/rays_d.norm(dim=-<span class="number">1</span>,keepdim=<span class="literal">True</span>))</span><br><span class="line">               rays_pts = rays_o[...,<span class="literal">None</span>,:] + rays_d[...,<span class="literal">None</span>,:] * interpx[...,<span class="literal">None</span>]</span><br><span class="line">               ones(rays_pts).<span class="built_in">sum</span>().backward()</span><br><span class="line">           <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">               count += (ones.grid.grad &gt; <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>这段代码也十分巧妙。作者首先构造了一个 <em>grid</em> 对象 <em>ones</em>，它是作者创建的一个类，这个类的作用就是构造可以训练的 voxel grid，并且可以完成插值等操作。然后在训练集视角上获取射线并在射线上采样点 <em>rays_pts</em>。对 <em>rays_pts</em> 中的每个点，找到它在 <em>ones</em> 中的插值结果，对应的语句是 <em>ones(rays_pts)。</em>然后将这些点的插值结果加起来：<em>ones(rays_pts).sum()。</em>可想而知，到目前为止，只有 <em>rays_pts</em> 中的那些点所在的 voxel 参与了加法，这个时候我们对加法结果进行反向传播：<em>ones(rays_pts).sum().backward()</em>，参与插值的那些 voxel 的梯度会是正的，<strong>我们找到这些梯度为正的 voxel：<em>ones.grid.grad &gt; 1</em>，就可以确定哪些 voxel 是可见的</strong>。count 是和 grid 同纬度的 tensor，<em>ones.grid.grad &gt; 1</em> 得到的是一个 bool tensor，其中为 True 的那些就是可见 voxel，两个加起来就能实现统计的目的。</p><h3 id="细粒度训练">细粒度训练</h3><h4 id="几何与颜色表示-1">几何与颜色表示</h4><p>与 coarse 阶段不同的是，fine 阶段采用了更密集的网格，并且使用显隐混合的表达方式（explicit-implicit hybrid representation），其实就是用一个 MLP 当解码器接在颜色网格后面，增强网络对颜色的表达能力。</p><p>fine 阶段查询一个空间点的密度和颜色的方式为： <span class="math display">\[\begin{array}{l}    \ddot{\sigma}^{\left( f \right)}=interp\left( x,\boldsymbol{V}^{\left( density \right) \left( f \right)} \right) ,\\    c^{\left( f \right)}=MLP_{\varTheta}^{\left( rgb \right)}\left( interp\left( x,\boldsymbol{V}^{\left( feat \right) \left( f \right)} \right) ,\boldsymbol{x,d} \right) ,\\\end{array}\]</span> 这对应于框架图的左半部分。</p><p>在 fine 阶段，作者采用了一系列的加速手段：</p><ol type="1"><li><strong>Known free space and unknown space</strong> 这里说明了怎么定义 free space 和 unknown space，方式就是手动设置一个阈值，之下的就是 free space。</li><li><strong>Fine voxels allocation</strong> coarse 阶段确定 grid 范围的方式是找到一个包含所有采样点的 BBox。但是有些地方是空的，所以我们可以更具 coarse 阶段得到的 density grid 确定 fine 阶段 grid 所在的区域，将有限的 voxel 尽可能地安排在有密度的地方。</li><li><strong>Progressive scaling</strong> 受到 NSVF 的启发，作者也是逐渐减小 voxel 的大小。随着训练不断的倍增 grid 的密度，直到最后达到指定数量的 voxel 停止倍增。</li><li><strong>Fine-stage points sampling</strong> 在 NeRF 中，一个场景的 near-bound 和 far-bound 是定好的一个大致的范围，但是由于我们这里明确知道场景就在空间的一个 grid 内，所以更好的方式是将 near-bound 和 far-bound 设置为每条射线进入 grid 和离开 grid 的点。（在代码中没找到对应部分 ）</li><li><strong>Free space skipping</strong> 意思是通过 coarse 阶段已经知道场景中哪些位置是空的了，那就不需要在这些位置采样点浪费时间了。</li><li><strong>Training objective for fine representation</strong> 使用与粗阶段相同的训练损失，但使用更小的权值作为正则化损失，因为作者发现，根据经验，它会带来略好的质量。</li></ol><h4 id="损失函数">损失函数</h4><p>Loss Function包含三个部分：来自NeRF的 MSE loss 以及两个从 DVGO 中借鉴的 loss。 <span class="math display">\[\begin{align}    \mathcal{L}_{photo}&amp;=\frac{1}{|\mathcal{R}|}\sum_{r\in \mathcal{R}}{|}\hat{C}\left( \boldsymbol{r} \right) -C\left( \boldsymbol{r} \right) |_{2}^{2}\\    \mathcal{L}_{all\_pts}&amp;=\sum_{i=1}^N{T}_i\left( 1-\exp \left( -\sigma _i\delta _i \right) \right) |\mathbf{c}_i-C\left( \mathbf{r} \right) |_{2}^{2}\mid\\    \mathcal{L}_{bg}&amp;=-T_{N+1}\log \left( T_{N+1} \right) -\left( 1-T_{N+1} \right) \log \left( 1-T_{N+1} \right)\\\\    L&amp;=L_{photo}+\alpha L_{all\_pts}+\beta L_{bg}\\\end{align}\]</span> MSE loss 的作用是使得建模出来的辐射场在训练集视角上渲染要和原图尽可能一致。权重为 1。</p><p><span class="math inline">\(L_{all\_pts}\)</span> 的意思是从一个像素中发射的射线上的所有像素点都应该和该像素尽可能接近，这个约束能够帮助网络一开始不要陷入 local minima，使得优化过程更加稳定。但是这个约束并不是符合实际的，因为一条射线上的点的颜色常常是不同的，所以作者给了一个很小的系数 <span class="math inline">\(\alpha=0.1\)</span>（coarse）和 <span class="math inline">\(\alpha=0.01\)</span>（coarse）。</p><p><span class="math inline">\(L_{bg}\)</span> 中 <span class="math inline">\(T_{N+1}\)</span> 表示射线击中背景的概率，也可以理解为射线一路上什么都没有击中，一路通到背景上。 <span class="math inline">\(L_{bg}\)</span> 的函数图像如下，它的作用是让一个像素点要么为背景颜色，要么没有背景颜色，尽量避免是二者的混合。作者设置其系数为 $  <span class="math inline">\(（coarse）和\)</span>$（fine）。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301193832857.png" alt="image-20240301214544618" /><figcaption aria-hidden="true">image-20240301214544618</figcaption></figure><h1 id="实验">实验</h1><h2 id="实现细节">实现细节</h2><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">coarse</th><th style="text-align: center;">fine</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">voxel 数量</td><td style="text-align: center;"><span class="math inline">\(100^3\)</span></td><td style="text-align: center;"><span class="math inline">\(160^3\)</span></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\alpha\)</span> 初始值</td><td style="text-align: center;"><span class="math inline">\(10^{-6}\)</span></td><td style="text-align: center;"><span class="math inline">\(10^{-2}\)</span></td></tr><tr class="odd"><td style="text-align: center;">采样间距/步长</td><td style="text-align: center;">coarse_voxel_size(<span class="math inline">\(\delta^{(c)}\)</span>)/2</td><td style="text-align: center;">fine_voxel_size(<span class="math inline">\(\delta^{(f)}\)</span>)/2</td></tr><tr class="even"><td style="text-align: center;">MLP</td><td style="text-align: center;">—</td><td style="text-align: center;">C -&gt; 128 -&gt; 128 -&gt; 3</td></tr><tr class="odd"><td style="text-align: center;">optimizer</td><td style="text-align: center;">Adam</td><td style="text-align: center;">Adam</td></tr><tr class="even"><td style="text-align: center;">lr for grid</td><td style="text-align: center;">0.1</td><td style="text-align: center;">0.1</td></tr><tr class="odd"><td style="text-align: center;">lr for MLP</td><td style="text-align: center;">—</td><td style="text-align: center;">0.001</td></tr></tbody></table><h2 id="实验结果">实验结果</h2><h2 id="消融实验">消融实验</h2><h1 id="代码">代码</h1><details><summary>问题箱</summary>1.DVGO是否可以与diffusion结合？ 2.DVGO是否可以与注意力机制结合？ 比如从多视角图像中提取特征时</details><h2 id="代码结构">代码结构</h2><details><summary>.ipynb_checkpoints /原始文件看不见这个，从云平台下载后显示的隐藏文件</summary>run-checkpoint.py /内容与主函数运行文件 run.py 相同</details><details><summary>configs /各种数据集的配置文件</summary><pre><code>blendedmvs 文件夹 &lt;/br&gt;</code></pre>co3d 文件夹</br> ... ...</br> default.py</details><details><summary>data /存放使用的数据</summary><p><span style="font-family: arial, helvetica, sans-serif; font-size: 14pt;">.ipynb_checkpoints 文件夹    /空的，也是从云平台下载后显示的隐藏文件夹</span></p><p><span style="font-family: arial, helvetica, sans-serif; font-size: 14pt;">nerf_synthetic</span></p><p style="padding-left: 30px;"><span style="font-family: arial, helvetica, sans-serif; font-size: 12pt;">lego</span></p><p style="padding-left: 60px;"><span style="font-family: arial, helvetica, sans-serif; color: #e67e23;">pre_train    /预训练用 200 张图片</span></p><p style="padding-left: 60px;"><span style="font-family: arial, helvetica, sans-serif; color: #e67e23;">test    /测试用 200 张图片，每张图片有原图、depth 深度图和 normal 法线图</span></p><p style="padding-left: 60px;"><span style="font-family: arial, helvetica, sans-serif; color: #e67e23;">train    /训练用 100 张图片</span></p><p style="padding-left: 60px;"><span style="font-family: arial, helvetica, sans-serif; color: #e67e23;">val    /验证用 100 张图片，和训练用的 100 张不一样哟</span></p><p style="padding-left: 60px;"><span style="font-family: arial, helvetica, sans-serif; color: #169179;">transforms_pre_train.json</span></p><p style="padding-left: 60px;"><span style="font-family: arial, helvetica, sans-serif; color: #169179;">transforms_test.json</span></p><p style="padding-left: 60px;"><span style="font-family: arial, helvetica, sans-serif; color: #169179;">transforms_train.json</span></p><p style="padding-left: 60px;"><span style="font-family: arial, helvetica, sans-serif; color: #169179;">transforms_val.json</span></p></details><details><summary>figs /存放两张示例图片</summary><pre><code>debug_cam_and_bbox.png &lt;/br&gt;</code></pre>debug_coarse_volume.png</details><details><summary>lib /C文件、dvgo模型的py文件和加载数据集的文件等</summary><p><span style="font-size: 12pt;">cuda    /存放用来加速的 cpp 文件、cu 文件</span></p><p style="padding-left: 40px;"><span style="color: #169179;">adam_upd.cpp</span></p><p style="padding-left: 40px;"><span style="color: #169179;">adam_upd_kernel.cu</span></p><p style="padding-left: 40px;"><span style="color: #e67e23;">render_utils.cpp</span></p><p style="padding-left: 40px;"><span style="color: #e67e23;">render_utils_kernel.cu</span></p><p style="padding-left: 40px;"><span style="color: #169179;">total_variation.cpp</span></p><p style="padding-left: 40px;"><span style="color: #169179;">total_variation_kernel.cu</span></p><p style="padding-left: 40px;"><span style="color: #e67e23;">ub360_utils.cpp</span></p><p style="padding-left: 40px;"><span style="color: #e67e23;">ub360_utils_kernels.cu</span></p><p><span style="color: #e03e2d; font-size: 12pt;">dbvgo.py    /DirectBiVoxGO</span></p><p><span style="color: #e03e2d; font-size: 12pt;">dcvgo.py    /DirectContractedVoxGO：elif cfg.data.unbounded_inward</span></p><p><span style="color: #e03e2d; font-size: 12pt;">dmpigo.py    /DirectMPIGO：if cfg.data.ndc</span></p><p><span style="color: #e03e2d; font-size: 12pt;">dvgo.py    /DirectVoxGO：else</span></p><p><span style="color: #e03e2d; font-size: 12pt;">grid.py    /DenseGrid 和 TensoRFGrid 两种模式</span></p><p><span style="color: #169179; font-size: 12pt;">load_blendedmvs.py</span></p><p><span style="color: #169179; font-size: 12pt;">load_blender.py</span></p><p><span style="color: #169179; font-size: 12pt;">load_co3d.py</span></p><p><span style="color: #169179; font-size: 12pt;">... ...</span></p><p><span style="color: #169179; font-size: 12pt;">load_tankstemple.py</span></p><p><span style="color: #e67e23; font-size: 12pt;">masked_adam.py    /扩展 Adam 优化器：1.使其支持 per-voxel 学习率；2.masked update (ignore zero grad) which speeduping training</span></p><p><span style="color: #3598db; font-size: 12pt;">utils.py    /Misc    |checkpoint utils    |Evaluation metrics (ssim,lpips)    </span></p><p style="padding-left: 40px;"> </p></details><details><summary>logs /存放输出的结果数据</summary><p><strong>nerf_synthetic</strong></p><p style="padding-left: 40px;"><strong>dvgo_lego</strong></p><p style="padding-left: 80px;"><span style="background-color: #fbeeb8;">render_test_fine_last</span></p><p style="padding-left: 120px;">video.depth.mp4</p><p style="padding-left: 120px;">video.rgb.mp4</p><p style="padding-left: 80px;"><span style="background-color: #fbeeb8;">args.txt</span></p><p style="padding-left: 80px;"><span style="background-color: #fbeeb8;">coarse_last.tar</span></p><p style="padding-left: 80px;"><span style="background-color: #fbeeb8;">fine_last.tar</span></p><p style="padding-left: 80px;"><span style="background-color: #fbeeb8;">config.py</span></p></details><details><summary>tools /神奇工具箱</summary><p><span style="color: #e03e2d; font-size: 14pt;">colmap_utils</span></p><p style="padding-left: 40px;"><span style="color: #3598db; font-size: 12pt;">colmap_read_model.py</span></p><p style="padding-left: 40px;"><span style="color: #3598db; font-size: 12pt;">colmap_wrapper.py</span></p><p style="padding-left: 40px;"><span style="color: #3598db; font-size: 12pt;">pose_utils.py</span></p><p><span style="color: #e03e2d; font-size: 14pt;">imgs2poses.py</span></p><p><span style="color: #e03e2d; font-size: 14pt;">vis_train.py</span></p><p><span style="color: #e03e2d; font-size: 14pt;">vis_volume.py  </span></p><p style="padding-left: 40px;"> </p></details><p>.gitignore /告诉Git忽略哪些文件<br> IMPROVING_LOG.md /数据集在不同显卡环境下的评价指标<br> LICNESE /许可类型<br> README.md /说明文件<br> requirements.txt /需要的python依赖包列表<br> run.py /主函数运行文件<br></p><h2 id="主程序-run.py">主程序 run.py</h2><ul><li><p><span style="background:#daf5e9;">def config_parser</span> /定义命令行参数</p></li><li><p><span style="background:#f9eda6;">def render_viewpoints(no grad)</span> /从给定视图渲染图像，如果真实图给了还会计算测试的评价指标</p></li><li><p><span style="background:#daf5e9;">def seed_everything</span> /固定随机种子以确保实验的可复现性（一些 pytorch 操作比如网格采样的反向传播是不确定的）</p></li><li><p><span style="background:#f9eda6;">def load_everything</span> /以字典形式加载图片、位姿、相机设置等信息，将其设置为张量</p></li><li><p><span style="background:#daf5e9;">def _compute_bbox_by_cam_frustrm_bounded</span> /根据有边界的相机视锥体计算 bbox</p></li><li><p><span style="background:#f9eda6;">def _compute_bbox_by_cam_frustrm_unbounded</span> /根据无边界的相机视锥体计算 bbox：找一个立方体紧紧地包住所有相机中心</p></li><li><p><span style="background:#daf5e9;">def compute_bbox_by_cam_frustrm</span> /根据cfg.data.unbounded_inward是否为真选择上面两种 bbox 计算方式</p></li><li><p><span style="background:#f9eda6;">def compute_bbox_by_coarse_geo(no grad)</span> /根据粗阶段几何（alpha值是否达到门槛确定）确定细阶段的计算区域</p></li><li><p><span style="background:#daf5e9;">def create_new_model</span> /创建并返回dvgo模型和优化器：model=dvgo.DirectVoxGO</p></li><li><p><span style="background:#f9eda6;">def load_existed_model</span> /和 def create_new_model 一样用于初始化模型和优化器（两者是if…else… 关系）</p></li><li><p><span style="background:#daf5e9;">def scene_rep_reconstruction</span> /细粒度阶段的训练重建函数</p><details><summary><p>点击查看该函数详情</p></summary><p><ol type="1"><li>init 初始化</p><p><ol start="2" type="1"><li>寻找是否存在 checkpoint 路径</p><p><ol start="3" type="1"><li>init 模型和优化器</p><p><ol start="4" type="1"><li>init 渲染参数</p><p><ol start="5" type="1"><li>init 批射线采样器（batch rays sampler）：<span style="background:#FFCC99;">def gather_training_rays</span></p><p><ol start="6" type="1"><li>基于视角的学习率</p><p><ol start="7" type="1"><li>运行 GO GO！</p><p>   ​    renew occupancy grid</p><p>   ​    progress scaling checkpoint</p><p>   ​    random sample rays</p><p>   ​    volume rendering</p><p>   ​    gradient descent step</p><p>   ​    update lr</p><p>   ​    check log &amp; save</p></details></li></ol></li></ol></li></ol></li></ol></li></ol></li></ol></li></ol></li><li><p><span style="background:#f9eda6;">def train</span> /训练函数</p><details><summary><p>点击查看 train 函数详情</p></summary><p><p><span style="color: #e03e2d;"><strong>init </strong>： 初始化存放输出结果的文件夹</span></p></p><p><p><span style="color: #3598db;"><strong>coarse geometry searching （only works for inward bounded scenes）</strong>：通过粗粒度训练中的 compute_bbox_by_cam_frustrm 寻找物体所在位置</span></p></p><p><p><span style="color: #e03e2d;"><strong>fine detail reconstruction </strong>：先通过 compute_bbox_by_coarse_geo 确定细粒度阶段的训练区域，再由 scene_rep_reconstruction 进行细粒度重建</span></p></p></details></li><li><p><strong><span style="background:#daf5e9;">if __name__=='__main__' </span></strong> /主函数</p><details><summary><p>点击查看主函数详情</p></summary><p><p><span style="color: #e67e23;"><strong>load setup </strong>：加载 parser、args 和 cfg （mmcv.Config.fromfile）等</span></p></p><p><p><span style="color: #169179;"><strong>init environment </strong>：设置环境为 cuda ，并通过 seed_everything 确保实验的可复现性</span></p></p><p><p><span style="color: #e67e23;"><strong>load images / poses / camera settings / data split </strong>：借助 load_everything 函数完成</span></p></p><p><p><span style="color: #169179;"><strong>export scene bbox and camera poses in 3d for debugging and visualization</strong> ：导出可视化等结果为 tar 压缩文件（coarse_last.tar）</span></p></p><p><p><span style="color: #e67e23;"><strong>train </strong>：如果 args.render_only 函数为假，通过 train 函数进行训练</span></p></p><p><p><span style="color: #169179;"><strong>load model for rendering</strong> ：根据 cfg.data 类型选择导入模型类型来进行相应渲染，设置渲染结果放入 fine_last.tar</span></p></p><p><p><span style="color: #e67e23;"><strong>render trainset and eval </strong>：如果相应渲染为 args.render_train，对训练渲染并将结果放入 render_train_{ckpt_name}</span></p></p><p><p><span style="color: #169179;"><strong>render testset and eval </strong>：如果相应渲染为 args.render_test，对测试渲染并将结果放入 render_test_{ckpt_name}</span></p></p><p><p><span style="color: #e67e23;"><strong>render video</strong> ：如果相应渲染为 args.video，对视频渲染并将结果放入 render_video_{ckpt_name}</span></p></p><p><p><span style="color: #169179;"><strong> 输出结束信息</strong></span></p></p></details></li></ul><h1 id="bib-citation">Bib Citation</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;SunSC22,</span><br><span class="line">  author    = &#123;Cheng Sun and Min Sun and Hwann&#123;-&#125;Tzong Chen&#125;,</span><br><span class="line">  title     = &#123;Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction&#125;,</span><br><span class="line">  booktitle = &#123;CVPR&#125;,</span><br><span class="line">  year      = &#123;2022&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="参考文献">参考文献</h1><p>[1] Sun C, Sun M, Chen H T. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction[C], Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 5459-5469.</p><p>[2] Mildenhall B, Srinivasan P P, Tancik M, et al. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis[C], European Conference on Computer Vision. Cham: Springer International Publishing, 2020: 405-421.</p><p>[3] Barron J T, Mildenhall B, Tancik M, et al. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields[C], Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021: 5855-5864.</p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2022 </tag>
            
            <tag> voxel grid </tag>
            
            <tag> 训练加速 </tag>
            
            <tag> 新视图合成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Typora 搜狗自定义短语设置</title>
      <link href="/2024/03/24/Typora%20%E6%90%9C%E7%8B%97%E5%BF%AB%E6%8D%B7%E9%94%AE%E8%AE%BE%E7%BD%AE/"/>
      <url>/2024/03/24/Typora%20%E6%90%9C%E7%8B%97%E5%BF%AB%E6%8D%B7%E9%94%AE%E8%AE%BE%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考网址：</p><p><a href="https://blog.csdn.net/kllo__/article/details/122494151">Typora：改变字体的背景颜色_typora文字背景色_kllo__的博客-CSDN博客</a></p><p><a href="https://www.cnblogs.com/USTHzhanglu/p/16073072.html">MD两种折叠方法对比（Typora/Jupyter设置代码折叠） - USTHzhanglu - 博客园 (cnblogs.com)</a></p><p><a href="https://nickxu.me/2022/02/20/Hexo-Butterfly-建站指南（五）日常写作/">Hexo + Butterfly 建站指南（五）日常写作 | NX の 博客 (nickxu.me)</a></p></blockquote><blockquote><p>在Typora中没有快捷设置字体颜色的方式，因此我们想到在搜狗输入法的自定义短语中定义快捷输入html代码来设置字体颜色。本文给出了一些常用的自定义短语。</p></blockquote><p>[TOC]</p><h2 id="设置方法">设置方法</h2><p>打开<code>搜狗-属性设置-高级-候选扩展-自定义短语</code>，点击<code>添加新定义</code>，即可定义新的自定义短语。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231206093545264.png" alt="image-20231206093545264" /><figcaption aria-hidden="true">image-20231206093545264</figcaption></figure><h2 id="设置字体颜色">设置字体颜色</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#常见颜色</span><br><span class="line">&lt;font color=red&gt;红色字体&lt;/font&gt;  #缩写red</span><br><span class="line">&lt;font color=blue&gt;蓝色字体&lt;/font&gt;  #缩写blu</span><br><span class="line">&lt;font color=green&gt;绿色字体&lt;/font&gt;  #缩写gree</span><br><span class="line">&lt;font color=orange&gt;橙色字体&lt;/font&gt;#缩写oran</span><br><span class="line">&lt;font color=purple&gt;紫色字体&lt;/font&gt;#缩写purp</span><br><span class="line">&lt;font color=yellow&gt;黄色字体&lt;/font&gt;#缩写yel</span><br><span class="line"></span><br><span class="line">&lt;font color=#1ec4d3&gt;藻蓝色字体&lt;/font&gt;</span><br></pre></td></tr></table></figure><h4 id="样式预览">样式预览：</h4><p><font color=red>红色字体</font><br /><font color=blue>蓝色字体</font><br /><font color=green>绿色字体</font><br /><font color=orange>橙色字体</font><br /><font color=purple>紫色字体</font><br /><font color=yellow>黄色字体</font></p><p><font color=#1ec4d3>藻蓝色字体</font></p><p>另外附上我从mubu上扒下来的字体颜色设置：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;font color=#ef042a&gt;红色字体&lt;/font&gt;  #缩写red</span><br><span class="line">&lt;font color=#0091ff&gt;蓝色字体&lt;/font&gt;  #缩写blu</span><br><span class="line">&lt;font color=#4eb434&gt;绿色字体&lt;/font&gt;  #缩写gree</span><br><span class="line">&lt;font color=#df8400&gt;橙色字体&lt;/font&gt;#缩写oran</span><br><span class="line">&lt;font color=#985fff&gt;紫色字体&lt;/font&gt;#缩写purp</span><br></pre></td></tr></table></figure><h4 id="样式预览-1">样式预览：</h4><p><font color=#df8400>这是橙色</font> <font color=#ef042a>这是红色</font> <font color=#4eb434>这是绿色</font> <font color=#0091ff>这是蓝色</font> <font color=#985fff>这是紫色</font></p><h2 id="给字体加底色">给字体加底色</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;span style=&quot;background:#eef0f4;&quot;&gt;浅灰色背景&lt;/span&gt;#缩写bggrey</span><br><span class="line">&lt;span style=&quot;background:#fbd4d0;&quot;&gt;浅红色背景&lt;/span&gt;#缩写bgred</span><br><span class="line">&lt;span style=&quot;background:#d4e9d5;&quot;&gt;浅绿色背景&lt;/span&gt;#缩写bggre</span><br><span class="line">&lt;span style=&quot;background:#dad5e9;&quot;&gt;浅紫色背景&lt;/span&gt;#缩写bgpur</span><br><span class="line">&lt;span style=&quot;background:#f9eda6;&quot;&gt;浅黄色背景&lt;/span&gt;#缩写bgyel</span><br><span class="line">&lt;span style=&quot;background:#FFCC99;&quot;&gt;浅橘色背景&lt;/span&gt;#缩写bgora</span><br><span class="line"></span><br><span class="line">&lt;span style=&quot;background:#daf5e9;&quot;&gt;亮绿色背景&lt;/span&gt;</span><br><span class="line">&lt;span style=&quot;background:#fff1c9;&quot;&gt;亮黄色背景&lt;/span&gt;  </span><br><span class="line">&lt;span style=&quot;background:#dcffff;&quot;&gt;亮蓝色背景&lt;/span&gt; </span><br></pre></td></tr></table></figure><h4 id="样式预览-2">样式预览</h4><p><span style="background:#eef0f4;">浅灰色背景</span> <span style="background:#fbd4d0;">浅红色背景</span> <span style="background:#d4e9d5;">浅绿色背景</span> <span style="background:#dad5e9;">浅紫色背景</span> <span style="background:#f9eda6;">浅黄色背景</span> <span style="background:#FFCC99;">浅橘色背景</span></p><p><span style="background:#daf5e9;">亮绿色背景</span> <span style="background:#fff1c9;">亮黄色背景</span> <span style="background:#dcffff;">亮蓝色背景</span></p><blockquote><p>配色参考网站：</p><p><a href="https://likexia.gitee.io/tools/peise/index.html">网页设计常用色彩搭配表 - 配色表 (gitee.io)</a></p><p><a href="https://encycolorpedia.cn/">十六进制颜色代码表，图表及调色板 - Encycolorpedia</a></p><p><a href="https://encycolorpedia.cn/e0ffff">Lightcyan／亮青／淺藍／淺藍色／#e0ffff十六进制颜色代码表，图表，调色板，绘图&amp;油漆 (encycolorpedia.cn)</a></p></blockquote><h2 id="给字体加底色且改颜色">给字体加底色且改颜色</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;span style=&quot;background:#f9eda6;color:red&quot;&gt;浅黄色背景红字&lt;/span&gt;</span><br></pre></td></tr></table></figure><p>如：<span style="background:#f9eda6;color:red">浅黄色背景</span></p><h2 id="设置图片题注tuzh">设置图片题注tuzh</h2><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">center</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">img</span> <span class="attr">style</span>=<span class="string">&quot;border-radius: 0.3125em;</span></span></span><br><span class="line"><span class="string"><span class="tag">    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);&quot;</span> </span></span><br><span class="line"><span class="tag">    <span class="attr">src</span>=<span class="string">&quot;这里输入图片地址&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">&quot;color:orange; border-bottom: 1px solid #d9d9d9;</span></span></span><br><span class="line"><span class="string"><span class="tag">    display: inline-block;</span></span></span><br><span class="line"><span class="string"><span class="tag">    color: #999;</span></span></span><br><span class="line"><span class="string"><span class="tag">    padding: 2px;&quot;</span>&gt;</span>这里输入题注<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">center</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="设置-html-超链接">设置 html 超链接</h2><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;链接地址&quot;</span>&gt;</span>链接名称<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这种形式的超链接在 Typora 中相较于原版，可以实现点击跳转页面！而且可以用于嵌入到折叠栏中，跳转功能也不受影响！</p><h2 id="设置内容折叠">设置内容折叠</h2><h3 id="普通内容折叠zhed">普通内容折叠zhed</h3><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">details</span>&gt;</span> <span class="tag">&lt;<span class="name">summary</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">summary</span>&gt;</span></span><br><span class="line">contents ...</span><br><span class="line"><span class="tag">&lt;/<span class="name">details</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在<code>content</code>中可以嵌套使用 Markdown 语法和 HTML 语法。</p><h4 id="效果如下">效果如下：</h4><details><summary>Title</summary>contents ...</details><h3 id="代码内容折叠czhed">代码内容折叠czhed</h3><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">details</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">summary</span>&gt;</span>点击时的区域标题：点击查看详细内容<span class="tag">&lt;/<span class="name">summary</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">pre</span>&gt;</span><span class="tag">&lt;<span class="name">code</span>&gt;</span>#define A B</span><br><span class="line">#endif</span><br><span class="line">void init(void)<span class="tag">&lt;/<span class="name">code</span>&gt;</span><span class="tag">&lt;/<span class="name">pre</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">details</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在Typora和网站上均可以折叠代码，但是代码不能高亮。<code>&lt;pre&gt;&lt;/pre&gt;</code>用于显示多行代码，若去除则渲染为单行代码。</p><h4 id="效果如下-1">效果如下：</h4><details><summary>点击时的区域标题：点击查看详细内容</summary><pre><code>#define A B#endifvoid init(void)</code></pre></details><h2 id="butterfly-标签外挂">Butterfly 标签外挂</h2><blockquote><p>官方文档：<a href="https://butterfly.js.org/posts/4aa8abbe/#標籤外掛（Tag-Plugins）">Butterfly 安裝文檔(三) 主題配置-1 | Butterfly</a></p></blockquote><h3 id="折叠栏-toggle类似内容折叠">折叠栏 Toggle（类似内容折叠）</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">&#123;% hideToggle 点击以打开 %&#125;</span><br><span class="line"></span><br><span class="line">内容</span><br><span class="line"></span><br><span class="line">&#123;% endhideToggle %&#125;</span><br></pre></td></tr></table></figure><h4 id="效果如下-2">效果如下：</h4><details class="toggle" ><summary class="toggle-button" style="">点击以打开</summary><div class="toggle-content"><p>内容</p></div></details><h3 id="选项卡-tabs">选项卡 Tabs</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">&#123;% tabs 样例 %&#125;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab 代码  --&gt;</span><br><span class="line"></span><br><span class="line">这里是代码</span><br><span class="line"></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab 预览  --&gt;</span><br><span class="line"></span><br><span class="line">这里是预览</span><br><span class="line"></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&#123;% endtabs %&#125;</span><br></pre></td></tr></table></figure><h4 id="效果如下-3">效果如下：</h4><div class="tabs" id="样例"><ul class="nav-tabs"><button type="button" class="tab  active" data-href="样例-1">代码</button><button type="button" class="tab " data-href="样例-2">预览</button></ul><div class="tab-contents"><div class="tab-item-content active" id="样例-1"><p>这里是代码</p></div><div class="tab-item-content" id="样例-2"><p>这里是预览</p></div></div><div class="tab-to-top"><button type="button" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div><h3 id="时间轴-timeline">时间轴 timeline</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">&#123;% timeline 2024 %&#125;</span><br><span class="line">&lt;!-- timeline 01-02 --&gt;</span><br><span class="line">这是测试文本</span><br><span class="line">&lt;!-- endtimeline --&gt;</span><br><span class="line">&#123;% endtimeline %&#125;</span><br></pre></td></tr></table></figure><h4 id="效果如下-4">效果如下：</h4><div class="timeline undefined"><div class='timeline-item headline'><div class='timeline-item-title'><div class='item-circle'><p>2024</p></div></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>01-02</p></div></div><div class='timeline-item-content'><p>这是测试页面</p></div></div></div>]]></content>
      
      
      <categories>
          
          <category> blog搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>胸中无笔墨，如何点江山</title>
      <link href="/2024/03/20/%E4%B8%80%E4%BA%9B%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D%E7%B4%A2%E5%BC%95/"/>
      <url>/2024/03/20/%E4%B8%80%E4%BA%9B%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D%E7%B4%A2%E5%BC%95/</url>
      
        <content type="html"><![CDATA[<h1 id="瓶颈层-bottleneck-layer">瓶颈层 bottleneck layer</h1><p>瓶颈层是指网络模型中<strong>输出维度明显小于输入维度的层</strong>，由此它极大地限制了网络的性能与表达，造出瓶颈。瓶颈层常用于降低网络维度，并提取高维特征。</p><h1 id="光栅化-rasterization">光栅化 rasterization</h1><blockquote><p>参考自：<a href="https://zhuanlan.zhihu.com/p/450540827">现代计算机图形学基础二：光栅化（Rasterization） - 知乎 (zhihu.com)</a></p></blockquote><p>Raster在德语中是屏幕的意思（Raster == screen in German），而光栅化就是把东西画在屏幕上的一个过程（Rasterize == drawing onto the screen ），也就是把名词变成动词。</p><p>屏幕是由一行行一列列像素组成的。我们先梳理下如何用相机拍摄物体，如苹果：</p><p>第一步摆放好苹果（model transformation）；</p><p>第二步找到一个好角度放相机（view transformation）；</p><p>第三步进行投影变换（projection transformation），将空间中的苹果转换到单位立方体中；</p><p>第四步通过视口变换矩阵把单位立方体映射到屏幕空间，得到屏幕空间中的很多三角形；</p><p>第五步把三角形打碎成像素，并给每个像素赋值，然后屏幕上就可以显示出苹果了。</p><p>这个完整的过程称为“<strong>光栅化</strong>”。其中第一到三步可以统称为<u>空间变换</u>。</p><p>第五步中，三角形打碎成像素其实就是判断三角形包围盒覆盖的范围中有哪些像素中心在包围盒内，然后给中心在盒内的像素均匀上色。事实上，这一步会导致“锯齿（Jaggies）”。<strong>有两个原因导致锯齿：一是像素本身有一定的大小，二是采样的速度更不上信号变化的速度（高频信号采样不足）。</strong> 我们将锯齿尽可能消除的方法就是光滑。</p><h1 id="数学用语">数学用语</h1><h2 id="kl散度与变分下界">KL散度与变分下界</h2><blockquote><p>出处：<a href="https://bluefisher.github.io/2020/02/06/理解-Variational-Lower-Bound/">理解 Variational Lower Bound | Fisher's Blog (bluefisher.github.io)</a></p></blockquote><h1 id="衡量指标">衡量指标</h1><h2 id="psnr">PSNR</h2><p>峰值信噪比（Peak Signal-to-Noise Ratio，PSNR）是一种用于衡量图像或视频质量的指标。它通过比较原始信号（通常是无失真的参考信号）和经过压缩或其他失真处理后的信号之间的差异来评估质量。</p><p>PSNR的计算公式如下： PSNR = 10 * log10((R^2) / MSE) 其中，R表示像素值的最大可能值（比如对于8位灰度图像，R=255），MSE表示均方误差（Mean Squared Error），它是原始信号和失真信号之间差异的平方的平均值。</p><p><span style="background:#daf5e9;">PSNR的值通常以分贝（dB）为单位，数值越高表示质量越好</span>。常见的PSNR阈值为30dB以上，30-40dB为较好的质量，40-50dB为很好的质量，50dB以上为极好的质量。</p><h2 id="ssim">SSIM</h2><p>结构相似性（Structural Similarity，简称SSIM）是一种用于衡量两幅图像相似程度的指标。与传统的均方误差（Mean Squared Error，MSE）相比，SSIM更能反映人眼对图像质量的感知。</p><p>SSIM指标结合了亮度、对比度和结构三个方面的信息，通过计算这些信息的相似性来评估图像的相似度。<span style="background:#daf5e9;">SSIM的取值范围在[0, 1]之间，值越接近1表示两幅图像越相似</span>。</p><p>SSIM的计算公式如下：</p><p><span class="math inline">\(SSIM(x, y)\)</span> = (2 * <span class="math inline">\(\mu_x\)</span> <em><span class="math inline">\(\mu_y\)</span> + <span class="math inline">\(c_1\)</span>) </em> (2 * <span class="math inline">\(\sigma_{xy}\)</span> + <span class="math inline">\(c_2\)</span>) / (<span class="math inline">\({\mu_x}^2\)</span> + <span class="math inline">\({\mu_y}^2\)</span> + c1) * (<span class="math inline">\({σ_x}^2\)</span> + <span class="math inline">\({σ_y}^2\)</span> + <span class="math inline">\(c_2\)</span>)</p><p>其中，x和y分别表示两幅图像，<span class="math inline">\(\mu\)</span> 表示图像的均值，<span class="math inline">\(\sigma\)</span> 表示图像的标准差，<span class="math inline">\(\sigma_{xy}\)</span> 表示图像的协方差，<span class="math inline">\(c_1\)</span> 和 <span class="math inline">\(c_2\)</span> 是常数，用于避免分母为0。</p><h2 id="lpips">LPIPS</h2><p>学习感知图像块相似度 （Learned Perceptual Image Patch Similarity，简称LPIPS）是一种用于计算图像质量和相似度的指标。它基于深度学习模型，通过学习人类感知图像质量的方式来评估图像之间的相似性。</p><p>LPIPS使用预训练的卷积神经网络（如VGG网络）来提取图像的特征表示，然后通过比较这些特征表示之间的差异来计算图像之间的相似度。与传统的像素差异度量方法（如均方误差）相比，LPIPS更能反映人类感知的差异。<span style="background:#daf5e9;">LPIPS 值越小越好</span>。</p><h1 id="数据集">数据集</h1><h2 id="inward-facing"><strong>inward-facing</strong></h2><blockquote><p><a href="https://immortalqx.github.io/2022/08/22/nerf-notes-3/#!">(ฅ&gt;ω&lt;*ฅ) 噫？又好了~ (immortalqx.github.io)</a></p></blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240101144152110.png" alt="image-20240101144152110" /><figcaption aria-hidden="true">image-20240101144152110</figcaption></figure><h1 id="sota-benchmark-和-baseline">SOTA, benchmark 和 baseline</h1><blockquote><p>参考链接：https://www.zhihu.com/question/433986039/answer/1618236738</p></blockquote><p>当在浩瀚无边的论文海洋中畅游时，最需要的找到的是SOTA论文。 SOTA，全称「state-of-the-art」，用于描述特定任务中取得当前最优效果的模型。 例如在图像分类任务上，某个模型在常用的数据集（如 ImageNet）上取得了当前最优的性能表现，我们就可以说这个模型达到了SOTA。</p><p>Benchmark和baseline都是指最基础的比较对象。你论文的motivation来自于想超越现有的baseline/benchmark，你的实验数据都需要以baseline/benckmark为基准来判断是否有提高。唯一的区别就是baseline讲究一套方法，而benchmark更偏向于一个目前最高的指标，比如precision，recall等等可量化的指标。举个例子，NLP任务中BERT是目前的SOTA，你有idea可以超过BERT。那在论文中的实验部分你的方法需要比较的baseline就是BERT，而需要比较的benchmark就是BERT具体的各项指标。</p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> implicit rendering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Diffusion Model（一）</title>
      <link href="/2024/03/16/Diffusion%20Model%20%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2024/03/16/Diffusion%20Model%20%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.bilibili.com/video/BV14o4y1e7a6/?vd_source=124ec79ebd3e16b0f454a3994a468f98">【Diffusion扩散模型+对比学习】2023最容易出论文的两个方向！由浅入深了解扩散模型＆对比学习，源码复现+模型解读（超详细保姆级入门教程）_哔哩哔哩_bilibili</a></p><p><a href="https://www.zhihu.com/question/545764550">(99+ 封私信 / 81 条消息) 怎么理解今年 CV 比较火的扩散模型（DDPM）？ - 知乎 (zhihu.com)</a></p><p><a href="https://arxiv.org/pdf/2209.00796.pdf">Diffusion Models: A Comprehensive Survey of Methods and Applications (arxiv.org)</a> (Diffusion 综述)</p><p><a href="https://zhuanlan.zhihu.com/p/562389931">扩散模型(Diffusion Model)首篇综述-Diffusion Models: A Comprehensive Survey of Methods and Applications - 知乎 (zhihu.com)</a></p></blockquote><blockquote><p><strong>Diffusion Model</strong> 是一种和 <strong>VAE、GAN、flow Model</strong> 等模型一样的生成模型(generative model)。</p></blockquote><h1 id="扩散模型的应用">扩散模型的应用</h1><p>根据综述文章（<a href="https://arxiv.org/pdf/2209.00796.pdf">Diffusion Models: A Comprehensive Survey of Methods and Applications (arxiv.org)</a>），我们可以将Diffusion的应用分为六类：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240225153110951.png" alt="image-20240225153110951" /><figcaption aria-hidden="true">image-20240225153110951</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240225132730831.png" alt="Diffusion_applications" /><figcaption aria-hidden="true">Diffusion_applications</figcaption></figure><ul><li><span style="background:#daf5e9;">Computer Vision 计算机视觉</span>——点云完整和生成</li><li>Natural Language Generation 自然语言生成</li><li>Temporal Data Modeling 时态数据建模</li><li><span style="background:#daf5e9;">Multi-Modal Learning 多模态生成</span>——由文本生成3D图像</li><li>Robust Learning 鲁棒地学习</li><li>Interdisciplinary Applications 跨学科应用</li></ul><h2 id="unconditional-and-conditional-diffusion-models">Unconditional and Conditional Diffusion Models</h2><p><strong>无条件扩散模型</strong>（unconditional diffusion models）和 <strong>条件扩散模型</strong>（conditional diffusion models）是扩散模型的两种基本应用范式。首先被发展的是无条件扩散模型，条件扩散模型则紧随其后。</p><p>无条件扩散模型通常被用于探索生成模型性能的上限。条件扩散模型则更多是用于应用级内容，因为它能够根据我们的倾向控制生成结果。</p><p><strong>条件扩散模型的四类在不同条件下的应用</strong>：</p><ul><li><p>扩散模型中的条件反射机制 Conditioning Mechanisms in Diffusion Models</p><p>条件反射机制主要有四种，包括<span style="background:#d4e9d5;">串联concatenation</span>、<span style="background:#fbd4d0;">基于梯度gradient-based</span>、<span style="background:#f9eda6;">交叉注意力gradient-based</span>和<span style="background:#dad5e9;">自适应层归一化adaptive layer normalization(adaLN)</span>。</p></li><li><p>标签和分类器上的条件扩散 Condition Diffusion on Labels and Classifiers</p></li><li><p>文本、图像和语义图上的条件扩散 Condition Diffusion on Texts, Images, and Semantic Maps</p></li><li><p>图形上的条件扩散 Condition Diffusion on Graphs</p></li></ul><h2 id="计算机视觉-computer-vision">计算机视觉 Computer Vision</h2><p><strong>1.图像超分辨率、修复、翻译和编辑 ( Image Super Resolution, Inpainting, Restoration, Translation, and Editing)</strong></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240225142347409.png" alt="image-20240225140505154" /><figcaption aria-hidden="true">image-20240225140505154</figcaption></figure><blockquote><p>图：由<strong>潜在扩散模型(Latent Diifusion Model, LDM)<sup>[1]</sup></strong>生成的图像超分辨率结果。潜在扩散模型（LDM）在不牺牲质量的情况下简化了扩散模型去噪的训练和采样过程。</p></blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240225140505154.png" alt="image-20240225141526648" /><figcaption aria-hidden="true">image-20240225141526648</figcaption></figure><blockquote><p>图：<strong>RePaint<sup>[2]</sup></strong> 生成的图像修复结果</p></blockquote><p><strong>2.语义分割 (Semantic Segmentation)</strong></p><p><strong>3.视频生成 (Video Generation)</strong></p><p><strong>4.点云完整和生成 (Point Cloud Completion and Generation)</strong></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240225141526648.png" alt="image-20240225142347409" /><figcaption aria-hidden="true">image-20240225142347409</figcaption></figure><blockquote><p>图：点云扩散过程的有向图形模型。<strong>Diffusion probabilistic models for 3d point cloud generation[3]</strong>采用了将点云视为热力学系统中的粒子的方法，使用热浴 (hit bath) 来促进从原始分布到噪声分布的扩散。</p></blockquote><p><strong>5.异常检测 (Anomaly Detection)</strong></p><h2 id="多模态生成-multi-modal-generation">多模态生成 Multi-Modal Generation</h2><p><strong>1.由文本生成图像 (Text-to-Image Generation)</strong></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240225151212293.png" alt="image-20240225143715162" /><figcaption aria-hidden="true">image-20240225143715162</figcaption></figure><blockquote><p>图：使用<strong>LDM<sup>[1]</sup>、Imagen<sup>[4]</sup> 和 ConPreDiff<sup>[5]</sup></strong> 进行文本到图像的合成示例</p></blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240225150546193.png" alt="image-20240225150546193" /><figcaption aria-hidden="true">image-20240225150546193</figcaption></figure><blockquote><p>图：与以前的SOTA模型相比，<strong>RPG<sup>[10]</sup></strong> 表现出在生成的图像中表达复杂和合成文本提示的卓越能力（彩色文本表示关键部分）。RPG 提出了一种全新的免训练文本到图像生成/编辑框架，利用多模态 LLM [339] 强大的思维链推理能力来增强文本到图像扩散模型的组合性。这个新的 RPG 框架以闭环方式统一了文本引导图像生成和图像编辑任务。值得注意的是，如图所示，RPG 优于所有 SOTA 方法，例如 SDXL 、DALL-E 3 和 InstructPix2Pix ，证明了其优越性。此外，RPG 框架是用户友好的，可以推广到不同的 MLLM 架构和扩散主干网（例如 ControlNet）。</p></blockquote><p><strong>2.由场景图生成图像 (Scene Graph-to-Image Generation)</strong></p><p><strong>3.<span style="background:#daf5e9;">由文本生成3D图像 (Text-to-3D Generation)</span></strong></p><p>3D内容生成在游戏、娱乐和机器人模拟等广泛应用中有着很高的需求。使用自然语言增强 3D 内容生成对新手和有经验的艺术家都有很大帮助。</p><p><strong>DreamFusion<sup>[6]</sup></strong>采用预先训练好的2D文本到图像扩散模型进行文本到3D合成。它优化了具有概率密度蒸馏损失的随机初始化的 3D 模型（神经辐射场或 NeRF），该模型利用 2D 扩散模型作为参数化图像生成器的先验优化。</p><p>为了获得NeRF的快速和高分辨率优化，<strong>Magic3D<sup>[7]</sup></strong>提出了一种基于级联低分辨率图像扩散先验和高分辨率潜在扩散先验的两阶段扩散框架。</p><p>为了实现高保真3D创作，<strong>Make-It-3D <sup>[8]</sup></strong>优化了神经辐射场，在正面视图处结合了参考图像的约束条件，在新视图处加入了扩散先验和扩散先验，从而优化了神经辐射场，将粗糙模型增强为有纹理的点云，并通过扩散先验和参考图像的高质量纹理来增加真实感。</p><p><strong>ProlificDreamer <sup>[9]</sup></strong> 提出了变分分数蒸馏 （VSD），基于文本提示作为随机变量优化 3D 场景的分布，以使用 KL 散度作为度量，将来自各个角度的渲染图像分布与预训练的 2D 扩散模型紧密对齐。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240225143715162.png" alt="image-20240225151212293" /><figcaption aria-hidden="true">image-20240225151212293</figcaption></figure><blockquote><p>图：ProlificDreamer 可以生成高保真和逼真的 3D 纹理网格。</p></blockquote><p><strong>4.由文本生成动作 (Text-to-Motion Generation)</strong></p><p><strong>5.由文本生成视频 (Text-to-Video Generation)</strong></p><p><strong>6.由文本生成音频 (Text-to-Audio Generation)</strong></p><h1 id="未来研究方向">未来研究方向</h1><p>A.重审假设。需要重新审视和分析扩散模型中的许多典型假设。例如，假设扩散模型的正向过程完全消除了数据中的所有信息并且使其等效于先前分布可能并不总是成立。实际上，完全删除信息是在有限时间内无法实现，了解何时停止前向噪声处理以在采样效率和采样质量之间取得平衡是非常有意义的。</p><p>B. 理论理解。diffusion model已经成为一个强大的框架，可以在大多数应用中与生成对抗性网络（GAN）竞争，而无需诉诸对抗性训练。对于特定的任务，我们需要了解为什么以及何时扩散模型会比其他网络更加有效，理解扩散模型和其他生成模型的区别将有助于阐明为什么扩散模型能够产生优秀的样本同时拥有高似然值。另外，系统地确定扩散模型的各种超参数也是很重要的。</p><p>C. 潜在表示。diffusion model如何在隐空间中提供良好的latent representation，以及如何将其用于data manipulation的任务也是值得研究的。</p><p>D. AIGC 和 Diffusion Foundation Models。将diffusion model和generative foundation model结合，探索更多类似于从AI 文生图扩散模型 Stable Diffusion 到 ChatGPT，GPT-4等的有趣的人工智能生成内容（AIGC）应用。</p><h1 id="参考文献">参考文献</h1><details><summary>点我查看</summary><p>[1] Rombach R, Blattmann A, Lorenz D, et al. High-resolution image synthesis with latent diffusion models[C], Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 10684-10695.</p><p>[2] Lugmayr A, Danelljan M, Romero A, et al. Repaint: Inpainting using denoising diffusion probabilistic models[C], Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 11461-11471.</p><p>[3] Luo S, Hu W. Diffusion probabilistic models for 3d point cloud generation[C], Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 2837-2845.</p><p>[4] Saharia C, Chan W, Saxena S, et al. Photorealistic text-to-image diffusion models with deep language understanding[J]. Advances in Neural Information Processing Systems, 2022, 35: 36479-36494.</p><p>[5] Yang L, Liu J, Hong S, et al. Improving diffusion-based image synthesis with context prediction[J]. Advances in Neural Information Processing Systems, 2024, 36.</p><p>[6] Poole B, Jain A, Barron J T, et al. Dreamfusion: Text-to-3d using 2d diffusion[J]. arxiv preprint arxiv:2209.14988, 2022.</p><p>[7] Lin C H, Gao J, Tang L, et al. Magic3d: High-resolution text-to-3d content creation[C], Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 300-309.</p><p>[8] Tang J, Wang T, Zhang B, et al. Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior[J]. arxiv preprint arxiv:2303.14184, 2023.</p><p>[9] Wang Z, Lu C, Wang Y, et al. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation[J]. Advances in Neural Information Processing Systems, 2024, 36.</p><p>[10] Yang L, Yu Z, Meng C, et al. Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs[J]. arxiv preprint arxiv:2401.11708, 2024.</p></details>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>信息熵</title>
      <link href="/2024/03/16/%E4%BF%A1%E6%81%AF%E7%86%B5/"/>
      <url>/2024/03/16/%E4%BF%A1%E6%81%AF%E7%86%B5/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信息论与编码 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>马尔科夫过程</title>
      <link href="/2024/03/16/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E8%BF%87%E7%A8%8B/"/>
      <url>/2024/03/16/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信息论与编码 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DreamFusion 论文笔记</title>
      <link href="/2024/03/11/DreamFusion/"/>
      <url>/2024/03/11/DreamFusion/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>论文名称： DreamFusion: Text-to-3D using 2D Diffusion (DreamFusion: 使用 2D 扩散模型实现从文本到3D模型)</strong></p><p>论文代码：</p><ul><li><p><a href="https://github.com/vikasTmz/Dreamfusion?tab=readme-ov-file">GitHub - vikasTmz/Dreamfusion: A pytorch implementation of text-to-3D dreamfusion, powered by stable diffusion.</a></p></li><li><p><a href="https://github.com/ashawkey/stable-dreamfusion">GitHub - ashawkey/stable-dreamfusion: Text-to-3D &amp; Image-to-3D &amp; Mesh Exportation with NeRF + Diffusion.</a></p><p>论文主页： <a href="https://dreamfusion3d.github.io/">DreamFusion: Text-to-3D using 2D Diffusion (dreamfusion3d.github.io)</a></p></li></ul><p>参考网址：</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> diffusion </tag>
            
            <tag> CVPR </tag>
            
            <tag> 2023 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparseFusion 论文笔记</title>
      <link href="/2024/03/11/SparseFusion/"/>
      <url>/2024/03/11/SparseFusion/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文名称： SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction (SparseFusion: 蒸馏提取有条件视图的扩散，用于三维重建)</p><p>论文代码： <a href="https://github.com/zhizdev/sparsefusion">GitHub - zhizdev/sparsefusion: Official PyTorch implementation of SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction</a></p><p>论文主页： <a href="https://sparsefusion.github.io/">SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction</a></p><p>参考网址：</p></blockquote><h1 id="论文基本信息">论文基本信息</h1>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> diffusion </tag>
            
            <tag> CVPR </tag>
            
            <tag> 2023 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DreamAvatar 论文笔记</title>
      <link href="/2024/03/11/DreamAvatar/"/>
      <url>/2024/03/11/DreamAvatar/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.bilibili.com/video/BV1SN411i7d8/?spm_id_from=333.337.search-card.all.click&amp;vd_source=124ec79ebd3e16b0f454a3994a468f98">2023082【跨模态学习驱动的三维理解与生成】韩锴：Text-and-Shape Guided 3D Human Avatar Generation……_哔哩哔哩_bilibili</a></p></blockquote><blockquote><p><strong>论文名称： DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via</strong> <strong>Diffusion Models （DreamAvatar: 文本和形状引导的基于扩散模型的3D人体角色生成）</strong></p><p>代码地址： 未开源</p><p>论文主页： <a href="https://yukangcao.github.io/DreamAvatar/">DreamAvatar (yukangcao.github.io)</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>群策群力战NeuS，乃知NeRF难自遮</title>
      <link href="/2024/03/11/NeuS/"/>
      <url>/2024/03/11/NeuS/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文名称: <strong>NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction</strong>（NeuS: 通过体积渲染学习神经隐式曲面，用于多视图重建）</p><p>代码地址：<a href="https://github.com/Totoro97/NeuS">Totoro97/NeuS: Code release for NeuS (github.com)</a></p><p>论文主页: <a href="https://lingjie0206.github.io/papers/NeuS/">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction (lingjie0206.github.io)</a></p><p>论文翻译：<a href="https://zhuanlan.zhihu.com/p/577893396">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction - 知乎 (zhihu.com)</a> （来自别的友友的翻译~）</p></blockquote><h1 id="论文基本信息">论文基本信息</h1><p>NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction<sup>[1]</sup>是<strong>2021</strong>年由来自香港大学、Max Planck研究所和德州农工大学的Peng Wang 等人发表在 <strong>NeurIPS (Spotlight)</strong> 会议上的SCI论文。</p><h2 id="论文摘要">论文摘要</h2><p>我们提出了一种新颖的神经表面重建方法，称为NeuS，用于从2D图像输入以高保真度重建对象和场景。</p><p>现有的神经表面重建方法，例如DVR [Niemeyer等人，2020] 和IDR [Yariv等人，2020]，需要前景掩模<span style="background:#eef0f4;">(foreground mask)</span>作为监督，容易被困在局部最小值中，并且因此与具有严重自遮挡<span style="background:#eef0f4;">(self-occlusion)</span>或薄结构的对象的重建作斗争。同时，用于新视图合成的最近的神经方法，例如NeRF [Mildenhall等人，2020] 及其变体，使用体积渲染来产生具有优化鲁棒性的神经场景表示，即使对于高度复杂的对象也是如此。但是，很难从这种学习的隐式表示中提取高质量的曲面，因为表示中没有足够的曲面约束<span style="background:#eef0f4;">(surface constraints)</span>。</p><p>在NeuS中，我们建议将曲面表示为有符号距离函数 (SDF) 的零级集<span style="background:#eef0f4;">(zero-level set)</span>，并开发一种新的体渲染方法来训练神经SDF表示。我们观察到传统的体绘制方法会导致表面重建的固有几何误差 (即偏差 <span style="background:#eef0f4;">bias</span>)，因此提出了一种新的公式，该公式在一阶近似中没有偏差，因此<strong>即使没有掩模监督</strong>，也可以实现更准确的表面重建。在DTU数据集和BlendedMVS数据集上的实验表明，NeuS在高质量的表面重建方面的性能优于现有技术，特别是对于具有复杂结构和自遮挡的对象和场景。</p><h2 id="创新点idea">创新点/IDEA</h2><p><strong>NeRF旨在进行新视图合成而不是表面重建</strong>，因此NeRF仅学习体积密度场，因此很难从中提取高质量的表面。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240211204429014.png" alt="image-20240211204429014" /><figcaption aria-hidden="true">image-20240211204429014</figcaption></figure><blockquote><p>图1 : ( a) 表面渲染和体积渲染的图示。(b) 竹花机的玩具实例，其中花机顶部有咬合物。与最先进的方法相比，我们的方法可以处理闭塞并获得更好的重建质量。</p></blockquote><p>针对该问题，该文章提出通过引入由SDF引起的密度分布，应用体渲染方法来学习隐式SDF表示。又因简单地将标准体渲染方法应用于与SDF相关的密度将导致重构表面中的可识别偏差，所以文章提出一种新颖的体绘制方案，以确保在SDF的一阶近似中进行无偏表面重建。</p><p><strong><span style="background:#daf5e9;">给定一组3D对象的有位姿图像{<span class="math inline">\(I_k\)</span>}，我们的目标是重建其表面<span class="math inline">\(S\)</span></span>。表面由神经隐式SDF的零级集表示。为了学习神经网络的权重，我们开发了一种新颖的体渲染方法，用于从隐式SDF渲染图像，并最小化渲染图像与输入图像之间的差异。这种体渲染方法可确保NeuS中的稳健优化，以重建复杂结构的对象。</strong></p><h1 id="相关工作">相关工作</h1><h2 id="classical-multi-view-surface-and-volumetric-reconstruction">Classical Multi-view Surface and Volumetric Reconstruction</h2><ul><li>基于点和表面的重建<ul><li>基于点和表面的重建方法通过利用图像间光度一致性来估计每个像素的深度图，然后将深度图融合为全局密集点云；</li><li>表面重建通常是通过诸如筛选的泊松表面重建之类的方法进行后处理 ；</li><li>总结：重建质量很大程度上依赖于对应匹配的质量，对于没有丰富纹理的对象，匹配对应的困难往往会导致重建结果出现严重的伪影和缺失部分；</li></ul></li><li>体积重建<ul><li>体积重建方法通过从多视图图像中估计体素网格中的占用率和颜色并评估每个体素的颜色一致性来规避显式对应匹配的困难；</li><li>总结：由于可实现的体素分辨率有限，这些方法无法实现高精度；</li></ul></li></ul><h2 id="neural-implicit-representation">Neural Implicit Representation</h2><ul><li>一些方法通过引入归纳偏差来在深度学习框架中强制实现3D理解。这些归纳偏差可以是显式表示，例如体素网格 ，点云 ，网格和隐式表示。</li><li>由于神经网络编码的隐式表示是连续的并且可以实现高空间分辨率，因此最近引起了很多关注。此表示已成功应用于形状表示，新颖的视图合成 和多视图3D重建。</li></ul><h1 id="算法框架">算法框架</h1><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240422182047922.png" alt="image-20240422182047922" /><figcaption aria-hidden="true">image-20240422182047922</figcaption></figure><h2 id="场景表示-scene-representation">场景表示 Scene representation</h2><p><u>第一步是构造从3D模型到图像的渲染方法</u>（在传统图形学大概可称为<u>光栅化</u>）。与NeRF类似（除了将密度 <span class="math inline">\(\sigma\)</span> 替换为 <span class="math inline">\(sdf\)</span> ），定义重建对象的<u>场景</u>由两个函数表示: <span class="math display">\[f:(x,y,z)-&gt;sdf \\c:(x,y,z,view dir)-&gt;color\]</span></p><ul><li><span class="math inline">\(f:\mathbb{R} ^3\rightarrow \mathbb{R}\)</span> 将空间位置<span class="math inline">\(x\in \mathbb{R} ^3\)</span>映射为从<span class="math inline">\(x\)</span>到对象的有符号距离</li><li><span class="math inline">\(c:\mathbb{R} ^3\times \mathbb{S}^2\rightarrow \mathbb{R} ^3\)</span> 对与点<span class="math inline">\(x\in \mathbb{R} ^3\)</span>和视角方向<span class="math inline">\(v\in \mathbb{S} ^2\)</span>相关的颜色进行编码</li></ul><p>这两个函数都由多层感知机MLP编码。重建对象的表面<span class="math inline">\(S\)</span>由SDF的零级集表示（<span style="background:#daf5e9;">SDF函数，即<span class="math inline">\(f(x)\)</span>的零值面表达形状）</span>，即 <span class="math display">\[\tag{1}\label{eq1}S=\left\{ x\in \mathbb{R} ^3|f\left( x \right) =0 \right\} .\]</span> <u>第二步是构造体渲染训练SDF网络。</u>为了应用体渲染方法来训练SDF网络，和传统体渲染不同，引入概率密度函数$_s( f( x ) ) $作为不透明度<code>opacity</code> <span class="math inline">\(\alpha\)</span>/ <code>“密度”</code><span class="math inline">\(\sigma\)</span> ，称为 <strong>S-density</strong> 。其中<span class="math inline">\(f(x)\)</span>是采样点<span class="math inline">\(x\)</span>的<code>符号距离函数SDF</code>，<span class="math inline">\(s\)</span>是一个<strong>trainable parameter</strong>，外层是逻辑密度分布函数(logistic density function): <span class="math display">\[\phi_s(x)=\Phi_s&#39;(x)=\frac{se^{-sx}}{(1+e^{-sx})^2}\tag{2}.\]</span> 逻辑密度分布函数是sigmoid函数 <span class="math inline">\(\Phi_s(x)=\frac{1}{1+e^{-sx}}\)</span> 的一阶导数。<span style="background:#fbd4d0;">原则上，概率密度函数<span class="math inline">\(\phi _S\left( x \right)\)</span>可以是以0为中心的任何单峰 unimodal（即铃形 bell-shaped）密度函数，这里我们选逻辑密度分布是为了计算方便。</span>逻辑密度分布函数<span class="math inline">\(\phi _s\left( x \right)\)</span>的标准差是<span class="math inline">\(1/s\)</span>，也是一个<strong>trainable parameter</strong>，即随着网络收敛<span class="math inline">\(1/s\)</span>趋向于0，同时也意味着此时概率密度函数pdf在射线上的某个空间点达到了峰值。</p><figure><img src="https://pic4.zhimg.com/80/v2-f85d8dcaa50d39f38f7b76a85ce0da4b_1440w.webp" alt="输入图片描述" /><figcaption aria-hidden="true">输入图片描述</figcaption></figure><p>从上图可知，<span class="math inline">\(\phi(x)\)</span>为一个在0点对称的概率密度函数，在0点达到最大值，数据从0点开始衰减。</p><h2 id="体渲染-rendering">体渲染 Rendering</h2><p>给定一个像素，相机中心为<span class="math inline">\(\mathbf{o}\)</span>，光线方向（即射线ray的单位向量）为<span class="math inline">\(\mathbf{v}\)</span>，从相机中心到该像素的射线上一点为<span class="math inline">\(\mathbf{p}(t)=\mathbf{o}+t\mathbf{v}，t\geqslant0\)</span>。该像素颜色<span class="math inline">\(C\left( \mathbf{o},\mathbf{v} \right)\)</span>的一般渲染通式为： <span class="math display">\[C\left( \mathbf{o},\mathbf{v} \right) =\int_0^{+\infty}{w\left( t \right) c\left( \mathbf{p}\left( t \right) ,\mathbf{v} \right) dt}\tag{3}.\]</span> &gt; 该通式其实就是 NeRF 中的未经离散化的体渲染公式： &gt; <span class="math display">\[&gt;   C=\int T(t)c(t)\sigma(t)dt\\&gt;   T(t)=e^{-\int_0^t \sigma(s)dt}\\&gt;   w(t)=T(t)\sigma(t)&gt;   \]</span></p><p><span class="math inline">\(c\left( \mathbf{p}\left( t \right) ,\mathbf{v} \right)\)</span>是在射线上一点<span class="math inline">\(\mathbf{p}\)</span>沿着视角方向<span class="math inline">\(\mathbf{v}\)</span>的颜色。<span class="math inline">\(w(t)\)</span>是关于颜色的权重函数，因此要求<span class="math inline">\(w(t)&gt;0\)</span>且<span class="math inline">\(\int_0^{+\infty}w(t)dt=1\)</span>。</p><p>作者认为从2D图像学习精确的SDF表达的关键是<strong>建立一个合适的基于SDF的权重函数 <span class="math inline">\(w(t)\)</span></strong>，有两点要求：</p><ul><li><strong>Unbiased</strong>：对于相机射线<span class="math inline">\(\mathbf{p}(t)\)</span>和表面surface的交点<span class="math inline">\(\mathbf{p}(t^*)\)</span>，<span class="math inline">\(w(t)\)</span>应在<span class="math inline">\(t^*\)</span>处取得局部最大值（即<u>相机射线</u>和<u>SDF零级集zero-level set（表面surface）</u>的交点处的像素贡献最大）。也就是说，<span class="math inline">\(\frac{dw}{dt}=0\ when \ t=t^*\)</span>。</li><li><strong>Occulusion-aware</strong>：同一条射线上两个深度<span class="math inline">\(t_0\)</span>和<span class="math inline">\(t_1\)</span>，若<span class="math inline">\(f\left( t_0 \right) =f\left( t_1 \right)\)</span>但<span class="math inline">\(t_0&lt;t_1\)</span>，那么应该有<span class="math inline">\(w(t_0)&gt;w(t_1)\)</span>（即考虑自体遮挡，如果一条射线多次交叉表面（穿越多个表面），应该更多地使用最靠近相机的交点的颜色（射线所遇到的第一个表面交点所带来的颜色））。</li></ul><p>作者讨论了两种<span class="math inline">\(w(t)\)</span>表达，最终选择了<span class="math inline">\((b)\)</span>形式。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240212171629587.png" alt="image-20240212171629587" /><figcaption aria-hidden="true">image-20240212171629587</figcaption></figure><blockquote><p>图2</p></blockquote><h3 id="naive的wt颜色权重函数设计">naive的<span class="math inline">\(w(t)\)</span>颜色权重函数设计</h3><p>直接利用先前NeRF中提出的 volume rendering pipeline： <span class="math display">\[w\left( t \right) =T\left( t \right) \sigma \left( t \right)\tag{4}.\]</span> - <span class="math inline">\(\sigma \left( t \right)\)</span>是传统体渲染中的体密度volume density，被设置为与S-density相同，其中 <span class="math inline">\(\mathbf{p}(t)\)</span> 表示采样点的<span class="math inline">\((x,y,x)\)</span>坐标：</p><p><span class="math display">\[\sigma(t)=S-density=\phi_s(f(\mathbf{p}(t))).\]</span></p><ul><li><span class="math inline">\(T(t)=\exp(-\int_0^t\sigma(u){\rm d}u)\)</span>是沿射线的累积透射率accumulated transmittance。</li></ul><p>该方法只能做到occulusion-aware，但是由于它在重建表面引入了固有误差，因此有bias，在射线打到表面之前<span class="math inline">\(w(t)\)</span>已经取得了局部最优，如图2(a)所示（具体推导可参考原文补充材料）。</p><blockquote><p><font color=#4eb434>naive 的权重设计不满足 <strong>unbiased</strong> 的原因推导</font></p><p>下面来算一下： <span class="math display">\[  \frac{dw}{dt}=\frac{dT(t)}{dt}\sigma(t)+T(t)\frac{d \sigma(t)}{dt}\\  =-e^{-\int_0^t \sigma(s)dt}\sigma(t)\sigma(t)+T(t)\frac{d \sigma(t)}{dt}\\  =T(t)(\frac{d \sigma(t)}{dt}-\sigma^2(t))  \]</span> 把符号距离函数的公式代入：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240422161748615.png" alt="image-20240422161748615" /> <span class="math display">\[  \frac{dw(t)}{dt}=T(t^*)(\frac{d\phi_s(f(p(t))}{dt}-\phi_s ^2(f(p(t)))\\  =T(t^*)(\phi_s&#39; (f(p(t))(\bigtriangledown f(p(t).\bold v)-\phi_s ^2(f(p(t)))\\  =T(t^*)(\phi_s&#39; (f(p(t))cos\theta-\phi_s ^2(f(p(t)))  \]</span> 假设<span class="math inline">\(t^*\)</span>为这一射线与曲面第一次相交的点。 <span class="math display">\[  f(p(t^*))=0\\  \phi_s&#39;(f(p(t^*))=0  \]</span> 所以： <span class="math display">\[  \frac{dw(t^*)}{dt^*}=-T(t^*)\phi_s^2(0)&lt;0  \]</span> 从上面的推导可知，在曲面上的一个点，梯度不为 0。因此原naive idea是有问题的。</p></blockquote><h3 id="neus的wt颜色权重函数设计">NeuS的<span class="math inline">\(w(t)\)</span>颜色权重函数设计</h3><p>首先讨论一种straight-forward 方式： <span class="math display">\[w(t) = \frac{ \phi_s(f(\mathbf{p}(t)))}{ \int_0^{+\infty} \phi_s( f( \mathbf{p}(u) ) ) {\rm d}u }. \tag{5}\label{eq5}\]</span> 该方式直接使用归一化的S-density作为权重函数，显然无偏，但是没有occlusion-aware：两个SDF交叉点将在<span class="math inline">\(w(t)\)</span>中产生两个等值的峰，即公式(5)值适用于每条射线与曲线只有一个交点的情况下。</p><p>接下来，我们需要再对其进行改造，使其能够满足occlusion-aware的性质。自然地，NeRF本身基本的公式就是occlusion-aware，文章还是采用了基本公式的逻辑，但是在其基础上做了微小的改动，即<span style="background:#daf5e9;">自行定义了一个不透明的密度函数<span class="math inline">\(\rho(t)\)</span>（opaque density），对应着传统公式中的 <span class="math inline">\(\sigma(t)\)</span></span>，因此，权重函数变为了： <span class="math display">\[w(t)=T(t)\rho(t),\\\,\,T(t)=\exp(-\int_0^t \rho(u){\rm d}u). \tag{6}\label{eq6}\]</span></p><p>注意，这里 <span class="math inline">\(\rho(t) \neq \textcolor{orange}{\phi_s(f(p(t))=S-density=\sigma(t)}\)</span> 。</p><h4 id="rhot的推导思路"><span class="math inline">\(\rho(t)\)</span>的推导思路</h4><p>首先，我们考虑一种简单情况：即只有一个表面交点且表面是平面，此时从<span class="math inline">\(\eqref{eq5}\)</span> <span class="math inline">\(\eqref{eq6}\)</span>式出发导出<span class="math inline">\(\rho(t)\)</span>以<span class="math inline">\(f(\mathbf{p}(t))\)</span>为输入的表达式，然后再推广到多次表明相交的情况。</p><p>在这种简单的特殊情况下，我们很容易地知道符号距离函数sdf，即<span class="math inline">\(f(\mathbf{p}(t))\)</span>有以下的关系： <span class="math display">\[f(\mathbf{p}(t))=-|cos(\theta)|\cdot(t-t^*),\,\,where\,\, f(\mathbf{p}(t^*))=0 \tag{7}.\]</span> 其中，<span class="math inline">\(\theta\)</span>是视角方向<span class="math inline">\(\mathbf{v}\)</span>与外表面法向量<span class="math inline">\(\mathbf{n}\)</span>之间的夹角。由于表面surface被假定为一个平面plane，因此<span class="math inline">\(|cos(\theta)|\)</span>是一个常量。此时根据<span class="math inline">\(\eqref{eq5}\)</span>式有： <span class="math display">\[\begin{aligned}w(t) &amp; =\lim _{t^{*} \rightarrow+\infty} \frac{\phi_{s}(f(\mathbf{p}(t)))}{\int_{0}^{+\infty} \phi_{s}(f(\mathbf{p}(u))) \mathrm{d} u} \\&amp; =\lim _{t^{*} \rightarrow+\infty} \frac{\phi_{s}(f(\mathbf{p}(t)))}{\int_{0}^{+\infty} \phi_{s}\left(-|\cos (\theta)|\left(u-t^{*}\right)\right) \mathrm{d} u} \\&amp; =\lim _{t^{*} \rightarrow+\infty} \frac{\phi_{s}(f(\mathbf{p}(t)))}{\int_{-t^{*}}^{+\infty} \phi_{s}\left(-|\cos (\theta)| u^{*}\right) \mathrm{d} u^{*}} \\&amp; =\lim _{t^{*} \rightarrow+\infty} \frac{\phi_{s}(f(\mathbf{p}(t)))}{|\cos (\theta)|^{-1} \int_{-|\cos (\theta)| t^{*}}^{+\infty} \phi_{s}(\hat{u}) \mathrm{d} \hat{u}} \\&amp; =|\cos (\theta)| \phi_{s}(f(\mathbf{p}(t))) .\end{aligned} \tag{8}\]</span> 又根据<span class="math inline">\(\eqref{eq6}\)</span>式， <span class="math display">\[T(t)\rho(t)=|\cos (\theta)| \phi_{s}(f(\mathbf{p}(t))).\tag{9}\]</span> 已知累积透射率accumulated transmittance <span class="math inline">\(T(t)=\exp(-\int_0^t \rho(u){\rm d}u)\)</span>，则： <span class="math display">\[T\left( t \right) \rho \left( t \right) =-\frac{dT}{dt}\left( t \right).\tag{10.a}\]</span></p><p><span class="math display">\[|\cos (\theta)| \phi_{s}(f(\mathbf{p}(t)))=-\frac{d\varPhi _s}{dt}\left( f\left( \mathbf{p}\left( t \right) \right) \right) .\tag{10.b}\]</span></p><p>从而， <span class="math display">\[\frac{dT}{dt}\left( t \right)=\frac{d\varPhi _s}{dt}\left( f\left( \mathbf{p}\left( t \right) \right) \right)\]</span> 对两边积分得， <span class="math display">\[T\left( t \right) =\varPhi _s\left( f\left( \mathbf{p}\left( t \right) \right) \right)\tag{11}\]</span> 最后取对数，再对两边积分， <span class="math display">\[\int_{0}^{t} \rho(u) \mathrm{d} u=-\ln \left(\Phi_{s}(f(\mathbf{p}(t)))\right) \\\Rightarrow \rho(t)=\frac{-\frac{\mathrm{d} \Phi_{s}}{\mathrm{~d} t}(f(\mathbf{p}(t)))}{\Phi_{s}(f(\mathbf{p}(t)))}. \tag{12}\]</span> 上式是单个surface的情况，当光线在两个surface之间时,<span class="math inline">\(-\frac{d\Phi_s(f(p(t)))}{dt}\)</span> 会变成负。因此为了防止它为负，拓展到多surface的情况时需要加一个为 0 的约束，即： <span class="math display">\[\rho(t)=\max \left(\frac{-\frac{\mathrm{d} \Phi_{s}}{\mathrm{~d} t}(f(\mathbf{p}(t)))}{\Phi_{s}(f(\mathbf{p}(t)))}, 0\right) .\tag{13}\label{eq13}\]</span> 且： <span class="math display">\[C\left( \mathbf{o},\mathbf{v} \right) =\int_0^{+\infty}{w\left( t \right) c\left( \mathbf{p}\left( t \right) ,\mathbf{v} \right) dt} \tag{3}\]</span> 在补充材料中我们给出了在单面交和多面交情况下，由<span class="math inline">\(\eqref{eq13}\eqref{eq6}\)</span>式定义的权重函数在SDF的一阶逼近中是无偏的定理证明。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240213173052919.png" alt="image-20240213173052919" /><figcaption aria-hidden="true">image-20240213173052919</figcaption></figure><blockquote><p>图3 多个表面相交情况下，权重分布图示。由图可见，相交的第一个面的贡献度是最大的，后面进行了衰减，符合 occlusion-aware的要求；且我们设计的<span class="math inline">\(w(t)\)</span>还保证了 SDF的估计是无偏 unbiased 的。</p></blockquote><h3 id="离散化处理-discretization">离散化处理 Discretization</h3><p>跟原始nerf一样，我们推出来的这个连续积分的形式（式(3)），计算机是无法处理的，所以我们需要对这个公式进行离散化处理，方便我们编程。为了获得不透明度opacity相关的 <span class="math inline">\(\rho(t)\)</span>和权重函数<span class="math inline">\(w(t)\)</span>的离散对应部分，我们采用与NeRF中使用的近似方案，该方案沿着射线对<span class="math inline">\(n\)</span>个点进行采样<span class="math inline">\(\{\mathbf{p}_i=\mathbf{o}+t_i\mathbf{v}|i=1,...,n,t_i&lt;t_{i+1}\}\)</span>，以计算射线的近似像素颜色<span class="math inline">\(\hat{C}\)</span>如下： <span class="math display">\[\hat{C}=\sum_{i=1}^n{T_i\alpha _ic_i},\]</span> <span class="math display">\[T_i=\prod_{j=1}^{i-1}{\left( 1-\alpha _j \right)},   \tag{14}\label{eq14}\]</span></p><p><span class="math display">\[\alpha _i=1-\exp \left( -\int_{t_i}^{t_{i+1}}{\rho \left( t \right) dt} \right) =\max \left( \frac{\Phi _s(f(\mathbf{p}(t_i)))-\Phi _s(f(\mathbf{p}(t_{i+1})))}{\Phi _s(f(\mathbf{p}(t_i)))},0 \right).\]</span></p><p>其中<span class="math inline">\(T_i\)</span>是离散的累积透射率accumulated transmittance，<span class="math inline">\(\alpha_i\)</span>是离散的不透明度opacity 值，关于<span class="math inline">\(\alpha_i\)</span>的详细推导在补充材料中给出。</p><blockquote><p><font color=#0091ff>关于离散化公式 (14) 的推导</font></p><p>原始 NeRF 的体渲染公式： <span class="math display">\[  C=\int T(t)c(t)\sigma(t)dt  \]</span> 在离散情况下，我们先把射线分为一段一段，假定每一段内颜色<span class="math inline">\(c(t)\)</span>和密度<span class="math inline">\(\sigma(t)\)</span>是不变的，先算出其每段的不透明度： <span class="math display">\[  \alpha_i=1-e^{\int_{t_i}^{t_{i+1}}\sigma(t)dt}=1-e^{\delta_i \sigma_i} \\  T_{i}=e^ {-\sum_{j=1}^{i-1} \sigma_{j} \delta_{j}}= \prod^{i-1}_{j=1}{(1-\alpha_i)}  \]</span> 最后的颜色预测为： <span class="math display">\[  C=c_0\alpha_0+c_1\alpha_1(1-\alpha_0)+...+c_N\alpha_N(1-\alpha_{N-1})...(1-\alpha_0)  \]</span> NeuS 的方法也是一样，只是把 <span class="math inline">\(\sigma(t)\)</span> 用 <span class="math inline">\(\rho(t)\)</span> 代替： <span class="math display">\[  \begin{align}  \alpha_i&amp;=1-e^{\int_{t_i}^{t_{i+1}}\rho(t)dt}\\  &amp;=1-e^{\int_{t_i}^{t_{i+1}}-\frac{\frac{d\Phi_s(f(p(t)))}{dt}}{\Phi_s(f(p(t)))}dt}\\  &amp;=1-e^{ln\Phi_s(f(p(t))+C|_{t_i}^{t_{i+1}}}\\  &amp;=1-e^{ln\Phi_s(f(p(t_{i+1}))-ln\Phi_s(f(p(t_{i}))}\\  &amp;=1-\frac{\Phi_s(f(p(t_{i+1}))}{\Phi_s(f(p(t_{i}))}\\  &amp;=\frac{\Phi_s(f(p(t_{i}))-\Phi_s(f(p(t_{i+1}))}{\Phi_s(f(p(t_{i}))}  \end{align}  \]</span></p></blockquote><h4 id="离散化的采样方式">离散化的采样方式</h4><p>在真正的实现过程中，实际上有两种采样方式：</p><ol type="1"><li>直接采样射线上的点：<span class="math inline">\(\mathbf{q}_i=\mathbf{o}+t_i\mathbf{v}\)</span></li><li>采样射线上某一小段的中点： <span class="math inline">\(\mathbf{p}_i = \mathbf{o} + \frac{ t_i + t_{i+1} }{2} \mathbf{v}\)</span></li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240213205224343.png" alt="image-20240213205224343" /><figcaption aria-hidden="true">image-20240213205224343</figcaption></figure><blockquote><p>图4 两种采样方式。</p><p>具体而言，当我们<strong>计算<span class="math inline">\(\alpha\)</span>值</strong>时，我们采用section points的方式，计算公式与<span class="math inline">\(\eqref{eq14}\)</span>保持一致，即<span class="math inline">\(\max \left( \frac{\Phi _s(f(\mathbf{q}(t_i)))-\Phi _s(f(\mathbf{q}(t_{i+1})))}{\Phi _s(f(\mathbf{q}(t_i)))},0 \right)\)</span>。当<strong>计算颜色值</strong>时，我们采用mid-point方式。</p></blockquote><h2 id="训练-training">训练 Training</h2><h3 id="损失函数">损失函数</h3><p>随机采样一批(a batch)像素，以及其对应的在世界坐标系下的射线，即： <span class="math display">\[P=\{C_k,M_k,\mathbf{o}_k,\mathbf{v}_k\}\]</span> 其中，<span class="math inline">\(C_k\)</span>是像素颜色，<span class="math inline">\(M_k\in\{0,1\}\)</span>是可选的mask值。采样像素数量（batch size）设为m，一条射线上的采样点数（point sampling size）设为n。损失函数定义为： <span class="math display">\[L=L_{color}+\lambda L_{reg}+\beta L_{mask}.\tag{15}\]</span> 其中，</p><ul><li><span class="math inline">\(L_{color}=\frac{1}{m}\sum_k{R\left( \hat{C}_k,C_k \right)}\)</span> ——渲染图和真实图差异。类似IDR，<span class="math inline">\(R\)</span>采用L1损失，这使得我们的观察对outliers鲁棒且训练稳定</li><li><span class="math inline">\(L_{reg}=\frac{1}{nm}\sum_{k,i}{\left( |\nabla f\left( \mathbf{\hat{p}}_{k,i} \right) |_2-1 \right) ^2}\)</span>——在采样点sample points上加的Eikonal项，用于正则化<span class="math inline">\(f_{\theta}\)</span>的SDF值，即SDF法向量的损失</li><li><span class="math inline">\(L_{mask}=BCE(M_k,\hat{O}_k)\)</span>——可选mask项，BCE（binary cross entropy）即二值交叉熵。其中<span class="math inline">\(\hat{O}_k=\sum_{i=1}^n{T_{k,i}\alpha _k}\)</span>是在相机射线上采样点的权重求和，这里的预测可微分mask就是通过射线上权重<span class="math inline">\(w(t)\)</span>的和求出的。</li></ul><h3 id="分层采样-hierarchical-sampling">分层采样 Hierarchical Sampling</h3><blockquote><p>原来的NeRF采样过程：</p><p>首先均匀采样一组点，然后 coarse importance sampling + fine importance sampling。<strong>coarse sampling</strong> 64个点，其概率值由 <strong>固定的很大的</strong>标准差 <span class="math inline">\(1/s\)</span>定义的<span class="math inline">\(\phi_s(f(x))\)</span>给出；<strong>fine sampling</strong> 64个点，其概率值由 <strong>learned</strong> <span class="math inline">\(1/s\)</span> 定义的 <span class="math inline">\(\phi_s(f(x))\)</span>给出。</p></blockquote><p>基本没怎么改动 NeRF 原来的 stratified sampling 过程（首先，均匀地在射线上进行采样，然后迭代地在粗概率估计峰值处执行重要性采样。）:</p><p>但不同于NeRF同时优化 coarse network和 fine network，这里只训练了一个network。首先均匀采样64个点作为coarse sampling 点，其概率由 <strong>固定的很大的</strong>标准差 <span class="math inline">\(1/s\)</span>定义的S-density <span class="math inline">\(\phi_s(f(x))\)</span>给出；然后利用这组coarse点、其对应的由<strong>固定的很大的</strong>标准差<span class="math inline">\(1/s\)</span>定义的<span class="math inline">\(\phi_s(f(x))\)</span>值和学得的标准差构造pdf，采样64个fine sampling点。</p><h1 id="代码">代码</h1><blockquote><p>代码注释小技巧：在注释代码时，如果想区别原来就有的代码和我们后来加上的代码，可以通过 Ctrl+R 将所有原来就有的注释符号 '#' 改为' '###'。在后面添加的代码注释则都以 '#' 开头。</p><p>代码解析参考链接：<a href="https://blog.csdn.net/yangyu0515/article/details/131608144">【三维重建】【深度学习】NeuS代码Pytorch实现--训练阶段代码解析(上)-CSDN博客</a></p></blockquote><h2 id="运行指令">运行指令</h2><ul><li><strong>Training without mask</strong></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python exp_runner.py --mode train --conf ./confs/womask.conf --<span class="keyword">case</span> &lt;case_name&gt;</span><br></pre></td></tr></table></figure><ul><li><strong>Training with mask</strong></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python exp_runner.py --mode train --conf ./confs/wmask.conf --<span class="keyword">case</span> &lt;case_name&gt;</span><br></pre></td></tr></table></figure><ul><li><strong>Extract surface from trained model</strong>——The corresponding mesh can be found in <code>exp/&lt;case_name&gt;/&lt;exp_name&gt;/meshes/&lt;iter_steps&gt;.ply</code>.</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python exp_runner.py --mode validate_mesh --conf &lt;config_file&gt; --<span class="keyword">case</span> &lt;case_name&gt; --is_continue <span class="comment"># use latest checkpoint</span></span><br></pre></td></tr></table></figure><ul><li><strong>View interpolation</strong>——The corresponding image set of view interpolation can be found in <code>exp/&lt;case_name&gt;/&lt;exp_name&gt;/render/</code>.</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python exp_runner.py --mode interpolate_&lt;img_idx_0&gt;_&lt;img_idx_1&gt; --conf &lt;config_file&gt; --<span class="keyword">case</span> &lt;case_name&gt; --is_continue <span class="comment"># use latest checkpoint</span></span><br></pre></td></tr></table></figure><h2 id="代码结构">代码结构</h2><details><summary>confs /配置文件 configurations</summary><p><span style="color: #800000;">thin_structure.conf    /薄结构的情况</span><br /><span style="color: #800000;">wmask.conf    /有mask</span><br /><span style="color: #800000;">womask.conf    /没有mask</span><br /><span style="color: #800000;">womask_dtu_large_roi.conf    /不知道是什么</span></p></details><details><summary>exp /输出的结果 export</summary><div><strong><span style="font-size: 14pt;">data_BlendedMVS</span></strong></div><div style="padding-left: 40px;"><strong><span style="font-size: 12pt;">bmvs_bear</span></strong></div><div style="padding-left: 80px;"><span style="color: #800000;">wmask/有mask</span></div><div style="padding-left: 120px;"><span style="color: #f1c40f;">logs/日志</span></div><div style="padding-left: 120px;"><span style="color: #f1c40f;">recording/训练记录</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">models</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">dataset.py</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">embedder.py</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">fields.py</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">renderer.py</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">config.conf</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">exp_runner.py</span></div><div style="padding-left: 80px;"><span style="color: #800000;">womask_sphere/无mask</span></div><div style="padding-left: 120px;"><span style="color: #f1c40f;">logs</span></div><div style="padding-left: 120px;"><span style="color: #f1c40f;">recording</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">models</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">dataset.py/加载数据</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">embedder.py</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">fields.py</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">renderer.py</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">config.conf</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">exp_runner.py</span></div></details><details><summary>models /模型</summary><p><span style="color: #800000;">dataset.py    /生成数据集，加载数据集</span><br /><span style="color: #800000;">embedder.py</span><br /><span style="color: #800000;">fields.py /存放了四个用于训练的网络类</span><br /><span style="color: #800000;">renderer.py</span></p></details><details><summary>preprocess_custom_data</summary><p><span style="font-size: 14pt; color: #800000;">aruco_preprocess</span><br /><span style="color: #3598db;">    calibration.cpp</span><br /><span style="color: #3598db;">    CMakeLists.txt</span><br /><span style="color: #3598db;">    gen_cameras.py</span><br /><span style="color: #3598db;">    run.sh</span><br /><span style="font-size: 14pt; color: #800000;">colmap_preprocess</span><br /><span style="color: #3598db;">    colmap_read_model.py</span><br /><span style="color: #3598db;">    colmap_wrapper.py</span><br /><span style="color: #3598db;">    gen_cameras.py</span><br /><span style="color: #3598db;">    imgs2poses.py</span><br /><span style="color: #3598db;">    pose_utils.py</span><br /><span style="font-size: 14pt; color: #800000;">static</span><br /><span style="color: #3598db;">    aruco_board<span style="color: #3598db;">.png</span><br /><span style="color: #3598db;">    interest_sparse_points.png</span><br /><span style="color: #3598db;">    raw_sparse_points.png</span><br /><span style="font-size: 14pt; color: #800000;">readme.md</span></span></p></details><details><summary>static</summary><p><span style="color: #800000;">intro_1_compressed.gif</span><br /><span style="color: #800000;">intro_2_compressed.gif</span></p></details><p>exp_runner.py /主程序：包含训练、验证、测试和预测<br> README.md /项目说明<br> requirements.txt /需要的python依赖包列表<br> public_data<br> .gitignore /告诉Git忽略哪些文件<br> LICENSE /许可类型<br></p><h2 id="主程序-exp_runner.py">主程序 exp_runner.py</h2><p>主要分为两部分：</p><ul><li><p><strong>class Runner</strong></p><p>|<span style="background:#daf5e9;">def __init__</span> /初始化函数</p><div class="line-block">  |self.Networks /同时将其加入 params_to _train(来自 /models/fields.py)</div><div class="line-block">  | |nerf_outside—NeRF</div><div class="line-block">  | |sdf_network—SDFNetwork</div><div class="line-block">  | |deviation_network—SingleVarianceNetwork</div><div class="line-block">  | |color_network—RenderingNetwork</div><div class="line-block">  |self.renderer /neus渲染器,继承自/models/renderer.py</div><div class="line-block">  | |self.nerf_outside</div><div class="line-block">  | |self.sdf_network</div><div class="line-block">  | |self.deviation_network</div><div class="line-block">  | |self.color_network</div><div class="line-block">  | |**self.conf['model.neus_renderer']</div><p>|<span style="background:#f9eda6;">def train(self)</span> /当mode为'train'时执行训练函数</p><p>|<span style="background:#daf5e9;">def get_image_perm(self)</span> /将图片打乱顺序</p><p>|<span style="background:#f9eda6;">def get_cos_anneal_ratio(self)</span></p><p>|<span style="background:#daf5e9;">def update_learning_rate(self)</span> /更新学习率</p><p>|<span style="background:#f9eda6;">def file_backup(self)</span></p><p>|<span style="background:#daf5e9;">def load_checkpoint(self, checkpoint_name)</span></p><p>|<span style="background:#f9eda6;">def save_checkpoint(self)</span> /存档中间结果</p><p>|<span style="background:#daf5e9;">def validate_image(self, idx=-1, resolution_level=-1)</span></p><p>|<span style="background:#f9eda6;">def render_novel_image</span> /渲染新图片</p><p>|<span style="background:#daf5e9;">def validate_mesh</span> /当mode为'validate_mesh'时执行</p><p>|<span style="background:#f9eda6;">def interpolate_view(self, img_idx_0, img_idx_1)</span> /当mode以'interpolate'开头时执行</p></li><li><p><strong>if __name__ == '__main__'</strong></p><p>|print开始提示信息</p><p>|log输出格式</p><p>|创建arg = parser.parse_args</p><div class="line-block">  |conf</div><div class="line-block">  |mode</div><div class="line-block">  |mcube_threshold</div><div class="line-block">  |is_continue</div><div class="line-block">  |gpu</div><div class="line-block">  |case: 样本集名称，默认值为空，需要由运行指令传入</div><p>|创建runner = Runner(args.xxx,...)，传入arg参数</p><p>|用if循环判断args.mode值以执行不同runner类的函数</p></li></ul><h2 id="配置文件夹confs">配置文件夹confs</h2><p>配置文件：主要是 general、dataset、train 和 model 四部分内容</p><ul><li><p>|<span style="background:#daf5e9;">general</span></p><p>|<span style="background:#daf5e9;">dataset</span></p><p>|<span style="background:#daf5e9;">train</span></p><p>|<span style="background:#daf5e9;">model</span></p><div class="line-block">  |nerf</div><div class="line-block">  |sdf_network</div><div class="line-block">  |variance_network</div><div class="line-block">  |rendering_network</div><div class="line-block">  |neus_renderer</div></li></ul><h2 id="models-模型文件夹">models 模型文件夹</h2><h3 id="fields.py">fields.py</h3><p>fields.py 中存放了从IDR（https://github.com/lioryariv/idr）中借鉴的两个网络 <span style="background:#daf5e9;">SDFNetwork</span> 和 <span style="background:#daf5e9;">RenderingNetwork</span> ，从nerf-pytorch（https://github.com/yenchenlin/nerf-pytorch）中借鉴的网络 <span style="background:#f9eda6;">NeRF</span> 和最后一个网络 <span style="background:#dad5e9;">SingleVarianceNetwork</span> 。</p><h3 id="renderer.py">renderer.py</h3><ul><li><p>|def extract_fields</p><p>|def extract_geometry</p><p>|def sample_pdf</p><p>|class NeuSRenderer</p></li></ul><h3 id="dataset.py">dataset.py</h3><ul><li><p>|def load_K_Rt_from_P</p><p>|class Dataset /用于加载处理数据data</p><div class="line-block">  |def __init__</div><div class="line-block">  |def gen_rays_at /Generate rays at world space from one camera</div><div class="line-block">  |def gen_random_rays_at /Generate random rays at world space from one camera</div><div class="line-block">  |def gen_rays_between /Interpolate pose between two cameras.</div><div class="line-block">  |def near_far_from_sphere</div><div class="line-block">  |def image_at</div></li></ul><h1 id="bib-citation">Bib Citation</h1><figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">@article&#123;wang2021neus,</span><br><span class="line">      title=&#123;NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction&#125;, </span><br><span class="line">      author=&#123;Peng Wang and Lingjie Liu and Yuan Liu and Christian Theobalt and Taku Komura and Wenping Wang&#125;,</span><br><span class="line">  journal=&#123;NeurIPS&#125;,</span><br><span class="line">      year=&#123;2021&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="参考文献">参考文献</h1><p><strong>[1] Wang P, Liu L, Liu Y, et al. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction[J]. arixv preprint arixv:2106.10689, 2021.</strong></p><h2 id="参考网址">参考网址：</h2><ul><li><p>https://www.jianguoyun.com/p/DWJ0GRYQx_2EDBjAi6kFIAA</p></li><li><p><a href="https://blog.csdn.net/pylittlebrat/article/details/127503069">Neus学习笔记-CSDN博客</a></p></li><li><p><a href="https://longtimenohack.com/posts/paper_reading/2021_wang_neus/">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction - Jianfei Guo (longtimenohack.com)</a></p></li><li><p><a href="https://blog.csdn.net/flow_specter/article/details/126222914">论文笔记：NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction_neus 论文解读-CSDN博客</a></p></li><li><p>[<a href="https://zhuanlan.zhihu.com/p/496752239">论文笔记]NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction - 知乎 (zhihu.com)</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NIPS2021 </tag>
            
            <tag> volume rendering </tag>
            
            <tag> neural surface reconstruction </tag>
            
            <tag> SDF </tag>
            
            <tag> 三维重建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>长江后浪扑前浪，NeRF现在沙滩上</title>
      <link href="/2024/03/05/NeRF/"/>
      <url>/2024/03/05/NeRF/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文主页： <a href="https://www.matthewtancik.com/nerf">NeRF: Neural Radiance Fields (matthewtancik.com)</a></p><p>参考链接：</p><p><a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODA3MDkyMA==&amp;mid=2247485459&amp;idx=1&amp;sn=3228ed101ae9ece2f65021cfb3aeccd4&amp;chksm=cf81fe74f8f67762b2d02998a6d9f76c862b13b78ff808655639f27441f567721f7ccf72e24d&amp;cur_album_id=2737490560171376640&amp;scene=189#wechat_redirect">NeRF入门之神经辐射场全貌 (qq.com)</a> （本文大多内容直接抄的这个大大的，建议自己亲自看一遍，强烈推荐！！！本文仅做记录）</p><p>[<a href="https://blog.csdn.net/g11d111/article/details/118959540">NeRF]代码+逻辑详细分析_nerf代码解读 csdn-CSDN博客</a></p></blockquote><h1 id="引入">引入</h1><p>NeRF 全称是 Neural Radiance Field (神经辐射场)，它是想做这样一件事情：</p><p><strong>给定一个场景，输入相机 (或者观察者) 的位置和朝向后，输出这个视角下的视图——新视图合成</strong></p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/640" /></p><p>更详细地说，NeRF 是一种用神经网络来渲染三维场景的技术。它可以在提供某几个角度的图像的前提下，生成其他角度下的图像，间接重建出场景的三维信息。</p><p>为什么要学习这个东西呢？</p><p>已故计算机视觉先驱 David Marr 曾经把计算机视觉终极问题定义为：<strong>从二维图像重建出三维物体的位置和形状。</strong></p><p>在 Marr 看来，只有当计算机可以学习到三维信息，才能说明计算机已经学会了视觉。</p><p>按照他的理论，CV 中的分类、检测、分割等任务，只能称为「模式识别」问题，而像 NeRF 这类三维重建任务，才能称为真正的计算机视觉。</p><p>NeRF 早在 2020 年就出现了，并在当年的 ECCV 会议上被提名最佳论文。由于 NeRF 优秀的 3D 建模能力，它可以做很多事情。</p><p>计算景深：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf1x.gif" /></p><p>设置对焦：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf2x.gif" /></p><p>重建三维场景（不是真正意义上，仅仅提供新视图）：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf3.gif" /></p><p>设置光影变换：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf4.gif" /></p><h1 id="ray-marching-体渲染">ray-marching 体渲染</h1><blockquote><p>参考链接：<a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODA3MDkyMA==&amp;mid=2247485148&amp;idx=1&amp;sn=4e6acc1b358e624ac2301729d35cc89d&amp;chksm=cf81f0bbf8f679ad3f990f91d5c84fcc67cc6948c3cbc0c34c59874f7795e9263f2382979414&amp;token=400584181&amp;lang=zh_CN#rd">NeRF入门之体渲染 (Volume Rendering) (qq.com)</a></p></blockquote><p><strong>体渲染（ Volume Rendering）能够对光线进行建模，计算出像素的光线强度/颜色值。</strong></p><p>不管是相机还是人眼，像素的诞生都是光线打在感光器件 (视锥细胞) 上的结果。以最经典的小孔成像为例，光线穿过孔洞后，打在屏幕上形成像素：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf5.png" /></p><p>因此，只要对光线渲染过程进行建模，计算出最终的像素值就可以了。当然啦，真实的相机中为了获得更高的信噪比，用的是镜头成像，不过，在 NeRF 中，小孔成像模型就足够建模整个渲染过程了。</p><p>而对光线进行建模，正好是<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODA3MDkyMA==&amp;mid=2247485148&amp;idx=1&amp;sn=4e6acc1b358e624ac2301729d35cc89d&amp;chksm=cf81f0bbf8f679ad3f990f91d5c84fcc67cc6948c3cbc0c34c59874f7795e9263f2382979414&amp;scene=21#wechat_redirect">体渲染</a>的拿手好戏。</p><p>ray-marching 体渲染把每一条光路建模成由一个个粒子构成。光子在光路传播中，可能因为粒子的遮挡损失能量，导致入射光减弱 (比如气体会遮挡部分光线，不透明固体则会遮挡所有光线)。光路中的粒子也可能本身会发光，抑或是周围光路的光子被弹射到当前光路，导致光线增强。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf6.png" /></p><p>在计算机中，由于计算资源限制，不可能建模出所有粒子的状态，因此通常会在每条光路上采样一些点，由这些离散采样点上的粒子来近似整条连续的光线。</p><h2 id="透射比-transmittance">透射比 transmittance</h2><p><strong>透射比</strong>（transmittance）的公式如下： <span class="math display">\[T(s)=\frac{I_{o}}{I_{i}}=\exp \left(-\int_{i}^{o} \tau_{a}(t) d t\right)\]</span> 其中，<span class="math inline">\(I_o\)</span>表示入射光辐射强度/颜色，<span class="math inline">\(I_i\)</span>表示出射光辐射强度/颜色。由上述公式知，<strong>透射比表示粒子群某一点的透明度，数值越大，说明粒子群越透明，光线衰减的幅度就越小</strong>。</p><p>而透明度本身是关于$_a( t ) $的方程， $_a( t ) <span class="math inline">\(越大，\)</span>T(s)$就越小。由公式 <span class="math inline">\(\tau _a\left( t \right) =\rho \left( t \right) A\)</span> 可以知道，它是由粒子密度<span class="math inline">\(\rho \left( t \right)\)</span>和粒子垂直光线方向的投影面积<span class="math inline">\(A\)</span>决定的。这在直觉上也很好理解，如果粒子密度大，粒子本身也比较大，那么遮住光线的概率也会相应提升，自然透明度也就下降了。</p><p><span class="math inline">\(\tau _a\left( t \right)\)</span>也被称为<strong>光学厚度 (optical depth)</strong>。下图为不同光学厚度下，光线透射度对比。</p><figure><img src="https://mmbiz.qpic.cn/mmbiz_jpg/bdpnCavfx2pcmvCruJHc0c3l8penibibHHg7jWW7W01Vb3hOHbB7UCjBJgQSdGX4j6GiaclqUQ0aIMsqibMhKNZ0lw/640?wx_fmt=jpeg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="不同光学厚度下，光线透射度对比" /><figcaption aria-hidden="true">不同光学厚度下，光线透射度对比</figcaption></figure><h2 id="体渲染公式">体渲染公式</h2><p><strong>最终每条光线渲染出来的颜色值都可以用下面的公式表示</strong> (该公式的详细说明与推导见：<a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODA3MDkyMA==&amp;mid=2247485148&amp;idx=1&amp;sn=4e6acc1b358e624ac2301729d35cc89d&amp;chksm=cf81f0bbf8f679ad3f990f91d5c84fcc67cc6948c3cbc0c34c59874f7795e9263f2382979414&amp;token=400584181&amp;lang=zh_CN#rd">NeRF入门之体渲染 (Volume Rendering) (qq.com)</a>)： <span class="math display">\[\begin{align}&amp;\hat{C}(\mathbf{r})=\sum_{i=1}^{N} T_{i}\left(1-\exp \left(-\sigma_{i} \delta_{i}\right)\right) \mathbf{c}_{i},\\&amp;\text { where } \  T_{i}=\exp \left(-\sum_{j=1}^{i-1} \sigma_{j} \delta_{j}\right)= \prod^{i-1}_{j=1}{(1-\alpha_i)},\\&amp; \ \ \ \ \ \ \ \ \ \ \ \ \ \alpha_i= 1-\exp \left(-\sigma_{i} \delta_{i}\right),\tag{1}\label{eq1}\end{align}\]</span> 这里的<span class="math inline">\(\sigma_i\)</span>可以理解为是第 i 个采样点处粒子的密度，<span class="math inline">\(\delta_i\)</span>是相邻采样点的距离，<span class="math inline">\(c_i\)</span>表示采样点处粒子发射的光线强度 (也可以理解为颜色)，<span class="math inline">\(T_i\)</span>则表示从第 i 个采样点到光线终点的透射比。从公式可知一共采样了<span class="math inline">\(N\)</span>个点。</p><p>公式计算结果<span class="math inline">\(\hat{C}(\mathbf{r})\)</span>就是这条光线最终渲染得到的像素值。</p><p>以上就是体渲染的精髓了。<strong>在传统的计算机图形学中，我们需要先知道整个场景中，每条光线上的每个采样点的粒子状态</strong> (即<span class="math inline">\(\sigma_i\)</span>这些数值，在NeRF中是颜色<span class="math inline">\(RGB\)</span>和密度<span class="math inline">\(\sigma\)</span>)，<strong>才能渲染出整个画面</strong>。但如何计算这些粒子的状态，本身是一件很复杂的事情。</p><p>那 NeRF 做了什么呢？</p><p>没错，它就是用来计算这些数值的。</p><h1 id="神经辐射场">神经辐射场</h1><p>在 NeRF 中，作者做了一个这样的尝试，既然体渲染中这些粒子状态这么复杂，那有没有可能让神经网络自己去学出来呢？</p><p>事实证明，这是可以的，而且效果相当好。</p><p>这个过程是这样的：</p><p><strong>给定相机位置和朝向后，我们可以确定出当前的成像平面。然后，将相机的位置坐标和平面上的某个像素相连，就确定了一条光线 (也即确定了光线的方向)。接着用网络预测出光线上每个采样点的粒子信息，就可以确定像素颜色。这个过程重复下去，直到每个像素都渲染完为止。</strong></p><p>这些排列整齐的光线，构成了类似磁场一样的东西，而光线本身就是一种辐射，因此叫辐射场。而每条光线上的粒子信息又都是由神经网络预测的，因此作者又给整个过程命名为<strong>神经辐射场</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf7.gif" /></p><h1 id="nerf-全貌">NeRF 全貌</h1><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf8.png" /></p><blockquote><p>NeRF 的训练流程图</p></blockquote><p><span style="background:#dad5e9;"><strong>NeRF 的整个模型可以被概括为以下函数：</strong></span> <span class="math display">\[f:(x,y,z,\theta,\phi) \rightarrow (c,\sigma)\]</span> 首先，选择一个特定的场景 (比如上图的乐高玩具)。</p><p>然后，在这个场景的四周摆放一些相机，并确定好相机位置<span class="math inline">\((x,y,z)\)</span>和光线方向 <span class="math inline">\((\theta,\phi)\)</span>。</p><p>沿着光线方向乘以不同的采样距离，可以确定光线上每个采样点的位置。<strong>注意，此时每个采样点对应的方向向量会转换成笛卡尔坐标系 (x', y', z')</strong>。</p><p>将相机位置以及方向向量送入网络后，让网络预测出光线上采样点的粒子信息<span class="math inline">\((RGB,\sigma)\)</span>，根据公式 <span class="math inline">\(\eqref{eq1}\)</span> 渲染出整个画面。</p><p>将模型渲染的结果和相机捕捉的真实结果计算损失 (均方误差)，由于公式<span class="math inline">\(\eqref{eq1}\)</span>是可导的，因此梯度可以正常回传，从而训练整个网络。</p><p>在预测的时候，我们直接将相机位置等参数输入网络，让网络计算出每根光线上的粒子信息后，便可以渲染出任意视角下的画面了。</p><p>以上便是 NeRF 的整个算法流程，是一种很经典的<strong>神经渲染</strong>方法。</p><blockquote><p><span style="background:#daf5e9;"><strong>需要注意一点，原始的 NeRF 只能针对一个三维场景进行重建</strong></span>，之后有不少论文对这一点做了改进，我们以后再聊。</p></blockquote><p>下面再介绍一下 NeRF 所使用的网络结构，以及其中用到的两个技巧。</p><h1 id="网络结构">网络结构</h1><p>NeRF 的网络结构是一个很简单的全连接网络。</p><p>作者把密度<span class="math inline">\(\sigma\)</span>和颜色<span class="math inline">\(RGB\)</span>分为两部分输出，这么做的考量在于，粒子密度是跟三维场景本身更强相关的属性，不管观察的方向怎么变，它都不会有太大的变化 (即论文中提到的 <u>multiview consistent</u>)，而颜色值在不同观察方向下，受光照影响，可能会发生大的变动，即它受相机位置和观察方向的影响都更大。</p><p>基于此，论文先把相机位置<span class="math inline">\((x,y,z)\)</span>经过位置编码 Positional encoding (下文会提及) 后拓展成 60 维的向量，送入一堆全连接网络 (FC 层, Full Connect) 后，分叉出两路，其中一路经过一个 FC 层和 ReLU 激活后输出<span class="math inline">\(\sigma\)</span>值；另一路和方向向量经过位置编码的向量 concat 后，一起送入之后的全连接网络，最后经过 Sigmoid 激活输出颜色值 <span class="math inline">\(RGB\)</span>。</p><blockquote><p>相机位置向量 <span class="math inline">\(\gamma(x)\)</span> 和方向向量 <span class="math inline">\(\gamma(d)\)</span> 都是三维。由于位置编码选择的参数<span class="math inline">\(L\)</span>不同，因此位置向量 <span class="math inline">\(\gamma(x)\)</span> 经过 positional encoding 后是 60 维，方向向量 <span class="math inline">\(\gamma(d)\)</span> 在 encoding 后则是 24 维。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf9.png" /></p><p>网络输出的<span class="math inline">\(\sigma\)</span>和<span class="math inline">\(RGB\)</span>就是一条光线上一个采样点对应的粒子密度和颜色值。在整个训练过程中，我们会收集所有光线上所有采样点的粒子密度和颜色，再根据体渲染得到预测的投影视图，然后和真实的投影视图计算 loss 来训练网络。</p><p>不过，仅靠这个网络训练的结果并不够逼真，论文使用了两个 trick 进一步优化。</p><h1 id="trick-1-位置编码-positional-encoding">trick 1: 位置编码 positional encoding</h1><p>第一个 trick 是位置编码 (positional encoding)。</p><p>它来自之前一些研究的实验发现：<strong>如果给网络输入的数据维度越高 (越高频)，那网络也能输出更加高频的信号 (即图像清晰度越好)</strong>。所谓“高频”可以理解为<u>参数的微小变化可导致编码结果的巨大差异</u>。</p><p><strong>由于<span class="math inline">\((x,y,z)\)</span>这类位置和方向向量只有三维</strong>，如果直接将它们投喂给网络，那网络也只能回馈给你低维度的信号，而如果能把输入拓展成更高维，那网络的输出信号会包含更高维的信息，图像内容会更丰富。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf10.png" /></p><blockquote><p>图：使用 positional encoding 的结果对比</p></blockquote><p>论文使用三角函数对输入向量的每个元素进行扩展 <span class="math display">\[\gamma(p)=\left(\sin \left(2^{0} \pi p\right), \cos \left(2^{0} \pi p\right), \cdots, \sin \left(2^{L-1} \pi p\right), \cos \left(2^{L-1} \pi p\right)\right)\tag{2}\]</span> 这个公式可以使用以下 pytorch 代码实现：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">positional_encoding</span>(<span class="params">inputs, L=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    inputs: 输入向量，包含(x,y,z)三个坐标</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    L = <span class="number">10</span></span><br><span class="line">    <span class="comment"># freq_bands: [2^0, 2^1, ..., 2^(L-1)]</span></span><br><span class="line">    freq_bands = <span class="number">2</span> ** torch.linespace(<span class="number">0</span>, L-<span class="number">1</span>, L)</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> freq <span class="keyword">in</span> freq_bands:</span><br><span class="line">        <span class="comment"># [sin(2^f \pi x), sin(2^f \pi y), sin(2^f \pi z)]</span></span><br><span class="line">        outputs += [torch.sin(freq * inputs)]</span><br><span class="line">        <span class="comment"># [cos(2^f \pi x), cos(2^f \pi y), cos(2^f \pi z)]</span></span><br><span class="line">        outputs += [torch.cos(freq * inputs)]</span><br><span class="line">    <span class="comment"># [sin(2^0 \pi x), sin(2^0 \pi y), sin(2^0 \pi z), cos(2^0 \pi x), cos(2^0 \pi y), cos(2^0 \pi z), ...]</span></span><br><span class="line">    outputs = torch.cat(outputs, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><p>对于位置向量<span class="math inline">\((x,y,z)\)</span>，论文选取的<span class="math inline">\(L=10\)</span>，即每个元素会拓展成 20 维，所以在之前的网络输入中，输入是 3*20 即 60 维的向量。而方向向量的<span class="math inline">\(L=4\)</span>，每个元素会拓展为 8 维，对应的位置编码 positional encoding 则是 24 维 (3*8)。</p><p>NeRF 采用的位置编码和 transformer 中的位置编码几乎一样，不过二者的目的不同：transformer 的位置编码是为了给模型提供序列中每个元素的位置信息，而 NeRF 纯粹是为了帮助网络拟合更高清的图像。</p><h1 id="trick-2-分层采样-hierarchical-sampling">trick 2: 分层采样 hierarchical sampling</h1><p>分层采样 (hierarchical sampling) 是另一个重要的 trick。</p><p>前面提到，NeRF 会对每根光线进行采样，然后用网络对每个采样点进行预测。而由于资源的限制，采样不可能做到很密集。<strong>在实际情况中，粒子在空间中的分布也是不均匀的，有些采样点可能粒子密度很高，有些则密度几乎为 0</strong>。</p><p>因此，在密度高的地方多放一些采样点是比较合适的，即做<strong>重要性采样 importance sampling</strong>。论文为此设计了一种由粗到细的分层采样方法：</p><ol type="1"><li>先在每条光线上均匀采样 64 个点，让网络预测其体密度；</li><li>然后根据“密度大的地方多采点”的想法，再次采样，并计算预测密度；</li><li>两轮预测的密度结果分别计算 loss.</li></ol><p>具体实施流程：</p><p>首先是<strong>粗粒度采样</strong>。论文在每根光线上<u>均匀采样</u> 64 个点，然后让网络预测出每个点的信息，包括颜色值 <span class="math inline">\(c_i\)</span> 、粒子密度 <span class="math inline">\(\sigma_i\)</span> 和透射比 <span class="math inline">\(T_i\)</span> 。其中，<span class="math inline">\(\sigma_i\)</span> 和 <span class="math inline">\(T_i\)</span> 可以整合成一个权重 <span class="math inline">\(w_i\)</span> : <span class="math display">\[\hat{C}_{c}(\mathbf{r})=\sum_{i=1}^{N_{c}} w_{i} c_{i}, \quad w_{i}=T_{i}\left(1-\exp \left(-\sigma_{i} \delta_{i}\right)\right)\tag{3}\]</span></p><blockquote><p>体渲染公式可以理解为采样点颜色的累积。</p><p><strong><span class="math inline">\(w_i\)</span>的值也是先大后小</strong>：在靠近物体表面时，透射比 <span class="math inline">\(T_i\)</span>逐渐变小，<span class="math inline">\(\sigma_i\)</span>逐渐变大，但是整体看 <span class="math inline">\(w_i\)</span>是增大的，<strong>即越靠近表面越不被遮挡的点贡献越大</strong>；远离物体时同理。</p></blockquote><p>怎么理解这里的 <span class="math inline">\(w_i\)</span> 呢？光线最终的辐射强度 (或者说颜色) 其实是由光线上每个点的粒子辐射<strong>累加</strong>而成的。既然是累加，那就可以简单地把 <span class="math inline">\(w_i\)</span> 理解为是每个采样点的粒子对最终成像的贡献度，或者说粒子的浓度。 <span class="math inline">\(w_i\)</span> 大的地方，证明这个区域粒子很浓，信息量大，需要重点采样。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf11.png" /></p><blockquote><p>图：粗粒度均匀采样</p></blockquote><p>那下面就是在第一轮预测的基础上，找出 <span class="math inline">\(w_i\)</span> 比较大的地方再重点采样，也即<strong>细粒度采样</strong>。</p><p>这个过程是这样，先将 <span class="math inline">\(w_i\)</span> 归一化：$<em>{i}=w</em>{i} / <em>{j=1}^{N</em>{c}} w_{j} $ （<span class="math inline">\(N_c\)</span> 表示粗粒度采样的点数），这样 <span class="math inline">\(\hat{w}_{i}\)</span> 就可以视为一个概率分布了。然后，为了在 <span class="math inline">\(\hat{w}_{i}\)</span> 数值更大的地方重采样，我们可以把 <span class="math inline">\(\hat{w}_{i}\)</span> 累加起来，求出<code>概率累积直方图 (cdf)</code>，然后<u>在 cdf 的<strong>纵坐标</strong>上再均匀采样</u>，这样，大部分采样点会对应到 <span class="math inline">\(\hat{w}_{i}\)</span> 更大的直方图，这个直方图的横坐标就是细粒度采样点。这个过程也称为<code>逆变换采样 (inverse transform sampling)</code>。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf12.png" /></p><blockquote><p>图：逆变换采样</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf13.png" /></p><blockquote><p>在粗粒度采样的基础上进行细粒度采样</p></blockquote><h2 id="实现整个分层采样的过程">实现整个分层采样的过程</h2><h1 id="实验效果">实验效果</h1><p>NeRF 这类新视图生成任务的主要评价标准，就是看生成的图片逼真度如何。因此评价指标采用的是图像复原中常用的 PSNR 、 SSIM 和 LPIPS 等衡量方法。</p><p>下图是 NeRF 和之前几种方法的对比结果：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf14.png" /></p><p>NeRF 之所以诞生后受到很大的关注，就在于它相比之前的方法提升太多了，对于 PSNR 这类指标来说，能提升两三个点证明方法本身已经取得质的突破，更何况 NeRF 在有些数据集上提升了五六个点！</p><p>对比方法中的 LLFF 和 NeRF 是同一个作者，所以人家也是在这个领域默默耕耘了很久。</p><p>效果对比图就不用说了，秒杀般的存在：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf15.png" /></p><h2 id="深度图">深度图</h2><p>NeRF 有一个副产品是，它可以输出场景的深度图。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf1x.gif" /></p><p>深度图代表的是场景中的物体离相机的距离。NeRF 输出深度图的方法是把光线上每个采样点的权重 <span class="math inline">\(w_i\)</span> 根据距离的远近累加起来：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># weights是光线上各个采样点的权重，z_vals是采样点离相机的距离</span></span><br><span class="line">depth_map = torch.<span class="built_in">sum</span>(weights*z_vals, -<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="为什么会有这种效果">为什么会有这种效果？</h3><h1 id="nerf与三维重建">NeRF与三维重建</h1><p>NeRF 可以预测三维场景中每个视角的二维投影信息，因此，它本身也具备了整个三维场景的信息，可以认为是一种三维重建算法。</p><p>但它跟我们以往认知的三维重建又有所区别。</p><p>最关键的一点，<strong>它并没有显示地学习出整个三维场景的结构信息，而是把整个三维场景的信息编码进了神经网络中</strong>。这个神经网络建立了观察视角与三维模型之间的映射，它就像一个查找表，通过输入观察视角，找出对应视角下三维场景的粒子和反射光等信息。</p><p>如果想通过 NeRF 直接获得三维模型，抱歉，办不到，NeRF 只能给你二维投影，需要你自己用这些投影图像去合成真正的三维模型。</p><p>在 NeRF 中存储的是一种很像点云的「雾」，只有在你观察它的时候，它才会给你具体某个投影面的信息，从这个角度想，这团雾又很像是「量子态」的。</p><h1 id="nerf的缺陷">NeRF的缺陷</h1><p>NeRF 相比之前的工作，最大的优势就是渲染出来的图像清晰度更高，更真实。但它也存在几个严重的命门。</p><p><strong>首先，NeRF 的渲染速度极其慢</strong>。假设要渲染一张 1024x1024 的图片，且每根光线的采样点为 128，那总共需要跑 1024x1024x128=134217728 次网络。我的天，家里没矿可不敢这么玩。</p><p><strong>其次，NeRF 只适用于一个场景</strong>，如果换了别的场景，就得重新训练了，这泛化能力约等于没有。</p><p>不过，作为奠基之作，NeRF 已经开了个好头，剩下的问题自然有追随者帮忙解决。在之后的文章中，我们会逐一看到那些精妙的破解之法。</p><h1 id="代码">代码</h1><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf9.png" /></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NeRF</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, D=<span class="number">8</span>, W=<span class="number">256</span>, input_ch=<span class="number">3</span>, input_ch_views=<span class="number">3</span>, output_ch=<span class="number">4</span>, skips=[<span class="number">4</span>], use_viewdirs=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(NeRF, self).__init__()</span><br><span class="line">        self.D = D</span><br><span class="line">        self.W = W</span><br><span class="line">        <span class="comment">## Position Encoding之后的 位置vector通道数（63）</span></span><br><span class="line">        self.input_ch = input_ch  </span><br><span class="line">        <span class="comment">## Position Encoding之后的 direction的vector通道数（27）</span></span><br><span class="line">        self.input_ch_views = input_ch_views</span><br><span class="line">        self.skips = skips   <span class="comment">## 在第4层有跳跃连接</span></span><br><span class="line">        self.use_viewdirs = use_viewdirs</span><br><span class="line">        </span><br><span class="line">        <span class="comment">## 前8层的MLP实现：输入为63，输出为 256</span></span><br><span class="line">        self.pts_linears = nn.ModuleList(</span><br><span class="line">            [nn.Linear(input_ch, W)] + [nn.Linear(W, W) <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> self.skips <span class="keyword">else</span> nn.Linear(W + input_ch, W) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(D-<span class="number">1</span>)])</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### 构建了第9层的输入为 第8层的输出 和 direction 进行concat,输出为128 维</span></span><br><span class="line">        self.views_linears = nn.ModuleList([nn.Linear(input_ch_views + W, W//<span class="number">2</span>)])</span><br><span class="line">   </span><br><span class="line">        <span class="keyword">if</span> use_viewdirs:</span><br><span class="line">            self.feature_linear = nn.Linear(W, W) <span class="comment"># 第9层 输出256维的向量</span></span><br><span class="line">            self.alpha_linear = nn.Linear(W, <span class="number">1</span>) <span class="comment"># 第9层输出 density alpha(1维)</span></span><br><span class="line">            self.rgb_linear = nn.Linear(W//<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.output_linear = nn.Linear(W, output_ch)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        input_pts, input_views = torch.split(x, [self.input_ch, self.input_ch_views], dim=-<span class="number">1</span>)</span><br><span class="line">        h = input_pts</span><br><span class="line">        <span class="keyword">for</span> i, l <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.pts_linears):</span><br><span class="line">            h = self.pts_linears[i](h)</span><br><span class="line">            h = F.relu(h)</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> self.skips:</span><br><span class="line">                h = torch.cat([input_pts, h], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.use_viewdirs:</span><br><span class="line">            alpha = self.alpha_linear(h)<span class="comment">#256—&gt;1</span></span><br><span class="line">            feature = self.feature_linear(h)<span class="comment">## 256—&gt;256</span></span><br><span class="line">            h = torch.cat([feature, input_views], -<span class="number">1</span>) <span class="comment">#第9层concat direction 向量</span></span><br><span class="line">        </span><br><span class="line">            <span class="keyword">for</span> i, l <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.views_linears):</span><br><span class="line">                h = self.views_linears[i](h)</span><br><span class="line">                h = F.relu(h)</span><br><span class="line"></span><br><span class="line">            rgb = self.rgb_linear(h)  <span class="comment">## 输出rgb 3维度向量 #128—&gt;3</span></span><br><span class="line">            outputs = torch.cat([rgb, alpha], -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            outputs = self.output_linear(h)<span class="comment">#256—&gt;4</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs   </span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="bib-citation">Bib Citation</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;mildenhall2020nerf,</span><br><span class="line"> title=&#123;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&#125;,</span><br><span class="line"> author=&#123;Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng&#125;,</span><br><span class="line"> year=&#123;2020&#125;,</span><br><span class="line"> booktitle=&#123;ECCV&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 新视图合成 </tag>
            
            <tag> implicit rendering </tag>
            
            <tag> ECCV2020 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>欲要通晓他人码，恰似平地起高楼</title>
      <link href="/2024/03/05/%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81%E5%87%BD%E6%95%B0%E8%AE%B0%E5%BD%95/"/>
      <url>/2024/03/05/%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81%E5%87%BD%E6%95%B0%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="configs-参数">configs 参数</h1><h4 id="white_bkgd"><code>white_bkgd</code>:</h4><blockquote><p><a href="https://blog.csdn.net/qq_41623632/article/details/126468034">Pytroch Nerf代码阅读笔记（LLFF 数据集pose 处理和Nerf 网络结构）-CSDN博客</a></p></blockquote><p>白色背景参数。使得渲染出的结果背景是白色滴</p><p>在Blender 的数据集图像有四个通道RGBA,其中A表示的是alpha通道，一般情况下就是两个取值【0,1】,当alpha=0 表示该处的pixel是透明的；当alpha=1 表示该处的pixel是不透明的。 而 white_bkgd 这个参数就是负责将透明像素的部分转化为白色的背景，转化的代码部分如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> args.white_bkgd:</span><br><span class="line">    images = images[...,:<span class="number">3</span>]*images[...,-<span class="number">1</span>:] + (<span class="number">1.</span>-images[...,-<span class="number">1</span>:])</span><br></pre></td></tr></table></figure><p>代码的解读： images是Normalize到【0,1】之间的图像，当alpha=0(也就是 images[…,-1:] = 0 )，那么images的像素将设置为1（纯白色)；当alpha=1的时候，那么images的像素的就是本来的RGB通道对应的颜色。</p><h1 id="checkpoint">checkpoint</h1><blockquote><p><a href="https://zhuanlan.zhihu.com/p/410548507">理解Checkpoint - 知乎 (zhihu.com)</a></p></blockquote><p>其实就是由于训练时间长，我们将中间产生的结果用 checkpoint 保存起来，在节省空间的同时防止丢失重要结果数据。类似游戏里的存档啦！只要有 checkpoint ，我们就能安心重新开启下一次训练，而不用担心此前工作白费！</p><h1 id="parser.add_argument">parser.add_argument()</h1><blockquote><p>参考链接：<a href="https://blog.csdn.net/qq_34243930/article/details/106517985">python之parser.add_argument()用法——命令行选项、参数和子命令解析器-CSDN博客</a></p></blockquote><h2 id="argparse-介绍">argparse 介绍</h2><blockquote><p>官方文档：<a href="https://docs.python.org/zh-cn/3/library/argparse.html">argparse --- 命令行选项、参数和子命令解析器 — Python 3.12.2 文档</a></p></blockquote><p>argparse 模块是 Python 内置的一个用于命令项选项与参数解析的模块。通过在程序中定义好我们需要的参数，然后 argparse 将会从 sys.argv 解析出这些参数，并在用户给程序传入无效参数时报出错误信息。</p><h2 id="argparse使用说明与示例">argparse使用说明与示例</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;test&#x27;</span>) <span class="comment">#Step 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--sparse&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>, <span class="built_in">help</span>=<span class="string">&#x27;GAT with sparse version or not.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">72</span>, <span class="built_in">help</span>=<span class="string">&#x27;Random seed.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10000</span>, <span class="built_in">help</span>=<span class="string">&#x27;Number of epochs to train.&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">ArgumentParser.add_argument(name or flags...[, action][, nargs][, const][, default][, type][, choices][, required][, help][, metavar][, dest])</span></span><br><span class="line"><span class="string">-name: 有&#x27;--&#x27;指定表示可选参数；不加&#x27;--&#x27;表示必选参数，需在命令行中手动指定，否则即使通过default设置默认参数，也还是会报错</span></span><br><span class="line"><span class="string">-action: 当参数在命令行中出现时使用的动作基本类型，如当其值为&#x27;store_true&#x27;时表示参数作用是存储True值，其默认值为&#x27;False&#x27;，const值为&#x27;True&#x27;(若在parse_args()的括号中被引用，使用&#x27;True&#x27;值，否则使用&#x27;False&#x27;值)</span></span><br><span class="line"><span class="string">-default: 不指定参数时其默认值</span></span><br><span class="line"><span class="string">-type: 参数应当被转换成的类型</span></span><br><span class="line"><span class="string">-help: 所添加参数的简要说明</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3</span></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"><span class="built_in">print</span>(args.sparse)</span><br><span class="line"><span class="built_in">print</span>(args.seed)</span><br><span class="line"><span class="built_in">print</span>(args.epochs)</span><br></pre></td></tr></table></figure><p>三个步骤：</p><ol type="1"><li>创建一个解析器——创建一个 <strong>argparse.ArgumentParser()</strong> 对象，名称为 <code>parser</code></li><li>添加参数——调用<code>parser</code>. <strong>add_argument()</strong>方法添加参数</li><li>解析参数——使用 <code>parser</code>. <strong>parse_args()</strong> 解析检查添加的参数集为<span style="background:#daf5e9;"><strong>args</strong></span>，调用时括号内通常不带参数</li></ol><h1 id="tqdm-python进度条库">tqdm — python进度条库</h1><blockquote><p>参考链接：<a href="https://zhuanlan.zhihu.com/p/163613814">python进度条库tqdm详解 - 知乎 (zhihu.com)</a></p></blockquote><p><strong>tqdm模块是python进度条库，主要分为两种运行方式</strong></p><p>1.基于迭代对象运行：tqdm(iterator)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm, trange</span><br><span class="line"></span><br><span class="line"><span class="comment">#trange(i)是tqdm(range(i))的一种简单写法</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> trange(<span class="number">100</span>):</span><br><span class="line">    time.sleep(<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">100</span>), desc=<span class="string">&#x27;Processing&#x27;</span>):</span><br><span class="line">    time.sleep(<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">dic = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>]</span><br><span class="line">pbar = tqdm(dic)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> pbar:</span><br><span class="line">    pbar.set_description(<span class="string">&#x27;Processing &#x27;</span>+i)</span><br><span class="line">    time.sleep(<span class="number">0.2</span>)</span><br><span class="line"><span class="number">100</span>%|██████████| <span class="number">100</span>/<span class="number">100</span> [<span class="number">00</span>:06&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">16.04</span>it/s]</span><br><span class="line">Processing: <span class="number">100</span>%|██████████| <span class="number">100</span>/<span class="number">100</span> [<span class="number">00</span>:06&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">16.05</span>it/s]</span><br><span class="line">Processing e: <span class="number">100</span>%|██████████| <span class="number">5</span>/<span class="number">5</span> [<span class="number">00</span>:01&lt;<span class="number">00</span>:<span class="number">00</span>,  <span class="number">4.69</span>it/s]</span><br></pre></td></tr></table></figure><p>2.手动进行更新</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tqdm(total=<span class="number">200</span>) <span class="keyword">as</span> pbar:</span><br><span class="line">    pbar.set_description(<span class="string">&#x27;Processing:&#x27;</span>)</span><br><span class="line">    <span class="comment"># total表示总的项目, 循环的次数20*10(每次更新数目) = 200(total)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">        <span class="comment"># 进行动作, 这里是过0.1s</span></span><br><span class="line">        time.sleep(<span class="number">0.1</span>)</span><br><span class="line">        <span class="comment"># 进行进度更新, 这里设置10个</span></span><br><span class="line">        pbar.update(<span class="number">10</span>)</span><br><span class="line">Processing:: <span class="number">100</span>%|██████████| <span class="number">200</span>/<span class="number">200</span> [<span class="number">00</span>:02&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">91.94</span>it/s]</span><br></pre></td></tr></table></figure><h2 id="tqdm模块参数说明">tqdm模块参数说明</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">tqdm</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, iterable=<span class="literal">None</span>, desc=<span class="literal">None</span>, total=<span class="literal">None</span>, leave=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">               file=sys.stderr, ncols=<span class="literal">None</span>, mininterval=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">               maxinterval=<span class="number">10.0</span>, miniters=<span class="literal">None</span>, <span class="built_in">ascii</span>=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">               disable=<span class="literal">False</span>, unit=<span class="string">&#x27;it&#x27;</span>, unit_scale=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">               dynamic_ncols=<span class="literal">False</span>, smoothing=<span class="number">0.3</span>, nested=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">               bar_format=<span class="literal">None</span>, initial=<span class="number">0</span>, gui=<span class="literal">False</span></span>):</span><br></pre></td></tr></table></figure><ul><li>iterable: 可迭代的对象, 在手动更新时不需要进行设置</li><li>desc: 字符串, 左边进度条描述文字description</li><li>total: 总的项目数</li><li>leave: bool值, 迭代完成后是否保留进度条</li><li>file: 输出指向位置, 默认是终端, 一般不需要设置</li><li>ncols: 调整进度条宽度, 默认是根据环境自动调节长度, 如果设置为0, 就没有进度条, 只有输出的信息</li><li>unit: 描述处理项目的文字, 默认是'it', 例如: 100 it/s, 处理照片的话设置为'img' ,则为 100 img/s</li><li>unit_scale: 自动根据国际标准进行项目处理速度单位的换算, 例如 100 000 it/s &gt;&gt; 100k it/s</li></ul><h1 id="torch">torch</h1><h2 id="torch.randperm">torch.randperm</h2><p>torch.randperm(n)：将 <span class="math inline">\(0\)</span> ~ <span class="math inline">\(n-1\)</span>（包括 <span class="math inline">\(0\)</span> 和 <span class="math inline">\(n-1\)</span>）随机打乱后获得的数字序列，函数名是random permutation缩写。'permutation' 译为‘排列组合’。下面是一个例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.randperm(<span class="number">10</span>)</span><br><span class="line"><span class="comment">#输出 tensor([2, 3, 6, 7, 8, 9, 1, 5, 0, 4])</span></span><br></pre></td></tr></table></figure><h2 id="torch.no_grad">torch.no_grad</h2><blockquote><p>进阶内容请看：<a href="https://zhuanlan.zhihu.com/p/406823590">几行代码让你搞懂torch.no_grad - 知乎 (zhihu.com)</a></p></blockquote><p>一般是 <code>with torch.no_grad()</code> 和 <code>@torch.no_grad()</code> 两种用法。</p><ul><li><p><code>requires_grad=True</code> 要求计算梯度</p></li><li><p><code>requires_grad=False</code> 不要求计算梯度</p></li><li><p><code>with torch.no_grad()</code>或者<code>@torch.no_grad()</code>中的数据<strong>不需要计算梯度，也不会进行反向传播</strong>。</p><p>在该模块下，所有计算得出的tensor的requires_grad都自动设置为False。即使一个tensor（命名为x）的requires_grad = True，在with torch.no_grad计算，由x得到的新tensor（命名为w-标量）requires_grad也为False，且grad_fn也为None,即不会对w求导。</p></li></ul><p>两种用法如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()                                <span class="comment"># 测试模式</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">   <span class="keyword">pass</span></span><br><span class="line">   </span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">eval</span>():</span><br><span class="line">...</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>岂知辗转复相见，阖应辨得故友来</title>
      <link href="/2024/02/15/%E5%B8%B8%E8%A7%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Pytorch%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E8%A7%84%E5%BE%8B/"/>
      <url>/2024/02/15/%E5%B8%B8%E8%A7%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Pytorch%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E8%A7%84%E5%BE%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>常见Pytorch代码模型结构规律总结</p><p>参考链接：<a href="https://www.zhihu.com/question/406133826">一个完整的Pytorch深度学习项目代码，项目结构是怎样的？ - 知乎 (zhihu.com)</a></p></blockquote><h1 id="常见模型结构">常见模型结构</h1><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">|--project<span class="emphasis">_name/</span></span><br><span class="line"><span class="emphasis">|    |--data/ # 数据</span></span><br><span class="line"><span class="emphasis">|    |--datasets/ # 生成数据集，加载处理数据集</span></span><br><span class="line"><span class="emphasis">|    |    |--data_</span>loader.py</span><br><span class="line">|    |--models/ # 模型的综合训练、测试、验证、预测等</span><br><span class="line">|    |    |--model.py</span><br><span class="line">|    |--configs/ # 配置文件，例如模型参数、优化器参数、训练和测试参数等</span><br><span class="line">|    |    |--config.py</span><br><span class="line">|    |--model<span class="emphasis">_hub/ # 预训练模型权重</span></span><br><span class="line"><span class="emphasis">|    |--utils/ # 大杂烩辅助模块，可以是日志、评价指标等常用接口文件</span></span><br><span class="line"><span class="emphasis">|    |    |--utils.py</span></span><br><span class="line"><span class="emphasis">|    |    |--metrics.py</span></span><br><span class="line"><span class="emphasis">|    |    |--visualization.py</span></span><br><span class="line"><span class="emphasis">|    |--tools/ # 训练，评估，预测的脚本</span></span><br><span class="line"><span class="emphasis">|    |    |--train.py</span></span><br><span class="line"><span class="emphasis">|    |    |--test.py</span></span><br><span class="line"><span class="emphasis">|    |    |--eval.py</span></span><br><span class="line"><span class="emphasis">|    |    |--predict.py</span></span><br><span class="line"><span class="emphasis">|    |--outputs/ # 输出的结果</span></span><br><span class="line"><span class="emphasis">|    |    |--checkpoints/ # 训练好的模型文件</span></span><br><span class="line"><span class="emphasis">|    |    |--logs/ # 日志</span></span><br><span class="line"><span class="emphasis">|    |    |--images/ # 可视化的结果图片</span></span><br><span class="line"><span class="emphasis">|    |--requirements.py # 依赖包列表</span></span><br><span class="line"><span class="emphasis">|    |--README.md # 项目说明</span></span><br><span class="line"><span class="emphasis">|    |--.gitignore #告诉Git忽略哪些文件</span></span><br></pre></td></tr></table></figure><p>关于配置文件configs，可以使用YAML或JSON格式，在<code>train.py</code>或<code>test.py</code>中，可以使用<code>PyYAML</code>库或<code>json</code>库来加载这些配置文件。例如，YAML配置文件可以包括以下内容：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">model:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">model1</span></span><br><span class="line">  <span class="attr">input_size:</span> <span class="number">224</span></span><br><span class="line">  <span class="attr">output_size:</span> <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">optimizer:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">Adam</span></span><br><span class="line">  <span class="attr">lr:</span> <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line"><span class="attr">train:</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">32</span></span><br><span class="line">  <span class="attr">epochs:</span> <span class="number">100</span></span><br><span class="line">  <span class="attr">validation_split:</span> <span class="number">0.2</span></span><br></pre></td></tr></table></figure><h1 id="模型的参数可选">模型的参数(可选)</h1><p>一个深度学习网络有很多的参数可以配置，一般分成以下三类：</p><ul><li>数据集参数（文件路径、batch_size等）</li><li>训练参数（学习率、训练epoch等）</li><li>模型参数（输入的大小，输出的大小）</li></ul><p>这些参数可以写一个类保存，也可以写一个字典，然后使用json保存，这些都是需要自己去实现的，但是这些都是一些细枝末节东西，写了几次，找到一个自己最喜欢的方式就可以，不是深度学习项目中必要的部分。</p><h1 id="network-模型的定义">Network 模型的定义</h1><p>创建一个Network类，继承<code>torch.nn.Module</code>，在构造函数中用初始化成员变量为具体的网络层，在forward函数中使用成员变量搭建网络架构，模型的使用过程中pytorch会自动调用forword进行参数的前向传播，构建计算图。<strong>以下拿一个简单的CNN图像分类模型举例</strong>：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Network, self).__init__()</span><br><span class="line">        <span class="comment"># 灰度图像的channels=1即in_channels=1 输出为10个类别即out_features=10</span></span><br><span class="line">        <span class="comment"># parameter(形参)=argument(实参) 卷积核即卷积滤波器 out_channels=6即6个卷积核 输出6个feature-maps(特征映射)</span></span><br><span class="line">        <span class="comment"># 权重shape 6*1*5*5</span></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">6</span>)  <span class="comment"># 二维批归一化 输入size=6</span></span><br><span class="line">        <span class="comment"># 权重shape 12*1*5*5</span></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">12</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 全连接层：fc or dense or linear out_features即特征(一阶张量)</span></span><br><span class="line">        <span class="comment"># 权重shape 120*192</span></span><br><span class="line">        self.fc1 = nn.Linear(in_features=<span class="number">12</span>*<span class="number">4</span>*<span class="number">4</span>, out_features=<span class="number">120</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm1d(<span class="number">120</span>)  <span class="comment"># 一维批归一化 输入size=120</span></span><br><span class="line">        <span class="comment"># 权重shape 60*120</span></span><br><span class="line">        self.fc2 = nn.Linear(in_features=<span class="number">120</span>, out_features=<span class="number">60</span>)</span><br><span class="line">        <span class="comment"># 权重shape 10*60</span></span><br><span class="line">        self.out = nn.Linear(in_features=<span class="number">60</span>, out_features=<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, t</span>):</span><br><span class="line">        <span class="comment"># (1) input layer</span></span><br><span class="line">        t = t</span><br><span class="line">        <span class="comment"># (2) hidden conv layer</span></span><br><span class="line">        t = F.relu(self.conv1(t))  <span class="comment"># (28-5+0)/1+1=24 输入为b(batch_size)*1*28*28 输出为b*6*24*24 relu后shape不变</span></span><br><span class="line">        t = F.max_pool2d(t, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)  <span class="comment"># (24-2+0)/2+1=12 输出为b*6*12*12</span></span><br><span class="line">        t = self.bn1(t)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (3) hidden conv layer</span></span><br><span class="line">        t = F.relu(self.conv2(t))  <span class="comment"># (12-5+0)/1+1=8 输出为b*12*8*8 relu后shape不变</span></span><br><span class="line">        t = F.max_pool2d(t, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)  <span class="comment"># (8-2+0)/2+1=4 输出为b*12*4*4</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (4) hidden linear layer</span></span><br><span class="line">        t = F.relu(self.fc1(t.reshape(-<span class="number">1</span>, <span class="number">12</span>*<span class="number">4</span>*<span class="number">4</span>)))  <span class="comment"># t.reshape后为b*192 全连接层后输出为b*120 relu后shape不变</span></span><br><span class="line">        t = self.bn2(t)</span><br><span class="line">        <span class="comment"># (5) hidden linear layer</span></span><br><span class="line">        t = F.relu(self.fc2(t))  <span class="comment"># 全连接层后输出为b*60 relu后shape不变</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (6) output layer</span></span><br><span class="line">        t = self.out(t)  <span class="comment"># 全连接层后输出为b*10 relu后shape不变</span></span><br><span class="line">        <span class="keyword">return</span> t</span><br></pre></td></tr></table></figure><h1 id="transforms">Transforms</h1><p>数据处理可以直接使用<code>torchvision.transforms</code>下的处理函数，包括均值，随机旋转，随机裁剪等等，也可以自己实现一些pytorch中没有实现的处理函数，<strong>下面拿一个分割网络的处理函数举例</strong>，可支持同时对传入的Image和GroundTruth进行处理，使用时直接按照顺序构造ProcessImgAndGt即可。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ProcessImgAndGt</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, transforms</span>):</span><br><span class="line">            self.transforms = transforms</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img, label</span>):</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> self.transforms:</span><br><span class="line">            img, label = t(img, label)</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Resize</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, height, width</span>):</span><br><span class="line">        self.height = height</span><br><span class="line">        self.width = width</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img, label</span>):</span><br><span class="line">        img = img.resize((self.width, self.height), Image.BILINEAR)</span><br><span class="line">        label = label.resize((self.width, self.height), Image.NEAREST)</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Normalize</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, mean, std</span>):</span><br><span class="line">        self.mean, self.std = mean, std</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img, label</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            img[:, :, i] -= <span class="built_in">float</span>(self.mean[i])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            img[:, :, i] /= <span class="built_in">float</span>(self.std[i])</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ToTensor</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.to_tensor = torchvision.transforms.ToTensor()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img, label</span>):</span><br><span class="line">        img, label = self.to_tensor(img), self.to_tensor(label).long()</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">transforms = ProcessImgAndGt([</span><br><span class="line">    Resize(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">    Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>]),</span><br><span class="line">    ToTensor()</span><br><span class="line">])</span><br></pre></td></tr></table></figure><h1 id="dataset-数据处理和加载">Dataset 数据处理和加载</h1><p>一般需要我们自己定义：创建一个数据集类，继承torch.utils.data.Dataset，只需重写__init__构造函数，__getitem__迭代器遍历函数以及__len__函数。</p><ul><li>在__init__函数中读取传入的数据集路径下的指定数据文件，<strong>还是拿一个分割网络的dataset流程举例</strong>，其他分类分类模型可以直接将GroundTruth替换为对应label即可，将拼接处理好的图片文件路径和GroundTruth文件路径作为元组存入一个为列表的成员变量file_list中；</li><li>在__getitem__中根据传入的索引从file_list取对应的元素，并且通过Transforms进行处理；</li><li>在__len__中返回len(self.file_list)即可。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset_path, transforms</span>):</span><br><span class="line">        <span class="built_in">super</span>(TrainDataset, self).__init()</span><br><span class="line">        self.dataset_path = dataset_path</span><br><span class="line">        self.transforms = transforms</span><br><span class="line">        <span class="comment"># 根据具体的业务逻辑读取全部数据路径作为加载数据的索引</span></span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">dir</span> <span class="keyword">in</span> os.listdir(dataset_path):</span><br><span class="line">            image_dir = os.path.join(dataset_path, <span class="built_in">dir</span>)</span><br><span class="line">            gt_path = image_dir + <span class="string">&#x27;/GT/&#x27;</span></span><br><span class="line">            img_path = image_dir + <span class="string">&#x27;/Frame/&#x27;</span></span><br><span class="line">            img_list = []</span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> os.listdir(img_path):</span><br><span class="line">                <span class="keyword">if</span> name.endswith(<span class="string">&#x27;.png&#x27;</span>):</span><br><span class="line">                    img_list.append(name)</span><br><span class="line">            self.file_list.extend([(img_path + name, gt_path + name) <span class="keyword">for</span> name <span class="keyword">in</span> img_list])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):   </span><br><span class="line">        img_path, label_path = self.file_list[idx]</span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_path).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">        label = Image.<span class="built_in">open</span>(label_path).convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">        img, label = self.transforms(img, label)</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.file_list)</span><br></pre></td></tr></table></figure><h1 id="train-and-validate-模型的训练和运行">Train and Validate 模型的训练和运行</h1><p><span style="background:#daf5e9;">训练模型一般分成以下几个步骤:</span></p><p>定义网络 定义数据 定义损失函数和优化器 开始训练 <strong>训练网络</strong> <u>将梯度置为0</u> <u>求loss</u> <u>反向传播</u> <u>更新参数</u> <u>更新优化器的学习率（可选）</u> <strong>测试网络</strong> <strong>可视化处理各种指标</strong> <strong>计算在验证集上的指标 （可选）</strong></p><h2 id="optimizer">Optimizer</h2><p>选择优化器进行模型参数更新，要创建优化器必须给它一个可进行迭代优化的包含了全部参数的列表 然后可以指定针对这些参数的学习率（learning_rate），权重衰减（weight_decay），momentum等，</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters(), lr = <span class="number">0.0001</span>)</span><br></pre></td></tr></table></figure><p>或者是可以指定针对哪些参数执行不一样的优化策略，根据不同层的name对不同层使用不同的优化策略。列表中的每一项都可以是一个dict，dict中params对应当前项的参数列表，可以对当前项指定学习率或者是衰减策略。对base_params使用的1e-4的学习率，对finetune_params使用1e-3的学习率，对两者一起使用1e-4的权重衰减</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">base_params = [params <span class="keyword">for</span> name, params <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> (<span class="string">&quot;xxx&quot;</span> <span class="keyword">in</span> name)]</span><br><span class="line">finetune_params = [params <span class="keyword">for</span> name, params <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> (<span class="string">&quot;yyy&quot;</span> <span class="keyword">in</span> name)]</span><br><span class="line">optimizer = optim.Adam([</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: base_params&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: finetune_params, <span class="string">&quot;lr&quot;</span>: <span class="number">1e-3</span>&#125;</span><br><span class="line">], lr=<span class="number">1e-4</span>, weight_decay=<span class="number">1e-4</span>);</span><br></pre></td></tr></table></figure><h2 id="run">Run</h2><p>基础组件都写好了，剩下的就是组成一个完整的模型结构。</p><ol type="1"><li>实例化模型对象，并将其加载到GPU中</li><li>根据需要构建数据预处理对象，传入数据集对象中进行读取数据时的数据处理</li><li>构建训练和测试的数据集对象，并将其传入torch.utils.data.DataLoader，指定batch_size（训练或测试是每次读取多少条数据）、shuffle（读取数据时是否打乱）、num_workers（开启多少线程进行数据加载，为0时(不推荐)用主线程在训练模型时进行数据加载）等参数</li><li>使用torch.optim.Adam构建优化器对象，这里根据不同层的name对不同层使用不同的优化策略</li><li>训练20个epoch，并且每5个epoch在测试集上跑一遍，这里只计算了损失，对于其他评价指标直接计算即可</li><li>根据条件对指定epoch的模型进行保存</li></ol><ul><li><strong>optimizer.zero_grad() # pytorch会积累梯度，在优化每个batch的权重的梯度之前将之前计算出的每个权重的梯度置0</strong><br /></li><li><strong>loss.backward() # 在最后一个张量上调用反向传播方法，在计算图中计算权重的梯度</strong><br /></li><li><strong>optimizer.step() # 使用预先设置的学习率等参数根据当前梯度对权重进行更</strong></li><li><strong>model.train()</strong> # <strong>保证BN层能够继续计算数据的均值和方差并进行更新，保证dropout层会按照设定的参数设置保留激活单元的概率（保留概率=p）</strong></li><li><strong>model.eval()</strong> # <strong>BN层会停止计算均值和方差，直接使用训练时的参数，dropout层利用了训练好的全部网络连接，不随机舍弃激活单元</strong></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Network().cuda()</span><br><span class="line"><span class="comment"># 构建数据预处理</span></span><br><span class="line">transforms = ProcessImgAndGt([</span><br><span class="line">    Resize(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">    Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>]),</span><br><span class="line">    ToTensor()</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 构建Dataset</span></span><br><span class="line">train_dataset = MyDataset(train_dataset_path, transforms)</span><br><span class="line"><span class="comment"># DataLoader</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset,</span><br><span class="line">                                                   batch_size=<span class="number">12</span>,</span><br><span class="line">                                                   shuffle=<span class="literal">True</span>,</span><br><span class="line">                                                   num_workers=<span class="number">4</span>,</span><br><span class="line">                                                   pin_memory=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># TestDataset</span></span><br><span class="line">test_dataset = MyDataset(test_dataset_path, transforms)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset,</span><br><span class="line">                                                  batch_size=<span class="number">4</span>,</span><br><span class="line">                                                  shuffle=<span class="literal">True</span>,</span><br><span class="line">                                                  num_workers=<span class="number">2</span>,</span><br><span class="line">                                                  pin_memory=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># optimizer需要传入全部需要更新的参数名称，这里是对不用的参数执行不同的更新策略 </span></span><br><span class="line">base_params = [params <span class="keyword">for</span> name, params <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> (<span class="string">&quot;xxx&quot;</span> <span class="keyword">in</span> name)]</span><br><span class="line">finetune_params = [params <span class="keyword">for</span> name, params <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> (<span class="string">&quot;yyy&quot;</span> <span class="keyword">in</span> name)]</span><br><span class="line">optimizer = torch.optim.Adam([</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: base_params, <span class="string">&quot;lr&quot;</span>: <span class="number">1e-3</span>, ...&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: finetune_params, <span class="string">&quot;lr&quot;</span>: <span class="number">1e-4</span>, ...&#125;</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> trian_loader:</span><br><span class="line">        images. gts = batch[<span class="number">0</span>].cuda(), batch[<span class="number">1</span>].cuda()</span><br><span class="line">        preds = model(iamges)</span><br><span class="line">        loss = F.cross_entropy(preds, gts)</span><br><span class="line">        optimizer.zero_grad()    <span class="comment"># pytorch会积累梯度，在优化每个batch的权重的梯度之前将之前计算出的每个权重的梯度置0</span></span><br><span class="line">        loss.backward()          <span class="comment"># 在最后一个张量上调用反向传播方法，在计算图中计算权重的梯度 </span></span><br><span class="line">        optimizer.step()         <span class="comment"># 使用预先设置的学习率等参数根据当前梯度对权重进行更新</span></span><br><span class="line">        epoch_loss += loss * trian_loader.batch_size</span><br><span class="line">        <span class="comment"># 计算其他标准</span></span><br><span class="line">    loss = epoch_loss / <span class="built_in">len</span>(train_loader.dataset)</span><br><span class="line">    <span class="comment"># .......</span></span><br><span class="line">    <span class="comment"># 每隔几个epoch在测试集上跑一下</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        test_epoch_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> test_batch <span class="keyword">in</span> test_loader:</span><br><span class="line">            test_images. test_gts = test_batch[<span class="number">0</span>].cuda(), test_batch[<span class="number">1</span>].cuda()</span><br><span class="line">            test_preds = model(test_iamges)</span><br><span class="line">            loss = F.cross_entropy(test_preds, test_gts)</span><br><span class="line">            test_epoch_loss += loss * test_loader.batch_size</span><br><span class="line">            <span class="comment"># 计算其他标准</span></span><br><span class="line">        test_loss = test_epoch_loss / (<span class="built_in">len</span>(test_loader.dataset))</span><br><span class="line">    <span class="comment"># .......</span></span><br><span class="line">    <span class="comment"># 根据条件对指定epoch的模型进行保存 将模型序列化到磁盘的pickle包</span></span><br><span class="line">    <span class="keyword">if</span> 精度最高:</span><br><span class="line">        torch.save(model.stat_dict(), <span class="string">f&#x27;<span class="subst">&#123;model_path&#125;</span>_<span class="subst">&#123;time_index&#125;</span>.pth&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="test">Test</h2><p>实际使用时需要将训练好的模型上在输入数据上运行，这里以测试集的数据为例，实际情况下只需要初始化模型之后将视频流中的图像帧作为模型的输入即可。</p><h3 id="torch.no_grad"><strong>torch.no_grad()</strong></h3><p>停止autograd模块的工作，不计算和储存梯度，一般在用训练好的模型跑测试集时使用，因为测试集时不需要计算梯度更不会更新梯度。使用后可以加速计算时间，节约gpu的显存</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_dataset = MyDataset(test_dataset_path, transforms)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset,</span><br><span class="line">                                                       batch_size=<span class="number">1</span>,</span><br><span class="line">                                                       shuffle=<span class="literal">False</span>,</span><br><span class="line">                                                       num_workers=<span class="number">2</span>)</span><br><span class="line">model = Network().cuda()</span><br><span class="line"><span class="comment"># 对磁盘上的pickle文件进行解包 将gpu训练的模型加载到cpu上</span></span><br><span class="line">model.load_stat_dict(torch.load(model_path, map_location=torch.device(<span class="string">&#x27;cpu&#x27;</span>)));</span><br><span class="line">mocel.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> test_loader:</span><br><span class="line">        test_images. test_gts = test_batch[<span class="number">0</span>].cuda(), test_batch[<span class="number">1</span>].cuda()</span><br><span class="line">        test_preds = model(test_iamges)</span><br><span class="line">        <span class="comment"># 保存模型输出的图片</span></span><br></pre></td></tr></table></figure><h1 id="补充-deepvacpytorch工程化规范项目">补充: DeepVAC——Pytorch工程化规范项目</h1><blockquote><p>Github源码地址：<a href="https://github.com/deepvac/deepvac">DeepVAC/deepvac: PyTorch Project Specification. (github.com)</a></p></blockquote><h1 id="其他补充">其他补充</h1><blockquote><p><a href="https://zhuanlan.zhihu.com/p/409662511">Pytorch实验代码的亿些小细节 - 知乎 (zhihu.com)</a></p><p>一个<strong>Pytorch框架包</strong>，只要按函数划分往里面填东西就行，还可以自动支持单机多卡，多机多卡的训练：<a href="https://github.com/Lightning-AI/pytorch-lightning?tab=readme-ov-file">Lightning-AI/pytorch-lightning: Pretrain, finetune and deploy AI models on multiple GPUs, TPUs with zero code changes. (github.com)</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积神经网络</title>
      <link href="/2024/01/23/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/"/>
      <url>/2024/01/23/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/</url>
      
        <content type="html"><![CDATA[<blockquote><p>原文链接：<a href="https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/index.html">6. 卷积神经网络 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p></blockquote><h1 id="从全连接层到卷积">从全连接层到卷积</h1><blockquote><p>本节主要讲述卷积层的原理。卷积的本质是<strong>有效提取相邻像素间的相关特征</strong>。</p><p>之前我们将softmax回归模型和多层感知机模型应用于Fashion-MNIST数据集中的服装图片。 为了能够应用softmax回归和多层感知机，我们首先将每个大小为28×28的图像展平为一个784维的固定长度的一维向量，然后用全连接层对其进行处理。 而现在，若我们掌握了卷积层的处理方法，我们就<strong>可以在图像中保留空间结构</strong>。 同时，用卷积层代替全连接层的另一个好处是：<strong>模型更简洁、所需的参数更少</strong>。</p></blockquote><p>多层感知机十分适合处理表格数据，其中行对应样本，列对应特征。 对于表格数据，我们寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构。 此时，多层感知机可能是最好的选择，然而对于高维感知数据（如图片数据），这种缺少结构的网络可能会变得不实用。</p><p>图像中本就拥有丰富的结构，而这些结构可以被人类和机器学习模型使用。 <strong>卷积神经网络（convolutional neural networks，CNN）是机器学习利用自然图像中一些已知结构的创造性方法</strong>。</p><h2 id="不变性">不变性</h2><p>假设我们想从一张图片中找到某个物体。 合理的假设是：<strong>无论哪种方法找到这个物体，都应该和物体的位置无关</strong>。 不妨假设我们要在人群中寻找小明的位置，由于小明的外表不取决于他所处的位置，因此我们可以使用一个“小明检测器“扫描图像。该检测器将图像分割为多个区域，并为每个区域包含小明的可能性打分。卷积神经网络正是将<strong>空间不变性（spatial invariance）</strong>的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。</p><p>现在，我们将上述想法总结一下，从而帮助我们设计适合于计算机视觉的神经网络架构。</p><ol type="1"><li><p>平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</p></li><li><p>局部性（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</p></li></ol><h2 id="多层感知机的限制">多层感知机的限制</h2><h3 id="平移不变性">平移不变性</h3><h3 id="局部性">局部性</h3><h2 id="卷积">卷积</h2><p>在数学中，两个函数（如<span class="math inline">\(f,g:R^d\rightarrow R\)</span>）之间的“卷积”被定义为 <span class="math display">\[\left( f*g \right) \left( \mathbf{x} \right) =\int{f\left( \mathbf{z} \right)}g\left( \mathbf{x}-\mathbf{z} \right) d\mathbf{z}.\]</span> 也就是说，卷积是当把一个函数“翻转”并移位<span class="math inline">\(\mathbf{x}\)</span>时，测量<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>之间的重叠。 当为离散对象时，积分就变成求和。对于二维张量，则为<span class="math inline">\(f\)</span>的索引(<span class="math inline">\(a\)</span>,<span class="math inline">\(b\)</span>)和<span class="math inline">\(g\)</span>的索引(<span class="math inline">\(i-a\)</span>,<span class="math inline">\(j-b\)</span>)上的对应加和： <span class="math display">\[\left( f*g \right) \left( i,j \right) =\sum_a{\sum_b{f\left( a,b \right) g\left( i-a,j-b \right)}}.\]</span></p><h2 id="通道">通道</h2><p>回到上面的找小明游戏，让我们看看它到底是什么样子。卷积层根据<strong>滤波器</strong><span class="math inline">\(V\)</span> <span style="background:#daf5e9;">（filter, 即卷积核 convolution kernel, 亦或简单地称之为该卷积层的<em>权重</em>，通常该权重是可学习的参数）</span>选取给定大小的窗口，并加权处理图片。我们的目标是学习一个模型，以便探测出“小明”最可能出现的地方。</p><p>然而这种方法有一个问题：我们忽略了图像一般包含三个通道/三种原色（红色、绿色和蓝色）。 实际上，<strong>图像不是二维张量，而是一个由高度、宽度和颜色组成的三维张量</strong>，比如包含1024×1024×3个像素。 前两个轴与像素的空间位置有关，而第三个轴可以看作每个像素的多维表示。因此，我们将输入<span class="math inline">\(X\)</span>索引为<span class="math inline">\([X]_{i,j,k}\)</span>。由此卷积相应地调整为<span class="math inline">\([V]_{a,b,c}\)</span>，而不是<span class="math inline">\([V]_{a,b}\)</span>。</p><p>此外，由于输入图像是三维的，我们的隐藏表示<span class="math inline">\(H\)</span>也最好采用三维张量。 换句话说，对于每一个空间位置，我们想要采用一组而不是一个隐藏表示。这样一组隐藏表示可以想象成一些互相堆叠的二维网格。 因此，我们可以把隐藏表示想象为一系列具有二维张量的<strong>通道（channel）</strong>。 这些通道有时也被称为<strong>特征映射（feature maps）</strong>，因为每个通道都向后续层提供一组空间化的学习特征。 直观上可以想象在靠近输入的底层，一些通道专门识别边缘，而一些通道专门识别纹理。</p><p>为了支持输入<span class="math inline">\(X\)</span>和隐藏表示<span class="math inline">\(H\)</span>中的多个通道，我们可以在<span class="math inline">\(V\)</span>中添加第四个坐标，即<span class="math inline">\([V]_{a,b,c,d}\)</span>。综上所述， <span class="math display">\[[\mathsf{H} ]_{i,j,d}=\sum_{a=-\Delta}^{\Delta}{\sum_{b=-\Delta}^{\Delta}{\sum_c{[}}}\mathsf{V} ]_{a,b,c,d}[\mathsf{X} ]_{i+a,j+b,c},\]</span> 其中隐藏表示<span class="math inline">\(H\)</span>中的索引<span class="math inline">\(d\)</span>表示输出通道，而随后的输出将继续以三维张量<span class="math inline">\(H\)</span>作为输入进入下一个卷积层。 所以，上式可以定义具有多个通道的卷积层，而其中<span class="math inline">\(V\)</span>是该卷积层的权重。</p><h2 id="从全连接层到卷积总结">从全连接层到卷积总结</h2><ul><li>图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。</li><li>局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。</li><li>在图像处理中，卷积层通常比全连接层需要更少的参数，但依旧获得高效用的模型。</li><li>卷积神经网络（CNN）是一类特殊的神经网络，它可以包含多个卷积层。</li><li>多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。</li></ul><h1 id="图像卷积">图像卷积</h1><blockquote><p>由于卷积神经网络的设计是用于探索图像数据，本节以图像为例讲述卷积的实际应用。</p></blockquote><h2 id="互相关运算">互相关运算</h2><p>严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是<em>互相关运算</em>（cross-correlation），而不是卷积运算。 根据<u>第一节</u>中的描述，在卷积层中，输入张量和核张量通过互相关运算产生输出张量。</p><p>首先，我们暂时忽略通道（第三维）这一情况，看看如何处理二维图像数据和隐藏表示。在下图中，输入是高度为3、宽度为3的二维张量（即形状为3×3）。卷积核的高度和宽度都是2，而卷积核窗口（或卷积窗口）的形状由内核的高度和宽度决定（即2×2）。</p><figure><img src="https://zh-v2.d2l.ai/_images/correlation.svg" alt="zh-v2.d2l.ai/_images/correlation.svg" /><figcaption aria-hidden="true">zh-v2.d2l.ai/_images/correlation.svg</figcaption></figure><p>二维互相关运算。阴影部分是第一个输出元素，以及用于计算输出的输入张量元素和核张量元素： <span class="math display">\[0\times0+1\times1+3\times2+4\times3=19\]</span> 在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。 在如上例子中，输出张量的四个元素由二维互相关运算得到，输出高度为2、宽度为2。</p><blockquote><p>注意，输出大小略小于输入大小。这是因为卷积核的宽度和高度大于1， 而卷积核只与图像中每个大小完全适合的位置进行互相关运算。 所以，输出大小等于输入大小<span class="math inline">\(n_h×n_w\)</span>减去卷积核大小<span class="math inline">\(k_h×k_w\)</span>，即： <span class="math display">\[(n_h-k_h+1) \times (n_w-k_w+1).\]</span> 这是因为我们需要足够的空间在图像上“移动”卷积核。稍后，我们将看到如何通过在图像边界周围填充零来保证有足够的空间移动卷积核，从而保持输出大小不变。</p></blockquote><p>接下来，我们在<code>corr2d</code>函数中实现如上过程，该函数接受输入张量<code>X</code>和卷积核张量<code>K</code>，并返回输出张量<code>Y</code>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">X, K</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算二维互相关运算&quot;&quot;&quot;</span></span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i:i + h, j:j + w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;验证上述二维互相关运算的输出&#x27;&#x27;&#x27;</span></span><br><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">K = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">corr2d(X, K)</span><br></pre></td></tr></table></figure><p>tensor([[19., 25.], [37., 43.]])</p><h2 id="卷积层">卷积层</h2><blockquote><p>高度和宽度分别为<span class="math inline">\(ℎ\)</span>和<span class="math inline">\(w\)</span>的卷积核可以被称为<span class="math inline">\(h×w\)</span>卷积或<span class="math inline">\(h×w\)</span>卷积核。 我们也将带有<span class="math inline">\(h×w\)</span>卷积核的卷积层称为<span class="math inline">\(h×w\)</span>卷积层。</p></blockquote><p>卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。 所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。 就像我们之前随机初始化全连接层一样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。</p><p>基于上面定义的<code>corr2d</code>函数实现二维卷积层。在<code>__init__</code>构造函数中，将<code>weight</code>和<code>bias</code>声明为两个模型参数<code>Parameter</code>。前向传播函数调用<code>corr2d</code>函数并添加偏置。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, kernel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#赋初值，nn.Parameter为可更新的参数（requires_grad=True）</span></span><br><span class="line">        self.weight = nn.Parameter(torch.rand(kernel_size))  <span class="comment">#随机初始化</span></span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure><h2 id="图像中目标的边缘检测">图像中目标的边缘检测</h2><p>如下是卷积层的一个简单应用：通过找到像素变化的位置，来检测图像中不同颜色的边缘。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.首先，我们构造一个6×8像素的黑白图像。中间四列为黑色（0），其余像素为白色（1）。</span></span><br><span class="line">X = torch.ones((<span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">X</span></span><br><span class="line"><span class="string">tensor([[1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.接下来，我们构造一个高度为1、宽度为2的卷积核K。当进行互相关运算时，如果水平相邻的两元素相同，则输出为零，否则输出为非零。</span></span><br><span class="line">K = torch.tensor([[<span class="number">1.0</span>, -<span class="number">1.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.现在，我们对参数X（输入）和K（卷积核）执行互相关运算。 如下所示，输出Y中的1代表从白色到黑色的边缘，-1代表从黑色到白色的边缘，其他情况的输出为0。</span></span><br><span class="line">Y = corr2d(X, K)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Y</span></span><br><span class="line"><span class="string">tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.最后，我们将输入的二维图像转置，再进行如上的互相关运算。 其输出如下，之前检测到的垂直边缘消失了。 不出所料，这个卷积核K只可以检测垂直边缘，无法检测水平边缘。</span></span><br><span class="line">corr2d(X.t(), K)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="学习卷积核">学习卷积核</h2><p>如果我们只需寻找黑白边缘，那么以上<code>[1, -1]</code>的边缘检测器足以。然而，当有了更复杂数值的卷积核，或者连续的卷积层时，我们不可能手动设计滤波器。因此考虑是否可以通过<code>仅查看“输入-输出”对</code>来学习由<code>X</code>生成<code>Y</code>的卷积核？</p><p>我们先构造一个卷积层，并将其卷积核初始化为随机张量。接下来，在每次迭代中，我们比较<code>Y</code>与卷积层输出的平方误差，然后计算梯度来更新卷积核。为了简单起见，我们在此使用内置的二维卷积层<code>nn.Conv2d</code>，并忽略偏置。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造一个二维卷积层，它具有1个输入通道、1个输出通道和形状为（1，2）的卷积核</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），</span></span><br><span class="line"><span class="comment"># 其中批量大小和通道数都为1</span></span><br><span class="line">X = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">Y = Y.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>))</span><br><span class="line">lr = <span class="number">3e-2</span>  <span class="comment"># 学习率0.03</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    loss = (Y_hat - Y) ** <span class="number">2</span></span><br><span class="line">    conv2d.zero_grad()</span><br><span class="line">    loss.<span class="built_in">sum</span>().backward()</span><br><span class="line">    <span class="comment"># 迭代卷积核</span></span><br><span class="line">    conv2d.weight.data[:] -= lr * conv2d.weight.grad</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:<span class="comment">#偶数行输出</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l.<span class="built_in">sum</span>():<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>epoch 2, loss 6.422 epoch 4, loss 1.225 epoch 6, loss 0.266 epoch 8, loss 0.070 epoch 10, loss 0.022</p><p>在10次迭代之后，误差已经降到足够低。现在我们来看看我们所学的卷积核的权重张量。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv2d.weight.data.reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>tensor([[ 1.0010, -0.9739]])</p><p>发现我们学习到的卷积核权重非常接近我们之前定义的卷积核<code>K</code>。</p><h2 id="互相关和卷积">互相关和卷积</h2><blockquote><p>为了与深度学习文献中的标准术语保持一致，我们将继续把“互相关运算”称为卷积运算，尽管严格地说，它们略有不同。 此外，对于卷积核张量上的权重，我们称其为<em>元素</em>。</p></blockquote><h2 id="特征映射和感受野">特征映射和感受野</h2><p>如前文所述，输出卷积层有时被称为<strong>特征映射</strong>（feature map），因为它可以被视为一个输入映射到下一层的空间维度的转换器。 在卷积神经网络中，对于某一层的任意元素<span class="math inline">\(x\)</span>，其<strong>感受野</strong>（receptive field）是指在前向传播期间可能影响<span class="math inline">\(x\)</span>计算的所有元素（来自所有先前层）。</p><blockquote><p>注意，感受野可能大于输入的实际大小。</p><p>因此，当一个特征图中的任意元素需要检测更广区域的输入特征时，我们可以构建一个更深的网络。</p></blockquote><h2 id="图像卷积总结">图像卷积总结</h2><ul><li>二维卷积层的核心计算是二维互相关运算。最简单的形式是，对二维输入数据和卷积核执行互相关操作，然后添加一个偏置。</li><li>我们可以设计一个卷积核来检测图像的边缘。</li><li>我们可以从数据中学习卷积核的参数。</li><li>学习卷积核时，无论用严格卷积运算或互相关运算，卷积层的输出不会受太大影响。</li><li>当需要检测输入特征中更广区域时，我们可以构建一个更深的卷积网络。</li></ul><h1 id="填充padding-和步幅stride">填充padding 和步幅stride</h1><p>正如我们在第二节中所概括的那样，假设输入形状为<span class="math inline">\(n_h×n_w\)</span>，卷积核形状为<span class="math inline">\(k_h×k_w\)</span>，那么输出形状将是<span class="math inline">\((n_h-k_h+1)×(n_w-k_w+1)\)</span>。可见，<font color=#4eb434>卷积的输出形状取决于输入形状和卷积核的形状。</font></p><p>还有什么因素会影响输出的大小呢？本节我们将介绍<font color=#985fff><strong>填充</strong>（padding）</font>和<font color=#df8400><strong>步幅</strong>（stride）</font>。假设以下情景： <font color=#985fff>有时，在应用了连续的卷积之后，我们最终得到的输出远小于输入大小。这是由于卷积核的宽度和高度通常大于1所导致的。比如，一个240×240像素的图像，经过10层5×5的卷积后，将减少到200×200像素。如此一来，原始图像的边界丢失了许多有用信息。而<em>填充</em>是解决此问题最有效的方法</font>； <font color=#df8400>有时，我们可能希望大幅降低图像的宽度和高度。例如，如果我们发现原始的输入分辨率十分冗余。<em>步幅</em>则可以在这类情况下提供帮助。</font></p><h2 id="填充">填充</h2><p><strong>填充</strong>（padding）：在输入图像的边界填充元素（通常填充元素是0）。</p><p>例如，在下图中，我们将3×3输入填充到5×5，那么它的输出就增加为4×4。阴影部分是第一个输出元素以及用于输出计算的输入和核张量元素： 0×0+0×1+0×2+0×3=0。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/conv-pad.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">带填充的二维互相关</div></center><p>通常，如果我们添加<span class="math inline">\(p_h\)</span>行填充（大约一半在顶部，一半在底部）和<span class="math inline">\(p_w\)</span>列填充（大约左侧一半，右侧一半），则输出形状将为 <span class="math display">\[(n_h-k_h+1+p_h)\times(n_w-k_w+1+p_w)。\]</span> 这意味着输出的高度和宽度将分别增加<span class="math inline">\(p_h\)</span>和<span class="math inline">\(p_w\)</span>。</p><p><strong>在许多情况下，我们需要设置<span class="math inline">\(p_h=k_h-1\)</span>和<span class="math inline">\(p_w=k_w-1\)</span>，使输入和输出具有相同的高度和宽度。</strong> 这样可以在构建网络时更容易地预测每个图层的输出形状。假设<span class="math inline">\(k_h\)</span>是奇数，我们将在高度的两侧填充<span class="math inline">\(p_h/2\)</span>行。 如果<span class="math inline">\(k_h\)</span>是偶数，则一种可能性是在输入顶部填充<span class="math inline">\(\lceil p_h/2\rceil\)</span>行，在底部填充<span class="math inline">\(\lfloor p_h/2\rfloor\)</span>行。同理，我们填充宽度的两侧。</p><p><span style="background:#daf5e9;">卷积神经网络中卷积核的高度和宽度通常为奇数，例如1、3、5或7。 </span>选择奇数的好处是，保持空间维度的同时，我们可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。</p><p>此外，使用奇数的核大小和填充大小也提供了书写上的便利。对于任何二维张量<code>X</code>，当满足： 1. 卷积核的大小是奇数； 2. 所有边的填充行数和列数相同； 3. 输出与输入具有相同高度和宽度 则可以得出：输出<code>Y[i, j]</code>是通过以输入<code>X[i, j]</code>为中心，与卷积核进行互相关计算得到的。</p><p>在下面的例子中，我们创建一个高度和宽度为3的二维卷积层，并在所有侧边填充1个像素。给定高度和宽度为8的输入，则输出的高度和宽度也是8；<span style="background:#daf5e9;">当卷积核的高度和宽度不同时，我们可以填充不同的高度和宽度，使输出和输入具有相同的高度和宽度。</span>在如下示例中，我们使用高度为5，宽度为3的卷积核，高度和宽度两边的填充分别为2和1。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了方便起见，我们定义了一个计算卷积层的函数。</span></span><br><span class="line"><span class="comment"># 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span><br><span class="line">    <span class="comment"># 这里的（1，1）表示批量大小和通道数都是1</span></span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>) + X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="comment"># 省略前两个维度：批量大小和通道</span></span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([8, 8])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([8, 8])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="步幅">步幅</h2><p>在计算互相关时，卷积窗口从输入张量的左上角开始，向下、向右滑动。 在前面的例子中，我们默认每次滑动一个元素。 但是，有时候<span style="background:#daf5e9;">为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素</span>。</p><p>我们将每次滑动元素的数量称为<strong>步幅（stride）</strong>。到目前为止，我们只使用过高度或宽度为1的步幅，那么如何使用较大的步幅呢？下图是垂直步幅为3，水平步幅为2的二维互相关运算。 着色部分是输出元素以及用于输出计算的输入和内核张量元素：0×0+0×1+1×2+2×3=8、0×0+6×1+0×2+0×3=6。</p><p>可以看到，为了计算输出中第一列的第二个元素和第一行的第二个元素，卷积窗口分别向下滑动三行和向右滑动两列。但是，当卷积窗口继续向右滑动两列时，没有输出，因为输入元素无法填充窗口（除非我们添加另一列填充）。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/conv-stride.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">垂直步幅为3，水平步幅为2的二维互相关运算</div></center><p>通常，当垂直步幅为<span class="math inline">\(s_h\)</span>、水平步幅为<span class="math inline">\(s_w\)</span>时，输出形状为 <span class="math display">\[\lfloor(n_h-k_h+s_h+p_h)/s_h\rfloor \times \lfloor(n_w-k_w+s_w+p_w)/s_w\rfloor.\]</span> <strong>如果我们设置了<span class="math inline">\(p_h=k_h-1\)</span>和<span class="math inline">\(p_w=k_w-1\)</span>，则输出形状将简化为<span class="math inline">\(\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor\)</span>。 更进一步，如果输入的高度和宽度可以被垂直和水平步幅整除，则输出形状将为<span class="math inline">\((n_h/s_h) \times (n_w/s_w)\)</span>。</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们将高度和宽度的步幅设置为2，从而将输入的高度和宽度减半。</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([4, 4])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 看一个稍微复杂的例子</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), stride=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([2, 2])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>为了简洁起见，当输入高度和宽度两侧的填充数量分别为<span class="math inline">\(p_h\)</span>和<span class="math inline">\(p_w\)</span>时，我们称之为填充<span class="math inline">\((p_h, p_w)\)</span>。当<span class="math inline">\(p_h = p_w = p\)</span>时，填充是<span class="math inline">\(p\)</span>。同理，当高度和宽度上的步幅分别为<span class="math inline">\(s_h\)</span>和<span class="math inline">\(s_w\)</span>时，我们称之为步幅<span class="math inline">\((s_h, s_w)\)</span>。特别地，当<span class="math inline">\(s_h = s_w = s\)</span>时，我们称步幅为<span class="math inline">\(s\)</span>。默认情况下，填充为0，步幅为1。在实践中，我们很少使用不一致的步幅或填充，也就是说，我们通常有<span class="math inline">\(p_h = p_w\)</span>和<span class="math inline">\(s_h = s_w\)</span>。</p><h2 id="填充和步幅总结">填充和步幅总结</h2><ul><li>填充可以增加输出的高度和宽度。在许多情况下，我们需要设置<span class="math inline">\(p_h=k_h-1\)</span>和<span class="math inline">\(p_w=k_w-1\)</span>，用来使输出与输入具有相同的高和宽。</li><li>步幅可以减小输出的高和宽，例如在设置了填充的情况下，若输入的高度和宽度可以被步幅s整除，输出的高和宽仅为输入的高和宽的1/s（步幅s是一个大于1的整数）。</li><li>填充和步幅可用于有效地调整输出数据的维度。</li></ul><h1 id="多输入多输出通道">多输入多输出通道</h1><p>虽然我们在第一节中描述了构成每个图像的多个通道和多层卷积层。例如彩色图像具有标准的RGB通道来代表红、绿和蓝。 但是到目前为止，我们仅展示了单个输入和单个输出通道的简化例子。 这使得我们可以将输入、卷积核和输出看作二维张量。</p><p>当我们添加通道时，我们的输入和隐藏的表示都变成了三维张量。例如，每个RGB输入图像具有<span class="math inline">\(3×ℎ×w\)</span>的形状。我们将这个大小为3的轴称为<strong>通道（channel）</strong>维度。本节将更深入地研究具有多输入和多输出通道的卷积核。</p><h2 id="多输入通道">多输入通道</h2><p><span style="background:#daf5e9;">当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便与输入数据进行互相关运算。</span>若输入和卷积核都有<span class="math inline">\(c_i\)</span>个通道，我们可以对每个通道输入的二维张量和卷积核的二维张量进行互相关运算，再对通道求和（将<span class="math inline">\(c_i\)</span>的结果相加）得到二维张量。这是多通道输入和多输入通道卷积核之间进行二维互相关运算的结果。</p><p>在下图中，我们演示了一个具有两个输入通道的二维互相关运算的示例。阴影部分是第一个输出元素以及用于计算这个输出的输入和核张量元素：(1×1+2×2+4×3+5×4)+(0×0+1×1+3×2+4×3)=56。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/conv-multi-in.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">两个输入通道的互相关计算</div></center><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 再来看下具体实现，我们实现一下多输入通道互相关运算。 简而言之，我们所做的就是对每个通道执行互相关操作，然后将结果相加。</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 先遍历“X”和“K”的第0个维度（通道维度），再把它们加在一起</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(d2l.corr2d(x, k) <span class="keyword">for</span> x, k <span class="keyword">in</span> <span class="built_in">zip</span>(X, K))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们可以构造与上图中的值相对应的输入张量X和核张量K，以验证互相关运算的输出。</span></span><br><span class="line">X = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]],</span><br><span class="line">               [[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]]])</span><br><span class="line">K = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]], [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]]])</span><br><span class="line"></span><br><span class="line">corr2d_multi_in(X, K)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 56.,  72.],</span></span><br><span class="line"><span class="string">        [104., 120.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>zip()</strong> 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的zip对象： &gt;&gt;&gt; a = [1,2,3] &gt;&gt;&gt; b = [4,5,6] &gt;&gt;&gt; zipped = zip(a,b) # 返回一个对象 &gt;&gt;&gt; zipped &lt;zip object at 0x103abc288&gt; &gt;&gt;&gt; list(zipped) # list() 转换为列表 [(1, 4), (2, 5), (3, 6)]</p><p>对于zip(args)这个函数，Python还提供了一种逆操作： &gt;&gt;&gt;origin = zip(<em>result) #前面加</em>号，事实上*号也是一个特殊的运算符，叫解包运算符。</p></blockquote><h2 id="多输出通道">多输出通道</h2><p>到目前为止，不论有多少输入通道，我们还只有一个输出通道。然而，正如我们在第一节中所讨论的，每一层有多个输出通道是至关重要的。在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。直观地说，我们可以将每个通道看作对不同特征的响应。而现实可能更为复杂一些，因为<u>每个通道不是独立学习的，而是为了共同使用而优化的</u>。因此，<u>多输出通道并不仅是学习多个单通道的检测器</u>。</p><p>用<span class="math inline">\(c_i\)</span>和<span class="math inline">\(c_o\)</span>分别表示输入和输出通道的数目，并让<span class="math inline">\(k_h\)</span>和<span class="math inline">\(k_w\)</span>为卷积核的高度和宽度。为了获得多个通道的输出，我们可以<span style="background:#daf5e9;">为每个输出通道创建一个形状为<span class="math inline">\(c_i\times k_h\times k_w\)</span>的卷积核张量</span>，这样卷积核的形状是<span class="math inline">\(c_o\times c_i\times k_h\times k_w\)</span>。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如下，我们实现一个计算多个通道的输出的互相关函数。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。</span></span><br><span class="line">    <span class="comment"># 最后将所有结果都叠加在一起</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过将核张量K与K+1（K中每个元素加1）和K+2连接起来，构造了一个具有3个输出通道的卷积核。</span></span><br><span class="line">K = torch.stack((K, K + <span class="number">1</span>, K + <span class="number">2</span>), <span class="number">0</span>)</span><br><span class="line">K.shape</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([3, 2, 2, 2])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面，我们对输入张量X与卷积核张量K执行互相关运算。现在的输出包含3个通道，第一个通道的结果与先前输入张量X和多输入单输出通道的结果一致。</span></span><br><span class="line">corr2d_multi_in_out(X, K)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[ 56.,  72.],</span></span><br><span class="line"><span class="string">         [104., 120.]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 76., 100.],</span></span><br><span class="line"><span class="string">         [148., 172.]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 96., 128.],</span></span><br><span class="line"><span class="string">         [192., 224.]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>输入张量X与具有3个输出通道的卷积核K执行互相关运算，得到输出的过程如下图：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nnn.jpg" alt="nnn" /><figcaption aria-hidden="true">nnn</figcaption></figure></blockquote><h2 id="卷积层-1">1×1卷积层</h2><p>1×1卷积，即<span class="math inline">\(k_h = k_w = 1\)</span>，看起来似乎没有多大意义。 毕竟，<span style="background:#daf5e9;">卷积的本质是有效提取相邻像素间的相关特征</span>，而1×1卷积显然没有此作用。 尽管如此，1×1仍然十分流行，经常包含在复杂深层网络的设计中。下面，让我们详细地解读一下它的实际作用。</p><p>因为使用了最小窗口，1×1卷积失去了卷积层的特有能力——在高度和宽度维度上，识别相邻元素间相互作用的能力。 其实1×1卷积的唯一计算发生在<strong>通道</strong>上。</p><p>下图展示了使用1×1卷积核与3个输入通道和2个输出通道的互相关计算。 这里输入和输出具有相同的高度和宽度，输出中的每个元素都是从输入图像中同一位置的元素的线性组合。 我们可以将1×1卷积层看作在每个像素位置应用的全连接层，以<span class="math inline">\(c_i\)</span>个输入值转换为<span class="math inline">\(c_o\)</span>个输出值。 因为这仍然是一个卷积层，所以跨像素的权重是一致的。 同时，1×1卷积层需要的权重维度为<span class="math inline">\(c_o\times c_i\)</span>，再额外加上一个偏置。</p><blockquote><p>这里的“权重”理解为“卷积核”</p></blockquote><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/conv-1x1.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">互相关计算使用了具有3个输入通道和2个输出通道的1×1卷积核。其中，输入和输出具有相同的高度3和宽度3</div></center><p>下面，我们使用全连接层实现1×1卷积。 请注意，我们需要对输入和输出的数据形状进行调整。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out_1x1</span>(<span class="params">X, K</span>):</span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]</span><br><span class="line">    X = X.reshape((c_i, h * w))</span><br><span class="line">    K = K.reshape((c_o, c_i))</span><br><span class="line">    <span class="comment"># 全连接层中的矩阵乘法</span></span><br><span class="line">    Y = torch.matmul(K, X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape((c_o, h, w))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当执行1×1卷积运算时，上述函数相当于先前实现的互相关函数corr2d_multi_in_out。让我们用一些样本数据来验证这一点。</span></span><br><span class="line">X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">K = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">Y1 = corr2d_multi_in_out_1x1(X, K)</span><br><span class="line">Y2 = corr2d_multi_in_out(X, K)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">float</span>(torch.<span class="built_in">abs</span>(Y1 - Y2).<span class="built_in">sum</span>()) &lt; <span class="number">1e-6</span></span><br></pre></td></tr></table></figure><h2 id="多输入多输出通道总结">多输入多输出通道总结</h2><ul><li>多输入多输出通道可以用来扩展卷积层的模型。</li><li>当以每像素为基础应用时，1×1卷积层相当于全连接层。</li><li>1×1卷积层通常用于调整网络层的通道数量和控制模型复杂性。</li></ul><h1 id="池化层-pooling">池化层 pooling</h1><p>本节将介绍<em>池化</em>（pooling）层，它具有双重目的：<strong>1.降低对空间降采样表示的敏感性</strong>，<font color=#ef042a><strong>2.降低卷积层对位置的敏感性</strong></font>。</p><ol type="1"><li><p>我们的机器学习任务通常会跟<u>全局图像的问题</u>有关（例如，“图像是否包含一只猫呢？”），所以我们<u>最后一层的神经元应该对整个输入的全局敏感</u>。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。因此，当我们处理图像时，我们希望<u>逐渐降低隐藏表示的空间分辨率、聚集信息</u>，这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野（输入）就越大。</p></li><li><p>此外，当检测较底层的特征时（例如第二节中所讨论的边缘），我们通常<u>希望这些特征保持某种程度上的平移不变性</u>。例如，如果我们拍摄黑白之间轮廓清晰的图像<code>X</code>，并将整个图像向右移动一个像素，即<code>Z[i, j] = X[i, j + 1]</code>，则新图像<code>Z</code>的输出可能大不相同。而<u>在现实中，随着拍摄角度的移动，任何物体几乎不可能发生在同一像素上</u>。即使用三脚架拍摄一个静止的物体，由于快门的移动而引起的相机振动，可能会使所有物体左右移动一个像素（除了高端相机配备了特殊功能来解决这个问题）。</p></li></ol><h2 id="最大池化层和平均池化层">最大池化层和平均池化层</h2><p>与卷积层类似，池化层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口（有时称为<strong>池化窗口</strong>）遍历的每个位置计算一个输出。 然而，<span style="background:#daf5e9;">不同于卷积层中的输入与卷积核之间的互相关计算，池化层不包含参数</span>。 相反，池运算是确定性的，我们通常计算池化窗口中所有元素的最大值或平均值。这些操作分别称为<strong>最大池化层</strong>（maximum pooling）和<strong>平均池化层</strong>（average pooling）。</p><p>在这两种情况下，与互相关运算符一样，池化窗口从输入张量的左上角开始，从左往右、从上往下的在输入张量内滑动。在池化窗口到达的每个位置，它计算该窗口中输入子张量的最大值或平均值。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/pooling.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">池化窗口形状为2×2的最大池化层。着色部分是第一个输出元素，以及用于计算这个输出的输入元素:max(0,1,3,4)=4.</div></center><p>池化窗口形状为<span class="math inline">\(p \times q\)</span>的池化层称为<u><span class="math inline">\(p \times q\)</span>池化层</u>，池化操作称为<u><span class="math inline">\(p \times q\)</span>池化</u>。</p><p><span style="background:#daf5e9;">回到本节开头提到的对象边缘检测示例，现在我们将使用卷积层的输出作为2×2最大池化的输入。 设置卷积层输入为<code>X</code>，池化层输出为<code>Y</code>。 无论<code>X[i, j]</code>和<code>X[i, j + 1]</code>的值相同与否，池化层始终输出<code>Y[i, j] = 1</code>。 也就是说，<u>使用2×2最大池化层，即使在高度或宽度上移动一个元素，卷积层仍然可以识别到模式</u>。</span></p><p>在下面的代码中的<code>pool2d</code>函数，我们实现池化层的前向传播。 这类似于第二节中的<code>corr2d</code>函数。 然而，这里我们没有卷积核中的类似参数，输出为输入中每个区域的最大值或平均值。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们可以构建上图中的输入张量X，验证二维最大池化层的输出。</span></span><br><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[4., 5.],</span></span><br><span class="line"><span class="string">        [7., 8.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们还可以验证平均池化层。</span></span><br><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>), <span class="string">&#x27;avg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[2., 3.],</span></span><br><span class="line"><span class="string">        [5., 6.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="填充和步幅">填充和步幅</h2><p>与卷积层一样，池化层也可以改变输出形状。和以前一样，我们可以通过填充和步幅以获得所需的输出形状。 下面，我们用深度学习框架中内置的二维最大池化层，来演示池化层中填充和步幅的使用。 我们首先构造了一个输入张量<code>X</code>，它有四个维度，其中样本数和通道数都是1。</p><blockquote><p>注意！默认情况下，深度学习框架中的步幅与池化窗口的大小相同</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">X</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 0.,  1.,  2.,  3.],</span></span><br><span class="line"><span class="string">          [ 4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="string">          [ 8.,  9., 10., 11.],</span></span><br><span class="line"><span class="string">          [12., 13., 14., 15.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认情况下，深度学习框架中的步幅与池化窗口的大小相同。 因此，如果我们使用形状为(3, 3)的池化窗口，那么默认情况下，我们得到的步幅形状为(3, 3)。</span></span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[10.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 填充和步幅可以手动设定。</span></span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 5.,  7.],</span></span><br><span class="line"><span class="string">          [13., 15.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当然，我们可以设定一个任意大小的矩形池化窗口，并分别设定填充和步幅的高度和宽度。</span></span><br><span class="line">pool2d = nn.MaxPool2d((<span class="number">2</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 5.,  7.],</span></span><br><span class="line"><span class="string">          [13., 15.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="多个通道">多个通道</h2><p>在处理多通道输入数据时，池化层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总。 这意味着<span style="background:#daf5e9;">池化层的输出通道数与输入通道数相同</span>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们在通道维度上连结张量X和X + 1，以构建具有2个通道的输入。</span></span><br><span class="line">X = torch.cat((X, X + <span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">X</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 0.,  1.,  2.,  3.],</span></span><br><span class="line"><span class="string">          [ 4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="string">          [ 8.,  9., 10., 11.],</span></span><br><span class="line"><span class="string">          [12., 13., 14., 15.]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[ 1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">          [ 5.,  6.,  7.,  8.],</span></span><br><span class="line"><span class="string">          [ 9., 10., 11., 12.],</span></span><br><span class="line"><span class="string">          [13., 14., 15., 16.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如下，池化后输出通道的数量仍然是2。</span></span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 5.,  7.],</span></span><br><span class="line"><span class="string">          [13., 15.]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[ 6.,  8.],</span></span><br><span class="line"><span class="string">          [14., 16.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="池化层总结">池化层总结</h2><ul><li>对于给定输入元素，最大池化层会输出该窗口内的最大值，平均池化层会输出该窗口内的平均值。</li><li>池化层的主要优点之一是<strong>减轻卷积层对位置的过度敏感</strong>。</li><li>使用最大池化层以及大于1的步幅，可减少空间维度（如高度和宽度）。</li><li>我们可以指定池化层的填充和步幅。</li><li>池化层的输出通道数与输入通道数相同。</li></ul><h1 id="卷积神经网络-lenet">卷积神经网络 LeNet</h1><p>本节将介绍LeNet，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。 这个模型是由AT&amp;T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），目的是识别图像中的手写数字。 当时，Yann LeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果。</p><p>当时，LeNet取得了与支持向量机（support vector machines）性能相媲美的成果，成为监督学习的主流方法。 LeNet被广泛用于自动取款机（ATM）机中，帮助识别处理支票的数字。 时至今日，一些自动取款机仍在运行Yann LeCun和他的同事Leon Bottou在上世纪90年代写的代码呢！</p><h2 id="lenet">LeNet</h2><p>总体来看，LeNet（LeNet-5）由两个部分组成：</p><ul><li>卷积编码器：由两个卷积层组成;</li><li>全连接层密集块：由三个全连接层组成。</li></ul><p>该架构如下所示：</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/lenet.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">LeNet中的数据流。输入是手写数字，输出为10种可能结果的概率</div></center><p>每个卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均池化层。请注意，虽然ReLU和最大池化层更有效，但它们在20世纪90年代还没有出现。每个卷积层使用5×5卷积核和一个sigmoid激活函数。这些层将输入映射到多个二维特征输出，通常同时增加通道的数量。第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个2×2池操作（步幅2）通过空间下采样将维数即特征图大小减少4倍。卷积的输出形状由批量大小、通道数、高度、宽度决定。</p><p>为了将卷积块的输出传递给全连接层密集块即稠密块，我们必须在小批量中展平每个样本。<span style="background:#daf5e9;">换言之，我们将这个四维输入转换成全连接层所期望的二维输入。</span>这里的二维表示的<span style="background:#FFCC99;">第一个维度索引小批量中的样本，第二个维度给出每个样本的平面向量表示</span>。LeNet的稠密块有三个全连接层，分别有120、84和10个输出。因为我们在执行分类任务，所以输出层的10维对应于最后输出结果的数量。</p><p>通过下面的LeNet代码，可以看出用深度学习框架实现此类模型非常简单。我们只需要实例化一个<code>Sequential</code>块并将需要的层连接在一起。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>我们对原始模型做了一点小改动，去掉了最后一层的高斯激活。除此之外，这个网络与最初的LeNet-5一致。</p><p>下面，我们将一个大小为28×28的单通道（黑白）图像通过LeNet。通过在每一层打印输出的形状，我们可以检查模型，以确保其操作与我们期望的数据流图一致。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/lenet-vert.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">LeNet 的简化版</div></center><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape: \t&#x27;</span>,X.shape)</span><br><span class="line">    </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Conv2d output shape:         torch.Size([1, 6, 28, 28])</span></span><br><span class="line"><span class="string">Sigmoid output shape:        torch.Size([1, 6, 28, 28])</span></span><br><span class="line"><span class="string">AvgPool2d output shape:      torch.Size([1, 6, 14, 14])</span></span><br><span class="line"><span class="string">Conv2d output shape:         torch.Size([1, 16, 10, 10])</span></span><br><span class="line"><span class="string">Sigmoid output shape:        torch.Size([1, 16, 10, 10])</span></span><br><span class="line"><span class="string">AvgPool2d output shape:      torch.Size([1, 16, 5, 5])</span></span><br><span class="line"><span class="string">Flatten output shape:        torch.Size([1, 400])</span></span><br><span class="line"><span class="string">Linear output shape:         torch.Size([1, 120])</span></span><br><span class="line"><span class="string">Sigmoid output shape:        torch.Size([1, 120])</span></span><br><span class="line"><span class="string">Linear output shape:         torch.Size([1, 84])</span></span><br><span class="line"><span class="string">Sigmoid output shape:        torch.Size([1, 84])</span></span><br><span class="line"><span class="string">Linear output shape:         torch.Size([1, 10])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>注意，在整个卷积块中，与上一层相比，每一层特征的高度和宽度都减小了。 第一个卷积层使用2个像素的填充（上下/左右两边就是4个像素），来补偿5×5卷积核导致的特征减少。 相反，第二个卷积层没有填充，因此高度和宽度都减少了4个像素。 随着层叠的上升，通道的数量从输入时的1个，增加到第一个卷积层之后的6个，再到第二个卷积层之后的16个。 同时，每个池化层的高度和宽度都减半。最后，每个全连接层减少维数，最终输出一个维数与结果分类数相匹配的输出。</p><h2 id="模型训练">模型训练</h2><p>现在我们已经实现了LeNet，让我们看看LeNet在Fashion-MNIST数据集上的表现。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br></pre></td></tr></table></figure><p>虽然卷积神经网络的参数较少，但与深度的多层感知机相比，它们的计算成本仍然很高，因为每个参数都参与更多的乘法。 通过使用GPU，可以用它加快训练。</p><p>为了进行评估，我们需要对之前描述的<code>evaluate_accuracy</code>函数进行轻微的修改。 由于完整的数据集位于内存中，因此在模型使用GPU计算数据集之前，我们需要将其复制到显存中。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 设置为评估模式</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    <span class="comment"># 正确预测的数量，总预测的数量</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">                <span class="comment"># BERT微调所需的（之后将介绍）</span></span><br><span class="line">                X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                X = X.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            metric.add(d2l.accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>为了使用GPU，我们还需要一点小改动。 与之前定义的<code>train_epoch_ch3</code>不同，在进行正向和反向传播之前，我们需要将每一小批量数据移动到我们指定的设备（例如GPU）上。</p><p>如下所示，训练函数<code>train_ch6</code>也类似于之前定义的<code>train_ch3</code>。 由于我们将实现多层神经网络，因此我们将主要使用高级API。 以下训练函数假定从高级API创建的模型作为输入，并进行相应的优化。 我们使用在之前介绍的Xavier随机初始化模型参数。 与全连接层一样，我们使用交叉熵损失函数和小批量随机梯度下降。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch6</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用GPU训练模型(在第六章定义)&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)<span class="comment">#小批量随机梯度下降</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()<span class="comment">#交叉熵损失函数</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                            legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># 训练损失之和，训练准确率之和，样本数</span></span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()</span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>现在，我们训练和评估LeNet-5模型。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">loss 0.469, train acc 0.823, test acc 0.779</span></span><br><span class="line"><span class="string">55296.6 examples/sec on cuda:0</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><figure><img src="https://zh-v2.d2l.ai/_images/output_lenet_4a2e9e_67_1.svg" alt="zh-v2.d2l.ai/_images/output_lenet_4a2e9e_67_1.svg" /><figcaption aria-hidden="true">zh-v2.d2l.ai/_images/output_lenet_4a2e9e_67_1.svg</figcaption></figure><h2 id="卷积神经网络-lenet总结">卷积神经网络 LeNet总结</h2><ul><li>卷积神经网络（CNN）是一类使用卷积层的网络。</li><li>在卷积神经网络中，我们组合使用卷积层、非线性激活函数和池化层。</li><li>为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。</li><li>在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。</li><li>LeNet是最早发布的卷积神经网络之一。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CBAM注意力机制</title>
      <link href="/2024/01/23/CBAM%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
      <url>/2024/01/23/CBAM%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文：《CBAM：Convolutional Block Attention Module》</p><p>论文参考样式：Woo S, Park J, Lee J Y, et al. Cbam: Convolutional block attention module[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 3-19.</p><p>论文链接：<a href="https://arxiv.org/pdf/1807.06521.pdf">CBAM：Convolutional Block Attention Module</a></p><p>demo: <a href="https://github.com/Jongchan/attention-module">GitHub - Jongchan/attention-module: Official PyTorch code for "BAM: Bottleneck Attention Module (BMVC2018)" and "CBAM: Convolutional Block Attention Module (ECCV2018)"</a></p></blockquote><blockquote><p>参考网址：</p><p><a href="https://zhuanlan.zhihu.com/p/101590167">CBAM：卷积注意力机制模块 - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/Roaddd/article/details/114646354">【注意力机制】CBAM详解（文末附代码）_cbam注意力-CSDN博客</a></p><p><a href="https://blog.csdn.net/m0_45447650/article/details/123983483">CBAM——即插即用的注意力模块（附代码）_cbam模块-CSDN博客</a></p></blockquote><h1 id="摘要">摘要</h1><p>本文（2018，ECCV）提出了卷积注意力模块——CBAM，这是一种用于前馈卷积神经网络的轻量级的注意力模块。 给定一个中间特征图，CBAM模块会沿着两个独立的维度（通道和空间）依次推断注意力图，然后将注意力图与输入特征图相乘以进行自适应特征优化。 由于CBAM是轻量级的通用模块，因此可以忽略的该模块的开销而将其无缝集成到任何CNN架构中，并且可以与基础CNN一起进行端到端训练。</p><p>论文在 ResNet 和 MobileNet 等经典结构上添加了 CBAM 模块并进行对比分析实验，同时也进行了CAM可视化，发现 CBAM 更关注识别目标物体，这也使得 CBAM 具有更好的解释性。本文验证所用的数据集有 ImageNet-1K，MS COCO检测和VOC 2007检测数据集。 实验表明，使用该模块在各种模型上，并在分类和检测性能方面的持续改进，证明了CBAM的广泛适用性。</p><blockquote><p>关于CAM可视化：<a href="https://aistudio.baidu.com/projectdetail/1655497">一文搞懂卷积网络之五（注意力可视化Grad-CAM） - 飞桨AI Studio星河社区 (baidu.com)</a></p></blockquote><h1 id="模型">模型</h1><p>CBAM模型结构如下所示：</p><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240109153927099.png" alt="image-20240109153927099" /><figcaption aria-hidden="true">image-20240109153927099</figcaption></figure><p>Convolutional Block Attention Module (CBAM) 表示卷积模块的注意力机制模块，可以看到 CBAM 包含2个独立的子模块：<font color=#4eb434>通道注意力模块（CAM，Channel Attention Module)</font> 和<font color=#985fff>空间注意力模块（SAM，Spartial Attention Module)</font> ，分别进行通道和空间的Attention。这样不只能够节约参数和计算力，并且保证了其能够做为即插即用的模块集成到现有的网络架构中去。相比于SENet 只关注<font color=#4eb434>通道（channel）</font>的注意力机制可以取得更好的效果。</p><blockquote><p>通道上的 Attention 机制在 2017 年的 SENet 就被提出，SENet可以参考<a href="https://blog.csdn.net/Roaddd/article/details/111357490">这篇文章</a>。事实上，CAM 与 SENet 相比，只是多了一个并行的 Max Pooling 层，提取到的高层特征更全面，更丰富。至于为何如此更改，论文也给出了解释和实验数据支持。</p></blockquote><p>由上图，CBAM由四个模块组成，分别是：输入特征、通道注意力模块、空间注意力模块和输出的精制特征图。整体流程大致描述如下</p><ol type="1"><li>输入特征<span class="math inline">\(F\in R^{C\times H\times W}\)</span>会先通过一个通道注意力模块，进行一维卷积<span class="math inline">\(M_c\in R^{C\times 1\times 1}\)</span>，将卷积结果乘原特征图<span class="math inline">\(F\)</span>得到加权结果<span class="math inline">\(F&#39;\)</span>。</li><li><span class="math inline">\(F&#39;\)</span>会再经过一个空间注意力模块，进行二维卷积<span class="math inline">\(M_s\in R^{1\times H\times W}\)</span>，再次将卷积结果与<span class="math inline">\(F&#39;\)</span>相乘，最终得到加权结果<span class="math inline">\(F&#39;&#39;\)</span>。</li></ol><p>用公式表示为 <span class="math display">\[F&#39;=M_c\left( F \right) \otimes F,\]</span> <span class="math display">\[F&#39;&#39;=M_s\left( F&#39; \right) \otimes F&#39;,\]</span></p><h2 id="channel-attention-modulecam">Channel Attention Module（CAM）</h2><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240109155441087.png" alt="image-20240109155441087" /><figcaption aria-hidden="true">image-20240109155441087</figcaption></figure><blockquote><p><strong>通道注意力模块</strong>：<strong>通道维度不变，压缩空间维度</strong>。该模块关注输入图片中<strong>有意义的信息</strong>(分类任务就关注因为什么分成了不同类别)。</p></blockquote><p>通道注意力模块CAM如上图所示。</p><ol type="1"><li>将输入的特征图feature map <span style="background:#dad5e9;">F</span>，分别经过并行的基于width和height的global max pooling 和global average pooling，将特征图从C×H×W变为C×1×1的大小，然后分别经过MLP。</li><li>在MLP中，它先将通道数压缩为原来的1/r（Reduction，减少率）倍，再扩张到原通道数，经过ReLU激活函数得到两个激活后的结果。</li><li>将MLP输出的两个结果进行基于element-wise的加和操作（即逐元素相加），再经过sigmoid激活操作，生成最终的channel attention feature-map <span style="background:#dad5e9;">M_c</span>。</li><li>将该channel attention feature-map和原来的input feature-map做element-wise乘法操作（乘完后变回C×H×W的大小），生成Spatial attention模块需要的输入特征 <span style="background:#dad5e9;">F'</span>。</li></ol><p>以上是通道注意力机制的步骤。</p><p>换一个角度考虑，通道注意力机制（Channel Attention Module）是将特征图在空间维度上进行压缩，得到一个一维矢量后再进行操作。在空间维度上进行压缩时，不仅考虑到了平均值池化（Average Pooling）还考虑了最大值池化（Max Pooling）。平均池化和最大池化可用来聚合特征映射的空间信息，送到一个共享网络，压缩输入特征图的空间维数，逐元素求和合并，以产生通道注意力图。单就一张图来说，通道注意力，关注的是这张图上哪些内容是有重要作用的。<u>平均值池化对特征图上的每一个像素点都有反馈，而最大值池化在进行梯度反向传播计算时，只有特征图中响应最大的地方有梯度的反馈。</u>通道注意力机制可以表达为： <span class="math display">\[\begin{aligned}    M_c\left( F \right) &amp;=\sigma \left( MLP\left( AvgPool\left( F \right) \right) +MLP\left( MaxPool\left( F \right) \right) \right)\\    &amp;=\sigma \left( W_1\left( W_0\left( F_{avg}^{c} \right) \right) +W_1\left( W_0\left( F_{\max}^{c} \right) \right) \right)\\\end{aligned}\]</span></p><blockquote><p>在channel attention中，表1对于pooling的使用进行了实验对比，发现avg &amp; max的并行池化的效果要更好。这里也有可能是池化丢失的信息太多，avg&amp;max的并行连接方式比单一的池化丢失的信息更少，所以效果会更好一点。</p></blockquote><h2 id="spatial-attention-modulesam">Spatial Attention Module（SAM）</h2><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240109163244599.png" alt="image-20240109163244599" /><figcaption aria-hidden="true">image-20240109163244599</figcaption></figure><blockquote><p><strong>空间注意力模块</strong>：<strong>空间维度不变，压缩通道维度</strong>。该模块关注的是<strong>目标的位置信息</strong>。</p></blockquote><p>空间注意力模块如上图所示。将Channel attention模块输出的特征图 <span style="background:#dad5e9;">F‘</span> 作为本模块的输入特征图。首先做一个基于channel的global max pooling 和global average pooling，得到两个1×H×W 的特征图，然后将这2个特征图基于channel 做concat操作（通道拼接）。然后经过一个7×7卷积操作（7×7比3×3效果要好），降维为1个channel的特征图，即1×H×W。再经过sigmoid生成spatial attention feature <span style="background:#dad5e9;">M_s</span>。最后将该feature和该模块的输入feature做乘法（乘之后变回C×H×W的大小），得到最终生成的特征 <span style="background:#dad5e9;">F’‘</span>。</p><p>同样，空间注意力机制（Spatial Attention Module）是对通道进行压缩，在通道维度分别进行了平均值池化和最大值池化。MaxPool的操作就是在通道上提取最大值，提取的次数是高乘以宽；AvgPool的操作就是在通道上提取平均值，提取的次数也是是高乘以宽；接着将前面所提取到的特征图（通道数都为1）合并得到一个2通道的特征图。空间注意力机制可以表达为： <span class="math display">\[\begin{aligned}    M_s\left( F \right) &amp;=\sigma \left( f^{7\times 7}\left( \left[ AvgPool\left( F \right) ;MaxPool\left( F \right) \right] \right) \right)\\    &amp;=\sigma \left( f^{7\times 7}\left( \left[ F_{avg}^{s};F_{\max}^{s} \right] \right) \right)\\\end{aligned}\]</span> 其中，σ 为sigmoid操作，7×7表示卷积核的大小，7×7的卷积核比3×3的卷积核效果更好。</p><h1 id="实验">实验</h1><p>本文中，进行了较多的对比实验，旨在验证注意力模块的积极作用。</p><p>首先，对比了通道、空间以及通道&amp;空间，不同注意力机制的效果。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240109164403936.png"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">仅使用Channel Attention，对比的是使用AvgPool、MaxPool以及都使用时的性能</div></center><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240109170246533.png"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">仅使用Spatial Attention，对比的是不同Avg，Max以及kernel_size的性能差异</div></center><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240109170551461.png"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">CBAM模块，对比的是不同顺序的性能差异</div></center><p>关于图3，通道注意力和空间注意力这两个模块能够以并行或者串行顺序的方式组合在一块儿，关于通道和空间上的串行顺序和并行作者进行了实验对比，可以看到的是，先使用Channel（AvgPool&amp;MaxPool），再使用Spatial（avg&amp;max，k=7）——即先通道后空间的性能是最优的。</p><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240109170744084.png" alt="image-20240109170744084" /><figcaption aria-hidden="true">image-20240109170744084</figcaption></figure><p>上图给出了ImageNet-1K数据集上，训练误差的曲线。同样的证明了，CBAM模块在训练集合验证集上，相比于Baseline和SE注意力机制，都有一定的提升。</p><p>最后，是使用Grad-CAM进行了可视化，以来证明CBAM是真正地提取出了积极有效的特征：利用 Grad-CAM 对不一样的网络进行可视化后，能够发现，引入 CBAM 后，特征覆盖到了待识别物体的更多部位，而且最终判别物体的几率也更高，这代表注意力机制的确让网络学会了关注重点信息。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240109171005799.png"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">CBAM，SE，Baseline等最后一层卷积层输出使用 grad-CAM 进行可视化的对比图</div></center><h1 id="总结">总结</h1><ol type="1"><li><p>论文提出了一种基于注意力机制的轻量型结构 CBAM ，基本上可以添加到所有常规的卷积层中。</p></li><li><p>文中验证了 Channel Attention Module 中 avg 与 max 并行的方式最好，接下来通过实验验证了 Channel Attention Module 和 Spatial Attention Module 的最佳先后顺序是先通道后空间，还对比了 CBAM 与 SENet 的性能。</p></li><li><p>文章还在实验中应用grad-CAM可视化了 CBAM 的关注区域（在图像分类任务中可以观察feature map的特征，解释了为什么模型将原图分类到某一类的结果），使得 CBAM 具有更好的解释性。</p></li><li><p>加入CBAM模块不一定会给网络带来性能上的提升，受自身网络还有数据等其他因素影响，甚至会下降。如果网络模型的泛化能力已经很强，而你的数据集不是benchmarks而是自己采集的数据集的话，不建议加入CBAM模块。CBAM性能虽然改进的比SE高了不少，但绝不是无脑加入到网络里就能有提升的。也要根据自己的数据、网络等因素综合考量。</p></li></ol><h1 id="代码">代码</h1>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初步认识 torch.nn</title>
      <link href="/2024/01/23/%E8%AE%A4%E8%AF%86%20torch.nn/"/>
      <url>/2024/01/23/%E8%AE%A4%E8%AF%86%20torch.nn/</url>
      
        <content type="html"><![CDATA[<blockquote><p>搬运链接：[<a href="https://blog.csdn.net/HiWangWenBing/article/details/120614234">Pytorch系列-30]：神经网络基础 - torch.nn库五大基本功能：nn.Parameter、nn.Linear、nn.functioinal、nn.Module、nn.Sequentia-CSDN博客</a></p><p>官方链接1：<a href="https://pytorch.org/docs/1.2.0/">PyTorch documentation — PyTorch master documentation</a></p><p>官方链接2：<a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#torchnn">torch.nn - PyTorch中文文档 (pytorch-cn.readthedocs.io)</a></p></blockquote><blockquote><p>nn是Neural Network的简称，帮助程序员方便执行如下的与神经网络相关的行为：创建、训练、保存和恢复神经网络。</p></blockquote><h1 id="一torch.nn简介">一、torch.nn简介</h1><h2 id="相关库的导入">相关库的导入</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#环境准备</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np              <span class="comment"># numpy数组库</span></span><br><span class="line"><span class="keyword">import</span> math                     <span class="comment"># 数学运算库</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 画图库</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> torch             <span class="comment"># torch基础库</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn    <span class="comment"># torch神经网络库</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><h1 id="二nn.linear类全连接层">二、nn.Linear类（全连接层）</h1><blockquote><p>nn.Linear本身并不包含激活函数（激活函数在Functional中)</p></blockquote><h2 id="函数说明">函数说明</h2><figure><img src="https://img-blog.csdnimg.cn/20191102164419608.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyMDc5Njg5,size_16,color_FFFFFF,t_70#pic_center" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><ul><li><strong>in_features</strong>: 输入的二维张量的大小。Y = WX + b, X的维度就是in_features，X的维度决定的W的维度， 总的参数个数 = in_features + 1。</li><li><strong>out_features：</strong>输出的二维张量的大小。</li></ul><p>从输入输出的张量的shape角度来理解，相当于一个输入为[batch_size, in_features]的张量变换成了[batch_size, out_features]的输出张量。</p><h2 id="使用nn.linear类创建全连接层">使用nn.Linear类创建全连接层</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nn.Linear</span></span><br><span class="line"><span class="comment"># 建立单层的多输入、多输出全连接层</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># in_features由输入张量的形状决定，out_features则决定了输出张量的形状</span></span><br><span class="line">full_connect_layer = nn.Linear(in_features=<span class="number">64</span> * <span class="number">64</span> * <span class="number">3</span>, out_features=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;full_connect_layer:&quot;</span>, full_connect_layer)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;parameters        :&quot;</span>, full_connect_layer.parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假定输入的图像形状为[64,64,3]</span></span><br><span class="line">x_input = torch.randn(<span class="number">1</span>, <span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>) <span class="comment">#或 x_input = x_input.view(1, -1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将四维张量转换为二维张量之后，才能作为全连接层的输入</span></span><br><span class="line">x_input = x_input.view(<span class="number">1</span>, <span class="number">64</span> * <span class="number">64</span> * <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input.shape:&quot;</span>, x_input.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用全连接层</span></span><br><span class="line">y_output = full_connect_layer(x_input)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_output.shape:&quot;</span>, y_output.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_output:&quot;</span>, y_output)</span><br></pre></td></tr></table></figure><p>full_connect_layer: Linear(in_features=12288, out_features=2, bias=True) parameters : &lt;bound method Module.parameters of Linear(in_features=12288, out_features=2, bias=True)&gt; x_input.shape: torch.Size([1, 12288]) y_output.shape: torch.Size([1, 2]) y_output: tensor([[-0.5883, -0.4527]], grad_fn=<AddmmBackward>)</p><h1 id="三nn.functional常见函数">三、nn.functional（常见函数）</h1><h2 id="nn.functional-概述">nn.functional 概述</h2><p>nn.functional（通常按惯例导入到 F 命名空间中）定义了创建神经网络所需要的一些常见的处理函数。如<strong>激活函数</strong>、<strong>损失函数</strong>、<strong>正则化函数</strong>和<strong>非状态（non-stateful）版本的层（如卷积层和线性层）</strong>等。</p><figure><img src="https://img-blog.csdnimg.cn/20211005202351328.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_14,color_FFFFFF,t_70,g_se,x_16" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h1 id="四nn.xxx和nn.functional.xxx比较">四、nn.Xxx和nn.functional.xxx比较</h1><h2 id="相同点">相同点</h2><p><code>nn.Xxx</code>和<code>nn.functional.xxx</code>的实际功能是相同的，即<code>nn.Conv2d</code>和<code>nn.functional.conv2d</code> 都是进行卷积，<code>nn.Dropout</code> 和<code>nn.functional.dropout</code>都是进行dropout，。。。。。；</p><ul><li>运行效率也是近乎相同。</li></ul><h2 id="不同点">不同点</h2><ul><li>形式看：<code>nn.functional.xxx</code>是<strong>小写</strong>字母开头，nn.Xxx中的函数是<strong>大写</strong>字母开头。</li><li><code>nn.functional.xxx</code>是API函数接口，而<code>nn.Xxx</code>是对原始API函数<code>nn.functional.xxx</code>的<strong>类封装</strong>。</li><li>所有<strong><code>nn.Xxx</code>都继承于于共同祖先<code>nn.Module</code>。</strong>这一点导致<code>nn.Xxx</code>除了具有<code>nn.functional.xxx</code>功能之外，内部附带了<code>nn.Module</code>相关的属性和方法，例如<code>train(), eval(),load_state_dict, state_dict</code>等。</li><li><code>nn.Xxx</code>继承于<code>nn.Module</code>， 能够很好的与<code>nn.Sequential</code>结合使用， 而<code>nn.functional.xxx</code>无法与<code>nn.Sequential</code>结合使用。</li><li><code>nn.Xxx</code> 需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据。<code>nn.functional.xxx</code>同时传入输入数据和weight, bias等其他参数 。</li><li><code>nn.Xxx</code>不需要你自己定义和管理weight；而<code>nn.functional.xxx</code>需要你自己定义weight，每次调用的时候都需要手动传入weight, 不利于代码复用。</li></ul><h1 id="五nn.parameter类">五、nn.Parameter类</h1><h2 id="nn.parameter概述">nn.Parameter概述</h2><p><code>Parameter</code>也是<code>Tensor</code>（或者说Tensor的包装器wrapper），也就是说是一个多维矩阵，是Variable类中的一个特殊子类。它告诉 <code>Module</code> 它具有在反向传播期间需要更新的权重。 只更新具有 <code>requires_grad</code> 属性的 <code>tensor</code>。</p><p>当我们创建一个Module时，nn会自动创建相应的参数parameter，并会自动累加到模型的Parameter成员列表中。</p><h4 id="语法">语法</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.nn.parameter.Parameter(data=<span class="literal">None</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li>data (Tensor) – parameter tensor. —— 输入得是一个<code>tensor</code></li><li>requires_grad (bool, optional) – if the parameter requires gradient. See Locally disabling gradient computation for more details. <strong>Default: True</strong> —— 这个不用解释，<strong>需要注意的是<code>nn.Parameter()</code>默认有梯度</strong>。</li></ul><p><code>torch.nn.Parameter()</code>将一个不可训练的tensor转换成可以训练的类型parameter，并将这个parameter绑定到这个module里面。即在定义网络时这个tensor就是一个可以训练的参数了。使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。</p><figure><img src="https://img-blog.csdnimg.cn/20211005202810135.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_13,color_FFFFFF,t_70,g_se,x_16" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="单个全连接层中参数的个数">单个全连接层中参数的个数</h2><blockquote><p><strong>in_features的数量，决定的参数的个数 Y = WX + b, X的维度就是in_features，X的维度决定的W的维度， 总的参数个数 = in_features + 1</strong></p><p><strong>out_features的数量，决定了全连接层中神经元的个数，因为每个神经元只有一个输出。</strong></p><p>多少个输出，就需要多少个神经元 (一个 WX + b 就是一个神经元)！</p></blockquote><p>总的W参数的个数= <strong>in_features * out_features</strong></p><p>总的b参数的个数= <strong>1 * out_features</strong></p><p>总的参数（W和B）的个数= (<strong>in_features + 1) * out_features</strong></p><h2 id="使用参数创建全连接层代码例子">使用参数创建全连接层代码例子</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nn.functional.linear( )</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x_input = torch.Tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input.shape:&quot;</span>, x_input.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input      :&quot;</span>, x_input)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">Weights1 = nn.Parameter(torch.rand(<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights.shape:&quot;</span>, Weights1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights      :&quot;</span>, Weights1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">Bias1 = nn.Parameter(torch.rand(<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bias.shape:&quot;</span>, Bias1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bias      :&quot;</span>, Bias1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">Weights2 = nn.Parameter(torch.Tensor(<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights.shape:&quot;</span>, Weights2.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights      :&quot;</span>, Weights2)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nfull_connect_layer&quot;</span>)</span><br><span class="line">full_connect_layer = nn.functional.linear(x_input, Weights1)</span><br><span class="line"><span class="built_in">print</span>(full_connect_layer)</span><br></pre></td></tr></table></figure><p>x_input.shape: torch.Size([3]) x_input : tensor([1., 1., 1.])</p><p>Weights.shape: torch.Size([3]) Weights : Parameter containing: tensor([0.8948, 0.0114, 0.3688], requires_grad=True)</p><p>Bias.shape: torch.Size([1]) Bias : Parameter containing: tensor([0.8087], requires_grad=True)</p><p>Weights.shape: torch.Size([3]) Weights : Parameter containing: tensor([1.4013e-45, 0.0000e+00, 0.0000e+00], requires_grad=True)</p><p>full_connect_layer tensor(1.2750, grad_fn=<DotBackward>)</p><h1 id="六nn.mudule类">六、nn.Mudule类</h1><p>创建一个可调用的对象，其行为类似于一个函数，但也可以包含状态（例如神经网络层权重）。 它知道它包含哪些参数，并且可以将所有梯度归零，循环遍历它们更新权重等。</p><figure><img src="https://img-blog.csdnimg.cn/2021100520265815.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_14,color_FFFFFF,t_70,g_se,x_16" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h1 id="七利用nn.sequential类创建神经网络继承于nn.module类">七、利用nn.Sequential类创建神经网络（继承于nn.Module类）</h1><p>nn.Sequential是一个有序的容器，该类将按照传入构造器的顺序，依次创建相应的函数，并记录在Sequential类对象的数据结构中。同时，以神经网络模块为元素的有序字典也可以作为传入参数。</p><p>因此，Sequential可以看成是有多个函数运算对象，串联成的神经网络，其返回的是Module类型的神经网络对象。</p><h2 id="以列表的形式串联函数运算构建串行执行的神经网络">以列表的形式，串联函数运算，构建串行执行的神经网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;利用系统提供的神经网络模型类：Sequential,以参数列表的方式来实例化神经网络模型对象&quot;</span>)</span><br><span class="line"><span class="comment"># A sequential container. Modules will be added to it in the order they are passed in the constructor.</span></span><br><span class="line"><span class="comment"># Example of using Sequential</span></span><br><span class="line">model_c = nn.Sequential(nn.Linear(<span class="number">28</span> * <span class="number">28</span>, <span class="number">32</span>), </span><br><span class="line">                        nn.ReLU(), </span><br><span class="line">                        nn.Linear(<span class="number">32</span>, <span class="number">10</span>), </span><br><span class="line">                        nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">                       )</span><br><span class="line"><span class="built_in">print</span>(model_c)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n显示网络模型参数&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_c.parameters)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n定义神经网络样本输入&quot;</span>)</span><br><span class="line">x_input = torch.randn(<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line">x_input = x_input.view(x_input.size()[<span class="number">0</span>], -<span class="number">1</span>)  <span class="comment">#将输入转为二维张量</span></span><br><span class="line"><span class="built_in">print</span>(x_input.size())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用神经网络进行预测&quot;</span>)</span><br><span class="line">y_pred = model_c.forward(x_input)</span><br><span class="line"><span class="built_in">print</span>(y_pred)  <span class="comment"># torch.Size([2, 10])</span></span><br></pre></td></tr></table></figure><p>利用系统提供的神经网络模型类：Sequential,以参数列表的方式来实例化神经网络模型对象 Sequential( (0): Linear(in_features=784, out_features=32, bias=True) (1): ReLU() (2): Linear(in_features=32, out_features=10, bias=True) (3): Softmax(dim=1) )</p><p>显示网络模型参数 &lt;bound method Module.parameters of Sequential( (0): Linear(in_features=784, out_features=32, bias=True) (1): ReLU() (2): Linear(in_features=32, out_features=10, bias=True) (3): Softmax(dim=1) )&gt;</p><p>定义神经网络样本输入 torch.Size([2, 28, 28, 1]) torch.Size([2, 784])</p><p>使用神经网络进行预测 tensor([[0.0773, 0.0843, 0.1366, 0.0933, 0.1107, 0.1086, 0.0721, 0.1129, 0.1267, 0.0777], [0.0875, 0.0570, 0.1179, 0.0981, 0.1261, 0.1177, 0.1174, 0.0735, 0.0932, 0.1116]], grad_fn=<SoftmaxBackward>)</p><h2 id="以字典的形式串联函数运算构建串行执行的神经网络">以字典的形式，串联函数运算，构建串行执行的神经网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;利用系统提供的神经网络模型类：Sequential,以字典的方式来实例化神经网络模型对象&quot;</span>)</span><br><span class="line">model = nn.Sequential(OrderedDict([(<span class="string">&#x27;h1&#x27;</span>, nn.Linear(<span class="number">28</span> * <span class="number">28</span>, <span class="number">32</span>)),</span><br><span class="line">                                   (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">                                   (<span class="string">&#x27;out&#x27;</span>, nn.Linear(<span class="number">32</span>, <span class="number">10</span>)),</span><br><span class="line">                                   (<span class="string">&#x27;softmax&#x27;</span>, nn.Softmax(dim=<span class="number">1</span>))])</span><br><span class="line">                     )</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n显示网络模型参数&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model.parameters)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n定义神经网络样本输入&quot;</span>)</span><br><span class="line">x_input = torch.randn(<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用神经网络进行预测&quot;</span>)</span><br><span class="line">y_pred = model.forward(x_input.view(x_input.size()[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(y_pred)   <span class="comment"># torch.Size([2, 10])</span></span><br></pre></td></tr></table></figure><p>利用系统提供的神经网络模型类：Sequential,以字典的方式来实例化神经网络模型对象 Sequential( (h1): Linear(in_features=784, out_features=32, bias=True) (relu1): ReLU() (out): Linear(in_features=32, out_features=10, bias=True) (softmax): Softmax(dim=1) )</p><p>显示网络模型参数 &lt;bound method Module.parameters of Sequential( (h1): Linear(in_features=784, out_features=32, bias=True) (relu1): ReLU() (out): Linear(in_features=32, out_features=10, bias=True) (softmax): Softmax(dim=1) )&gt;</p><p>定义神经网络样本输入 torch.Size([2, 28, 28, 1])</p><p>使用神经网络进行预测 tensor([[0.0967, 0.1006, 0.0714, 0.0752, 0.1209, 0.1088, 0.1215, 0.1156, 0.0879, 0.1015], [0.0912, 0.1031, 0.0914, 0.0962, 0.1124, 0.0999, 0.0885, 0.1082, 0.0953, 0.1137]], grad_fn=<SoftmaxBackward>)</p><h1 id="八-自定义神经网络模型类继承于module类">八、 自定义神经网络模型类（继承于Module类）</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义网络模型：带relu的两层全连接神经网络</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;自定义新的神经网络模型的类&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">#原始写法</span></span><br><span class="line"><span class="string">class NetC(torch.nn.Module):</span></span><br><span class="line"><span class="string">    # 定义神经网络</span></span><br><span class="line"><span class="string">    def __init__(self, n_feature, n_hidden, n_output):</span></span><br><span class="line"><span class="string">        super(NetC, self).__init__()</span></span><br><span class="line"><span class="string">        self.h1 = nn.Linear(n_feature, n_hidden)  #subModule: Linear</span></span><br><span class="line"><span class="string">        self.relu1 = nn.ReLU()</span></span><br><span class="line"><span class="string">        self.out = nn.Linear(n_hidden, n_output)</span></span><br><span class="line"><span class="string">        self.softmax = nn.Softmax(dim=1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # 定义前向运算</span></span><br><span class="line"><span class="string">    def forward(self, x_input):</span></span><br><span class="line"><span class="string">        h1 = self.h1(x_input)</span></span><br><span class="line"><span class="string">        a1 = self.relu1(h1)</span></span><br><span class="line"><span class="string">        out = self.out(a1)</span></span><br><span class="line"><span class="string">        a_out = self.softmax(out)</span></span><br><span class="line"><span class="string">        return a_out</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#用torch.nn.functional引入激活函数的写法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NetC</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 定义神经网络</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">        <span class="built_in">super</span>(NetC, self).__init__()</span><br><span class="line">        self.h1 = nn.Linear(n_feature, n_hidden)  <span class="comment">#subModule: Linear</span></span><br><span class="line">        self.out = nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向运算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_input</span>):</span><br><span class="line">        h1 = F.relu(self.h1(x_input))</span><br><span class="line">        out = F.softmax(self.out(h1),dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n实例化神经网络模型对象&quot;</span>)</span><br><span class="line">model = NetC(<span class="number">28</span> * <span class="number">28</span>, <span class="number">32</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n显示网络模型参数&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model.parameters)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n定义神经网络样本输入&quot;</span>)</span><br><span class="line">x_input = torch.randn(<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line"><span class="comment"># 得到的数据格式torch.Size([64, 1, 28, 28])需要转变为（64,784）</span></span><br><span class="line">x_input = x_input.view(x_input.size()[<span class="number">0</span>], -<span class="number">1</span>)  <span class="comment"># -1表示自动匹配</span></span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用神经网络进行预测&quot;</span>)</span><br><span class="line">y_pred = model.forward(x_input)</span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br></pre></td></tr></table></figure><p>自定义新的神经网络模型的类</p><p>实例化神经网络模型对象 NetC( (h1): Linear(in_features=784, out_features=32, bias=True) (out): Linear(in_features=32, out_features=10, bias=True) )</p><p>显示网络模型参数 &lt;bound method Module.parameters of NetC( (h1): Linear(in_features=784, out_features=32, bias=True) (out): Linear(in_features=32, out_features=10, bias=True) )&gt;</p><p>定义神经网络样本输入 torch.Size([2, 28, 28, 1]) torch.Size([2, 784])</p><p>使用神经网络进行预测 tensor([[0.0884, 0.0733, 0.0890, 0.1088, 0.1589, 0.0944, 0.0861, 0.1191, 0.0876, 0.0944], [0.1134, 0.0963, 0.0595, 0.1051, 0.0881, 0.1059, 0.0627, 0.1023, 0.1605, 0.1063]], grad_fn=<SoftmaxBackward>)</p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对象和类 学习笔记</title>
      <link href="/2024/01/08/%E7%B1%BB%E5%92%8C%E5%AF%B9%E8%B1%A1%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2024/01/08/%E7%B1%BB%E5%92%8C%E5%AF%B9%E8%B1%A1%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<blockquote><p>部分内容参考网址：<a href="https://www.cnblogs.com/jialexu/articles/14188861.html">【Python学习】对象和类 - xujiale - 博客园 (cnblogs.com)</a></p></blockquote><h1 id="什么是面向对象编程">什么是面向对象编程</h1><h2 id="背景知识">背景知识</h2><p>20世纪70年代中期，人们开始撰写文章来介绍这种编程方法的优点。大概在同一时间， 编程语言SmallTalk（施乐研究中心）和CLU（麻省理工学院）为这种思想提供了语言上 的支持。</p><p>1967年5月20日，挪威奥斯陆计算中心的科学家Ole-Johan Dahl和Kristen Nygaard正式 发布了Simula 67语言。它被认为是最早的 面向对象程序设计 语言，首次引入类、对象、 继承、动态绑定等重要概念。因此，1983年共同获得图灵奖。</p><p>SmallTalk语言起源于施乐帕罗奥多研究中心（Xerox PARC）的一项研究项目。它被公认 为历史上第二个面向对象的程序设计语言和第一个真正的集成开发环境 (IDE) 。</p><p>但C++和Java的出现才真正使这种思想成为现实。</p><h2 id="具体概念">具体概念</h2><blockquote><p>传统的程序设计，即<strong>面向过程的编程</strong>主张将程序看作一系列函数的集合，或者直接就是一系列对计算机下达的指令。</p></blockquote><p><strong>面向对象程序设计（Object Oriented Programming, OOP）</strong>可以看作一种<strong>在程序中包含各种独立而又互相调用的对象</strong>的思想。面向对象编程的一个重要特点就是<span style="background:#f9eda6;color:red"><strong>数据封装</strong></span>。</p><p><strong>对象（Object）</strong>则指的是<strong>类（Class）</strong>的实例。 在面向对象程序编程里，计算机程序将<strong>对象</strong>作为程序的基本单元，将程序和数据封装其中，以提高软件的重用性、灵活性和扩展性。</p><p>面向对象程序设计中的每一个对象都应该能够接受数据、处理数据并将数据传达给其它对象，因此它们都可以被看作一个小型的“机器”，即<strong>对象</strong>。<u>Python里面所有的东西都是对象(<em>objects</em>)，连同一个整数也是一种对象</u>，该语法设计可以巧妙的隐藏诸多细节。面向对象程序设计推广了程序的灵活性和可维护性，并且<strong>在大型项目设计中广为应用</strong>。</p><p>面向对象不仅指一种程序设计方法。它更多意义上是一种程序开发方式。许多流行的编程语言是面向对象的: <span style="background:#fbd4d0;">Python、C++、Objective-C、Java、Swift、C#、Perl、Ruby 与 PHP</span>等。</p><p><strong>Python支持面向过程、面向对象等编程方式。</strong>Python不强制使用任何一种编程方式，可以使用面向过程方式编写任何程序，但在中大型项目中，面向对象会带来很多优势。</p><h3 id="一个栗子">一个栗子</h3><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231218105617559.png" alt="image-20231218105617559" /><figcaption aria-hidden="true">image-20231218105617559</figcaption></figure><h1 id="什么是对象和类">什么是对象和类</h1><h2 id="对象的概念">对象的概念</h2><p><strong>对象是封装的数据抽象：对象是一个数据和操作的封装体。</strong>封装的目的就是<u>阻止非法的访问</u>，因此<strong>对象实现了信息的隐藏</strong>，外部只能通过操作接口访 问对象数据。</p><p>对象包含<strong>属性（attribute）</strong>和<strong>方法（method）</strong>。当你要创建一个新的object时，就必须先定义一个新的类，用它来清楚规范其类别可以创造出来的对象有什么样的属性与方法。</p><p><font color=orange>对象</font>像名词，<font color=blue>方法</font>就像个动词。<font color=orange>对象</font>代表一个独立的事物，<font color=blue>方法</font>用来定义它如何与其他<font color=orange>对象</font>相互作用。与模块不同的是，你可以同时创建多个同类别的<font color=orange>对象</font>，他们之间的属性值可能各有不同。</p><blockquote><p><strong><u>抽象数据类型</u></strong>是一个由对象以及对象上的操作组成的集合，对象和操作被捆绑为一个整体，可以从程序的一个部分传递到另一个部分。</p><p>这些操作的规范定义了抽象数据类型和程序其他部分之间的<strong><u>接口</u></strong>。</p><p><u>接口定义了操作的行为，即它们做什么，但没有说明如何去做。</u>于是，接口建立了一个<strong><u>抽象边界</u></strong>，将程序的其他部分与实现类型抽象的数据结构、算法和代码隔离开来。</p></blockquote><h2 id="类的概念">类的概念</h2><p>类和变量之间存在着一定的联系，类型是模板，而变量则是具有这种模板的一个实体。<strong>类（class）是对客观世界中事物的抽象，对象是类的实例(Instance)。</strong></p><p>从本质上说，对象是一组<u>数据</u>以及<u>操作这些数据的函数</u>。之前介绍的数字、字符串、列表、 字典和函数都是Python提供的内置对象。<strong>要创建新型对象，必须先创建类</strong>。类就类似于内置数据类型，可用于创建特定类型的对象。 <u>类指定了对象将包含哪些数据和函数，还指定了对象与其他类的关系</u>。</p><blockquote><p>一个重要的OOP功能是<strong>继承</strong>：创建新类时，可让其继承父类的数据和函数。使用好继承可避免重新编写代码，还可让程序更容易理解。对象和</p></blockquote><h1 id="对象和类">对象和类</h1><h2 id="定义">定义</h2><h3 id="定义类">定义类</h3><p>Python使用<font color=red>class关键字</font>定义一个类，类名首字符一般要大写。类定义中存在一个函数定义时，被定义的函数称为<strong>方法</strong>，并与这个类相关联。这些方法有时称为类的<strong>方法属性</strong>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Leiming</span>(<span class="title class_ inherited__">继承的父类名</span>)：</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,xx</span>):</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Xinfangfa</span>(<span class="params">xx</span>):</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">XXinfangfa</span>(<span class="params">self,xx</span>):</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><h3 id="在类中定义对象">在类中定义对象</h3><p>创建对象的过程称为<strong>实例化</strong>。</p><p>当一个对象被创建之后，包含3方面的特性：对象的<strong>标识</strong>、<strong>属性</strong>和<strong>方法</strong>。对象的标识用于区分不同的对象，当对象被创建之后，该对象会获取一块存储空间，<strong>存储空间的地址</strong>即为对象的标识。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自定义一个Person()</span></span><br><span class="line"><span class="comment"># __init__为定义属性部分</span></span><br><span class="line"><span class="comment"># self为object自己</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Person</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, email</span>): <span class="comment">#类的构造函数，用来初始化对象</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.email = email</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">say</span>(<span class="params">self, tem</span>):  <span class="comment">#类中定义的函数，也成为方法</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;I am &#x27;</span> + self.name + tem</span><br><span class="line"></span><br><span class="line">hunter = Person(<span class="string">&#x27;Elmer Fudd&#x27;</span>, <span class="string">&quot;QQ@WW.tw&quot;</span>)   <span class="comment">#实例化</span></span><br><span class="line"></span><br><span class="line">Husky = Person(<span class="string">&#x27;Hsuky&#x27;</span>, <span class="string">&quot;XDD@WW.tw&quot;</span>)</span><br><span class="line">Husky.ff = “oo”  <span class="comment"># 定义新对象后可自定义不在类中的新属性（属性的动态添加）</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(Husky.ff)</span><br><span class="line"><span class="comment"># print(hunter.ff)  报错&#x27;Person&#x27; object has no attribute &#x27;ff&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(hunter.name)</span><br><span class="line"><span class="built_in">print</span>(hunter.email)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(hunter.say(<span class="string">&#x27;!!!&#x27;</span>))  <span class="comment">#方法引用：通过点标记法，访问与类关联的方法</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(Husky.name)</span><br><span class="line"><span class="built_in">print</span>(Husky.email)</span><br></pre></td></tr></table></figure><p>oo Elmer Fudd QQ@WW.tw I am Elmer Fudd!!! Hsuky XDD@WW.tw</p><p>值得注意的是，当在类中定义函数时，若函数的第一个参数不是self，则该函数方法只能被类所使用。<span style="background:#fbd4d0;">因此在类中定义一般对象使用的方法时（<strong>除了静态方法外</strong>），第一个参数必须写<strong>self</strong>！</span>下面是个栗子</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">test</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sayhi</span>():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Hi&quot;</span>)</span><br><span class="line"></span><br><span class="line">gg = test()</span><br><span class="line"><span class="comment">#gg.sayhi() #报错 test.sayhi() takes 0 positional arguments but 1 was given</span></span><br><span class="line">test.sayhi() <span class="comment">#Hi</span></span><br></pre></td></tr></table></figure><h2 id="类的属性">类的属性</h2><p>Python并没有真正的私有化支持，但可用下划线得到伪私有。</p><h3 id="protected类型">protected类型</h3><p><font color=#ef042a>"单下划线 "</font> 开始的成员变量叫做保护变量，意思是只有<font color=#df8400><strong>类实例</strong></font>和<font color=#df8400><strong>子类实例</strong></font>能访问到这些变量， 需通过类提供的接口进行访问；不能用'from module import *'导入</p><h3 id="private类型">private类型</h3><p>在Python中，若<u>不希望类中的属性在类外被直接访问</u>，可以将实例的变量名改为<font color=#ef042a>以__（双下划线）开头</font>，就变成了一个私有类型（<font color=#ef042a>private</font>），只有内部可以访问，外部不能访问。</p><p>双下划线开头的类型是不是一定不能从外部访问呢？其实也不是。不能直接访问__xxx是因为Python解释器对外把__xxx变量/方法改成了_Class__xxx，所以，仍然可以通过<font color=#ef042a><strong>instance._Class__xxx</strong></font>来访问__xxx变量/方法。Python提供了直接访问私有属性的方式，可用于程序的测试和调试。当不知道类名时，只有<font color=#df8400><strong>类对象自己内部</strong></font>能访问，连子类对象也不能访问到这个数据。</p><h4 id="使用属性对特性进行访问和设置">使用属性对特性进行访问和设置</h4><p>在其他语言中，可以设置getter 和 setter來确保私有属性的读写。但是在python一切都是公开的，可以通过property()来达到python风格的写法，即可將属性值藏起來，不用通过调用每个getter()和setter()来达到改变私有变量。</p><p>若沒有给定setter函数，则无法通过property()来改变属性值，当然前提是在別人不知道实际存储变量的属性名称是什么。</p><blockquote><p>关于property()的用法请看这里：<a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017502538658208">使用@property - 廖雪峰的官方网站 (liaoxuefeng.com)</a>，里面说得很详细啦！</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Duck</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        self.hidden_name = input_name</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#取的 name 的函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_name</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用get函数---&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.hidden_name + <span class="string">&#x27;!!&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#设定 name 的函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_name</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用set函数---&#x27;</span>)</span><br><span class="line">        self.hidden_name = input_name + <span class="string">&#x27;??&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#使用property(get,set)来包裝，让使用上更方便</span></span><br><span class="line">    name = <span class="built_in">property</span>(get_name, set_name)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义Object为Duck类，并给定name，从头到尾都沒有直接抄作hidden_name來改变属性值</span></span><br><span class="line">fowl = Duck(<span class="string">&#x27;Howard&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;提取名称时，则调用get函数&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n设定名称时，则调用set函数&#x27;</span>)</span><br><span class="line">fowl.name = <span class="string">&#x27;Daffy&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;nname被改成Daffy&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n当然也可以通过原始的set_name()与get_name()进行修改私有属性&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.get_name())</span><br><span class="line">fowl.set_name(<span class="string">&#x27;Daffyyyy&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.get_name())</span><br></pre></td></tr></table></figure><p>提取名称时，则调用get函数 ---使用get函数--- Howard!!</p><p>设定名称时，则调用set函数 ---使用set函数--- nname被改成Daffy ---使用get函数--- Daffy??!!</p><p>当然也可以通过原始的set_name()与get_name()进行修改私有属性 ---使用get函数--- Daffy??!! ---使用set函数--- ---使用get函数--- Daffyyyy??!!</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#当然可以通过装饰器decorator，来写得更漂亮!!!</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Duck</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        self.hidden_name = input_name</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">name</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用get函数---&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.hidden_name</span><br><span class="line"></span><br><span class="line"><span class="meta">    @name.setter</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">name</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用set函数---&#x27;</span>)</span><br><span class="line">        self.hidden_name = input_name</span><br><span class="line">        </span><br><span class="line"><span class="comment">#定义Object为Duck类，并给定name</span></span><br><span class="line">fowl = Duck(<span class="string">&#x27;Howard&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;提取名称时，则调用get函数&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n设定名称时，则调用set函数&#x27;</span>)</span><br><span class="line">fowl.name = <span class="string">&#x27;Daffy&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;nname被改成Daffy&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br></pre></td></tr></table></figure><p>提取名称时，则调用get函数 ---使用get函数--- Howard</p><p>设定名称时，则调用set函数 ---使用set函数--- nname被改成Daffy ---使用get函数--- Daffy</p><h4 id="使用名称重整保持私有性推荐">使用名称重整保持私有性（推荐）</h4><p>前面的用法如果被知道实际储存属性的名称为什么，也是可以对其修改 所以可以通过名称重整来把实际储存的名称改写</p><p>在属性名称前面加上( __ )来重整名称，虽然不能完全的防止修改私有属性，但可以通过有效的方法降低有意或无意的修改</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Duck</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        self.__name = input_name</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">name</span>(<span class="params">self</span>):  <span class="comment">#该函数名不一定为name，也可以改为names等其他名字</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用get函數---&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.__name</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @name.setter  </span><span class="comment">#这里相应的也要改改为names等其他名字，同理访问时也是</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">name</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用set函數---&#x27;</span>)</span><br><span class="line">        self.__name = input_name</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">fowl = Duck(<span class="string">&#x27;Howard&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br><span class="line">fowl.name = <span class="string">&#x27;Donald&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br><span class="line"></span><br><span class="line"><span class="comment">#fowl.__name        #直接访问或修改会错误</span></span><br><span class="line"><span class="comment">#fowl._Duck__name   #重整完的名称</span></span><br><span class="line"><span class="comment">#print(fowl.__name) # 报错</span></span><br><span class="line"><span class="comment">#print(fowl._Duck__name) # OK</span></span><br><span class="line"></span><br><span class="line">fowl.__name=<span class="string">&quot;gg&quot;</span>  <span class="comment">#相当于属性的动态添加，即添加了一个名为__name的新属性</span></span><br><span class="line"><span class="built_in">print</span>(fowl.__name)      <span class="comment">#gg</span></span><br><span class="line"><span class="built_in">print</span>(fowl._Duck__name) <span class="comment">#Donald</span></span><br><span class="line"><span class="built_in">print</span>(fowl.name)        <span class="comment">#仍然为Donald，说明使用双下划线后无法轻易地改变类内的属性</span></span><br><span class="line"></span><br><span class="line">fowl._Duck__name=<span class="string">&quot;GGBond&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)  <span class="comment">#只有知道类名，才能改变该属性</span></span><br></pre></td></tr></table></figure><p>---使用get函數--- Howard ---使用set函數--- ---使用get函數--- Donald gg Donald ---使用get函數--- Donald</p><p>---使用get函數--- GGBond</p><h3 id="public类型">public类型</h3><p>C++有定义属性的关键字（public、private、protect），而Python没有这类关键字，默认情况下所有的属性都是“公有的”，对公有属性的访问没有任何限制，且都会被子类继承，也能从子类中进行访问。</p><h3 id="特殊类型">特殊类型</h3><h4 id="特殊变量">特殊变量</h4><p>在Python中，变量名类似<font color=#4eb434>__xxx__</font>的，也就是<font color=#4eb434>以双下划线开头，并且以双下划线结尾</font>的，是<font color=#4eb434>特殊变量</font>，特殊变量是可以直接访问的，不是private变量，如 init（）代表类的构造函数。</p><h4 id="特殊方法">特殊方法</h4><p>在python中，存在一些特殊方法( special method )或者称为( magic method )。我们可以通过<strong>在类中重写特殊方法</strong>，使编程更加便捷！ 这些方法为<span style="background:#fbd4d0;">双下划线( __ )开头与结尾</span>的用法。前面介绍过的( <strong>init</strong> )就是一个特殊方法，<u>只要一个类被实例化，就会调用该类中定义的__init__方法</u>。<u>__init__方法的第一个参数永远都是self</u>，表示创建实例本身，在__init__方法内部，可以把各种属性绑定到self，因为形参self指向创建的实例实参本身。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#---------------采用一般方法写法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Word</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, text</span>):</span><br><span class="line">        self.text = text</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">equals</span>(<span class="params">self, word2</span>):</span><br><span class="line">        <span class="keyword">return</span> self.text.lower() == word2.text.lower()</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建3个字符Object</span></span><br><span class="line">first = Word(<span class="string">&#x27;ha&#x27;</span>)</span><br><span class="line">second = Word(<span class="string">&#x27;HA&#x27;</span>)</span><br><span class="line">third = Word(<span class="string">&#x27;eh&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#进行比较</span></span><br><span class="line"><span class="built_in">print</span>(first.equals(second))  <span class="comment">#True</span></span><br><span class="line"><span class="built_in">print</span>(first.equals(third))   <span class="comment">#False</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#---------------采用特殊方法写法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Word</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, text</span>):</span><br><span class="line">        self.text = text</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__eq__</span>(<span class="params">self, word2</span>):   <span class="comment">#在Word类中重写__eq__方法</span></span><br><span class="line">        <span class="keyword">return</span> self.text.lower() == word2.text.lower()</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建3个字符对象</span></span><br><span class="line">first = Word(<span class="string">&#x27;ha&#x27;</span>)</span><br><span class="line">second = Word(<span class="string">&#x27;HA&#x27;</span>)</span><br><span class="line">third = Word(<span class="string">&#x27;eh&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#进行比较</span></span><br><span class="line"><span class="built_in">print</span>(first == second)  <span class="comment">#True</span></span><br><span class="line"><span class="built_in">print</span>(first == third)   <span class="comment">#False</span></span><br></pre></td></tr></table></figure><p>由上可见，特殊方法将使输出更加漂亮!</p><p>我们再来看一些常见的特殊方法与普通方法对比：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Element</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,name,symbol,number</span>):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.symbol = symbol</span><br><span class="line">        self.number = number</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dump</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;name=&#123;&#125;,\nsymbol=&#123;&#125;,\nnumber=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(self.name,self.symbol,self.number))</span><br><span class="line">        <span class="comment"># 若将 print 改为 return，输出为：&#x27;name=GGBond,\nsymbol=G,\nnumber=666&#x27;</span></span><br><span class="line">    </span><br><span class="line">hydrogen = Element(<span class="string">&#x27;GGBond&#x27;</span>,<span class="string">&#x27;G&#x27;</span>,<span class="number">666</span>)</span><br><span class="line">hydrogen.dump()</span><br></pre></td></tr></table></figure><p>name=GGBond, symbol=G, number=666</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Element</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,name,symbol,number</span>):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.symbol = symbol</span><br><span class="line">        self.number = number</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):  <span class="comment">#相当于重写__str__方法</span></span><br><span class="line">        <span class="keyword">return</span>(<span class="string">&#x27;name=&#123;&#125;,\nsymbol=&#123;&#125;,\nnumber=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(self.name,self.symbol,self.number))</span><br><span class="line">        <span class="comment"># 若将 return 改为 print, 报错：TypeError: __str__ returned non-string (type NoneType)</span></span><br><span class="line">        </span><br><span class="line">hydrogen = Element(<span class="string">&#x27;GGBond&#x27;</span>,<span class="string">&#x27;G&#x27;</span>,<span class="number">888</span>)</span><br><span class="line"><span class="built_in">print</span>(hydrogen)</span><br></pre></td></tr></table></figure><p>name=GGBond, symbol=G, number=888</p><blockquote><p>关于__str__和__repr__的区别：<a href="https://www.cnblogs.com/huhuxixi/p/10619289.html">__rept__和<strong>str</strong> - 呼呼嘻嘻 - 博客园 (cnblogs.com)</a></p><p>我们在用print输出任何东西的时候，都会有一个渲染步骤，而且默认的就是用str进行渲染，因为任何一样东西都可以看做一个对象，那么它必有一个类型，如果它的类里面没有定义str和repr也没关系，object里面定义了str和repr，object是一切类的父类，所以输出的对象一定会是渲染过的。这个类里面自己写了str和repr，它覆盖了object里面的str和repr，相当于print的重定向。</p><p>调用 print(i1) （#等同与print(str(i1))）的时候，解释器第一个寻找的就是i1这个类的方法里面有没有<u>重新定义str</u>，如果没有，那么它第二步会去寻找这个类里面有没有<u>重新定义repr</u>，如果有则会用类方法的重新定义的repr，如果还没有，那么解释器会找这个类的上一层父类，按同样的规则进行寻找。</p><p>调用print（repr（i1））的时候就不一样了，repr只会调用repr方法，当自定义的类中没有重写repr方法的时候，它会直接找上一级的父类中有没有repr方法，而不会考虑调用str方法。</p></blockquote><p>常用的特殊方法整理如下：</p><p><strong>非常常用</strong></p><table><thead><tr class="header"><th>方法名</th><th>使用</th></tr></thead><tbody><tr class="odd"><td>__init__(self)</td><td>定义新对象时</td></tr><tr class="even"><td>__str__(self)</td><td>print(对象名) 或 print(str(对象名))</td></tr><tr class="odd"><td>__repr__(self)</td><td>print(对象名) 或 print(repr(对象名))</td></tr><tr class="even"><td>__len__(self)</td><td>len(对象名)</td></tr><tr class="odd"><td>__type__(self)</td><td>type(对象名) == 类名</td></tr></tbody></table><p><strong>比较用</strong></p><table><thead><tr class="header"><th>方法名称</th><th>使用</th></tr></thead><tbody><tr class="odd"><td>__eq__(self,other)</td><td>对象名 == other</td></tr><tr class="even"><td>__ne__(self,other)</td><td>对象名 != other</td></tr><tr class="odd"><td>__lt__(self,other)</td><td>对象名 &lt; other</td></tr><tr class="even"><td>__gt__(self,other)</td><td>对象名 &gt; other</td></tr><tr class="odd"><td>__le__(self, other)</td><td>对象名 &lt;= other</td></tr><tr class="even"><td>__ge__(self, other)</td><td>对象名 &gt;= other</td></tr></tbody></table><p><strong>数学用</strong></p><table><thead><tr class="header"><th>方法名</th><th>使用</th></tr></thead><tbody><tr class="odd"><td>__add__(self, other)</td><td>对象名 + other</td></tr><tr class="even"><td>__sub__(self, other)</td><td>对象名 - other</td></tr><tr class="odd"><td>__mul__(self, other)</td><td>对象名 * other</td></tr><tr class="even"><td>__floordiv__(self, other)</td><td>对象名 // other</td></tr><tr class="odd"><td>__truediv__(self, other)</td><td>对象名 / other</td></tr><tr class="even"><td>__mod__(self, other)</td><td>对象名 % other</td></tr><tr class="odd"><td>__pow__(self, other)</td><td>对象名**other</td></tr></tbody></table><blockquote><p>完整清单见官方清单：<a href="https://docs.python.org/3/reference/datamodel.html#special-method-names">3. Data model — Python 3.12.1 documentation</a></p></blockquote><h3 id="其他类型">其他类型</h3><h4 id="类变量静态变量"><font color=#985fff>类变量/静态变量</font></h4><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231218165934920.png" alt="image-20231218165934920" /><figcaption aria-hidden="true">image-20231218165934920</figcaption></figure><p>C++中有一类特殊的属性称为静态变量。静态变量可以被类直接调用，而不被实例化对象调用。当创建新的实例化对象后，静态变量并不会获取新的内存空间，而是使用类创建的内存空间。因此，静态变量能够被多个实例化对象共享。<font color=#ef042a>在Python中静态变量称为类变量，类变量可以在该类的所有实例中被共享</font>。</p><blockquote><p><strong>有实例属性的变量：</strong>实例属性是以<font color=#ef042a>self</font>为前缀的属性，没有该前缀的属性是普通的局部变量。</p><p>如上图中的普通局部变量zone只有在__init__()方法中能被访问到，而有实例属性的变量color则能在外部被实例调用访问。</p></blockquote><h4 id="静态方法和类方法"><font color=#985fff>静态方法和类方法</font></h4><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231218165535528.png" alt="image-20231218165535528" /><figcaption aria-hidden="true">image-20231218165535528</figcaption></figure><blockquote><p>类的方法也分为公有方法和私有方法。私有方法不能被模块外的类或方法调用，私有方法也不能被外部的类或函数调用。</p></blockquote><h5 id="静态方法"><font color=#4eb434>静态方法</font></h5><p>Python使用<font color=#ef042a>函数staticmethod()</font>或 <font color=#ef042a>@ staticmethod修饰器</font>将普通的函数转换为静态方法。在开发中，我们常常需要定义一些方法，这些方法跟类有关，但在实现时并不需要引用类或者实例，例如，设置环境变量，修改另一个类的变量，等。这个时候，我们可以使用静态方法。</p><blockquote><p>静态方法一定要加函数staticmethod()或@ staticmethod修饰器，否则会被识别为没有self参数的普通方法，进而只能被类调用！</p></blockquote><ol type="1"><li>静态方法可以使用<font color=#df8400><strong>类调用</strong></font>也可以使<font color=#df8400><strong>用对象调用</strong></font>。</li><li>一般不需要传参数self。</li><li>Python的静态方法并没有和类的实例进行名称绑定，只是名义上归类管理，<u>实际上在静态方法里面访问不了类或者实例的任何属性</u>。</li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231218165603714.png" alt="image-20231218165603714" /><figcaption aria-hidden="true">image-20231218165603714</figcaption></figure><h5 id="类方法"><font color=#4eb434>类方法</font></h5><p>使用<font color=#ef042a>函数classmethod()</font>或 <font color=#ef042a>@ classmethod修饰器</font>，没有被调用的类中其余参数不会加载进内存中。</p><ol type="1"><li><font color=#df8400><strong>只能访问类变量</strong></font>，不能访问实例变量。需要有参数。类方法能被<u>类本身</u>和<u>类的实例</u>调用。</li><li>类方法的第一个参数长什么样不重要，它指向的都是类本身。</li><li>在类方法中，可以调用类里面的类属性和普通方法/静态方法。</li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231218165618888.png" alt="image-20231218165618888" /><figcaption aria-hidden="true">image-20231218165618888</figcaption></figure><h5 id="静态方法和类方法的栗子"><font color=#4eb434>静态方法和类方法的栗子</font></h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>():</span><br><span class="line">    count = <span class="number">0</span>           <span class="comment">#类属性</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,name</span>):</span><br><span class="line">        A.count += <span class="number">1</span>    <span class="comment">#修改类属性,修改时必须用A.调用</span></span><br><span class="line">        self.name = name</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">exclaim</span>():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;I&#x27;m an A!&quot;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod        </span><span class="comment">#类方法(methond)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kids</span>(<span class="params">cls</span>):</span><br><span class="line">        cls.exclaim()  <span class="comment">#类方法能调用普通方法/静态方法，但在调用实例方法时会报错</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;A has&quot;</span>, cls.count, <span class="string">&quot;little objects.&quot;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="meta">    @classmethod        </span><span class="comment">#类方法(methond)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kids2</span>(<span class="params">pp</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;A has&quot;</span>, A.count, <span class="string">&quot;little objects.&quot;</span>)</span><br><span class="line"></span><br><span class="line">easy_a = A(<span class="string">&quot;easy&quot;</span>)</span><br><span class="line">breezy_a = A(<span class="string">&quot;breezy&quot;</span>)</span><br><span class="line">wheezy_a = A(<span class="string">&quot;wheezy&quot;</span>)</span><br><span class="line">A.kids()</span><br><span class="line">A.kids2()</span><br><span class="line">easy_a.kids()  <span class="comment">#类方法也能被类的实例所调用 </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CoyoteWeapon</span>():</span><br><span class="line">    yyo = <span class="number">1</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">commercial</span>():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;This CoyoteWeapon has been brought to you by Acme&#x27;</span>)</span><br><span class="line">        <span class="comment"># print(yyo) 报错，在静态方法中访问不了类的任何实例和变量</span></span><br><span class="line">        </span><br><span class="line">CoyoteWeapon.commercial()</span><br><span class="line">ppp = CoyoteWeapon()</span><br><span class="line">ppp.commercial()</span><br></pre></td></tr></table></figure><p>I'm an A! A has 3 little objects. A has 3 little objects. I'm an A! A has 3 little objects. This CoyoteWeapon has been brought to you by Acme This CoyoteWeapon has been brought to you by Acme</p><h2 id="属性和方法的动态添加">属性和方法的动态添加</h2><blockquote><p>动态语言目前非常具有活力。例如JavaScript， PHP 、 Ruby 、 Python 等。而 C 、 C++ 等语言则不属于动态语言。</p></blockquote><h3 id="属性的动态添加">属性的动态添加</h3><ol type="1"><li>运行过程中可以给<strong>对象/类</strong>添加属性</li><li>语法格式：xx.new_attribute = attri_value</li></ol><h3 id="方法的动态添加">方法的动态添加</h3><ol type="1"><li>语法格式：class_name.method_name=function_name</li><li>可以动态添加/更改类的方法，将某个已经定义的函数添加到类中。当method_name表示已经存在的方法名，function_name表示1个已经存在的函数，该赋值表达式表示将函数的内容更新到方法。</li></ol><h2 id="继承与多态">继承与多态</h2><blockquote><p>这一章有个例子，感兴趣的可以搜一下 Undercut游戏~</p></blockquote><h3 id="继承">继承</h3><p>不同的类型中有许多通用的属性。例如，list类型和str类型都具有len函数，意义也完全 一样，这是因为它们继承了同一个父类。<strong>“继承”</strong>是面向对象编程 (OOP) 语言的一个主要功能。</p><p>在编写类时，如果发现已经有前人开发过，那就可以不用整段赋值，<strong>可以采用<font color=#985fff>继承</font>的方法取得他的属性与方法</strong>。 <strong>并且补充自己会用的功能</strong>，一方面可以减少去改已有的类的辛苦，也可以省去复制粘贴的功夫。<font color=#985fff>原始的类称为父类或超类（Base class、Super class），新类称为子类（Subclass）</font>。Python在类名后使用一对括号表示继承关系，括号中即为父类。</p><blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219152748146.png" alt="image-20231219152748146" /><figcaption aria-hidden="true">image-20231219152748146</figcaption></figure><p>如果子类中除了pass没有其他的内容，则pass必写；否则，报错下一行的缩进问题</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">math</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;add:&quot;</span>, a + b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">mean</span>(<span class="title class_ inherited__">math</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">ab = mean()</span><br><span class="line">ab.add(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">ac = math()</span><br><span class="line">ac.add(<span class="number">1</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>add: 4 add: 6</p><p>继承的过程，就是从一般到特殊的过程。object类在最顶层。因为在Python中，存在于运行时的一切都是对象。</p><h4 id="构造函数的继承">构造函数的继承</h4><p>python中<u>如果子类有自己的构造函数</u>，<u>不会自动调用父类的构造函数</u>，<font color=#ef042a>如果需要用到父类的构造函数，则需要在子类的构造函数中显式地调用，且调用代码应位于第一行</font>。</p><p>如果子类需要扩展父类的行为，可以添加__init__方法的参数。一种方法如图，即<code>Fruit.__init__(self,color)</code>; 另一种也可以用<code>super().__init__(color)</code>。</p><p>如果子类没有自己的构造函数，则会直接从父类继承构造函数。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219151850198.png" alt="image-20231219151850198" /><figcaption aria-hidden="true">image-20231219151850198</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219152043676.png" alt="image-20231219152043676" /><figcaption aria-hidden="true">image-20231219152043676</figcaption></figure><blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219152242748.png" alt="image-20231219152242748" /><figcaption aria-hidden="true">image-20231219152242748</figcaption></figure><p>为什么P4&lt;P1能够执行，而P1&lt;P4不行？</p><ul><li>P4&lt;P1相当于调用P4.__lt__(P1)，相当于使用与P4对象相关的Person的方法，根据人的姓名做比较，因此可正常执行；</li><li>P1&lt;P4相当于调用P1.__ lt __(P4)，相当于使用与P1对象相关的MITPeron的方法，根据人的IDNum做比较，而P4是一个Person，没有IDNum，因此无法比较。</li></ul></blockquote><blockquote><p>说明:函数isinstance是内置在Python中的一个常用函数,其中第一个参数可以是任何对象,但第二个参数必须是一个type类型的对象。函数当且仅当第一个参数是第二个 参数的一个实例时,才返回True。</p><p>例如,isinstance([1,2],list)的值是True。</p><p>常见用法： def isStudent(self): ​ return isinstance(self, Student)</p><p>请注意，isinstance(p6, Student)与type(p6) == Student在意义上是截然不同的。与p6绑定的对象类型是UnderGraduate，不是Student， 但因为UnderGraduate是Student的子类，所以p6绑定的对象被认为是Student类的一个实例（也是MITPerson类和Person类的实例）。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">math</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;add:&quot;</span>, a + b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">mean</span>(<span class="title class_ inherited__">math</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;add:&quot;</span>, a + b)</span><br><span class="line">    </span><br><span class="line">ab = mean()</span><br><span class="line">ac = math()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(ab)==mean) <span class="comment">#True</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(ab)==math) <span class="comment">#False</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">isinstance</span>(ab,mean)) <span class="comment">#True</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">isinstance</span>(ab,math)) <span class="comment">#True</span></span><br></pre></td></tr></table></figure></blockquote><h4 id="覆盖">覆盖</h4><p>覆盖——也就是替换——超类中的方法。</p><p><font color=#0091ff>如果一个方法被覆盖，那么调用这个方法时使用的版本就要根据调用这个方法的对象来确定。</font>如果这个对象的类型是子类，那么就使用定义在子类中的方法版本；如果对象的 类型是超类，那么就使用超类中的版本。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">math</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;add:&quot;</span>, a + b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">mean</span>(<span class="title class_ inherited__">math</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;add:&quot;</span>, a + b + b)</span><br><span class="line">    </span><br><span class="line">ab = mean()</span><br><span class="line">ab.add(<span class="number">1</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>add: 7</p><h4 id="添加新的属性或方法">添加新的属性或方法</h4><p>添加新的属性。例如，子类MITPerson中新增了类变量nextIdNum、实例变量idNum 和方法getIdNum。我们也可以在新的类中加入新的方法：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class math():</span><br><span class="line">    def add(self, a, b):</span><br><span class="line">        print(&quot;add:&quot;, a + b)</span><br><span class="line"></span><br><span class="line">class mean(math):</span><br><span class="line">    def less(self, a, b):</span><br><span class="line">        print(&quot;add:&quot;, a - b)</span><br><span class="line">    </span><br><span class="line">ab = mean()</span><br><span class="line">ab.add(1, 3)</span><br><span class="line">ab.less(1, 3)</span><br><span class="line"></span><br><span class="line">ac = math()</span><br><span class="line">ac.add(1, 5)</span><br></pre></td></tr></table></figure><p>add: 4 add: -2 add: 6</p><h3 id="多态">多态</h3><p>继承机制说明子类具有父类的公有属性和方法，而且子类可以扩展自身的功能，添加新的属性和方法。因此，子类可以替代父类对象，这种特性称为<strong>多态性</strong>。</p><p><strong>多态（polymorphism）</strong>是指父类的同一个方法在不同的子类对象中具有不同的表现和 行为。 （<u>事实上，即使是同一个类的对象，也可以有不同的属性</u>）</p><p>子类继承了父类的属性和方法之后，还会增加某些特定的属性和方法，同时还会对继承来的某些方法进行改变，都是多态的表现形式。</p><p>Python大多数运算符可以作用于不同类型的操作数，且对不不同类型的操作数往往有不 同的表现，这本身就是多态，是通过重写特殊方法与运算符重载实现的。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219160503820.png" alt="image-20231219160503820" /><figcaption aria-hidden="true">image-20231219160503820</figcaption></figure><p>多态的好处就是，当我们需要传入Dog、Cat、Tortoise……时，我们只需要接收Animal类型就可以了，因为Dog、Cat、Tortoise……都是Animal类型，然后，按照Animal类型进行操作即可。由于Animal类型有run()方法，因此，传入的任意类型，只要是Animal类或者子 类，就会自动调用实际类型的run()方法，这就是多态的意思。</p><p>对于一个变量，我们只需要知道它是Animal类型，无需确切地知道它的子类型，就可以放心地调用run()方法，而具体调用的run()方法是作用在Animal、Dog、Cat还是Tortoise对象上，由运行时该对象的确切类型决定，这就是多态真正的威力：</p><p><font color=#985fff>调用方只管调用，不管细节，而当我们新增一种Person的子类时，只要确保新方法编写正确，而不用管原来的代码。这就是著名的<strong>“开闭”原则</strong>：</font></p><p><font color=#985fff><strong>对扩展开放（Open for extension）：允许子类重写方法函数</strong></font> <font color=#985fff><strong>对修改封闭（Closed for modification）：不重写，直接继承父类方法函数</strong></font></p><h3 id="多重继承">多重继承</h3><p>Python支持多重继承，即一个类可以继承多个父类。多重继承的语法格式： <code>class_name(parent_class1, parent_class2…)</code>。其中class_name是类名，parent_class1和parent_class2是父类名。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219160947542.png" alt="image-20231219160947542" /><figcaption aria-hidden="true">image-20231219160947542</figcaption></figure><h4 id="多重继承关系中的构造函数">多重继承关系中的构造函数</h4><p>子类从多个父类派生，而子类又没有自己的构造函数时， （1）按顺序继承，哪个父类在最前面且它又有自己的构造函数，就继承它的构造函数； （2）如果最前面第一个父类没有构造函数，则继承第2个的构造函数，第2个没有的话， 再往后找，以此类推。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219161134386.png" alt="image-20231219161134386" /><figcaption aria-hidden="true">image-20231219161134386</figcaption></figure><h2 id="抽象基类">抽象基类</h2><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/kkk" alt="image-20231218165633785" /><figcaption aria-hidden="true">image-20231218165633785</figcaption></figure><ol type="1"><li>抽象基类是对一类事物的特征行为的抽象，由抽象方法组成。在Python3中可以使用<font color=#985fff>abc模块</font>，该模块中有一个<font color=#985fff>元类ABCMeta</font>和<font color=#985fff>修饰器 @ abstractmethod</font>。抽象基类不能被直接实例化。</li><li><strong>继承抽象类的子类必须重写抽象函数</strong>。</li></ol><h2 id="组合">组合</h2><p>如果要新建的类有相似的类可以继承的話就可以采用继承来取得父类的所有， 但若两个类差异太大，或是沒有关系，我们就可以采用组合來合并这些类</p><p>例如，鸭子是鸟的一种，所以可以继承鸟的类， 但是嘴巴和尾巴不是鸟的一种，而是鸭子的组成。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Bill</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, description</span>):</span><br><span class="line">        self.description = description</span><br><span class="line">        </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tail</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, length</span>):</span><br><span class="line">        self.length = length</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Duck</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, bill, tail</span>):</span><br><span class="line">        self.bill = bill</span><br><span class="line">        self.tail = tail</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">about</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;这只鸭子有一个&#x27;</span>, bill.description, <span class="string">&#x27;嘴巴，然后有&#x27;</span>, tail.length, <span class="string">&#x27;长的尾巴&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">bill = Bill(<span class="string">&#x27;红色的&#x27;</span>)</span><br><span class="line">tail = Tail(<span class="string">&#x27;白色，15cm&#x27;</span>)</span><br><span class="line"></span><br><span class="line">duck = Duck(bill, tail)</span><br><span class="line">duck.about()</span><br></pre></td></tr></table></figure><p>这只鸭子有一个 红色的 嘴巴，然后有 白色，15cm 长的尾巴</p><h2 id="何时使用类和对象而不是模块">何时使用类和对象而不是模块</h2><p>有一些方法可以帮助你決定是把你的代码封裝到类里还是模块里。</p><ul><li>当你需要许多具有相似行为（方法），但不同状态（特性）的实例时，使用对象是最好的选择。</li><li>类支持继承，但模块不支持。</li><li>如果你想要保证实例的唯一性，使用模块是最好的选择。不管模块在程序中被引用多少次，始终只有一个实例被加载。</li><li>如果你有一系列包含多个值的变量，并且他们能作为参数传入不同的函数，那么最好將它们封裝到类里面。举个例子，你可能会使用以大小和颜色为键的字典代表一张 彩色图片。你可以在程序中为每张图片创建不同的字典，并把它们作为参数传递给像规模（）或者变换（）之类的函数。但这么做的话，一旦你想要添加其他的键或者函数会变得非常麻烦。为了保证统一性，应该定义一个图片类，把大小和颜色作为特性，把规模（）和变换（）定义为方法。这么一来，关于一张图片的所有数据和可执行的操作都存储在了统一的位置。</li><li>用最简单的方式解決问题。使用字典，列表和元組往往要比使用模块更加简单，简介且快速。而使用类则更为复杂。</li></ul><h3 id="命名tuplenamed-tuple">命名Tuple(named tuple)</h3><p>可以用来创造可以用名称访问的Tuple子类</p><p>跟Tuple一样，不可被改变，但是可以透过替换來产生新的命名Tuple</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple <span class="comment">#引入函数库</span></span><br><span class="line"></span><br><span class="line">Duck = namedtuple(<span class="string">&#x27;Duck&#x27;</span>, <span class="string">&#x27;bill tail&#x27;</span>) <span class="comment">#定义为命名Tuple，並且有bill和tail两种名称</span></span><br><span class="line">duck = Duck(<span class="string">&#x27;wide orange&#x27;</span>, <span class="string">&#x27;long&#x27;</span>)     <span class="comment">#赋值</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(duck)</span><br><span class="line"><span class="built_in">print</span>(duck.bill)</span><br><span class="line"><span class="built_in">print</span>(duck.tail)</span><br><span class="line"></span><br><span class="line">parts = &#123;<span class="string">&#x27;bill&#x27;</span>: <span class="string">&#x27;wide orange&#x27;</span>, <span class="string">&#x27;tail&#x27;</span>: <span class="string">&#x27;long&#x27;</span>&#125;  <span class="comment">#使用dictionary赋值</span></span><br><span class="line">duck2 = Duck(**parts)  <span class="comment">#键名相同，因此可以用**提取字典内容导入</span></span><br><span class="line"><span class="built_in">print</span>(duck2)</span><br><span class="line"></span><br><span class="line">duck3 = duck2._replace(tail=<span class="string">&#x27;magnificent&#x27;</span>, bill=<span class="string">&#x27;crushing&#x27;</span>)  <span class="comment">#替换內容</span></span><br><span class="line"><span class="built_in">print</span>(duck3)</span><br></pre></td></tr></table></figure><p>Duck(bill='wide orange', tail='long') wide orange long Duck(bill='wide orange', tail='long') Duck(bill='crushing', tail='magnificent')</p><blockquote><p>学习资料链接：</p><p>链接：https://pan.baidu.com/s/1fg10feuxUbm4bdk5faX9MA?pwd=9k78 提取码：9k78 --来自百度网盘超级会员V1的分享</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> course </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计学习及监督学习概论第一章 读书笔记</title>
      <link href="/2023/12/25/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/12/25/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="统计学习">统计学习</h1><p><strong>统计学习(statistical learning)</strong>，也称统计机器学习(statistical machine learning)，是关于计算机基于<u>数据</u>构建<u>概率统计模型</u>并运<u>用模型对数据进行预测与分析</u>的一门学科。现在,当人们提及机器学习时,往往是指统计机器学习。所以可以认为本书介绍的是机器学习方法。</p><p>统计学习研究的对象是<strong>数据(data)</strong>。它从数据出发,提取数据的特征,抽象出数 据的模型,发现数据中的知识,又回到对数据的分析与预测中去。统计学习关于数据的基本假设是<u>同类数据具有一定的统计规律性</u>,这是统计学习的前提。比如,<em>可以用随机变量描述数据中的特征,用概率分布描述数据的统计规律</em>。在统计学习中,以变量或变量组表示数据。数据分为由<u>连续变量</u><u>和表示的类型</u>。本书以讨论离散变量的方法为主。另外,本书只涉及利用数据构建模型及利用模型对数据进行分析与预测,对数据的观测和收集等问题不作讨论。</p><p>统计学习的方法是<u>基于数据构建概率统计模型从而对数据进行预测与分析</u>。统计学习由<strong>监督学习(supervised learning)</strong>、<strong>无监督学习(unsupervised learning)</strong>和<strong>强化学习(reinforcement learning)</strong>等组成。其中，监督学习、无监督学习方法是最主要的统计学习方法。</p><p>统计学习方法可以概括如下:从<u>给定的、有限的、用于学习</u>的训练数据(training data)集合出发,假设数据是独立同分布产生的;并且假设要学习的模型属于某个函数的集合,称为<font color=#df8400>假设空间(hypothesis space)</font>;应用某个<font color=#0091ff>评价准则(evaluation criterion)</font>,从假设空间中选取一个最优模型,使它对已知的训练数据及未知的测试数据(test data)在给定的评价准则下有最优的预测; 最优模型的选取由<font color=#4eb434>算法</font>实现。这样,统计学习方法包括<font color=#df8400>模型的假设空间</font>、<font color=#0091ff>模型选择的准则</font>以及<font color=#4eb434>模型学习的算法</font>。称其为统计学习方法的三要素,简称为<font color=#df8400><strong>模型(model)</strong></font>、<font color=#0091ff><strong>策略(strategy)</strong></font>和<font color=#4eb434><strong>算法(algorithm)</strong></font>。</p><p><strong>实现统计学习方法的步骤如下:</strong> (1) 得到一个有限的<u>训练数据集合</u>; (2)确定包含所有可能的<u>模型的假设空间</u>,即学习模型的集合; (3)确定模型选择的准则,即<u>学习的策略/选择最优模型的评价准则</u>; (4)实现求解最优模型的算法,即<u>学习的算法/选取最优模型的方法</u>; (5)通过学习方法选择最优模型; (6)利用学习的最优模型对新数据进行预测或分析。</p><p>本书第1篇介绍监督学习方法,主要包括用于<u>分类、标注与回归问题的方法</u>。这些方法在<u>自然语言处理、信息检索、文本数据挖掘等领域</u>中有着极其广泛的应用。统计学习研究一般包括统计学习方法、统计学习理论及统计学习应用三个方面。统计学习是计算机科学发展的一个重要组成部分。可以认为计算机科学由三维组成: <u>系统、计算、信息</u>。统计学习主要属于<u>信息</u>这一维,并在其中起着核心作用。</p><h2 id="统计学习的分类">统计学习的分类</h2><h3 id="基本分类">基本分类</h3><p>统计学习或机器学习一般包括<u>监督学习</u>、<u>无监督学习</u>、<u>强化学习</u>。有时还包括半 监督学习、主动学习。</p><h4 id="监督学习">监督学习</h4><p>监督学习(supervised learning)的本质是学习输入到输出的映射的统计规律。</p><hr /><p>几个概念：</p><ul><li><strong>输入空间(input space)</strong>：输入所有可能取值的集合</li><li><strong>输出空间(output space)</strong>：输出所有可能取值的集合</li><li><strong>实例(instance)</strong>：每个具体的输入是一个实例，通常由特征向量(feature vector)表示。</li><li><strong>特征空间(feature space)</strong>：所有特征向量存在的空间称为特征空间。特征空间的每一维对应于一个特征。模型实际上都是定义在特征空间上的。</li></ul><hr /><p>在监督学习中,将输入与输出看作是定义在输入(特征)空间与输出空间上的随机变量的取值。输入输出变量用大写字母表示,习惯上输入变量写作X,输出变量写作Y。输入输出变量的取值用小写字母表示,输入变量的取值写作x,输出变量的取值写作y。变量可以是标量或向量,都用相同类型字母表示。除特别声明外,本书中向量均为列向量。</p><p>输入变量X和输出变量Y有不同的类型,可以是连续的,也可以是离散的。人们根据输入输出变量的不同类型,对预测任务给予不同的名称：</p><ul><li><strong>回归问题</strong>：输入变量与输出变量均为连续变量的预测问题;</li><li><strong>分类问题</strong>：输出变量为有限个离散变量的预测问题;</li><li><strong>标注问题</strong>：输入变量与输出变量均为变量序列的预测问题。</li></ul><h4 id="无监督学习">无监督学习</h4><p>无监督学习(unsupervised learning)是指从无标注数据中学习预测模型的机器学习问题。无标注数据是自然得到的数据,预测模型表示数据的类别、转换或概率。无监督学习的本质是学习数据中的统计规律或潜在结构。</p><h4 id="强化学习">强化学习</h4><p>强化学习(reinforcement learning)是指智能系统在与环境的连续互动中学习 最优行为策略的机器学习问题。假设智能系统与环境的互动基于马尔可夫决策过 程(Markov decision process),智能系统能观测到的是与环境互动得到的数据序列。 强化学习的本质是学习最优的序贯决策。</p><h4 id="半监督学习">半监督学习</h4><p>半监督学习(semi-supervised learning)是指利用标注数据和未标注数据学习预 测模型的机器学习问题。通常有少量标注数据、大量未标注数据,因为标注数据的构 建往往需要人工,成本较高,未标注数据的收集不需太多成本。半监督学习旨在利用 未标注数据中的信息,辅助标注数据,进行监督学习,以较低的成本达到较好的学习 效果。</p><h4 id="主动学习">主动学习</h4><p>主动学习(active learning)是指机器不断主动给出实例让教师进行标注,然后利 用标注数据学习预测模型的机器学习问题。通常的监督学习使用给定的标注数据,往 往是随机得到的,可以看作是“被动学习”,主动学习的目标是找出对学习最有帮助的 实例让教师标注,以较小的标注代价,达到较好的学习效果。</p><blockquote><p>半监督学习和主动学习更接近监督学习。</p></blockquote><h3 id="按模型分类">按模型分类</h3><h4 id="概率模型与非概率模型">概率模型与非概率模型</h4><p>统计学习的模型可以分为概率模型(probabilistic model)和非概率模型(nonprobabilistic model)或者确定性模型(deterministic model)。</p><h4 id="线性模型与非线性模型">线性模型与非线性模型</h4><p>统计学习模型,特别是非概率模型,可以分为线性模型(linear model)和非线性模型(non-linear model)。如果函数y=f(x)或z=g(x)是线性函数,则称模型是线 性模型,否则称模型是非线性模型。</p><h4 id="参数化模型与非参数化模型">参数化模型与非参数化模型</h4><p>统计学习模型又可以分为参数化模型(parametric model)和非参数化模型(nonparametric model)。参数化模型假设模型参数的维度固定,模型可以由有限维参数完全刻画;非参数化模型假设模型参数的维度不固定或者说无穷大,随着训练数据量的增加而不断增大。</p><p>参数化模型适合问题简单的情况,现实中问题往往比较复杂,非参数化模型更加有效。</p><h3 id="按算法分类">按算法分类</h3><p>统计学习根据算法,可以分为在线学习(online learning)与批量学习(batch learning)。在线学习是指每次接受一个样本,进行预测,之后学习模型,并不断重复该操作的机器学习。与之对应,批量学习一次接受所有数据,学习模型,之后进行预测。有些实际应用的场景要求学习必须是在线的。比如,数据依次达到无法存储,系统需要及时做出处理;数据规模很大,不可能一次处理所有数据;数据的模式随时间动态变化,需要算法快速适应新的模式(不满足独立同分布假设)。</p><p>在线学习通常比批量学习更难,很难学到预测准确率更高的模型,因为每次模型更新中,可利用的数据有限。</p><h3 id="按技巧分类">按技巧分类</h3><h4 id="贝叶斯学习">贝叶斯学习</h4><p>贝叶斯学习(Bayesian learning), 又称为贝叶斯推理(Bayesian inference), 是 统计学、机器学习中重要的方法。其主要想法是, <u>在概率模型的学习和推理中, 利用贝</u> <u>叶斯定理, 计算在给定数据条件下模型的条件概率,即后验概率, 并应用这个原理进</u><u>行模型的估计, 以及对数据的预测</u>。将模型、未观测要素及其参数用变量表示, 使用模型的先验分布是贝叶斯学习的特点。贝叶斯学习中也使用基本概率公式(图1.4)。</p><p>本书介绍的<u>朴素贝叶斯、潜在狄利克雷分配的学习</u>属于贝叶斯学习。</p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3D Gaussian Splatting for Real-Time Radiance Field Rendering 论文笔记</title>
      <link href="/2023/12/20/3D%20Gaussian%20Splatting%20for%20Real-Time%20Radiance%20Field%20Rendering%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/12/20/3D%20Gaussian%20Splatting%20for%20Real-Time%20Radiance%20Field%20Rendering%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p><strong>数据集</strong>：Mip-NeRF360 数据集、Deep Blending 数据集[Hedman et al. 2018]</p><h1 id="abstract">Abstract</h1><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231220144503261.png" alt="image-20231220144503261" /><figcaption aria-hidden="true">image-20231220144503261</figcaption></figure><p>优化时间与<strong>InstantNGP</strong>相当 优化质量与<strong>光学体素</strong>相当 训练时间达到51分钟时，我们的质量达到最好，甚至比Mip-NeRF更胜一筹</p><p>mip_splatting</p><p>我们引入了三个关键元素，使我们能够在保持竞技训练时间的同时实现最先进的视觉质量，重要的是允许在1080p分辨率下实现高质量的实时(≥30 fps)新视图合成。<strong>首先</strong>，从相机标定过程中产生的稀疏点云出发，用三维高斯模型表示场景，该模型保留了场景优化所需的连续体亮度场特性，同时避免了在空空间中进行不必要的计算; <strong>其次</strong>，我们对三维高斯模型进行了交错优化/密度控制，特别是优化了各向异性协方差，以实现对场景的准确表示; <strong>最后</strong>，我们开发了一个快速的可视性感知的渲染算法，支持各向异性喷溅，既加速训练过程，又允许实时渲染。我们在几个已建立的数据集上展现出了最先进的视觉质量和实时渲染。</p><p>其他关键词和短语: <strong>新视图合成(novel view synthesis)，亮度场(radiance fields)，三维高斯(3D gaussians)，实时渲染(real-time rendering)</strong></p><h1 id="introduction">Introduction</h1><p>网格和点是最常见的3D场景表示，因为它们是显式的，并且非常适合基于GPU/ cuda的快速栅格化。相比之下，最近的神经辐射场(NeRF)方法建立在连续场景表示的基础上，通常使用体积射线行进优化多层感知器(MLP)，以获得捕获场景的新视图合成。类似地，迄今为止最有效的亮度场解决方案建立在连续表示的基础上，通过插值存储在体素[fridovic - keil和Yu等人，2022]或哈希[Müller等人，2022]网格或点[Xu等人，2022]中的值。虽然这些方法的连续性质有助于优化，但渲染所需的随机采样代价高昂，并可能导致噪声。我们引入了一种新方法，它结合了两个方面的优点: 我们的3D高斯表示允许优化最先进的(SOTA)视觉质量和有竞争性的训练时间，而我们的基于瓦片的喷溅解决方案确保在几个之前发布的数据集上以1080p分辨率的SOTA质量进行实时渲染[Barron等，2022; Hedman等人2018年;Knapitsch等人，2017](见图1)。</p><p>我们的目标是允许对多张照片捕获的场景进行实时渲染，并在优化时间内创建典型真实场景的表示，速度与之前最有效的方法一样快。最近的方法实现了快速训练[fridovic - keil和Yu等人，2022;Müller等人，2022]，但很难达到目前SOTA NeRF方法，即Mip-NeRF360 [Barron等人，2022]获得的视觉质量，这需要多达48小时的训练时间。快速但质量较低的亮度场方法可以根据场景实现<u>交互渲染</u>时间(10-15帧/秒)，但在高分辨率的实时渲染上达不到要求。</p><blockquote><p>fridovic - keil和Yu等人，2022;Müller等人，2022</p><p>InstantNGP <strong>光学体素</strong></p></blockquote><p>我们的解决方案构建在三个主要组件上。<font color=#985fff>我们首先介绍三维高斯模型作为一种灵活和富有表现力的场景表示。</font>我们从与之前类似nerf的方法相同的输入开始，即使用结构从运动(SfM)校准的相机[Snavely等人，2006]，并使用SfM过程中免费生成的稀疏点云初始化3D高斯模型。与大多数需要多视图立体视觉(MVS)数据的基于点的解决方案相比[Aliev等人，2020;Kopanas等人2021年;Rückert et al. 2022]，我们仅以SfM点作为输入就获得了高质量的结果。注意，<u>对于nerf合成数据集，我们的方法即使在随机初始化的情况下也能实现高质量</u>。我们表明，3D Gaussians是一个非常好的选择，因为它们是可微分的体积表示，但它们也可以非常有效地栅格化，通过将它们投影到2D，并应用标准的𝛼-blending，使用等效的图像生成模型作为NeRF。</p><p><font color=#df8400>我们方法的第二部分</font>是优化<u>三维高斯函数的性质</u>—<span style="background:#fbd4d0;">三维位置，不透明度𝛼，各向异性协方差，和 球面调和(SH)系数</span>—与<u>自适应密度控</u>制步骤交错，在优化过程中我们添加和偶尔删除三维高斯函数。优化过程生成了一个相当紧凑的、非结构化的、精确的场景表示(测试的所有场景的1-5百万高斯值)。</p><p><font color=#4eb434>我们方法的第三个也是最后一个部分是我们的实时渲染解决方案</font>，它使用了快速的GPU排序算法，这是受到基于贴图的栅格化的启发，遵循最近的工作[Lassner和Zollhofer 2021]。然而，由于我们的3D高斯表示，我们可以执行各向异性溅射，它遵循可见性排序——这要归功于排序和𝛼blending——并通过跟踪所需的多个排序splats (splats是排序好的) 的遍历来实现快速和准确的向后遍历。</p><blockquote><p>可以不提供真实相机参数, 运行colmap，可以估计图像的相机参数</p><p>3DGS只有两个数据加载器一个是Blender另一个是colmap数据加载器; Blender数据集用的是随机点云，不是从SFM生成的, blender数据集用的是box内随机生成点云</p><p>抛雪球算法：splatting</p></blockquote><p><strong>我们的贡献主要如下:</strong></p><ul><li><p>引入各向异性三维高斯函数作为高质量、非结构化的亮度场表示。</p></li><li><p>3D高斯属性的优化方法，与自适应密度控制交织，为捕获的场景创建高质量的表示。</p></li><li><p>GPU的一种快速、可微分的渲染方法，它是可见感知的，允许各向异性喷溅和快速反向传播，以实现高质量的新视图合成。</p></li></ul><p>我们在之前发布的数据集上的结果表明，我们可以从多视图捕获的图像中优化3D高斯，并获得与之前的隐式亮度场方法相同或更好的质量。我们还可以达到与最快的方法相似的训练速度和质量，重要的是，为新视图合成提供第一次高质量的实时渲染。</p><p>我们首先简要概述了传统的重建，然后讨论了基于点的渲染和亮度场工作，讨论了它们的相似性;亮度场是一个很大的区域，所以我们只关注直接相关的工作。有关该领域的完整报道，请参阅最近的优秀调查[Tewari等人。2022;谢等。2022]。</p><h2 id="traditional-scene-reconstruction-and-rendering">2.1 Traditional Scene Reconstruction and Rendering</h2><p>第一个新视觉合成方法是基于光场，第一个密集采样[Gortler等人，1996;Levoy和Hanrahan 1996]然后允许非结构化捕获[Buehler等人2001]。结构从运动(SfM)的出现[Snavely等人，2006]使一个全新的领域，一组照片可以用来合成新的视图。SfM在相机标定过程中估计了一个稀疏的点云，最初用于简单的三维空间可视化。随后的多视图立体视觉(MVS)在过去几年里产生了令人印象深刻的完整的三维重建算法[Goesele等人，2007]，使几个视图合成算法的发展成为可能[Chaurasia等人，2013;Eisemann等人2008年;Hedman等人2018年;Kopanas等人，2021]。所有这些方法都将输入图像重新投影和混合到新的视图相机中，并使用几何图形来引导这种重新投影。这些方法在许多情况下都产生了很好的效果，但是当MVS生成不存在的几何时，通常无法从未重建区域或“过度重建”区域完全恢复。最近的神经渲染算法[Tewari等人，2022]大大减少了这些工件，并避免了在GPU上存储所有输入图像的巨大成本，在大多数方面都优于这些方法。</p><blockquote><p>工件 (artifacts, 理解为人为在软件运行过程中造出的阶段性产物，可以是软件代码、文档、图纸、数据等非天然存在的资产，<a href="https://www.zhihu.com/question/455298119">(99+ 封私信 / 80 条消息) artifact 一词在计算机编程里面表示什么意思？ - 知乎 (zhihu.com)</a>)</p></blockquote><h2 id="neural-rendering-and-radiance-fields">2.2 Neural Rendering and Radiance Fields</h2><p>深度学习技术很早就被用于新视角合成[Flynn等人，2016;周等2016;cnn被用于估计混合权重[Hedman等人2018]，或用于纹理空间解决方案[Riegler和Koltun 2020;Thies等。2019]。使用基于MVS的几何是这些方法的一个主要缺点; 此外，使用CNNs进行最终渲染经常会导致时间闪烁(temporal flickering)。</p><p>新颖视图合成的体积表示由Soft3D发起[Penner和Zhang 2017]; 随后提出了深度学习技术和体积射线推进技术[Henzler等人，2019; Sitzmann等人。2019]基于连续可微密度场来表示几何。由于查询体积需要大量的样本，使用体积射线行进进行渲染的成本非常高。神经辐射场(NeRFs) [Mildenhall等人，2020]引入了重要性采样和位置编码来提高质量，但使用了大型多层感知器，对速度产生了负面影响。NeRF的成功导致了后续方法的爆炸式增长，这些后续方法通常通过引入正则化策略来解决质量和速度问题; 当前最先进的新型视图合成图像质量是Mip-NeRF360 [Barron等人，2022]。虽然渲染质量卓越，训练和渲染时间仍然非常高; 在提供快速训练和实时渲染的同时，我们能够达到甚至在某些情况下超过这个质量。</p><p>最近的方法主要通过三种设计选择来实现更快的训练和/或渲染: <font color=#4eb434>使用空间数据结构来存储(神经)特征，这些特征随后在体积射线行进过程、不同的编码和MLP容量中被插值，</font>。这些方法包括空间离散化的不同变体[Chen et al. 2022b,a; fridovic - keil和Yu等人，2022年;Garbin等人2021年; Hedman等人，2021年; Reiser等人，2021年; Takikawa等人，2021年; Wu等人，2022; Yu等人，2021年]，码本[Takikawa等人，2022年]，<font color=#ef042a>编码如哈希表</font>[Müller等人，2022年]，<font color=#0091ff>完全允许使用较小的MLP或先前提到的神经网络</font>[fridovic - keil和Yu等人，2022年; Sun等，2022]。</p><blockquote><p>Plenoxels [fridovic - keil和Yu等人，2022] 即 光学体素</p><p>360场景合成——处理图片并生成带背景的场景</p><p>3dgs用到了两个cuda扩展包，主体还是pytorch</p></blockquote><p>这些方法中最值得注意的是 <strong>InstantNGP</strong> [Müller等人。2022]，它使用哈希网格和占用网格来加速计算，并使用较小的 MLP 来表示密度和外观; 和 <strong>Plenoxels</strong> [fridovic - keil和Yu等人，2022]使用稀疏体素网格插值连续密度场，并能够完全放弃神经网络。<u>两者都依赖于球面谐波: 前者直接表示方向效果，后者将其输入编码到颜色网络</u>。虽然这两种方法都提供了出色的结果，但这些方法仍然难以有效地表示空白空间，这部分取决于场景/捕获类型。此外，图像质量在很大程度上受到用于加速的结构化网格的选择的限制，而渲染速度则因需要为给定的光线推进步骤查询许多样本而受到阻碍。我们使用的非结构化的、显式gpu友好的3D高斯函数，在没有神经组件的情况下，实现了更快的渲染速度和更好的质量。</p><h2 id="point-based-rendering-and-radiance-fields">2.3 Point-Based Rendering and Radiance Fields</h2><p>基于点的方法可以有效地渲染不连接的和非结构化的几何样本(即点云)[Gross和Pfister 2011]。在最简单的形式中，点采样渲染[Grossman和Dally 1998]栅格化一个固定大小的非结构化点集，它可以利用图形API中自然支持的点类型[Sainz和Pajarola 2004]或GPU上的并行软件栅格化[Laine和Karras 2011;Schütz et al. 2022]。虽然对底层数据来说是真实的，但点采样呈现会出现漏洞，导致混叠，并且严格来说是不连续的。基于点的高质量渲染的开创性工作通过“喷溅”大于一个像素的点基元来解决这些问题，例如圆形或椭圆形圆盘、椭球或面元surfels [Botsch et al. 2005;Pfister等人2000年;Ren等人，2002年;Zwicker等人。2001b]。</p><p>最近人们对基于可微点的渲染技术产生了兴趣[Wiles等人，2020;Yifan等，2019。用神经特征增强了点，并使用CNN渲染[Aliev等人，2020;Rückert等人，2022]导致快速甚至实时的视图合成; 然而，他们仍然依赖于MVS得到初始几何，并继承其工件，最明显的过度或重构不足的情况下，如无特征/闪亮的区域或薄结构。</p><p>基于点的𝛼-blending和NeRF风格的体绘制在本质上共享相同的图像生成模型。具体来说，颜色𝐶是由沿着光线的体积渲染给出的: $$</p><p>$$</p><blockquote><p>该公式与volsdf中的公式26下面一个公式类似</p><p>splatting的另外相关论文包括，mip-splatting</p><p>instantngp比mipNeRF跑得好，但是比mipNeRF360差</p></blockquote><h1 id="overview">Overview</h1><p>我们的方法的输入是一组静态场景的图像，以及由SfM [Schönberger和Frahm 2016]校准的相应摄像机，这将产生一个稀疏的点云作为副作用。从这些点出发，我们创建了一组3D高斯(第4节)，由位置(均值)、协方差矩阵和不透明度𝛼定义，这允许一个非常灵活的优化机制。这就产生了一个合理紧凑的3D场景表示，部分原因是高度各向异性的体块可以用来紧凑地表示精细结构。亮度场的方向外观分量(颜色)通过球面谐波(SH)表示，遵循标准实践[fridovic - keil和Yu等人。2022;Müller et al. 2022]。我们的算法通过三维高斯参数的一系列优化步骤，即位置、协方差、𝛼和SH系数与高斯密度的自适应控制操作交织，来创建亮度场表示(第5节)。我们的方法效率的关键是我们的基于tile的光栅化(第6节)，它允许𝛼-blending的各向异性碎片，由于快速排序而尊重可见性顺序。我们的快速光栅化还包括一个快速的向后通过跟踪累积的𝛼值，没有限制高斯的数量，可以接收梯度。我们的方法概述如图2所示。</p><h1 id="differentiable-3d-gaussian-splatting">DIFFERENTIABLE 3D GAUSSIAN SPLATTING</h1><p>我们的目标是优化一个允许高质量的新视图合成的场景表示，从一个没有法线的稀疏集(SfM)点开始。为了做到这一点，我们需要一个原语，它继承了可微体积表示的属性，同时是非结构化和显式的，以允许非常快速的渲染。我们选择3D高斯，这是可微分的，可以很容易地投影到2D splats允许快速𝛼-blending渲染。我们的表示方法与之前使用2D点的方法相似[Kopanas等人，2021;并假设每个点都是一个带【数】法线的平面小圆。由于SfM点的极度稀疏性，很难估计法线。类似地，</p>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SIGGRAPH 2023 </tag>
            
            <tag> 辐射场 </tag>
            
            <tag> NeRF </tag>
            
            <tag> splatting </tag>
            
            <tag> CUDA kernels 加速 </tag>
            
            <tag> InstantNGP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>制作电脑纹理壁纸</title>
      <link href="/2023/12/16/%E8%87%AA%E5%88%B6%E7%94%B5%E8%84%91%E7%BA%B9%E7%90%86%E5%A3%81%E7%BA%B8/"/>
      <url>/2023/12/16/%E8%87%AA%E5%88%B6%E7%94%B5%E8%84%91%E7%BA%B9%E7%90%86%E5%A3%81%E7%BA%B8/</url>
      
        <content type="html"><![CDATA[<h1 id="查看电脑壁纸尺寸">查看电脑壁纸尺寸</h1><p>在桌面右键，选择<font color=orange>显示设置</font></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216200845959.png" alt="image-20231216200845959" /><figcaption aria-hidden="true">image-20231216200845959</figcaption></figure><p>便可查看到目前电脑的分辨率是：<font color=orange>1920×1080</font></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216201136929.png" alt="image-20231216201136929" /><figcaption aria-hidden="true">image-20231216201136929</figcaption></figure><h1 id="准备纹理图片">准备纹理图片</h1><blockquote><p>补充于2023/12/17</p><p>如果选择的纹理图片和电脑显示器尺寸一样，可以跳过这一步哦~</p></blockquote><blockquote><p>纹理图片可参考网站：</p><ol type="1"><li><a href="https://texturelabs.org/?pg=1">Free Textures, Tutorials, and More (texturelabs.org)</a></li><li><a href="https://www.transparenttextures.com/">Transparent Textures</a></li><li><a href="https://www.cgbookcase.com/textures">Textures | cgbookcase.com</a></li><li><a href="https://www.toptal.com/designers/subtlepatterns/">Subtle Patterns | Free textures for your next web project (toptal.com)</a></li><li><a href="https://polyhaven.com/">Poly Haven</a></li><li><a href="https://www.poliigon.com/search?credit=0">Search - Poliigon</a></li></ol></blockquote><p>考虑到纹理图片和电脑显示器图片可能不一致，由此制作壁纸的最终效果可能稍有不完美，所以我们可以<strong>设法先将尺寸不匹配的纹理图片铺满整个屏幕</strong>，再导出作为制作壁纸使用的纹理图片。</p><ol type="1"><li><p>在PS软件中点击<font color=purple>文件-打开</font>，或者<font color=purple>Ctrl+O</font>打开纹理图片</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231217150953342.png" alt="image-20231217150953342" /><figcaption aria-hidden="true">image-20231217150953342</figcaption></figure></li><li><p>点击<font color=blue>编辑-定义图案</font>，给纹理图案起一个好记的名字，点击<font color=blue>确定</font></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231217151110446.png" alt="image-20231217151110446" /><figcaption aria-hidden="true">image-20231217151110446</figcaption></figure></li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/wwwwww.png" alt="wwwwww" /><figcaption aria-hidden="true">wwwwww</figcaption></figure><ol start="3" type="1"><li><p>新建项目，将尺寸设置为电脑显示器大小</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/屏幕截图%202023-12-17%20151328.png" alt="屏幕截图 2023-12-17 151328" /><figcaption aria-hidden="true">屏幕截图 2023-12-17 151328</figcaption></figure></li><li><p>选择<font color=red>油漆桶工具</font>，将填充物设置为<font color=red>图案</font>，<font color=red><strong>在图案里找到我们定义的纹理图案</strong></font>，进行填充</p></li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231217151756597.png" alt="image-20231217151756597" /><figcaption aria-hidden="true">image-20231217151756597</figcaption></figure><ol start="4" type="1"><li>最后导出即可</li></ol><h1 id="制作壁纸">制作壁纸</h1><blockquote><p>这里使用的图案编辑工具是在线PS工具（<a href="https://www.photopea.com/">Photopea | Online Photo Editor</a>），也可以直接在PS软件中编辑</p></blockquote><blockquote><p>颜色设置可参考配色网站：<a href="http://zhongguose.com/">zhongguose － 传统颜色</a></p></blockquote><ol type="1"><li><p>新建项目，设置分辨率为<strong>电脑显示器分辨率</strong>，设置<strong>背景色</strong>为你喜欢的颜色（或者直接打开一张你喜欢的背景图片）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216202126809.png" alt="image-20231216202126809" /><figcaption aria-hidden="true">image-20231216202126809</figcaption></figure></li><li><p>点击<strong><font color=green>文件-打开</font></strong>或者<font color=blue>Ctrl+O</font>打开下载好的纹理图片</p><blockquote><p>这一步用的纹理图案尺寸比电脑显示器尺寸小一些，最后的纹理效果有点放大失真。如想避免这一点可以跳回去看第二步：准备纹理图片</p></blockquote></li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216210041490.png" alt="image-20231216210041490" /><figcaption aria-hidden="true">image-20231216210041490</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216214805133.png" alt="image-20231216214805133" /><figcaption aria-hidden="true">image-20231216214805133</figcaption></figure><ol start="3" type="1"><li><p>在纹理图案界面，点击<font color=red>编辑-定义新的图案</font>，定义成功后回到壁纸界面</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216215020114.png" alt="image-20231216215020114" /><figcaption aria-hidden="true">image-20231216215020114</figcaption></figure></li><li><p>点击图层，右键点击<font color=purple>混合模式</font></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216215322738.png" alt="image-20231216215322738" /><figcaption aria-hidden="true">image-20231216215322738</figcaption></figure></li></ol><p>在出现的界面选择<font color=orange>纹理</font>，然后<strong>选择我们定义的图案</strong>，点击确定</p><blockquote><p>纹理界面的深度为正，代表纹理效果是凸出的；深度为负，表示纹理效果是凹陷的</p></blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216215555128.png" alt="image-20231216215555128" /><figcaption aria-hidden="true">image-20231216215555128</figcaption></figure><ol start="5" type="1"><li><p>最后依次点击<font color=green>文件-导出为-PNG</font>，将图片导出即可</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216215902983.png" alt="image-20231216215902983" /><figcaption aria-hidden="true">image-20231216215902983</figcaption></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> blog搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Butterfly主题优化 </tag>
            
            <tag> PS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Butterfly主题 网页标题崩溃欺骗特效</title>
      <link href="/2023/12/05/Butterfly%E4%B8%BB%E9%A2%98%20%E7%BD%91%E9%A1%B5%E6%A0%87%E9%A2%98%E5%B4%A9%E6%BA%83%E6%AC%BA%E9%AA%97%E7%89%B9%E6%95%88/"/>
      <url>/2023/12/05/Butterfly%E4%B8%BB%E9%A2%98%20%E7%BD%91%E9%A1%B5%E6%A0%87%E9%A2%98%E5%B4%A9%E6%BA%83%E6%AC%BA%E9%AA%97%E7%89%B9%E6%95%88/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考网址：<a href="https://asdfv1929.github.io/2018/01/25/crash-cheat/">Hexo NexT主题中添加网页标题崩溃欺骗搞怪特效 | asdfv1929 's Home</a></p></blockquote><p>在butterfly主题中给网页标题增加一些搞怪特效</p><h4 id="创建js文件-crash_cheat.js">创建js文件 crash_cheat.js</h4><p>打开theme文件夹下的<code>\butterfly\source\js</code>，创建文件<span style="background:#f9eda6;color:red">crash_cheat.js</span>，添加代码：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;!--崩溃欺骗--&gt;</span><br><span class="line"> var OriginTitle = document.title;</span><br><span class="line"> var titleTime;</span><br><span class="line"> document.addEventListener(&#x27;visibilitychange&#x27;, function () &#123;</span><br><span class="line">     if (document.hidden) &#123;</span><br><span class="line">         $(&#x27;[rel=&quot;icon&quot;]&#x27;).attr(&#x27;href&#x27;, &quot;/img/favicon.png&quot;);</span><br><span class="line">         document.title = &#x27;╭(°A°`)╮ 页面崩溃啦 ~&#x27;;</span><br><span class="line">         clearTimeout(titleTime);</span><br><span class="line">     &#125;</span><br><span class="line">     else &#123;</span><br><span class="line">         $(&#x27;[rel=&quot;icon&quot;]&#x27;).attr(&#x27;href&#x27;, &quot;/favicon.png&quot;);</span><br><span class="line">         document.title = &#x27;(ฅ&gt;ω&lt;*ฅ) 噫又好了~&#x27; + OriginTitle;</span><br><span class="line">         titleTime = setTimeout(function () &#123;</span><br><span class="line">             document.title = OriginTitle;</span><br><span class="line">         &#125;, 2000);</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;)</span><br></pre></td></tr></table></figure><h4 id="inject-引用">inject 引用</h4><p>在主题配置文件<code>_config.butterfly.yml</code>的<span style="background:#f9eda6;color:red">inject-bottom</span>下直接引入js文件：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Inject</span><br><span class="line"># Insert the code to head (before &#x27;&lt;/head&gt;&#x27; tag) and the bottom (before &#x27;&lt;/body&gt;&#x27; tag)</span><br><span class="line"># 插入代码到头部 &lt;/head&gt; 之前 和 底部 &lt;/body&gt; 之前</span><br><span class="line">inject:</span><br><span class="line">  head:</span><br><span class="line">    # - &lt;link rel=&quot;stylesheet&quot; href=&quot;/xxx.css&quot;&gt;</span><br><span class="line"></span><br><span class="line">  bottom:</span><br><span class="line">    - &lt;script src=&quot;/js/crash_cheat.js&quot;&gt;&lt;/script&gt;     #页面崩溃欺骗特效</span><br><span class="line">    # - &lt;script src=&quot;xxxx&quot;&gt;&lt;/script&gt;     主题/source/js文件夹中的.js文件</span><br></pre></td></tr></table></figure><h4 id="解决引入js文件时出现inject不生效的问题">解决引入js文件时出现inject不生效的问题</h4><p>在上面一通操作后，再hexo三连，发现页面标题并未出现变化。经过多方查找解决方案，得到如下方法：</p><ol type="1"><li><p>在网站按<code>F12</code>检查，发现控制台报错：<span style="background:#fbd4d0;">Uncaught ReferenceError: $ is not defined</span></p></li><li><p>再次查找，发现原因是没有引用jquery库的jquery.min.js文件</p></li><li><p>引入jquery库：一种是从本地项目路径引用；另一种是通过网页链接引入（<span style="background:#f9eda6;color:red">无论哪种引用库的方式，都要把jquery库的引用放到第一个&lt;s<!--断开script关键字-->cript&gt;引用的前面，这样才能使后面的js文件顺序执行时被成功识别</span>）</p><p>如果我们的项目是https安全域名，那么引入代码为：</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://libs.baidu.com/jquery/2.1.4/jquery.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p>反之：</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;http://libs.baidu.com/jquery/2.1.4/jquery.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> blog搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Butterfly主题优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>学术英语写作与沟通 笔记</title>
      <link href="/2023/11/25/%E5%AD%A6%E5%A0%82%E5%9C%A8%E7%BA%BF%20%E5%AD%A6%E6%9C%AF%E8%8B%B1%E8%AF%AD%E5%86%99%E4%BD%9C%E4%B8%8E%E6%B2%9F%E9%80%9A/"/>
      <url>/2023/11/25/%E5%AD%A6%E5%A0%82%E5%9C%A8%E7%BA%BF%20%E5%AD%A6%E6%9C%AF%E8%8B%B1%E8%AF%AD%E5%86%99%E4%BD%9C%E4%B8%8E%E6%B2%9F%E9%80%9A/</url>
      
        <content type="html"><![CDATA[<p>课程链接：https://www.xuetangx.com/learn/hfut05021002478/hfut05021002478/16907237/video/36265667</p><p><strong>本文只作学习交流使用！</strong></p><h1 id="ch1-title-author-affiliation-论文题目-作者姓名-单位">Ch1 Title + Author + Affiliation 论文题目 + 作者姓名 + 单位</h1><h2 id="how-to-write-the-title-of-academic-paper">1.1 How to Write the Title of Academic Paper</h2><p>Titles are succinct descriptive labels of texts and are meant to fulfifil different purposes, such as to individualize a publication, summarize its content and appeal to its audience among others. They are ideally relevant to present the content of a study and, in general, they are self-explanatory to their readers. This topic will cover 5 sections; they are function, basic requirements, classification, syntax and tips.</p><h2 id="ex-1.1">ex 1.1</h2><p>1.Which type does this title belong to?</p><p>Effect of non-pharmaceutical interventions to contain COVID-19 in China</p><p><font color=red>Noun phrase title </font></p><p>2.Analyze the grammatical construction of the two following titles from top academic journals?</p><p>An investigation of transmission control measures during the first 50 days of the covid-19 epidemic in China ------ From <em>Science</em></p><p><font color=red>Nominal group construction</font></p><h2 id="author-affiliation">1.2 Author + Affiliation</h2><p>Author and affiliation provide important information of a published paper so we need to write them correctly in English. This section will talk about how to write author and affiliation for an academic paper from four aspects, namely definition, function, layout and writing tips.</p><h2 id="ex-1.2">ex 1.2</h2><p>1.Affiliations lie just above authors’ names in published papers, which usually contain such information as authors’ institutions or addresses.</p><p><font color=red>false</font></p><p>2.Corresponding author has the authority to act on behalf of all authors and is the contact person for the research paper.</p><p><font color=red>true</font></p><p>3.In terms of author’s affiliated information, we tend to put bigger unit first then followed by smaller one.</p><p><font color=red>false</font></p><p>4.We usually use the word “and” or the sign “&amp;” to connect the last two authors’ names.</p><p><font color=red>true</font></p><h1 id="ch2-outline-大纲">Ch2 Outline 大纲</h1><h2 id="ex-2">ex 2</h2><p>1.Which of the following is not true about the reverse outlining?</p><p><font color=red>It is useful only when your paper focuses on complex issues in detail.</font></p><p>2.Analyze the sample outline, and try to tell the type of the sample outline.</p><p><strong>Thesis:</strong> Explorers who went to conquer Mt. Everest have achieved much in many ways, but their expeditions also have exerted impacts, negative as well as positive, on Mt. Everest and the local community.</p><ol type="I"><li><pre><code>Background Information</code></pre></li></ol><p>​ A. Location of Mt. Everest</p><p>​ B. Geography of the Surrounding Area</p><p>​ C. Facts about Mt. Everest</p><p>​ 1. Height of the Mountain</p><p>​ 2. How the Mountain Was Named</p><p>​ a. Peak XV</p><p>​ b. Jomolungma (Tibetan name)</p><p>​ c. Sagarmatha (Nepalese name)</p><p>​ 3. The Number of People Who Have Climbed Everest to Date</p><ol start="2" type="I"><li><pre><code>Major Explorers Covered in this Paper</code></pre></li></ol><p>​ A. Sir Edmund Hillary</p><p>​ 1. First to Reach the Summit (1953)</p><p>​ 2. Leader of a Team of Experienced Mountain Climbers Who Worked Together</p><p>​ B. Tenzing Norgay and the Sherpas</p><p>​ 1. Norgay the Experienced Climber and Guide Who Accompanied Hillary</p><p>​ 2. Sherpas Still Used to Guide Expeditions</p><p>​ C. Rob Hall</p><p>​ 1. Leader of the Failed 1996 Expedition</p><p>​ 2. Leading Group of Tourists with Little Mountain Climbing Experience</p><ol start="3" type="I"><li><pre><code>The Impact Expeditions have had on Mt. Everest and Local Community</code></pre></li></ol><p>​ A. Ecological Effects</p><p>​ 1. Loss of Trees Due to High Demand for Wood for Cooking and Heating for Tourists.</p><p>​ 2. Piles of Trash Left by Climbing Expeditions</p><p>​ B. Economic Effects</p><p>​ 1. Expedition Fees Providing Income for the Country</p><p>​ 2. Expeditions Providing Work for the Sherpas, Contributing to the Local Economy.</p><p>​ C. Cultural Effects</p><p>​ 1. Introduction of Motor Vehicles</p><p>​ 2. Introduction of Electricity</p><p><font color=red>The topic outline</font></p><h2 id="importantce-of-an-outline-in-reading-and-writing">Importantce of an outline in reading and writing</h2><p><em>Outline can help the paper readers sort out the important and unimportant details, so that they can read more effectively.</em></p><p><em>When writing, the authors tend to use brainstorming, a useful tool for generating ideas in a free thinking way. Then outlining can help organize all the generated ideas, and help the authors prioritize the important information and eliminate the trivial details.</em></p><p><em>Therefore, although outline is not a part of your paper, it is an important tool to facilitate your academic career.</em></p><h2 id="outline-巩固题">outline 巩固题</h2><p>1.A thesis statement should be _____.</p><p><font color=red>a sentence</font></p><p>2.What is the role of thesis statement in an outline?</p><p><font color=red>the writer’s position about the topic</font></p><h1 id="ch3-abstract-摘要">Ch3 Abstract 摘要</h1><h2 id="ex-3.1">ex 3.1</h2><p>1.An abstract is a ____ summary of your published or unpublished research paper, usually about a paragraph long.</p><p><font color=red>short</font></p><p>2.An abstract lets readers get the gist or essence of your paper or article quickly, in order todecide whether to read the ____ paper.</p><p><font color=red>full</font></p><p>3.An abstract is a short statement about a paper designed to give a complete yet concise understanding of its research and finding.</p><p><font color=red>true</font></p><p>4.An abstract should include a ____ but ____ statement of the problem or issue, a description of the research method and design, the major findings and their significance, and the conclusion.</p><p><font color=red>brief</font> <font color=red>precise</font></p><p>5.The function of an abstract is to ___ .</p><p><font color=red>let the reader understand the main content of the paper </font></p><p><font color=red>provide convenience for the construction and maintenance of scientific and technological literature retrieval database</font></p><p>6.The features of an abstract is ___ .</p><p><font color=red>concise、objective、consistent、complete</font></p><h2 id="ex-3.2">ex 3.2</h2><p>1.An abstract prepares readers to follow the ____ information, analyses, and arguments in your full paper.</p><p><font color=red>detailed</font></p><p>2.An abstract helps readers remember ____ from your paper.</p><p><font color=red>key points</font></p><p>3.An abstract is often composed of____.</p><p><font color=red>Background or purpose、Methods、Results、Conclusion</font></p><p>4.The premises, objectives and tasks of the research work, and the scope of the topics covered will be introduced in the abstract.</p><p><font color=red>true</font></p><p>5.Methods are going to define how do you get answers to your research question, so ____, conditions, ____, means, ____, procedures employed in the paper will be illustrated define what material, what method, and what procedures are to be used.</p><p><font color=red>theories、materials、equipment</font></p><p>6.You may choose ____ as keywords.</p><p><font color=red>3-5 highly relevant terms</font></p><h2 id="samples-of-abstract">Samples of Abstract</h2><h3 id="descriptive-abstract"><strong>Descriptive abstract</strong></h3><p>Translation is not only a linguistic transference, but also an intercultural communication. For quite a long time, translation studies have been concentrated on the prescription of translation methods, with scant attention paid to the description of macro-cultural factors involved in the translating. In this paper, the writer contends that the study of the macro-cultural factors will surely enlarge the scope and enrich the content of the translation studies. The paper is largely a rudimentary step, both in theory and in practice, to expose some of the factors influencing Mr. Fu’s translation of <em>Gone with the Wind</em>, with a historic and descriptive approach employed.</p><h3 id="informative-abstract"><strong>Informative abstract</strong></h3><p>This study investigated the role of "signaling" in helping good readers comprehend expository text. As the existing literature on signaling, reviewed in the last issue of the Journal, pointed to deficiencies in previous studies' methodologies, one goal of this study was to refine prose research methods. Two passages were designed in one of eight signaled versions each. The design was constructed to assess the individual and combined effect of headings, previews, and logical connectives. The study also assessed the effect of passage length, familiarity and difficulty. The results showed that signals do improve a reader's comprehension, particularly comprehension two weeks after the reading of a passage and comprehension of subordinate and superordinate inferential information. This study supports the hypothesis that signals can influence retention of text-based information, particularly with long, unfamiliar, or difficult passages.</p><h1 id="ch4-data-collection-and-evaluation-数据收集与评价">Ch4 Data Collection and Evaluation 数据收集与评价</h1><h2 id="ex-4">ex 4</h2><p>1.There are ____ formats of data collection and evaluation.</p><p><font color=red>3</font></p><p>2.When introducing background information, if the content of a sentence is a general fact that is not affected by time, apply_____.</p><p><font color=red>the present tense</font></p><p>3.Data evaluation refers to the process of ____.</p><p><font color=red>critically reading and evaluating the sources</font></p><p>4.The titles of tables or diagrams are usually expressed in ____.</p><p><font color=red>phrases</font></p><h2 id="tips-for-data-collection-and-evaluation">Tips for Data Collection and Evaluation</h2><h3 id="i.-data-collection"><strong>I</strong>. <strong>Data Collection</strong></h3><p><strong>Collecting primary sources</strong></p><p><strong>Texts</strong>: Once the main arguments are in mind, the text should be re-read while highlighting, and underlining, scribbling in the margins, or using sticky notes to pick out what is needed.</p><p><strong>Interviews</strong>: Locate someone through friends and family networks.</p><p><strong>Collecting secondary sources</strong></p><p>Searching the Web for government documents. Government records may be helpful but in most cases secondary-source research begins at the library.</p><p>The library is the main source of data collection, where readers can access the relevant literature, books, periodicals, audio and video recording, radio, television and the Internet.</p><p><strong>As to the types of information</strong>, in the library there are data in print and electronic resources, including encyclopedias, almanacs (年鉴)，indexes (索引), abstracts, dictionaries, reference literature, compilations, bibliographic catalogs, online databases, web sites, online communities, search engines and so on.</p><p>Researchers often follow the following four steps in collecting data from the library:</p><p>l Search encyclopedias.</p><p>l Search biographical references, yearbooks, atlases（地图册），gazettes（公报，报纸）and professional dictionaries relevant to the topic.</p><p>l Search library catalogs to find appropriate books. Search by author, title or subject.</p><p>l Retrieve journal articles citation from journals or newspapers index with the relevant terms found in encyclopedias, dictionaries and other reference directories (目录).</p><h3 id="ii.-data-evaluation"><strong>II. Data Evaluation</strong></h3><p>Data evaluation refers to the process of critically reading and evaluating the sources. The gist of being critical is not just to criticize, but to question and, not take anything read at face value.</p><p>Structure, purpose, audience and author are four important dimensions of the text to pay close attention to in critical thinking and reading.</p><p><strong>Structure</strong></p><p>If starting with a book, look at the table of contents. See the shape of what is to come and identify places where the thesis or question might be most directly addressed. Notice the subsections. Is there anything very obviously missing?</p><p>Glance at any appendices, diagrams, tables, or figures and see what kinds of things make it into the Endnotes section if there is one.</p><p>Look at the topics listed in the Index at the back. Which of the entries has the most page numbers listed next to it? This will give you an indication of the subjects that contribute to the real scope of the book.</p><p><strong>Purpose</strong></p><p>Examine the title and first few paragraphs. What is the author trying to do? What is his or her bias? Any assumptions to be challenged?</p><p><strong>Audience</strong></p><p>Who is the intended audience? How narrow or broad is it? To answer this, look at stylistic choices such as diction and tone. Who is the target audience?</p><p><strong>Author</strong></p><p>Who is the author? Is it someone a professor has mentioned or one that you come across in the course of other reading? Has the person been mentioned in other texts or bibliographies of other texts? Is the person a teacher or researcher from a reputable academic institution?</p><p>Does the person have considerable knowledge of what he or she is talking about? Is the author respected and well-received（深受好评的）?</p><p><strong>Evaluating Web Pages</strong></p><p>Authorship</p><p>Who wrote this?</p><p>The publishing body</p><p>Is the name of any organization given on the document? Are there headers (页眉) , footers, or a distinctive watermark that show the document to be part of an official academic or scholarly web site?</p><p>Point of view or bias</p><p>Referral to or display of knowledge of the literature</p><p>Accuracy or verifiability of details</p><p>Currency: the date of publication</p><p>All information needs to be evaluated by readers for authority, appropriateness and other personal criteria for value. Never use information that cannot be verified. Establishing and learning criteria to filter information found on the Internet is a good beginning for becoming a critical consumer of information in all forms.</p><p>Learn to be skeptical and then learn to trust your instincts.</p><h2 id="useful-expressions-and-sentence-patterns">Useful Expressions and Sentence Patterns</h2><ol type="1"><li><h3 id="useful-expressions">Useful Expressions</h3></li></ol><ol type="1"><li><p>开头 图表类型：table（表格）、chart（图表）、graph（多指曲线图）、diagram（图标）、column chart（柱状图）、pie graph（饼图）、tree diagram（树形图） 描述：show, describe, illustrate, can be seen from, clear, apparent, reveal, represent 内容：figure, statistic, number, percentage, proportion</p></li><li><p>表示数据变化的单词或者词组 rapid/rapidly 迅速的/地，飞快的/地，险峻的/地 dramatic/dramatically 戏剧性的/地</p></li></ol><p>significant/significantly 有意义的/地，重大的/地 sharp/sharply 锐利的/地，急剧的/地 steep/steeply 急剧升降的/地</p><p>gradual/gradually 渐进的/地，逐渐的/地 slow/slowly 缓慢的/地 slight/slightly 稍微的/略微地</p><p>stable/stably 稳定的/地</p><p>steady/steadily 稳固的/地，坚定不移的/地</p><ol start="3" type="1"><li>其它在描述中的常用到的词</li></ol><p>grow 增长 distribute 分布 unequally 不相等地</p><p>measure n. 方法，措施 v. 估量，调节 forecast n.先见，预见 v. 猜测</p><p>significant changes 图中一些较大变化 noticeable trend 明显趋势 during the same period 在同一时期 in the case of 在……的情况下 in terms of/in respect of/regarding 在……方面 in contrast 相反，大不相同</p><ol start="2" type="1"><li><h3 id="useful-sentence-patterns">Useful Sentence Patterns</h3></li></ol><ol type="1"><li><p>The table shows the changes in the number of…over the period from…to…. 该表格描述了在……年至……年间……数量的变化。</p></li><li><p>The bar chart illustrates that…. 该柱状图展示了……</p></li><li><p>The graph provides some interesting data regarding…. 该图为我们提供了有关……有趣数据。</p></li><li><p>The diagram shows（that）…. 该图向我们展示了……</p></li><li><p>The pie graph depicts （that）…. 该圆形图揭示了……</p></li><li><p>This is a cure graph which describes the trend of…. 这个曲线图描述了……的趋势。</p></li><li><p>The figures/statistics show （that）…. 数据（字）表明……</p></li><li><p>The tree diagram reveals how…. 该树型图向我们揭示了如何……</p></li><li><p>The data/statistics show （that）…. 该数据（字）可以这样理解……</p></li><li><p>The data/statistics/figures lead us to the conclusion that….</p></li><li><p>From the table/chart/diagram/figure，we can see clearly that……or it is clear/apparent from the chart that…. 从图表我们可以很清楚（明显）看到……</p></li><li><p>This is a graph which illustrates…. 这个图表向我们展示了……</p></li><li><p>This table shows the changing proportion of a and b from……to…. 该表格描述了……年到……年间a与b的比例关系。</p></li><li><p>The graph，presented in a pie chart，shows the general trend in…. 该图以圆形图形式描述了……总的趋势。</p></li><li><p>This is a column chart showing…. 这是一个柱型图，描述了……</p></li><li><p>As can be seen from the graph，the two curves show the fluctuation of…. 如图所示，两条曲线描述了……的波动情况。</p></li></ol><h1 id="ch5-summary-概要">Ch5 Summary 概要</h1><h2 id="ex-5">ex 5</h2><p>1.A good summary is an accurate reflection of the author’s viewpoint.</p><p><font color=red>true</font></p><p>2.When you write a summary, you need to________.</p><p><font color=red>Write in your own words</font></p><p>3.How would you avoid plagiarism when you want to cite source materials?</p><p><font color=red>Summarize、Quote、Paraphrase</font></p><p>4.Most summaries start with a sentence containing two elements:____ and ____.</p><p><font color=red>the source、the main idea</font></p><h1 id="ch6-literature-review-文献综述">Ch6 Literature Review 文献综述</h1><h2 id="ex-6.1">ex 6.1</h2><p>1.Is it plagiarism if I use big parts of someone’s literature review for the introduction section of my paper?</p><p><font color=red>Yes</font></p><p>2.The most important reason for you to write literature review in an article is to ____.</p><p><font color=red>find a gap or something contradictory in the previous studies to justify your research</font></p><p>3.Which of the following title looks like a literature review?</p><p><font color=red>Background subtraction techniques: a review.</font></p><h2 id="summarize-vs-synthesize">Summarize VS Synthesize</h2><p><strong>Summarizing and synthesizing information from multiple sources is an indispensable step for writing a literature review.</strong> <strong>But are you really clear about their differences?</strong></p><ul><li><p><strong>A summary reiterates what the study is about.</strong></p><p><strong>Summary is important in a literature review because some research may not be familiar to the reader. It helps the reader develop an understanding of the subject matter. But a literature review goes beyond retelling what the data points out.</strong></p></li><li><p><strong>Synthesis pulls several sources together and explains through the writer’s words what his interpretation of the data means to the writer in his own voice.</strong></p><p><strong>For example, you have five research studies and they all point to the same conclusion.</strong></p></li></ul><h2 id="ex-6.2">ex 6.2</h2><p>1.We can use past tense when describing an action in a research beginning in the past and continuing to the present.</p><p><font color=red>false</font></p><p>2.The literature review is a synthesis and analysis of research on your topic in your own words. Most ideas can be and should be paraphrased. Paraphrase is a preferred choice over direct quotations on most occasions.</p><p><font color=red>true</font></p><h1 id="ch7-proposal-开题报告">Ch7 Proposal 开题报告</h1><h2 id="ex-7.1">ex 7.1</h2><p>1.The research proposal can __________.</p><ul><li><font color=red> clearly and systematically present the research problem and objective</font></li><li><font color=red>indicate the significance and introduce the specific methodology and research procedures synthesize current knowledge, seek gaps and formulate a plan to address the problem</font></li><li><font color=red>synthesize current knowledge, seek gaps and formulate a plan to address the problem</font></li><li><font color=red>provide a timetable for the study and a budget of the investigation or experiments</font></li></ul><p>2.A research proposal is intended to_________.</p><ul><li><font color=red>convince the readers that you are ready to do a research</font></li><li><font color=red>demonstrate that you have the knowledge, full understanding and the expertise to complete the project</font></li><li><font color=red>show your competency in a particular area of study</font></li><li><font color=red>serve as a planning tool</font></li></ul><p>3.The main elements of a research proposal include __________.</p><p><font color=red>what、why、how、expected result</font></p><p>4.Examine carefully the following to determine to what extent the topic chosen meets the criteria for a proposal:</p><ul><li><font color=red>It must be interesting to you.</font></li><li><font color=red>It must be within your competence.</font></li><li><font color=red> It must be feasible.</font></li><li><font color=red>It must be sufficiently delimited.</font></li><li><font color=red>It must have the potential to make a contribution to knowledge or practice in the appropriate area.</font></li></ul><p>5.<strong>The potential supervisors use research proposals to assess</strong></p><ul><li><font color=red>The quality and originality of your ideas</font></li><li><font color=red>Your skills in critical thinking</font></li><li><font color=red>The feasibility of the research project</font></li></ul><h2 id="ex-7.2">ex 7.2</h2><p>1.The budget of a research proposal consists of __________.</p><p><font color=red>a section details the amount of cost of equipment and service</font></p><p><font color=red>a section provides the justification for the funding requested for reviewers to check and see if it is reasonable</font></p><p>2.A thesis statement is a simple sentence that formulates both your topic and _________ toward it.</p><p><font color=red>point of view</font></p><p>3.In exploring data for a research proposal, sources can be divided into ____________.</p><p><font color=red>primary source and secondary source</font></p><p>4.Sometimes the literature review section is incorporated into ( ) section.</p><p><font color=red> Introduction</font></p><p>5.Most students’ literature reviews suffer from the following problems:</p><ul><li><font color=red>Lacking organization and structure</font></li><li><font color=red>Lacking focus, unity and coherence</font></li><li><font color=red>Failing to cite influential papers</font></li><li><font color=red>Failing to critically evaluate cited papers</font></li><li><font color=red>Depending too much on secondary sources</font></li></ul><h2 id="ex-7.3">ex 7.3</h2><p>1.In a successful writing of a research proposal, we should __________.</p><ul><li><font color=red>use accepted scientific terms</font></li><li><font color=red> try to use full forms instead of abbreviations and avoid contractions</font></li><li><font color=red>use formal word and phrases instead of nonstandard or informal expressions</font></li><li><font color=red>use impersonal expressions and passive voice in a proper manner</font></li></ul><p>2.In proposal, the discussion section will include:</p><ul><li><font color=red>An analysis of sources of error in the data</font></li><li><font color=red>Integration with what was previously known</font></li><li><font color=red>Implications for future study</font></li></ul><h1 id="ch8-how-to-write-a-research-paper-introduction">Ch8 How to Write a Research Paper Introduction</h1><h2 id="ex-8">ex 8</h2><p>1.Generally speaking, what will be covered in Introduction Part?</p><ul><li><font color=red>Significance and necessity of the study</font></li><li><font color=red>Background, scope of the issue being study </font></li><li><font color=red>Clear definitions of key term involved in the study</font></li><li><font color=red>Theoretical foundations for the study</font></li><li><font color=red>Objectives of the present study</font></li><li><font color=red>Brief literature review and comment on the previous studies</font></li></ul><h1 id="ch9-material-and-methods-材料与方法">Ch9 Material and Methods 材料与方法</h1><h2 id="ex-9">ex 9</h2><p>1.Among the following research methods, which one is cheaper and easier?</p><p><font color=red>Opinion-based research method.</font></p><p>2.Compared with qualitative research method, quantitative research method_________.</p><p><font color=red>focuses on specific and narrow area.</font></p><p>3.In the materials and methods section, the ____ tense is more natural since you are describing work that is already completed at the time of writing, and the____ voice is preferable since this section focuses more on research than on researcher.</p><p><font color=red>past、passive</font></p><h1 id="ch10-resultsfindings-figures-tables-结果图表">Ch10 Results/Findings + Figures &amp; Tables 结果+图表</h1><p>The Results/Findings describes the statistical results/ findings of a research, which directly answers the research questions raised previously. It is important as it is the section where authors present new information and make new knowledge claims. The results/ findings can be given in the form of numerical data, verbal description or the combination of the above two.</p><h2 id="ex-10.1">ex 10.1</h2><p>1.In which form can the Results/ Findings section be given? ( )</p><p><font color=red>numerical data、verbal description、the combination of B and C</font></p><p>2.What are the three main functions of Results/ Findings section? ( )</p><p><font color=red>locating results、reporting results、explaining results</font></p><p>3.What are the three writing principles for the Results/ Findings section?</p><p><font color=red>faithfulness、innovation、generalization</font></p><p>4.How can the main findings be presented in the Results/ Findings section?</p><ul><li><font color=red>in a certain logical order</font></li><li><font color=red>in chronological order</font></li><li><font color=red>in order of importance</font></li></ul><h2 id="tips-for-writing-the-results-findings">Tips for Writing the Results/ Findings</h2><h3 id="structure-of-the-resultsfindings-section"><strong>1) structure of the Results/Findings section</strong></h3><p><strong>The first subdivision</strong> in this section is to provide preparatory information for the presentation of the results and it functions as a reminder and connector between the Methods section and the Results section. Authors often introduce source of data such as the type of data, the size of data, and the data collection method to prepare for the presentation of the significant results.</p><p><strong>The second subdivision</strong> in this section is to present results. Authors present the results of the study with relevant evidence such as statistics and examples. To report results is obligatory while to locate results and to explain results are optional.</p><h3 id="writing-principles-for-the-results-findings"><strong>2) writing principles for the Results/ Findings</strong></h3><p>There are some writing principles to follow here. They are faithfulness, innovation and generalization, or FIG.</p><p><strong>Principle of faithfulness.</strong> Whether research results can hold water depends on whether they can be tested repeatedly. Never add or delete the research results subjectively. Conflicting results sometimes lead to more meaningful further hypotheses and even more scientific conclusions. So be faithful in presenting your results.</p><p><strong>Principle of innovation.</strong> It is not necessary to cite too much of others' research results here. Therefore, when this section is written, the content of authors’ own original findings should be highlighted.</p><p><strong>Principle of generalization.</strong> Authors need to generalize essential facts and summarize key results in this section. No need to report all specific raw data here. State the main findings in a certain logical order, say, in chronological order or in order of importance.</p><h2 id="ex-10.2">ex 10.2</h2><p><em>Tables and figures are very important in academic writing and communication. They provide visual ways of presenting data and each type has its own advantages and disadvantages.</em></p><p>1.Tables and figures provide visual ways of presenting data and each type has its own advantages and disadvantages.</p><p><font color=red>true</font></p><p>2.Figures can display exact data or statistical information.</p><p><font color=red>false</font></p><p>3.There is no need to classify, process or select data in tables or figures.</p><p><font color=red>false</font></p><p>4.Which one is a flow chart?</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/ba2ccc22-193a-4164-8a21-f41d0087b960.png" alt="chart 4.png" /><figcaption aria-hidden="true">chart 4.png</figcaption></figure><h2 id="tips-for-various-tables-and-figures">Tips for Various Tables and Figures</h2><p>A table is an arrangement of data in rows and columns, or possibly in a more complex structure. Tables are widely used in communication, research and data analysis. They can display exact data or statistical information.</p><p>Figures have many various types. Here we just focus on pie chart, bar chart, line chart and flow chart.</p><ol type="1"><li>A pie chart is a circular statistical graphic which is divided into slices to illustrate numerical proportions. But sometimes it is not easy to compare different sections of a given pie chart or compare data across different pie charts.</li><li>A bar chart is a chart to present grouped data with rectangular bars whose lengths are proportional to the values they present. It can show comparisons among various categories.</li><li>A line chart displays information as a series of data points connected by straight line segments. It is often used to visualize a trend in data changing with the time or condition.</li><li>A flow chart is a diagram to show the order of operations or sequence of tasks for solving a problem or managing a complex project.</li></ol><h1 id="ch11-discussion-and-conclusion">Ch11 Discussion and Conclusion</h1><h2 id="ex-11.1">ex 11.1</h2><p>1.The results show that those who reported driving unsupervised 1-12 times were 80% more likely to be involved in crash than those who reported never having done this.</p><p><font color=red>Reporting results</font></p><p>2.The research has some potential limitations.</p><p><font color=red>Indicating limitations</font></p><p>3.We bring everything together in this part by discussing the ______ of our findings and its relationship to previous research in the area.</p><p><font color=red>significance</font></p><p>4.Each result reported should be followed by a proper ______.</p><p><font color=red>discussion</font></p><h2 id="tips-for-discussion">Tips for Discussion</h2><ol type="1"><li>summarized the main results.</li><li>interpreted (not described) the results.</li><li>discussed the significance of the results.</li><li>explained whether the results prove or disprove the hypothesis.</li><li>discussed the results in the light of previous research.</li><li>explained the wider implications of the work.</li><li>discussed any problems with or limitations of the study.</li><li>made suggestions for improvements.</li><li>suggested directions for future research.</li></ol><h2 id="ex-11.2">ex 11.2</h2><p>1.Given that a significant portion of learner license holders report driving unsupervised and those that violate this condition the most are more likely to crash, improving compliance with learner license supervised driving conditions varrants increased attention.</p><p><font color=red>Interpreting results</font></p><p>2.Evaluations of the initiatives designed to increase compliance supervised driving condition should be a research priority.</p><p><font color=red>Recommending further research</font></p><p>3.Though components of a conclusion may vary, a conclusion generally contains a restatement of the thesis in the ________, a summary of key points in the body, and a broad statement.</p><p><font color=red>introduction </font></p><p>4.This exploratory case study ________the stated beliefs and actual instructional practices of two experienced teachers of English language in a primary school in Singapore.</p><p><font color=red>investigated </font></p><h2 id="tips-for-conclusion">Tips for Conclusion</h2><p>The structure of a conclusion generally follows a pattern, moving from specific to general.</p><ol type="1"><li>Restatement of the main premise.</li><li>Summary of key points in the essay.</li><li>Broad statement.</li></ol><h1 id="ch12-how-to-write-acknowledgements-致谢">Ch12 How to Write Acknowledgements 致谢</h1><h2 id="ex-12">ex 12</h2><p>1.请翻译：国家自然科学基金</p><p><font color=red>National Natural Science Foundation of China</font></p><p>2.Whom will we thank in Acknowledgement Part?</p><ul><li><font color=red>Those who have helped the author's scientific research</font></li><li><font color=red>Reviewers and editors </font></li><li><font color=red>Funding / program</font></li></ul><h1 id="ch13-references-参考文献">Ch13 References 参考文献</h1><h2 id="ex-13">ex 13</h2><p>1.The functions of the references section include the following points except:</p><p><font color=red>It concludes the whole researc</font></p><p>2.The following two samples are documented in ____ style.</p><p>Wysocki, Anne Frances, et al. <em>Writing New Media: Theory and Applications for Expanding the Teaching of Composition</em>. Logan, UT: Utah State UP, 2004. Print.</p><p>Foucault, Michel. <em>Madness and Civilization: A History of Insanity in the Age of Reason</em>. Trans. Richard Howard. New York: Vintage-Random House, 1988. Print.</p><p><font color=red>MLA</font></p><p>3.Which of the following is a documentation tool?</p><p><font color=red>Latex、Endnote、Bibtex（Trados 是翻译软件）</font></p><h2 id="samples-of-apa-and-mla-style">Samples of APA and MLA style</h2><p>Anderson, A. K., Christoff, K., Panitz, D., De Rosa, E., &amp; Gabrieli, J. D. E. (2003). Neural correlates of the automatic processing of threat facial signals. <em>Journal of Neuroscience, 23,5627-5633.</em></p><p>Chow T. W., &amp; Cummings, J. L. (2000). The amygdala and Alzheimer<em>’</em>s disease. In J. P. Aggleton (Ed.), <em>The amygdala: A functional analysis</em> (pp. 656–680). Oxford,England: Oxford University Press.</p><p>Shipley, W. C. (1986). <em>Shipley Institute of Living Scale</em>. Los Angeles, CA: Western Psychological Services.</p><p>Wheeler, D.P., &amp; Bragin, M. (2007). Bringing it all back home: Social work and the challenge of returning veterans. Health and Social Wor, 32, 297-300. Retrieved from <a href="http://www/">http://www</a>. naswpressonline. org</p><p>Samples of MLA style</p><p>Wysocki, Anne Frances, et al. <em>Writing New Media: Theory and Applications for Expanding the Teaching of Composition</em>. Logan, UT: Utah State UP, 2004. Print.</p><p>Foucault, Michel. <em>Madness and Civilization: A History of Insanity in the Age of Reason</em>. Trans. Richard Howard. New York: Vintage-Random House, 1988. Print.</p><p>Crowley, Sharon, and Debra Hawhee. <em>Ancient Rhetorics for Contemporary Students</em>. 3rd ed. New York: Pearson/Longman, 2004. Print.</p><p><strong>The New Jerusalem Bible</strong>. Ed. Susan Jones. New York: Doubleday, 1985. Print.</p><p>Bishop, Karen Lynn. <em>Documenting Institutional Identity: Strategic Writing in the IUPUI Comprehensive Campaign</em>. Diss. Purdue University, 2002. Ann Arbor: UMI, 2004. Print.</p><p>Poniewozik, James. "TV Makes a Too-Close Call." <em>Time</em> 20 Nov. 2000: 70-71. Print.</p><p>Buchman, Dana. "A Special Education." <em>Good Housekeeping</em> Mar. 2006: 143-48. Print.</p><p>"Of Mines and Men." Editorial. <em>Wall Street Journal</em> east. ed. 24 Oct. 2003: A14. Print.</p><p>Hamer, John. Letter. <em>American Journalism Review</em> Dec. 2006/Jan. 2007: 7. Print.</p><p>Bagchi, Alaknanda. "Conflicting Nationalisms: The Voice of the Subaltern in Mahasweta Devi's Bashai Tudu." <em>Tulsa</em> <em>Studies in Women's Literature</em> 15.1 (1996): 41-50. Print.</p><p>Aristotle. <em>Poetics</em>. Trans. S. H. Butcher. <em>The Internet Classics Archive</em>. Web Atomic and Massachusetts Institute of Technology, 13 Sept. 2007. Web. 4 Nov. 2008. <a href="http://classics.mit.edu/Aristotle.html" class="uri">http://classics.mit.edu/Aristotle.html</a>.</p><h2 id="references-作业">references 作业</h2><p>1.If writers do not give credit for borrowed ideas or words, they make a serious error, i.e. ____.</p><p><font color=red>plagiarism</font></p><p>2.When you use a direct quotation in APA style, you must state the following elements except _____.</p><p><font color=red>the edition of the journal</font></p><h2 id="文献引用的三种格式">文献引用的三种格式</h2><p>文献引用一般有三种形式。（参考链接：<a href="https://blog.csdn.net/programmer_jiang/article/details/118557929#:~:text=Author%20Prominent%20Citation**：一般过去时**%20关注完成研究的人。%20一般以作者姓放在句首做主语，%20其后括号标注引注年份%20，汇报动词（reporting%20verb）做谓语动词。,higher%20in%20order%20to%20yield%20acceptable%20sliceability.%204">文献英语期末_hedging模糊语有哪些-CSDN博客</a>）</p><ul><li>Information Prominent Citation<strong>：一般现在时</strong></li></ul><p>关注前人研究的内容。一般陈述研究内容，引注将作者和年份放在句末括号里。 如：Scientific paper writing skill is usually adopted with learning by doing and formal training (Auvinen, 2015).</p><ul><li>Author Prominent Citation<strong>：一般过去时</strong></li></ul><p>关注完成研究的人。一般以作者姓放在句首做主语，<strong>其后括号标注引注年份</strong>，汇报动词（reporting verb）做谓语动词。 如：Theno et al. (1978) concluded that salt content had to be 2% or higher in order to yield acceptable sliceability. 4</p><ul><li>Weak Author Prominent Citation<strong>：现在完成时</strong></li></ul><p>关注一系列研究的相似结果。一般以 many researchers，many scholars 等一群人作为句子主语，谓语动词用完成时，引注将诸多作者和年份排列在句末括号里。 如： Researchers have noted that resolving this debate hinges on understanding the relationship between yield and biodiversity, the likelihood of land being spared, and external consequences of practices raising yield, such as agrochemical runoff (Grau et al., 2013; Green et al., 2005; Phalan et al., 2011).</p><h1 id="ch14-academic-poster-学术海报">Ch14 Academic Poster 学术海报</h1><h2 id="ex-14.1">ex 14.1</h2><p>1.Important information should be ____ from about 10 feet away.</p><p><font color=red>readable</font></p><p>2.Title is ____ and draws interest.</p><p><font color=red>short</font></p><p>3.Academic posters could be an ____way of communicating concisely, visually and attractively.</p><p><font color=red>effective</font></p><p>4.Use ____ language to present your work. Avoid ____ unless you're really positive that yours will be a specialist-only audience.</p><p><font color=red>plain、jargon and acronyms</font></p><p>5.An effective poster lets ____ tell the story, uses ____ sparingly, and keeps the sequence well-ordered and obvious.</p><p><font color=red>graphs and images、text</font></p><p>6.Academic posters summarize information or research concisely and attractively, to help publicize information or research and generate discussion.</p><p><font color=red>true</font></p><p>7.What roles does an academic poster play in academic communication？</p><ul><li><font color=red>Posters are widely used in the academic community.</font></li><li><font color=red>Most conferences include poster presentations in their program. </font></li><li><font color=red>Academic posters may be displayed at national or international conferences.</font></li><li><font color=red>They may also be published online as part of conference proceedings.</font></li><li><font color=red>Academic Posters can be used as assessment at university.</font></li></ul><h2 id="ex-14.2">ex 14.2</h2><p>1.An effective poster can make a ____impact, so it's worth developing your poster planning skills.</p><p><font color=red>strong</font></p><p>2.In some courses, ____ and ____ may be weighted at 60%, with ____ and ____weighted at 40%.</p><p><font color=red>content、structure、visual organization、presentation</font></p><p>3.If you are reporting on a piece of research with an academic poster , you can turn to the some language signals to ____, ____, ____, and ____.</p><p><font color=red>introduce the poster、locate a point on the poster、answer directly、handle complex questions</font></p><p>4.Academic posters need to show evidence of reading and research, so you must always include Literature cited.</p><p><font color=red>true</font></p><p>5.Acknowledgments section is to thank individuals for specific contributions to the project Also include in this section explicit disclosures for any conflicts of interest and conflicts of commitment.</p><p><font color=red>true</font></p><h2 id="poster">poster</h2><p>• An effective poster is a visual communications tool;</p><p>• An effective poster will get your main point(s) across to as many people as possible;</p><p>• An effective poster is focused on a single message; lets graphs and images tell the story; uses text sparingly; keeps the sequence well-ordered and obvious.</p><h1 id="ch15-linguistic-features-of-academic-english-学术英语的语言特点">Ch15 Linguistic Features of Academic English 学术英语的语言特点</h1><h2 id="ex-15.1">ex 15.1</h2><p>1.In writing academic paper, we should avoid expressing____ arising out of intuition, feeling, prejudice or your own experience.</p><p><font color=red>personal opinions</font></p><p>2.Another common feature of academic writing is nominalization, whereby verbs become _____.</p><p><font color=red>nouns</font></p><p>3.We can make tentative statements by applying ____ into our academic writing.</p><p><font color=red>models、verbs、adverbs、adjectives</font></p><h2 id="tips-for-academic-writing">Tips for Academic Writing</h2><p><strong>Why do we need a formal language style?</strong></p><p>To fulfil the expectations of academic readers.</p><p>It is important to follow the specific genre requirements in academic writing. This is a defining feature of academic writing.</p><p>Term papers and research reports are generally formal.</p><p><strong>How to achieve formality?</strong></p><p>Advanced and academic vocabulary</p><p>Long and complex sentences</p><p>The minimized use of 1st - and 2nd –person pronouns</p><p><strong>The use of advanced and academic vocabulary</strong></p><p>Avoid using simple and colloquial words</p><p><strong>Simple words</strong> <strong>Advanced words</strong></p><p>eat consume</p><p>try attempt/endeavor</p><p>keep remain/maintain</p><p>good (for) beneficial</p><p>bad(for) harmful/detrimental</p><p><strong>Avoid using phrasal verbs</strong></p><p>It would be better to use a single verb instead of a phrasal verb.</p><p><strong>Phrasal verbs</strong> <strong>One-word verbs</strong></p><p>look into investigate</p><p>find out discover</p><p>cut down reduce</p><p>go up increase</p><p>get rid of eliminate</p><p><strong>Avoid using shortened forms of words/contractions</strong></p><p><strong>Contractions Full forms</strong></p><p>won’t will not</p><p>didn’t did not</p><p>can’t cannot</p><p>it’s it is</p><p>you’re you are</p><p>​</p><p><strong>Academic Word List (AWL)</strong></p><p>Developed by Averil Coxhead, a scholar in New Zealand</p><p>570 word families</p><p>Formal, advanced and academic</p><p><strong>The use of long and complex sentences</strong></p><p>Subordinate clauses: add extra information to the main clause</p><p>Subordinate conjunctions 从属连词</p><p>Some youngsters admit getting restless if their phones is not nearby.</p><p>Relative pronouns 关系代词</p><p>It is advisable to write an outline, which will help you organize the essay in a more logical way.</p><p><strong>Subordinate conjunctions</strong> <strong>Relative pronouns</strong></p><p>Once after until that who whose</p><p>Provided that although when</p><p>Rather than as whenever which whoever whosever</p><p>Since because where</p><p>So that before whereas whichever whom whomever than even if wherever whether if</p><p>Though while unless why</p><p>l Do not always use too long and complex sentences in your essay, because readers will find them difficult to understand.</p><p>l The best way is to use a combination of simple and complex sentences.</p><p><strong>The minimized use of 1st - and 2nd –person pronouns</strong></p><p>First-person pronouns: I, we, us, my, our</p><p>second-person pronouns: you and your</p><p>E.g. Recently, we have discussed COVID-19 and China-US relations a lot.</p><p>→ Recently, COVID-19 and China-US relations have been much discussed.</p><p>Tip: Use third-person and passive voice.</p><h2 id="useful-expressions-in-results-and-tables-figures">Useful Expressions in Results and Tables &amp; Figures</h2><h3 id="useful-expressions-in-results"><strong>1.</strong> <strong>Useful Expressions in Results</strong></h3><p><strong>1.1 Active voice VS Passive voice</strong></p><p>Table 1 presents…and Table 2 presents…</p><p>Our hypothesis predicted….</p><p>Information…was obtained….</p><p>…this information was collected when</p><p><strong>1.2 Suitable reporting verbs</strong> show, indicate, reveal, report,</p><p>describe, explain, display, present…</p><p><strong>1.2.1 Locating the data</strong></p><p>As can be seen from Table 1…</p><p>… are shown/given/provided/ summarized in Table 1.</p><p>Table 1 demonstrates/</p><p>indicates/suggests….</p><p><strong>1.2.2 Highlighting the data</strong></p><p>…is exactly/approximately/almost the same as</p><p>… is completely/entirely/quite different from…</p><p>The main difference between…and… is that…</p><p><strong>1.2.3 Discussing the data</strong></p><p>The data clarify the relationship</p><p>between… and…</p><p>There is some evidence in the data to support our hypothesis, which proposed that…</p><p>This particular result may be</p><p>attributed to the influence of …</p><p><strong>1.3 Phrases of generality</strong></p><p>Overall</p><p>In general</p><p>On the whole</p><p>In the main</p><p>With …exception(s)</p><p>Overall, the results indicate that students performed above the 12th-grade level.</p><p>The overall results indicate…</p><p>The results indicate, overall, that…</p><p>In general, the experimental samples resisted…</p><p>With one exception, the experimental samples resisted…</p><h3 id="useful-expressions-in-tables-and-figures"><strong>2.</strong> <strong>Useful Expressions in</strong> <strong>Tables and Figures</strong></h3><p><strong>2.1 Very frequent and appropriate verbs</strong></p><p>reported, show, characterized, suggests, used, intended, contradict, suggest, prevail, focused, enables, speculate, maintain, compared, focused, claimed, shows, tend, represent</p><p><strong>2.2 Tense of verbs</strong></p><p>The tables show that those who reported driving unsupervised 1-12 times were 80% more likely to be involved in crash than those who reported never having done this.</p><h2 id="ex-15.2">ex 15.2</h2><p>1.Regardless of the placement, each figure must be _____ consecutively and complete with caption.</p><p><font color=red>numbered</font></p><p>2.What kind of tense do we usually use if we start a sentence with words like “Table 1 or Table n”?</p><p><font color=red>The present tense</font></p><p>3.We can place figures and tables within ____, or we can include them at____.</p><p><font color=red>the text of the results、the end of the report</font></p><h2 id="additional-material-for-describing-tables-and-figures-in-academic-language">Additional material for describing tables and figures in academic language</h2><h3 id="图形种类及概述法">1、 图形种类及概述法：</h3><p>泛指一份数据图表： a data graph(曲线图)/chart/diagram/illustration/table 饼图：pie chart 直方图或柱形图：bar chart/histogram 趋势曲线图：line chart/curve diagram 表格图：table 流程图或过程图：flow chart/sequence diagram 程序图：processing/procedures diagram</p><h3 id="常用的描述用法">2、常用的描述用法</h3><p>The table/chart diagram/graph shows （that） According to the table/chart diagram/graph As （is） shown in the table/chart diagram/graph As can be seen from the table/chart/diagram/graph/figures， figures/statistics shows （that）…… It can be seen from the figures/statistics We can see from the figures/statistics It is clear from the figures/statistics It is apparent from the figures/statistics table/chart/diagram/graph figures （that） …… table/chart/diagram/graph shows/describes/illustrates</p><h3 id="图表中的数据data具体表达法">3、图表中的数据（Data）具体表达法</h3><p>数据（Data）在某一个时间段固定不变：fixed in time 在一系列的时间段中转变：changes over time 持续变化的data在不同情况下： 增加：increase/raise/rise/go up …… 减少：decrease/grow down/drop/fall …… 波动：fluctuate/rebound/undulate/wave …… 稳定：remain stable/stabilize/level off ……</p><h3 id="二相关常用词组">二、相关常用词组</h3><h4 id="主章开头">1、主章开头</h4><p>图表类型：table（表格）、chart（图表）、diagram（图标）、graph（多指曲线图）、column chart（柱状图）、pie graph（饼图）、tree diagram（树形图） 描述：show、describe、illustrate、can be seen from、clear、apparent、reveal、represent 内容：figure、statistic、number、percentage、proportion</p><h4 id="表示数据变化的单词或者词组">2、表示数据变化的单词或者词组</h4><p>rapid/rapidly 迅速的，飞快的，险峻的 dramatic/dramatically 戏剧性的，生动的 significant/significantly 有意义的，重大的，重要的 sharp/sharply 锐利的，明显的，急剧的 steep/steeply 急剧升降的 steady/steadily 稳固的，坚定不移的 gradual/gradually 渐进的，逐渐的 slow/slowly 缓慢的，不活跃的 slight/slightly 稍微的、略微地 stable/stably 稳定的</p><h4 id="其它在描述中的常用到的词">3、其它在描述中的常用到的词</h4><p>significant changes 图中一些较大变化 noticeable trend 明显趋势 during the same period 在同一时期 grow/grew 增长 distribute 分布 unequally 不相等地 in the case of 在……的情况下 in terms of/in respect of/regarding 在……方面 in contrast 相反，大不相同 government policy 政府政策 market forces <a href="https://www.baidu.com/s?wd=市场力量&amp;tn=SE_PcZhidaonwhc_ngpagmjz&amp;rsv_dl=gh_pc_zhidao">市场力量</a> measure n.尺寸，方法，措施 v.估量，调节 forecast n.先见，预见 v.猜测</p><h3 id="三图表描述套句精选">三、图表描述套句精选</h3><p>1.The table shows the changes in the number of…over the period from…to…. 该表格描述了在……年之……年间……数量的变化。 2.The bar chart illustrates that…. 该柱状图展示了…… 3.The graph provides some interesting data regarding…. 该图为我们提供了有关……有趣数据。 4.The diagram shows （that）…. 该图向我们展示了…… 5.The pie graph depicts （that）…. 该圆形图揭示了…… 6.This is a cure graph which describes the trend of…. 这个曲线图描述了……的趋势。 7.The figures/statistics show （that）…. 数据（字）表明……</p><p>8.The tree diagram reveals how….</p><p>该树型图向我们揭示了如何……</p><p>9.The data/statistics show （that）….</p><p>该数据（字）可以这样理解……</p><p>10.The data/statistics/figures lead us to the conclusion that….</p><p>这些数据资料令我们得出结论…… 11.As is shown/demonstrated/exhibited in the diagram/graph/chart/table…. 如图所示…… 12.According to the chart/figures…. 根据这些表（数字）…… 13.As is shown in the table…. 如表格所示…… 14.As can be seen from the diagram，great changes have taken place in…. 从图中可以看出，……发生了巨大变化。 15.From the table/chart/diagram/figure，we can see clearly that……or it is clear/apparent from the chart that…. 从图表我们可以很清楚（明显）看到…… 16.This is a graph which illustrates…. 这个图表向我们展示了…… 17.This table shows the changing proportion of a &amp; b from……to…. 该表格描述了……年到……年间a与b的比例关系。 18.The graph，presented in a pie chart，shows the general trend in…. 该图以圆形图形式描述了……总的趋势。 19.This is a column chart showing…. 这是一个柱型图，描述了…… 20.As can be seen from the graph，the two curves show the fluctuation of…. 如图所示，两条曲线描述了……的波动情况。 21.Over the period from…to…, the…remained level. 在……至……期间，……基本不变。 22.In the year between…and…. 在……年到……期间…… 23.In the 3 years spanning from 1995 through 1998…. 1995年至1998三年里…… 24.From then on/from this time onwards…. 从那时起…… 25.The number of…remained steady/stable from （month/year） to （month/year）. ……月（年）至……月（年）……的数量基本不变。 26.The number sharply went up to…. 数字急剧上升至…… 27.The percentage of…stayed the same between…and…. ……至……期间……的比率维持不变。 28.The figures peaked at…in（month/year）. ……的数目在……月（年）达到顶点，为…… 29.The percentage remained steady at…. 比率维持在…… 30.The percentage of…is slightly larger/smaller than that of…. ……的比例比……的比例略高（低）。 31.There is not a great deal of difference between…and… ……与……的区别不大。 32.The graphs show a threefold increase in the number of…. 该图表表明……的数目增长了三倍。 33…decreased year by year while…increased steadily. ……逐年减少，而……逐步上升。 34.The situation reached a peak（a high point at）of …%. ……的情况（局势）到达顶（高）点，为……百分点。 35.The figures/situation bottomed out in…. 数字（情况）在……达到底部。</p><p>36.The figures reached the bottom/a low point/hit a trough.</p><p>数字（情况）达到底部（低谷）。</p><p>37.A is …times as much/many as b.</p><p>a是b的……倍。 38.A increased by…. a增长了…… 39.A increased to…. a增长到……</p><p>40.high/low/great/small/ percentage.</p><p>比率高（低） 41.There is an upward trend in the number of…. ……数字呈上升趋势。 42.Aconsiderable increase/decrease occurred from…to…. ……到……发生急剧上升。 43.From…to…the rate of decrease slow down. 从……到……，下降速率减慢。 44.from this year on，there was a gradual decline/ reduction in the…，reaching a figure of…. 从这年起，……逐渐下降至…… 45.be similar to… 与……相似 46.be the same as… 与……相同 47.There are a lot similarities/differences between…and…. ……与……之间有许多相似（不同）之处 48.A has something in common with b. a与b有共同之处。 49.The difference between a and b lies in…. a与b之间的差别在于…… 50.…（year）witnessed/saw a sharp rise//in….</p><p>……年……急剧上升。</p><h2 id="ex-15.3">ex 15.3</h2><p>1.We analyzed ____ variety of tissue samples.</p><p><font color=red>a</font></p><p>2.____colors affect our perception of reality.</p><p><font color=red>x</font></p><p>3.Those interested in donating the kidney should notify the hospital’s donation committee.</p><p><font color=red>false</font></p><p>4.The cheetah is the quickest of the land animals.</p><p><font color=red>true</font></p><h2 id="samples-for-articles">Samples for Articles</h2><ol type="1"><li><p>Becoming an expert takes a lot of ✗ experience.</p></li><li><p>The Cheetah is the quickest of the land animals.</p></li></ol><h2 id="culture-and-ethics-练习">culture and ethics 练习</h2><p>1.Ethics in academic writing exclude the following action(s):</p><p><font color=red>academic theft</font></p><p>2.Scientific writing is objective, impersonal and detached, so cultural difference cannot be detected in the academic field.</p><p><font color=red>false</font></p><h2 id="ex-15.4">ex 15.4</h2><p>1.In the paper discussed in this lecture, the word “Creator” should not be used because it ____.</p><p><font color=red>makes references to Creationism</font></p><p>2.Academic Integrity and ethics includes:</p><ul><li><font color=red>honesty</font></li><li><font color=red>no fabrication, falsification, or plagiarism</font></li><li><font color=red>honoring property rights</font></li></ul><h1 id="ch16-academic-correspondence-with-the-editors-与编辑的学术联系">Ch16 Academic Correspondence with the Editors 与编辑的学术联系</h1><h2 id="ex-16.1">ex 16.1</h2><p>1.You write a cover letter to______.</p><ul><li><font color=red>Introduce your paper to the editor</font></li><li><font color=red>Recommend reviewers to the editor</font></li><li><font color=red>Oppose reviewers to the editor</font></li></ul><p>2.What is a cover letter?</p><p><font color=red>A cover letter also called submission letter is a letter of transmittal to the editor of the journal for possible publication.</font></p><p>3.In order to choose your target journal, you need to find a journal without any peer review.</p><p><font color=red>false</font></p><h2 id="ex-16.2">ex 16.2</h2><p>1.If you want to know whether your paper is accepted or not, you need to write______.</p><p><font color=red>An inquiry letter</font></p><p>2.Peer review will provide you with comments and suggestions from ____and editors.</p><p><font color=red>reviewers</font></p><p>3.You can write a rebuttal letter to accuse the reviewers of bias or incompetence.</p><p><font color=red>false</font></p><h1 id="ch17-international-academic-conference-presentation-skills-国际学术会议宣讲技巧">Ch17 International Academic Conference Presentation Skills 国际学术会议宣讲技巧</h1><h2 id="ex-17">ex 17</h2><p>1.A presentation is the act of effective _____ communication with an audience.</p><p><font color=red>oral</font></p><p>2.Presentation skills consist of three major parts: ____,____ , and____ .</p><p><font color=red>audience analysis、delivery、managing stage fright</font></p><p>3.Which of the following should NOT be done in international academic conference presentation?</p><p><font color=red> speak towards the screen</font></p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> course </tag>
            
            <tag> 论文写作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to write an outline 笔记</title>
      <link href="/2023/11/09/How%20to%20write%20an%20outline%20%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/11/09/How%20to%20write%20an%20outline%20%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="how-to-write-an-outline-笔记">How to write an outline 笔记</h1><h2 id="outline-定义">outline 定义</h2><p>Outline是你的写作地图。写作者使用outline来以一种有逻辑的、清晰的顺序展示思想，并有目的地组织话题及支撑的细节。</p><p>它展示了每一段或每一节将包含的信息，以及它们之间的顺序。outline可以被用来辨别并减少文章中可能存在的缺点或缺少的重点。</p><h2 id="outline-的功能">outline 的功能</h2><p>撰写outline将帮助你集中于眼前的任务，并避免不必要的切题、逻辑谬误和段落的不完善。</p><p>在进行大型研究项目时，很容易混淆资源来源，或者在研究过程中忘记阅读到的内容。为了让你对阅读的内容保持清晰，阅读时可以在页边空白处或另一张纸上记录reverse outline。这个reverse outline可以让你轻松地浏览你的资源，突出显示那些可能有助于你研究的信息。</p><h2 id="outline-的类型和结构">outline 的类型和结构</h2><p><strong>Topic outline</strong> 由简短的短语组成。当您处理的问题可能会以各种不同的方式出现在您的论文中时，这种方法非常有用。</p><p><strong>Sentence outline</strong> 为完整句子。当您的论文侧重于复杂问题的细节时，这种方法非常有用。</p><p>两种outline都遵循严格的格式，使用罗马数字和阿拉伯数字以及字母表中的大写和小写字母。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/654cdb69c458853aefce1aba.png" alt="image-20231109124822463" /><figcaption aria-hidden="true">image-20231109124822463</figcaption></figure><h2 id="outlining-步骤">outlining 步骤</h2><ol type="1"><li><strong>初步工作</strong>：写下主题，然后开始头脑风暴。</li><li><strong>确定研究问题</strong>：研究问题是outline其余部分的重点。试着用一句话或短语来概括你的论文要点。它也是决定论文标题的关键。</li><li><strong>确定主要类别</strong>：你将分析哪些要点？导言（Introduction）描述了你所有的要点；你论文的其余部分可以用来阐述这些要点。</li><li><strong>创建第一个类别</strong>：您要介绍的第一点是什么？如果文章围绕一个复杂的术语展开，那么可以从定义开始。对于一篇涉及特定理论的应用和测验的论文来说，提供该理论的一般背景是一个很好的切入点。</li><li><strong>创建子类别</strong> ：完成这些步骤后，在其下方叙述支撑主要论点的材料。使用的类别数量取决于尝试涵盖的信息量。</li></ol><h2 id="tips">Tips</h2><ul><li><p>在开始outlining之前，需要一个清晰的论文陈述或明确的目的或论点，因为outline中的其他内容都将用于支撑主题。</p></li><li><p>论文陈述应该是一个完整的肯定陈述句，而不是疑问句、短语或从属分句。</p></li><li><p>避免混合类型。与 topic outline 或 sentence outline 保持一致。请勿混合使用两种类型。</p></li><li><p>使用平行。每个标题和副标题都应保持与其他标题平行的结构。最明显的是 "topic" 和 "sentence" 的 outline 格式；平行性也指词性和时态。</p></li><li><p>使信息相互关联协调。第一个主标题所提供的信息应该与第二个主标题所提供的信息同等重要。副标题也是如此。</p></li><li><p>学会划分。每个主标题应该被划分为两个或多个部分。换言之，每个主标题至少应有两个副标题。</p></li><li><p>大写问题。在写outline、主标题和副标题时，几乎总是按照正确的句子大写规则书写。</p></li></ul><h2 id="附在读写中-outline-的重要性">附：在读写中 outline 的重要性</h2><p>Outline can help the paper readers sort out the important and unimportant details, so that they can read more effectively.</p><p>When writing, the authors tend to use brainstorming, a useful tool for generating ideas in a free thinking way. Then outlining can help organize all the generated ideas, and help the authors prioritize the important information and eliminate the trivial details.</p><p>Therefore, although outline is not a part of your paper, it is an important tool to facilitate your academic career.</p><p>参考网址：https://www.xuetangx.com/learn/hfut05021002478/hfut05021002478/16907237/video/36265680</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> course </tag>
            
            <tag> 论文写作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/11/06/hello-world/"/>
      <url>/2023/11/06/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
