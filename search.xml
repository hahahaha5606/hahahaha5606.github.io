<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>leetcode——链表</title>
      <link href="/2025/04/05/leetcode%E2%80%94%E2%80%94%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
      <url>/2025/04/05/leetcode%E2%80%94%E2%80%94%E5%93%88%E5%B8%8C%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/youngyangyang04/leetcode-master/blob/master/README.md">leetcode-master/README.md at master · youngyangyang04/leetcode-master · GitHub</a></p><p><strong>C++</strong></p><ul><li><p>一些常见函数：</p><ul><li><p>```c++ memset(hashOtherStr, 0, 26 * sizeof(int)); //从地址hashOtherStr开始，将此后26个整数字节的内存值重置为0 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">-   ```c++</span><br><span class="line">    vector&lt;string&gt; result;</span><br><span class="line">    string s(1, i + &#x27;a&#x27;); // char -&gt; string</span><br><span class="line">    result.push_back(s); //在字符串型向量的末尾添加新元素 ‘s’</span><br></pre></td></tr></table></figure></p></li><li><p>```c++ //关于unordered_set用法，例子如下：</p><p>class Solution { public: vector<int> intersection(vector<int>&amp; nums1, vector<int>&amp; nums2) { unordered_set<int> result_set; // 存放结果，之所以用set是为了给结果集去重 unordered_set<int> nums_set(nums1.begin(), nums1.end()); for (int num : nums2) { // 发现nums2的元素 在nums_set里又出现过 if (nums_set.find(num) != nums_set.end()) { result_set.insert(num); } } return vector<int>(result_set.begin(), result_set.end()); //注意此处把 set转成vector 需要O（m）的时间复杂度 } }; <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">-   ```c++</span><br><span class="line">    // 关于 unsorted_map，例子如下：</span><br><span class="line">    class Solution &#123;</span><br><span class="line">    public:</span><br><span class="line">        vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) &#123;</span><br><span class="line">            std::unordered_map &lt;int,int&gt; map;</span><br><span class="line">            for(int i = 0; i &lt; nums.size(); i++) &#123;</span><br><span class="line">                // 遍历当前元素，并在map中寻找是否有匹配的key</span><br><span class="line">                auto iter = map.find(target - nums[i]); </span><br><span class="line">                if(iter != map.end()) &#123;</span><br><span class="line">                    return &#123;iter-&gt;second, i&#125;; // iter-&gt;second 表示访问值</span><br><span class="line">                &#125;</span><br><span class="line">                // 如果没找到匹配对，就把访问过的元素和下标加入到map中</span><br><span class="line">                map.insert(pair&lt;int, int&gt;(nums[i], i)); </span><br><span class="line">            &#125;</span><br><span class="line">            return &#123;&#125;;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    </span><br><span class="line">    // 在 C++ 的 std::unordered_map 中，如果访问一个尚未存在的 key，会自动插入这个 key，并且使用默认构造函数来初始化它的 value。 对于 int 类型的 value，默认构造函数会将其初始化为 0。</span><br></pre></td></tr></table></figure></p></li></ul></li></ul><p><strong>Python3</strong></p><h2 id="链表定义">链表定义</h2><p>取值运算符：* 取地址运算符：&amp;</p><p>更多细节参考：<a href="https://blog.csdn.net/weixin_44884357/article/details/105480596">C指针 取地址符&amp; 取值符*_指针取值-CSDN博客</a></p><p><strong>C++</strong></p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//以下为C/C++的自定义链表节点方式（推荐）</span></span><br><span class="line"><span class="comment">//单链表</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> &#123;</span></span><br><span class="line">  <span class="type">int</span> val; <span class="comment">//节点存储的值</span></span><br><span class="line">  ListNode *next; <span class="comment">//指向下一个节点的指针</span></span><br><span class="line">  ListNode(<span class="type">int</span> x) : val(x), next(<span class="literal">NULL</span>) &#123;&#125; <span class="comment">//节点的构造函数，这里用NULL或nullptr都可</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化头结点</span></span><br><span class="line">ListNode* head = new ListNode(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><p>最后一行代码是 <strong>链表节点 (ListNode)</strong> 的构造函数 (constructor)。它用 C++ 编写，定义了一个链表节点如何被创建并初始化。 让我们分解一下：</p><p><strong>ListNode(int x) : val(x), next(NULL) {}</strong></p><ul><li><p><strong>ListNode(int x)</strong>: 这是构造函数的声明。它表示这个函数名为 <code>ListNode</code>，接受一个整数类型的参数 <code>x</code>。 由于函数名和类名相同，所以它是构造函数。构造函数的作用是初始化类的成员变量。</p></li><li><p><strong>: val(x), next(NULL)</strong>: 这是初始化列表 (initializer list)。它在构造函数体执行之前，直接初始化类的成员变量。这样做通常更高效。</p><ul><li><p><strong>val(x)</strong>: 将参数 <code>x</code> 的值赋给链表节点的数据域 <code>val</code>。 <code>val</code> 变量通常存储节点本身的数据，比如一个整数。</p></li><li><p><strong>next(NULL)</strong>: 将链表节点的指针域 <code>next</code> 初始化为 <code>NULL</code>。 <code>next</code> 指针指向链表中的下一个节点。 <code>NULL</code> (或者在一些更现代的 C++ 中会使用 <code>nullptr</code>) 表示该节点是链表的最后一个节点，或者尚未连接到任何其他节点。</p></li></ul></li><li><p><strong>{}</strong>: 这是构造函数的函数体。 在这个例子中，函数体是空的，因为所有的初始化都已经在初始化列表中完成了。</p></li></ul><p>或者，不自定义构造函数，使用默认构造函数(该构造函数不会初始化任何成员变量，不推荐)：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ListNode* head = new ListNode();</span><br><span class="line">head-&gt;val = 5;</span><br></pre></td></tr></table></figure><p><strong>Python3</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ListNode</span>:</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, val, <span class="built_in">next</span>=<span class="literal">None</span></span>):</span><br><span class="line">    self.val = val</span><br><span class="line">    self.<span class="built_in">next</span> = <span class="built_in">next</span></span><br></pre></td></tr></table></figure><h2 id="链表-vs-数组">链表 vs 数组</h2><p><strong>时间复杂度分析：</strong></p><table><thead><tr class="header"><th></th><th>插入/删除</th><th>查询</th></tr></thead><tbody><tr class="odd"><td>数组</td><td>O(n)</td><td>O(1)</td></tr><tr class="even"><td>链表</td><td>O(1)</td><td>O(n)</td></tr></tbody></table><ul><li><strong>数组</strong>适用于数据量固定，频繁查询，较少增删；<strong>链表</strong>适用于数据量不固定，频繁增删，较少查询。</li><li><strong>数组</strong>在定义时，长度就是固定的，如果改动需要新定义数组；<strong>链表</strong>长度不固定，可以动态增删；</li></ul><h2 id="移除链表元素">0203.移除链表元素</h2><p><strong>C++</strong></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">removeElements</span><span class="params">(ListNode* head, <span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        ListNode* dummyHead = <span class="keyword">new</span> <span class="built_in">ListNode</span>(<span class="number">0</span>); <span class="comment">// 设置一个虚拟头结点</span></span><br><span class="line">        dummyHead-&gt;next = head; <span class="comment">// 将虚拟头结点指向head，这样方便后面做删除操作</span></span><br><span class="line">        ListNode* cur = dummyHead;</span><br><span class="line">        <span class="keyword">while</span> (cur-&gt;next != <span class="literal">NULL</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span>(cur-&gt;next-&gt;val == val) &#123;</span><br><span class="line">                ListNode* tmp = cur-&gt;next;</span><br><span class="line">                cur-&gt;next = cur-&gt;next-&gt;next;</span><br><span class="line">                <span class="keyword">delete</span> tmp;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                cur = cur-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        head = dummyHead-&gt;next;</span><br><span class="line">        <span class="keyword">delete</span> dummyHead;</span><br><span class="line">        <span class="keyword">return</span> head;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>Python3</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">（版本一）虚拟头节点法</span><br><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, next=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.next = next</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeElements</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode], val: <span class="built_in">int</span></span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        <span class="comment"># 创建虚拟头部节点以简化删除过程</span></span><br><span class="line">        dummy_head = ListNode(<span class="built_in">next</span> = head)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 遍历列表并删除值为val的节点</span></span><br><span class="line">        current = dummy_head</span><br><span class="line">        <span class="keyword">while</span> current.<span class="built_in">next</span>:</span><br><span class="line">            <span class="keyword">if</span> current.<span class="built_in">next</span>.val == val:</span><br><span class="line">                current.<span class="built_in">next</span> = current.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                current = current.<span class="built_in">next</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dummy_head.<span class="built_in">next</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>虚拟头结点法分析：</strong></p><table><thead><tr class="header"><th>时间复杂度</th><th>空间复杂度</th></tr></thead><tbody><tr class="odd"><td>O（n）</td><td>O(1)</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代码箱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RISC-V 入门</title>
      <link href="/2025/03/31/RISC-V/"/>
      <url>/2025/03/31/RISC-V/</url>
      
        <content type="html"><![CDATA[<p>编译与链接</p><p>练习3.1</p><p>gcc -c hello.c -o hello.o //生成 hello.o</p><p>readelf -h hello.o // 看 ELF header</p><p>readelf -S(W) hello.o // 看 链接视图 (W: Wide)</p><p>gcc -g -c hello.c //调试信息</p><p>objdump -S hello.o //反汇编</p><p>模拟器：QEMU</p><p>Visual Studio Code</p><p>第5章前搭好环境</p><p>gnu汇编器（与 ARM、Intel 汇编有所区别）</p><p>第4章</p><p>练习4.2</p><p>riscv64-unkown-elf-gcc -march-rv32ima -mabi=ilp32 hello.c // 交叉编译环境</p><p>file a.cout // 是 riscv ，不是 x86 的</p><p>qemu-riscv32 ./a.out //在模拟器上运行</p><p>// 以上为 user mode (application 层)</p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编码 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>leetcode——链表</title>
      <link href="/2025/03/04/leetcode%E2%80%94%E2%80%94%E9%93%BE%E8%A1%A8/"/>
      <url>/2025/03/04/leetcode%E2%80%94%E2%80%94%E9%93%BE%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/youngyangyang04/leetcode-master/blob/master/README.md">leetcode-master/README.md at master · youngyangyang04/leetcode-master · GitHub</a></p><p><strong>C++</strong></p><ul><li>链表的“删除”节点，并不是真正意义上的删除，只是修改了指针的指向。因此在C++中，最好再手动释放删除的节点内存；相比下，JAVA、Python 有自己的内存回收机制，无需手动释放。</li></ul><p><strong>Python3</strong></p><h2 id="链表定义">链表定义</h2><p>取值运算符：* 取地址运算符：&amp;</p><p>更多细节参考：<a href="https://blog.csdn.net/weixin_44884357/article/details/105480596">C指针 取地址符&amp; 取值符*_指针取值-CSDN博客</a></p><p><strong>C++</strong></p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//以下为C/C++的自定义链表节点方式（推荐）</span></span><br><span class="line"><span class="comment">//单链表</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> &#123;</span></span><br><span class="line">  <span class="type">int</span> val; <span class="comment">//节点存储的值</span></span><br><span class="line">  ListNode *next; <span class="comment">//指向下一个节点的指针</span></span><br><span class="line">  ListNode(<span class="type">int</span> x) : val(x), next(<span class="literal">NULL</span>) &#123;&#125; <span class="comment">//节点的构造函数，这里用NULL或nullptr都可</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化头结点</span></span><br><span class="line">ListNode* head = new ListNode(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><p>最后一行代码是 <strong>链表节点 (ListNode)</strong> 的构造函数 (constructor)。它用 C++ 编写，定义了一个链表节点如何被创建并初始化。 让我们分解一下：</p><p><strong>ListNode(int x) : val(x), next(NULL) {}</strong></p><ul><li><p><strong>ListNode(int x)</strong>: 这是构造函数的声明。它表示这个函数名为 <code>ListNode</code>，接受一个整数类型的参数 <code>x</code>。 由于函数名和类名相同，所以它是构造函数。构造函数的作用是初始化类的成员变量。</p></li><li><p><strong>: val(x), next(NULL)</strong>: 这是初始化列表 (initializer list)。它在构造函数体执行之前，直接初始化类的成员变量。这样做通常更高效。</p><ul><li><p><strong>val(x)</strong>: 将参数 <code>x</code> 的值赋给链表节点的数据域 <code>val</code>。 <code>val</code> 变量通常存储节点本身的数据，比如一个整数。</p></li><li><p><strong>next(NULL)</strong>: 将链表节点的指针域 <code>next</code> 初始化为 <code>NULL</code>。 <code>next</code> 指针指向链表中的下一个节点。 <code>NULL</code> (或者在一些更现代的 C++ 中会使用 <code>nullptr</code>) 表示该节点是链表的最后一个节点，或者尚未连接到任何其他节点。</p></li></ul></li><li><p><strong>{}</strong>: 这是构造函数的函数体。 在这个例子中，函数体是空的，因为所有的初始化都已经在初始化列表中完成了。</p></li></ul><p>或者，不自定义构造函数，使用默认构造函数(该构造函数不会初始化任何成员变量，不推荐)：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ListNode* head = new ListNode();</span><br><span class="line">head-&gt;val = 5;</span><br></pre></td></tr></table></figure><p><strong>Python3</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ListNode</span>:</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, val, <span class="built_in">next</span>=<span class="literal">None</span></span>):</span><br><span class="line">    self.val = val</span><br><span class="line">    self.<span class="built_in">next</span> = <span class="built_in">next</span></span><br></pre></td></tr></table></figure><h2 id="链表-vs-数组">链表 vs 数组</h2><p><strong>时间复杂度分析：</strong></p><table><thead><tr class="header"><th></th><th>插入/删除</th><th>查询</th></tr></thead><tbody><tr class="odd"><td>数组</td><td>O(n)</td><td>O(1)</td></tr><tr class="even"><td>链表</td><td>O(1)</td><td>O(n)</td></tr></tbody></table><ul><li><strong>数组</strong>适用于数据量固定，频繁查询，较少增删；<strong>链表</strong>适用于数据量不固定，频繁增删，较少查询。</li><li><strong>数组</strong>在定义时，长度就是固定的，如果改动需要新定义数组；<strong>链表</strong>长度不固定，可以动态增删；</li></ul><h2 id="移除链表元素">0203.移除链表元素</h2><p><strong>C++</strong></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">removeElements</span><span class="params">(ListNode* head, <span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        ListNode* dummyHead = <span class="keyword">new</span> <span class="built_in">ListNode</span>(<span class="number">0</span>); <span class="comment">// 设置一个虚拟头结点</span></span><br><span class="line">        dummyHead-&gt;next = head; <span class="comment">// 将虚拟头结点指向head，这样方便后面做删除操作</span></span><br><span class="line">        ListNode* cur = dummyHead;</span><br><span class="line">        <span class="keyword">while</span> (cur-&gt;next != <span class="literal">NULL</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span>(cur-&gt;next-&gt;val == val) &#123;</span><br><span class="line">                ListNode* tmp = cur-&gt;next;</span><br><span class="line">                cur-&gt;next = cur-&gt;next-&gt;next;</span><br><span class="line">                <span class="keyword">delete</span> tmp;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                cur = cur-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        head = dummyHead-&gt;next;</span><br><span class="line">        <span class="keyword">delete</span> dummyHead;</span><br><span class="line">        <span class="keyword">return</span> head;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>Python3</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">（版本一）虚拟头节点法</span><br><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, next=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.next = next</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeElements</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode], val: <span class="built_in">int</span></span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        <span class="comment"># 创建虚拟头部节点以简化删除过程</span></span><br><span class="line">        dummy_head = ListNode(<span class="built_in">next</span> = head)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 遍历列表并删除值为val的节点</span></span><br><span class="line">        current = dummy_head</span><br><span class="line">        <span class="keyword">while</span> current.<span class="built_in">next</span>:</span><br><span class="line">            <span class="keyword">if</span> current.<span class="built_in">next</span>.val == val:</span><br><span class="line">                current.<span class="built_in">next</span> = current.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                current = current.<span class="built_in">next</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dummy_head.<span class="built_in">next</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>虚拟头结点法分析：</strong></p><table><thead><tr class="header"><th>时间复杂度</th><th>空间复杂度</th></tr></thead><tbody><tr class="odd"><td>O（n）</td><td>O(1)</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代码箱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>leetcode——数组</title>
      <link href="/2025/02/10/leetcode%E2%80%94%E2%80%94%E6%95%B0%E7%BB%84/"/>
      <url>/2025/02/10/leetcode%E2%80%94%E2%80%94%E6%95%B0%E7%BB%84/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/youngyangyang04/leetcode-master/blob/master/README.md">leetcode-master/README.md at master · youngyangyang04/leetcode-master · GitHub</a></p><p><strong>C++</strong></p><ul><li>C++ 代码 面对大量数据 读取 输出操作，最好用scanf 和 printf（相比 cin 和 cout），耗时会小很多；</li><li>如果用到最大整数 <code>res = INI_MAX</code>，要引入头文件 <code>#include &lt;climits&gt;</code>;</li><li>二维数组的写法：<code>vector&lt;vector&lt;int&gt;&gt; vec(n, vector&lt;int&gt;(m,0));</code>。这段代码意为创建一个名为 <code>vec</code> 的二维向量（矩阵），其大小为 <code>n</code> 行 <code>m</code> 列，并且所有元素初始化为 0；</li><li><code>cout</code> 和 <code>printf</code> 不能输出向量，只能输出单个值；</li></ul><p><strong>Python 3</strong></p><ul><li><p>python 3中一次性读取大量数据一般使用以下结构:</p><p>import sys</p><p>input = sys.stdin.read</p><p>data = input().split()</p><p>first_num = int(data[0])</p></li><li><p>定义无穷大值的语句为：<code>res = float('inf')</code></p></li></ul><h2 id="二分查找">0704.二分查找</h2><p>给定一个 n 个元素有序的（升序）整型数组 nums 和一个目标值 target ，写一个函数搜索 nums 中的 target，如果目标值存在返回下标，否则返回 -1。（<a href="https://leetcode.cn/problems/binary-search/submissions/598191630/">704. 二分查找 - 力扣（LeetCode）</a>）</p><p><strong>C++题解</strong></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 版本一</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">search</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = nums.<span class="built_in">size</span>() - <span class="number">1</span>; <span class="comment">// 定义target在左闭右闭的区间里，[left, right]</span></span><br><span class="line">        <span class="keyword">while</span> (left &lt;= right) &#123; <span class="comment">// 当left==right，区间[left, right]依然有效，所以用 &lt;=</span></span><br><span class="line">            <span class="type">int</span> middle = left + ((right - left) / <span class="number">2</span>);<span class="comment">// 防止溢出 等同于(left + right)/2</span></span><br><span class="line">            <span class="keyword">if</span> (nums[middle] &gt; target) &#123;</span><br><span class="line">                right = middle - <span class="number">1</span>; <span class="comment">// target 在左区间，所以[left, middle - 1]</span></span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nums[middle] &lt; target) &#123;</span><br><span class="line">                left = middle + <span class="number">1</span>; <span class="comment">// target 在右区间，所以[middle + 1, right]</span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123; <span class="comment">// nums[middle] == target</span></span><br><span class="line">                <span class="keyword">return</span> middle; <span class="comment">// 数组中找到目标值，直接返回下标</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 未找到目标值</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>Python3题解</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span>  <span class="comment"># 定义target在左闭右闭的区间里，[left, right]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">            middle = left + (right - left) // <span class="number">2</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> nums[middle] &gt; target:</span><br><span class="line">                right = middle - <span class="number">1</span>  <span class="comment"># target在左区间，所以[left, middle - 1]</span></span><br><span class="line">            <span class="keyword">elif</span> nums[middle] &lt; target:</span><br><span class="line">                left = middle + <span class="number">1</span>  <span class="comment"># target在右区间，所以[middle + 1, right]</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> middle  <span class="comment"># 数组中找到目标值，直接返回下标</span></span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>  <span class="comment"># 未找到目标值</span></span><br></pre></td></tr></table></figure><p><strong>二分法分析：</strong></p><table><thead><tr class="header"><th>时间复杂度</th><th>空间复杂度</th></tr></thead><tbody><tr class="odd"><td>O（log n）</td><td>O(1)</td></tr></tbody></table><h2 id="移除元素">0027.移除元素</h2><p>给你一个数组 nums 和一个值 val，你需要 原地 移除所有数值等于 val 的元素，并返回移除后数组的新长度。（<a href="https://leetcode.cn/problems/remove-element/description/">27. 移除元素 - 力扣（LeetCode）</a>）</p><p><strong>C++题解</strong></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 时间复杂度：O(n)</span></span><br><span class="line"><span class="comment">// 空间复杂度：O(1)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">removeElement</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> slowIndex = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> fastIndex = <span class="number">0</span>; fastIndex &lt; nums.<span class="built_in">size</span>(); fastIndex++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (val != nums[fastIndex]) &#123;</span><br><span class="line">                nums[slowIndex++] = nums[fastIndex];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> slowIndex;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><strong>Python3题解</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeElement</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], val: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        fastIndex = slowIndex = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> fastIndex <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> nums[fastIndex] != val:</span><br><span class="line">                nums[slowIndex] = nums[fastIndex]</span><br><span class="line">                slowIndex += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> slowIndex</span><br></pre></td></tr></table></figure><p><strong>双指针法（快慢指针法）分析：</strong></p><table><thead><tr class="header"><th>时间复杂度</th><th>空间复杂度</th></tr></thead><tbody><tr class="odd"><td>O（n）</td><td>O(1)</td></tr></tbody></table><h2 id="有序数组的平方">977.有序数组的平方</h2><p>给你一个按 非递减顺序 排序的整数数组 nums，返回 每个数字的平方 组成的新数组，要求也按 非递减顺序 排序。（<a href="https://leetcode.cn/problems/squares-of-a-sorted-array/description/">977. 有序数组的平方 - 力扣（LeetCode）</a>）</p><p><strong>C++题解</strong></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">sortedSquares</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; A)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> k = A.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">result</span><span class="params">(A.size(), <span class="number">0</span>)</span></span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>, j = A.<span class="built_in">size</span>() - <span class="number">1</span>; i &lt;= j;) &#123; <span class="comment">// 注意这里要i &lt;= j，因为最后要处理两个元素</span></span><br><span class="line">            <span class="keyword">if</span> (A[i] * A[i] &lt; A[j] * A[j])  &#123;</span><br><span class="line">                result[k--] = A[j] * A[j];</span><br><span class="line">                j--;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                result[k--] = A[i] * A[i];</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><strong>Python3题解</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">（版本一）双指针法</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sortedSquares</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        l, r, i = <span class="number">0</span>, <span class="built_in">len</span>(nums)-<span class="number">1</span>, <span class="built_in">len</span>(nums)-<span class="number">1</span></span><br><span class="line">        res = [<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)] * <span class="built_in">len</span>(nums) <span class="comment"># 需要提前定义列表，存放结果</span></span><br><span class="line">        <span class="keyword">while</span> l &lt;= r:</span><br><span class="line">            <span class="keyword">if</span> nums[l] ** <span class="number">2</span> &lt; nums[r] ** <span class="number">2</span>: <span class="comment"># 左右边界进行对比，找出最大值</span></span><br><span class="line">                res[i] = nums[r] ** <span class="number">2</span></span><br><span class="line">                r -= <span class="number">1</span> <span class="comment"># 右指针往左移动</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                res[i] = nums[l] ** <span class="number">2</span></span><br><span class="line">                l += <span class="number">1</span> <span class="comment"># 左指针往右移动</span></span><br><span class="line">            i -= <span class="number">1</span> <span class="comment"># 存放结果的指针需要往前平移一位</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p><strong>双指针法分析：</strong></p><table><thead><tr class="header"><th>时间复杂度</th><th>空间复杂度</th></tr></thead><tbody><tr class="odd"><td>O（n）</td><td>O(1)</td></tr></tbody></table><h2 id="长度最小的子数组">209.长度最小的子数组</h2><p>给定一个含有 n 个正整数的数组和一个正整数 s ，找出该数组中满足其和 ≥ s 的长度最小的 连续 子数组，并返回其长度。如果不存在符合条件的子数组，返回 0。(<a href="https://github.com/youngyangyang04/leetcode-master/blob/master/problems/0209.长度最小的子数组.md">leetcode-master/problems/0209.长度最小的子数组.md at master · youngyangyang04/leetcode-master · GitHub</a>)</p><p><strong>C++题解</strong></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">minSubArrayLen</span><span class="params">(<span class="type">int</span> s, vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> result = INT32_MAX;</span><br><span class="line">        <span class="type">int</span> sum = <span class="number">0</span>; <span class="comment">// 滑动窗口数值之和</span></span><br><span class="line">        <span class="type">int</span> i = <span class="number">0</span>; <span class="comment">// 滑动窗口起始位置</span></span><br><span class="line">        <span class="type">int</span> subLength = <span class="number">0</span>; <span class="comment">// 滑动窗口的长度</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; nums.<span class="built_in">size</span>(); j++) &#123;</span><br><span class="line">            sum += nums[j];</span><br><span class="line">            <span class="comment">// 注意这里使用while，每次更新 i（起始位置），并不断比较子序列是否符合条件</span></span><br><span class="line">            <span class="keyword">while</span> (sum &gt;= s) &#123;</span><br><span class="line">                subLength = (j - i + <span class="number">1</span>); <span class="comment">// 取子序列的长度</span></span><br><span class="line">                result = result &lt; subLength ? result : subLength;</span><br><span class="line">                sum -= nums[i++]; <span class="comment">// 这里体现出滑动窗口的精髓之处，不断变更i（子序列的起始位置）</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 如果result没有被赋值的话，就返回0，说明没有符合条件的子序列</span></span><br><span class="line">        <span class="keyword">return</span> result == INT32_MAX ? <span class="number">0</span> : result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><strong>Python3题解</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">（版本一）滑动窗口法</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minSubArrayLen</span>(<span class="params">self, s: <span class="built_in">int</span>, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        l = <span class="built_in">len</span>(nums)</span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = <span class="number">0</span></span><br><span class="line">        min_len = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">        cur_sum = <span class="number">0</span> <span class="comment">#当前的累加值</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> right &lt; l:</span><br><span class="line">            cur_sum += nums[right]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">while</span> cur_sum &gt;= s: <span class="comment"># 当前累加值大于目标值</span></span><br><span class="line">                min_len = <span class="built_in">min</span>(min_len, right - left + <span class="number">1</span>)</span><br><span class="line">                cur_sum -= nums[left]</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> min_len <span class="keyword">if</span> min_len != <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>) <span class="keyword">else</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><p><strong>滑动窗口法分析：</strong></p><table><thead><tr class="header"><th>时间复杂度</th><th>空间复杂度</th></tr></thead><tbody><tr class="odd"><td>O（n）</td><td>O(1)</td></tr></tbody></table><h2 id="区间和">58.区间和</h2><p><a href="https://kamacoder.com/problempage.php?pid=1070">58. 区间和（第九期模拟笔试）</a></p><p>给定一个整数数组 Array，请计算该数组在每个指定区间内元素的总和。</p><p>输入描述</p><p>第一行输入为整数数组 Array 的长度 n，接下来 n 行，每行一个整数，表示数组的元素。随后的输入为需要计算总和的区间，直至文件结束。</p><p>输出描述</p><p>输出每个指定区间内元素的总和。</p><p><strong>C++题解</strong></p><p><font color=#ef042a>C++ 代码 面对大量数据 读取 输出操作，最好用scanf 和 printf，耗时会小很多：</font></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123; <span class="comment">//前缀和解法</span></span><br><span class="line">    <span class="type">int</span> n, a, b;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">vec</span><span class="params">(n)</span></span>;</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">p</span><span class="params">(n)</span></span>;</span><br><span class="line">    <span class="type">int</span> presum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;vec[i]);</span><br><span class="line">        presum += vec[i];</span><br><span class="line">        p[i] = presum;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (~<span class="built_in">scanf</span>(<span class="string">&quot;%d%d&quot;</span>, &amp;a, &amp;b)) &#123;</span><br><span class="line">        <span class="type">int</span> sum;</span><br><span class="line">        <span class="keyword">if</span> (a == <span class="number">0</span>) sum = p[b];</span><br><span class="line">        <span class="keyword">else</span> sum = p[b] - p[a - <span class="number">1</span>];</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, sum);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Python3题解</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="built_in">input</span> = sys.stdin.read</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    data = <span class="built_in">input</span>().split()</span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    n = <span class="built_in">int</span>(data[index])</span><br><span class="line">    index += <span class="number">1</span></span><br><span class="line">    vec = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        vec.append(<span class="built_in">int</span>(data[index + i]))</span><br><span class="line">    index += n</span><br><span class="line"></span><br><span class="line">    p = [<span class="number">0</span>] * n</span><br><span class="line">    presum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        presum += vec[i]</span><br><span class="line">        p[i] = presum</span><br><span class="line"></span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">while</span> index &lt; <span class="built_in">len</span>(data):</span><br><span class="line">        a = <span class="built_in">int</span>(data[index])</span><br><span class="line">        b = <span class="built_in">int</span>(data[index + <span class="number">1</span>])</span><br><span class="line">        index += <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> a == <span class="number">0</span>:</span><br><span class="line">            sum_value = p[b]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sum_value = p[b] - p[a - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        results.append(sum_value)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">        <span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h2 id="开发商购买土地">44.开发商购买土地</h2><p><a href="https://kamacoder.com/problempage.php?pid=1044">44. 开发商购买土地（第五期模拟笔试）</a></p><p>在一个城市区域内，被划分成了n * m个连续的区块，每个区块都拥有不同的权值，代表着其土地价值。目前，有两家开发公司，A 公司和 B 公司，希望购买这个城市区域的土地。</p><p>现在，需要将这个城市区域的所有区块分配给 A 公司和 B 公司。</p><p>然而，由于城市规划的限制，只允许将区域按横向或纵向划分成两个子区域，而且每个子区域都必须包含一个或多个区块。</p><p>为了确保公平竞争，你需要找到一种分配方式，使得 A 公司和 B 公司各自的子区域内的土地总价值之差最小。</p><p>注意：区块不可再分。</p><p>【输入描述】</p><p>第一行输入两个正整数，代表 n 和 m。</p><p>接下来的 n 行，每行输出 m 个正整数。</p><p>输出描述</p><p>请输出一个整数，代表两个子区域内土地总价值之间的最小差距。</p><p>【输入示例】</p><p>3 3</p><p>1 2 3</p><p>2 1 3</p><p>1 2 3</p><p>【输出示例】</p><p>0</p><p>【提示信息】</p><p>如果将区域按照如下方式划分：</p><p>1 2 | 3</p><p>2 1 | 3</p><p>1 2 | 3</p><p>两个子区域内土地总价值之间的最小差距可以达到 0。</p><p>【数据范围】：</p><ul><li>1 &lt;= n, m &lt;= 100；</li><li>n 和 m 不同时为 1。</li></ul><p><strong>C++题解</strong></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;climits&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span> <span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n, m;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m;</span><br><span class="line">    <span class="type">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">vec</span>(n, <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(m, <span class="number">0</span>)) ;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; m; j++) &#123;</span><br><span class="line">            cin &gt;&gt; vec[i][j];</span><br><span class="line">            sum += vec[i][j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 统计横向</span></span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">horizontal</span><span class="params">(n, <span class="number">0</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span> ; j &lt; m; j++) &#123;</span><br><span class="line">            horizontal[i] += vec[i][j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 统计纵向</span></span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">vertical</span><span class="params">(m , <span class="number">0</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; m; j++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span> ; i &lt; n; i++) &#123;</span><br><span class="line">            vertical[j] += vec[i][j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> result = INT_MAX;</span><br><span class="line">    <span class="type">int</span> horizontalCut = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span> ; i &lt; n; i++) &#123;</span><br><span class="line">        horizontalCut += horizontal[i];</span><br><span class="line">        result = <span class="built_in">min</span>(result, <span class="built_in">abs</span>(sum - horizontalCut - horizontalCut));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> verticalCut = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; m; j++) &#123;</span><br><span class="line">        verticalCut += vertical[j];</span><br><span class="line">        result = <span class="built_in">min</span>(result, <span class="built_in">abs</span>(sum - verticalCut - verticalCut));</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; result &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Python3题解</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="keyword">import</span> sys</span><br><span class="line">    <span class="built_in">input</span> = sys.stdin.read</span><br><span class="line">    data = <span class="built_in">input</span>().split()</span><br><span class="line"></span><br><span class="line">    idx = <span class="number">0</span></span><br><span class="line">    n = <span class="built_in">int</span>(data[idx])</span><br><span class="line">    idx += <span class="number">1</span></span><br><span class="line">    m = <span class="built_in">int</span>(data[idx])</span><br><span class="line">    idx += <span class="number">1</span></span><br><span class="line">    <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">    vec = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        row = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            num = <span class="built_in">int</span>(data[idx])</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line">            row.append(num)</span><br><span class="line">            <span class="built_in">sum</span> += num</span><br><span class="line">        vec.append(row)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 统计横向</span></span><br><span class="line">    horizontal = [<span class="number">0</span>] * n</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            horizontal[i] += vec[i][j]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 统计纵向</span></span><br><span class="line">    vertical = [<span class="number">0</span>] * m</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            vertical[j] += vec[i][j]</span><br><span class="line"></span><br><span class="line">    result = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">    horizontalCut = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        horizontalCut += horizontal[i]</span><br><span class="line">        result = <span class="built_in">min</span>(result, <span class="built_in">abs</span>(<span class="built_in">sum</span> - <span class="number">2</span> * horizontalCut))</span><br><span class="line"></span><br><span class="line">    verticalCut = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        verticalCut += vertical[j]</span><br><span class="line">        result = <span class="built_in">min</span>(result, <span class="built_in">abs</span>(<span class="built_in">sum</span> - <span class="number">2</span> * verticalCut))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p><strong>前缀和法分析：</strong> 时间复杂度 <span class="math inline">\(O(mn)\)</span> .</p><h2 id="螺旋矩阵">59.螺旋矩阵||</h2><p>给定一个正整数 n，生成一个包含 1 到 n^2 所有元素，且元素按顺时针顺序螺旋排列的正方形矩阵。</p><p>示例:</p><p>输入: 3 输出: [ [ 1, 2, 3 ], [ 8, 9, 4 ], [ 7, 6, 5 ] ]</p><p>提示：坚持循环不变量原则（即坚持左闭右开，或坚持左开右闭）</p><p><strong>C++题解</strong></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">generateMatrix</span>(<span class="type">int</span> n) &#123;</span><br><span class="line"><span class="type">int</span> count = <span class="number">1</span>;</span><br><span class="line">vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">outMatrix</span>(n, <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n, <span class="number">0</span>));</span><br><span class="line"><span class="type">int</span> loop = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n / <span class="number">2</span>; i++)&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt;= n - (i + <span class="number">1</span>) * <span class="number">2</span>; j++) &#123;</span><br><span class="line">outMatrix[i][j+loop] = count;</span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt;= n - (i + <span class="number">1</span>) * <span class="number">2</span>; j++) &#123;</span><br><span class="line">outMatrix[j + loop][n - i - <span class="number">1</span>] = count;</span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt;= n - (i + <span class="number">1</span>) * <span class="number">2</span>; j++) &#123;</span><br><span class="line">outMatrix[n - i - <span class="number">1</span>][n - j - loop - <span class="number">1</span>] = count;</span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt;= n - (i + <span class="number">1</span>) * <span class="number">2</span>; j++) &#123;</span><br><span class="line">outMatrix[n - j - loop <span class="number">-1</span>][i] = count;</span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line">loop++;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (n % <span class="number">2</span> != <span class="number">0</span>)&#123;</span><br><span class="line">            outMatrix[n / <span class="number">2</span>][n / <span class="number">2</span>] = count;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> outMatrix;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><strong>Python3题解</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generateMatrix</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        nums = [[<span class="number">0</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        </span><br><span class="line">        count = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        p = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n // <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">2</span> * i - <span class="number">1</span>):</span><br><span class="line">                j += p</span><br><span class="line">                nums[i][j] = count</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">                    </span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">2</span> * i - <span class="number">1</span>):</span><br><span class="line">                j += p</span><br><span class="line">                nums[j][n - i - <span class="number">1</span>] = count</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">                    </span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">2</span> * i - <span class="number">1</span>):</span><br><span class="line">                j += p</span><br><span class="line">                nums[n - i - <span class="number">1</span>][n - j - <span class="number">1</span>] = count</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">2</span> * i - <span class="number">1</span>):</span><br><span class="line">                j += p</span><br><span class="line">                nums[n - j - <span class="number">1</span>][i] = count</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            p += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> n % <span class="number">2</span> != <span class="number">0</span>: </span><br><span class="line">            nums[n // <span class="number">2</span>][n // <span class="number">2</span>] = count</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure><p><strong>性能分析：</strong></p><table><thead><tr class="header"><th>时间复杂度</th><th>空间复杂度</th></tr></thead><tbody><tr class="odd"><td>O（n^2）</td><td>O(1)</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代码箱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>代码规范</title>
      <link href="/2025/01/27/work_%E4%BB%A3%E7%A0%81%E8%A7%84%E8%8C%83/"/>
      <url>/2025/01/27/work_%E4%BB%A3%E7%A0%81%E8%A7%84%E8%8C%83/</url>
      
        <content type="html"><![CDATA[<p>不同语言的项目代码规范具体参考：<a href="https://zh-google-styleguide.readthedocs.io/en/latest/">Google 开源项目风格指南——中文版 — Google 开源项目风格指南</a></p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代码箱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2025/01/14/%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A6%82%E8%BF%B0/"/>
      <url>/2025/01/14/%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="dtu-格式">dtu 格式</h1><p>dtu 标定 1-64</p><p>每个文件夹中含有64个 <code>pos_xxx.txt</code> ，每个txt文件中是一个3×4的数值矩阵，如<code>pos_001.txt</code>：</p><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">2607.429996 -3.844898 1498.178098 -533936.661373</span><br><span class="line">-192.076910 2862.552532 681.798177 23434.686572</span><br><span class="line">-0.241605 -0.030951 0.969881 22.540121</span><br></pre></td></tr></table></figure><p>dtu 中每个 scan 含有 49 张图，每张图有多个格式分辨率（一般使用形式为“rect_xxx_3_r5000”的图片）。</p><h1 id="合成数据集blender格式">合成数据集/blender格式</h1><p>一个表示相机参数的 <code>transform.json</code> 文件，里面有64个以下格式的相机参数块组成:</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="attr">&quot;cam_001&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;ext&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">          <span class="punctuation">[</span></span><br><span class="line">              <span class="number">0.43296581506729126</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">0.8215289115905762</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">-0.37098634243011475</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">-0.2599387764930725</span></span><br><span class="line">          <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">          <span class="punctuation">[</span></span><br><span class="line">              <span class="number">-0.8851349949836731</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">0.3096177279949188</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">-0.347380131483078</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">-0.22276368737220764</span></span><br><span class="line">          <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">          <span class="punctuation">[</span></span><br><span class="line">              <span class="number">-0.17051884531974792</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">0.4787766933441162</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">0.8612178564071655</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">0.6124556064605713</span></span><br><span class="line">          <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">          <span class="punctuation">[</span></span><br><span class="line">              <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">1.0</span></span><br><span class="line">          <span class="punctuation">]</span></span><br><span class="line">      <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;intr&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">          <span class="punctuation">[</span></span><br><span class="line">              <span class="number">2222.222222222222</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">799.5</span></span><br><span class="line">          <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">          <span class="punctuation">[</span></span><br><span class="line">              <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">2222.222222222222</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">599.5</span></span><br><span class="line">          <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">          <span class="punctuation">[</span></span><br><span class="line">              <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">1.0</span></span><br><span class="line">          <span class="punctuation">]</span></span><br><span class="line">      <span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br></pre></td></tr></table></figure><h1 id="colmap格式">COLMAP格式</h1><p>在COLMAP软件中，相机参数的数量和类型取决于所选的相机模型，一般都涉及内参和外参。</p><p>COLMAP 设定了 <span style="background:#FFCC99;">SIMPLE_PINHOLE </span>，<span style="background:#FFCC99;">PINHOLE </span>， <span style="background:#FFCC99;">SIMPLE_RADIAL </span>，<span style="background:#FFCC99;">RADIAL</span>， <span style="background:#FFCC99;">OPENCV</span> ， <span style="background:#FFCC99;">FULL_OPENCV</span> ， <span style="background:#FFCC99;">SIMPLE_RADIAL_FISHEYE </span>，<span style="background:#FFCC99;">RADIAL_FISHEYE</span>，<span style="background:#FFCC99;">OPENCV_FISHEYE</span>，<span style="background:#FFCC99;">FOV</span>，<span style="background:#FFCC99;">THIN_PRISM_FISHEYE</span> 共11种相机模型，其中最常用的为PINHOLE，即小孔相机模型。一般正常拍摄的照片，不考虑畸变即为该模型。手动参数的情况下官方推荐OPENCV相机模型，相比PINHOLE，考虑了xy轴畸变。<u>3DGS 中使用到的是<span style="background:#FFCC99;">SIMPLE_PINHOLE </span>，<span style="background:#FFCC99;">PINHOLE </span>两种 undistorted 模型。</u></p><h2 id="poses_bounds.npy文件">poses_bounds.npy文件</h2><p>后缀 “npy” 表示文件存储的是 numpy 数组。若文件中数组形状为：N×17, N 表示图像的数量；17 = 3*5+2，前者是位姿矩阵，后者表示两个深度值——视角到场景的最近和最远距离。</p><h2 id="txt-型文件">txt 型文件</h2><h3 id="内参camera.txt">内参/camera.txt</h3><p><code>camera.txt</code> 文件中存储内参（camera intrinsics）。示例如下：</p><p><font color=#4eb434># Camera list with one line of data per camera:</font> <font color=#4eb434># CAMERA_ID, MODEL, WIDTH, HEIGHT, PARAMS[fx,fy,cx,cy]</font> <font color=#4eb434># Number of cameras: 2</font> <font color=#4eb434>1 PINHOLE 1280 720 771.904 771.896 639.993 360.001</font> <font color=#4eb434>2 PINHOLE 1280 720 771.899 771.898 639.999 360.001</font></p><p>上面的宽和高表示图片的宽和高，以像素为单位。以 pinhole 相机模型为例，有四个主要的相机内参：</p><ul><li><strong>fx</strong>：表示相机的焦距在图像坐标系中的水平方向上的缩放因子。它决定了图像中相同物体的像素之间的距离和物理世界中的实际距离之间的关系。</li><li><strong>fy</strong>：表示相机的焦距在图像坐标系中的垂直方向上的缩放因子。与 <code>fx</code> 类似，它也影响了图像中像素距离和实际距离的关系。</li><li><strong>cx</strong>：表示图像平面中心在水平方向上的偏移量。它表示图像的中心点在 x 轴方向上的像素位置。</li><li><strong>cy</strong>：表示图像平面中心在垂直方向上的偏移量。它表示图像的中心点在 y 轴方向上的像素位置。</li></ul><blockquote><p>文件的读取代码为：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#copy from: https://github.com/Fyusion/LLFF/blob/master/llff/poses/colmap_read_model.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_cameras_text</span>(<span class="params">path</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    see: src/base/reconstruction.cc</span></span><br><span class="line"><span class="string">        void Reconstruction::WriteCamerasText(const std::string&amp; path)</span></span><br><span class="line"><span class="string">        void Reconstruction::ReadCamerasText(const std::string&amp; path)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    cameras = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> fid:</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            line = fid.readline()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            line = line.strip()</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(line) &gt; <span class="number">0</span> <span class="keyword">and</span> line[<span class="number">0</span>] != <span class="string">&quot;#&quot;</span>:</span><br><span class="line">                elems = line.split()</span><br><span class="line">                camera_id = <span class="built_in">int</span>(elems[<span class="number">0</span>])</span><br><span class="line">                model = elems[<span class="number">1</span>]</span><br><span class="line">                width = <span class="built_in">int</span>(elems[<span class="number">2</span>])</span><br><span class="line">                height = <span class="built_in">int</span>(elems[<span class="number">3</span>])</span><br><span class="line">                params = np.array(<span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">float</span>, elems[<span class="number">4</span>:])))</span><br><span class="line">                cameras[camera_id] = Camera(<span class="built_in">id</span>=camera_id, model=model,</span><br><span class="line">                                            width=width, height=height,</span><br><span class="line">                                            params=params)</span><br><span class="line">    <span class="keyword">return</span> cameras</span><br></pre></td></tr></table></figure></blockquote><h3 id="外参images.txt">外参/images.txt</h3><p><code>images.txt</code> 文件中存储相机外参（camera extrinsics）。示例如下：</p><p><font color=#4eb434># Image list with two lines of data per image:</font> <font color=#4eb434># IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME</font> <font color=#4eb434># POINTS2D[] as (X, Y, POINT3D_ID)</font> <font color=#4eb434># Number of images: 2, mean observations per image: 2</font> <font color=#4eb434>1 0.430115 0.411564 0.555504 -0.580543 10468.491287 380.313066 1720.465175 1 image001.jpg</font> <font color=#4eb434># Make sure every other line is left empty</font> <font color=#4eb434>2 0.309712 0.337960 0.655221 -0.600456 10477.663284 446.4208 -1633.886712 2 image002.jpg</font> <font color=#4eb434>3 0.375916 0.401654 0.609703 -0.570633 10592.122754 263.672534 600.636247 3 image003.jpg</font></p><p>其中，QW, QX, QY, QZ 为四元数表示的相机旋转信息（<strong>旋转矩阵R</strong>），TX, TY, TZ为平移向量（<strong>平移矩阵T</strong>）。</p><blockquote><p>文件的读取代码为：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#copy from:https://github.com/Fyusion/LLFF/blob/master/llff/poses/colmap_read_model.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_images_text</span>(<span class="params">path</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    see: src/base/reconstruction.cc</span></span><br><span class="line"><span class="string">        void Reconstruction::ReadImagesText(const std::string&amp; path)</span></span><br><span class="line"><span class="string">        void Reconstruction::WriteImagesText(const std::string&amp; path)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    images = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> fid:</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            line = fid.readline()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            line = line.strip()</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(line) &gt; <span class="number">0</span> <span class="keyword">and</span> line[<span class="number">0</span>] != <span class="string">&quot;#&quot;</span>:</span><br><span class="line">                elems = line.split()</span><br><span class="line">                image_id = <span class="built_in">int</span>(elems[<span class="number">0</span>])</span><br><span class="line">                qvec = np.array(<span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">float</span>, elems[<span class="number">1</span>:<span class="number">5</span>])))</span><br><span class="line">                tvec = np.array(<span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">float</span>, elems[<span class="number">5</span>:<span class="number">8</span>])))</span><br><span class="line">                camera_id = <span class="built_in">int</span>(elems[<span class="number">8</span>])</span><br><span class="line">                image_name = elems[<span class="number">9</span>]</span><br><span class="line">                elems = fid.readline().split()</span><br><span class="line">                xys = np.column_stack([<span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">float</span>, elems[<span class="number">0</span>::<span class="number">3</span>])),</span><br><span class="line">                                       <span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">float</span>, elems[<span class="number">1</span>::<span class="number">3</span>]))])</span><br><span class="line">                point3D_ids = np.array(<span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, elems[<span class="number">2</span>::<span class="number">3</span>])))</span><br><span class="line">                images[image_id] = Image(</span><br><span class="line">                    <span class="built_in">id</span>=image_id, qvec=qvec, tvec=tvec,</span><br><span class="line">                    camera_id=camera_id, name=image_name,</span><br><span class="line">                    xys=xys, point3D_ids=point3D_ids)</span><br><span class="line">    <span class="keyword">return</span> images</span><br></pre></td></tr></table></figure></blockquote><h3 id="点云points3d.txt">点云/points3D.txt</h3><p>示例如下：</p><p><font color=#4eb434># 3D point list with one line of data per point:</font> <font color=#4eb434># POINT3D_ID, X, Y, Z, R, G, B, ERROR, TRACK[] as (IMAGE_ID, POINT2D_IDX)</font> <font color=#4eb434># Number of points: 8772, mean track length: 7.0926812585499315</font> <font color=#4eb434>7328 19.581902231208609 -8.9584345600751494 38.559312268337116 0 0 0 0.41406486712668422 3 3203 5 3397</font> <font color=#4eb434>7327 16.864630744945362 5.1149227685898264 25.929447341399616 0 0 0 0.19639330687077938 6 3355 5 3379</font></p><blockquote><p>文件的读取代码为：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#copy from: https://github.com/Fyusion/LLFF/blob/master/llff/poses/colmap_read_model.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_points3D_text</span>(<span class="params">path</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    see: src/base/reconstruction.cc</span></span><br><span class="line"><span class="string">        void Reconstruction::ReadPoints3DText(const std::string&amp; path)</span></span><br><span class="line"><span class="string">        void Reconstruction::WritePoints3DText(const std::string&amp; path)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    points3D = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> fid:</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            line = fid.readline()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            line = line.strip()</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(line) &gt; <span class="number">0</span> <span class="keyword">and</span> line[<span class="number">0</span>] != <span class="string">&quot;#&quot;</span>:</span><br><span class="line">                elems = line.split()</span><br><span class="line">                point3D_id = <span class="built_in">int</span>(elems[<span class="number">0</span>])</span><br><span class="line">                xyz = np.array(<span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">float</span>, elems[<span class="number">1</span>:<span class="number">4</span>])))</span><br><span class="line">                rgb = np.array(<span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, elems[<span class="number">4</span>:<span class="number">7</span>])))</span><br><span class="line">                error = <span class="built_in">float</span>(elems[<span class="number">7</span>])</span><br><span class="line">                image_ids = np.array(<span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, elems[<span class="number">8</span>::<span class="number">2</span>])))</span><br><span class="line">                point2D_idxs = np.array(<span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, elems[<span class="number">9</span>::<span class="number">2</span>])))</span><br><span class="line">                points3D[point3D_id] = Point3D(<span class="built_in">id</span>=point3D_id, xyz=xyz, rgb=rgb,</span><br><span class="line">                                               error=error, image_ids=image_ids,</span><br><span class="line">                                               point2D_idxs=point2D_idxs)</span><br><span class="line">    <span class="keyword">return</span> points3D</span><br></pre></td></tr></table></figure></blockquote><h2 id="bin-型文件">bin 型文件</h2><h3 id="内参camera.bin">内参/camera.bin</h3><blockquote><p>文件的读取代码为：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#copy from: https://github.com/Fyusion/LLFF/blob/master/llff/poses/colmap_read_model.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_cameras_binary</span>(<span class="params">path_to_model_file</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    see: src/base/reconstruction.cc</span></span><br><span class="line"><span class="string">        void Reconstruction::WriteCamerasBinary(const std::string&amp; path)</span></span><br><span class="line"><span class="string">        void Reconstruction::ReadCamerasBinary(const std::string&amp; path)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    cameras = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path_to_model_file, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> fid:</span><br><span class="line">        num_cameras = read_next_bytes(fid, <span class="number">8</span>, <span class="string">&quot;Q&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> camera_line_index <span class="keyword">in</span> <span class="built_in">range</span>(num_cameras):</span><br><span class="line">            camera_properties = read_next_bytes(</span><br><span class="line">                fid, num_bytes=<span class="number">24</span>, format_char_sequence=<span class="string">&quot;iiQQ&quot;</span>)</span><br><span class="line">            camera_id = camera_properties[<span class="number">0</span>]</span><br><span class="line">            model_id = camera_properties[<span class="number">1</span>]</span><br><span class="line">            model_name = CAMERA_MODEL_IDS[camera_properties[<span class="number">1</span>]].model_name</span><br><span class="line">            width = camera_properties[<span class="number">2</span>]</span><br><span class="line">            height = camera_properties[<span class="number">3</span>]</span><br><span class="line">            num_params = CAMERA_MODEL_IDS[model_id].num_params</span><br><span class="line">            params = read_next_bytes(fid, num_bytes=<span class="number">8</span>*num_params,</span><br><span class="line">                                     format_char_sequence=<span class="string">&quot;d&quot;</span>*num_params)</span><br><span class="line">            cameras[camera_id] = Camera(<span class="built_in">id</span>=camera_id,</span><br><span class="line">                                        model=model_name,</span><br><span class="line">                                        width=width,</span><br><span class="line">                                        height=height,</span><br><span class="line">                                        params=np.array(params))</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(cameras) == num_cameras</span><br><span class="line">    <span class="keyword">return</span> cameras</span><br></pre></td></tr></table></figure></blockquote><h3 id="外参images.bin">外参/images.bin</h3><blockquote><p>文件的读取代码为：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#copy from: https://github.com/Fyusion/LLFF/blob/master/llff/poses/colmap_read_model.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_images_binary</span>(<span class="params">path_to_model_file</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    see: src/base/reconstruction.cc</span></span><br><span class="line"><span class="string">        void Reconstruction::ReadImagesBinary(const std::string&amp; path)</span></span><br><span class="line"><span class="string">        void Reconstruction::WriteImagesBinary(const std::string&amp; path)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    images = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path_to_model_file, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> fid:</span><br><span class="line">        num_reg_images = read_next_bytes(fid, <span class="number">8</span>, <span class="string">&quot;Q&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> image_index <span class="keyword">in</span> <span class="built_in">range</span>(num_reg_images):</span><br><span class="line">            binary_image_properties = read_next_bytes(</span><br><span class="line">                fid, num_bytes=<span class="number">64</span>, format_char_sequence=<span class="string">&quot;idddddddi&quot;</span>)</span><br><span class="line">            image_id = binary_image_properties[<span class="number">0</span>]</span><br><span class="line">            qvec = np.array(binary_image_properties[<span class="number">1</span>:<span class="number">5</span>])</span><br><span class="line">            tvec = np.array(binary_image_properties[<span class="number">5</span>:<span class="number">8</span>])</span><br><span class="line">            camera_id = binary_image_properties[<span class="number">8</span>]</span><br><span class="line">            image_name = <span class="string">&quot;&quot;</span></span><br><span class="line">            current_char = read_next_bytes(fid, <span class="number">1</span>, <span class="string">&quot;c&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">while</span> current_char != <span class="string">b&quot;\x00&quot;</span>:   <span class="comment"># look for the ASCII 0 entry</span></span><br><span class="line">                image_name += current_char.decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">                current_char = read_next_bytes(fid, <span class="number">1</span>, <span class="string">&quot;c&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">            num_points2D = read_next_bytes(fid, num_bytes=<span class="number">8</span>,</span><br><span class="line">                                           format_char_sequence=<span class="string">&quot;Q&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">            x_y_id_s = read_next_bytes(fid, num_bytes=<span class="number">24</span>*num_points2D,</span><br><span class="line">                                       format_char_sequence=<span class="string">&quot;ddq&quot;</span>*num_points2D)</span><br><span class="line">            xys = np.column_stack([<span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">float</span>, x_y_id_s[<span class="number">0</span>::<span class="number">3</span>])),</span><br><span class="line">                                   <span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">float</span>, x_y_id_s[<span class="number">1</span>::<span class="number">3</span>]))])</span><br><span class="line">            point3D_ids = np.array(<span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, x_y_id_s[<span class="number">2</span>::<span class="number">3</span>])))</span><br><span class="line">            images[image_id] = Image(</span><br><span class="line">                <span class="built_in">id</span>=image_id, qvec=qvec, tvec=tvec,</span><br><span class="line">                camera_id=camera_id, name=image_name,</span><br><span class="line">                xys=xys, point3D_ids=point3D_ids)</span><br><span class="line">    <span class="keyword">return</span> images</span><br></pre></td></tr></table></figure></blockquote><h3 id="点云points3d.bin">点云/points3D.bin</h3><blockquote><p>文件的读取代码为：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#copy from: https://github.com/Fyusion/LLFF/blob/master/llff/poses/colmap_read_model.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_points3d_binary</span>(<span class="params">path_to_model_file</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    see: src/base/reconstruction.cc</span></span><br><span class="line"><span class="string">        void Reconstruction::ReadPoints3DBinary(const std::string&amp; path)</span></span><br><span class="line"><span class="string">        void Reconstruction::WritePoints3DBinary(const std::string&amp; path)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    points3D = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path_to_model_file, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> fid:</span><br><span class="line">        num_points = read_next_bytes(fid, <span class="number">8</span>, <span class="string">&quot;Q&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> point_line_index <span class="keyword">in</span> <span class="built_in">range</span>(num_points):</span><br><span class="line">            binary_point_line_properties = read_next_bytes(</span><br><span class="line">                fid, num_bytes=<span class="number">43</span>, format_char_sequence=<span class="string">&quot;QdddBBBd&quot;</span>)</span><br><span class="line">            point3D_id = binary_point_line_properties[<span class="number">0</span>]</span><br><span class="line">            xyz = np.array(binary_point_line_properties[<span class="number">1</span>:<span class="number">4</span>])</span><br><span class="line">            rgb = np.array(binary_point_line_properties[<span class="number">4</span>:<span class="number">7</span>])</span><br><span class="line">            error = np.array(binary_point_line_properties[<span class="number">7</span>])</span><br><span class="line">            track_length = read_next_bytes(</span><br><span class="line">                fid, num_bytes=<span class="number">8</span>, format_char_sequence=<span class="string">&quot;Q&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">            track_elems = read_next_bytes(</span><br><span class="line">                fid, num_bytes=<span class="number">8</span>*track_length,</span><br><span class="line">                format_char_sequence=<span class="string">&quot;ii&quot;</span>*track_length)</span><br><span class="line">            image_ids = np.array(<span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, track_elems[<span class="number">0</span>::<span class="number">2</span>])))</span><br><span class="line">            point2D_idxs = np.array(<span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, track_elems[<span class="number">1</span>::<span class="number">2</span>])))</span><br><span class="line">            points3D[point3D_id] = Point3D(</span><br><span class="line">                <span class="built_in">id</span>=point3D_id, xyz=xyz, rgb=rgb,</span><br><span class="line">                error=error, image_ids=image_ids,</span><br><span class="line">                point2D_idxs=point2D_idxs)</span><br><span class="line">    <span class="keyword">return</span> points3D</span><br></pre></td></tr></table></figure></blockquote><h1 id="数据集加载">数据集加载</h1><h2 id="llffold">LLFF（old）</h2><p>以下参考自：<a href="https://blog.csdn.net/qq_41623632/article/details/126468034">链接</a></p><p>在实际的LLFF数据集读取中，主要使用 <code>load_llff_data</code> 函数得到<span style="background:#daf5e9;"><strong>相机外参 pose matrix</strong> </span>如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">images, poses, bds, render_poses, i_test = load_llff_data(args.datadir, args.factor,recenter=<span class="literal">True</span>, bd_factor=<span class="number">.75</span>,spherify=args.spherify)</span><br></pre></td></tr></table></figure><p>从load_llff_data 中取出的pose 是一个（20，3，5）的list。20代表一共有20张image，3×5是每一个image 的pose matrix。</p><p>pose matrix 的前四列是旋转矩阵 rotation matrix 和平移向量 translation vector，即一般意义上的位姿矩阵 T (camera-to-world affine)。最后一列的三行，分别代表图像的高、宽和相机焦距：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hwf = poses[<span class="number">0</span>,:<span class="number">3</span>,-<span class="number">1</span>]  // 取出前三行最后一列元素（红色部分）</span><br><span class="line">poses = poses[:,:<span class="number">3</span>,:<span class="number">4</span>]  // 取出pose里的平移和旋转部分</span><br><span class="line">....中间代码略去.......</span><br><span class="line">H, W, focal = hwf   // 分别赋予 Hieight、Width、focal</span><br></pre></td></tr></table></figure><h1 id="格式转换">格式转换</h1><h2 id="blender格式与colmap格式">blender格式与COLMAP格式</h2><p>参见 <a href="https://blog.csdn.net/m0_46246301/article/details/137842904">Blender生成COLMAP数据集_blender立体视觉数据生成-CSDN博客</a></p><p><a href="https://blog.csdn.net/qq_35831906/article/details/131317381#:~:text=1%20LLFF（Layered%20Light%20Field%20Flow）数据集：%20LLFF数据集是用于视角一致性和深度学习重建的数据集。%20它包含通过拍摄相机移动的图像序列和深度图像。%20...,Object%20Detection）数据集是用于物体检测和位姿估计的数据集。%20它包含多个物体的RGB图像和相应的标注信息，例如物体的二维边界框和位姿。%20...%204%20DeepVoxel数据集：%20DeepVoxel数据集是用于稠密三维重建的数据集。">NeRF-pytorch源码解读：论文Representing Scenesas Neural Radiance Fieldsfor View Synthesis_nerf源码-CSDN博客</a></p><h2 id="llff格式与dtu格式">LLFF格式与DTU格式</h2><p><a href="https://blog.csdn.net/weixin_47343723/article/details/129707401">关于colmap+nerf对数据集进行预处理的使用总结_colmap nerf-CSDN博客</a></p><p>参见 <a href="https://blog.csdn.net/weixin_59961223/article/details/135429437">NeuS（隐式重建）自制数据集流程（LLFF、DTU）_dtu数据集-CSDN博客</a></p><p><a href="https://blog.csdn.net/weixin_43041894/article/details/136098701">【三维重建】【深度学习】【数据集】基于COLMAP制作自己的NeuS(DTU格式)数据集_dtu数据集-CSDN博客</a></p><h2 id="其它">其它</h2><p><a href="https://blog.csdn.net/zp_jlu/article/details/124429361">NeRF数据集准备--毕设记录_nerf lego数据集-CSDN博客</a></p><p><a href="https://blog.csdn.net/toro180/article/details/129973285">Nerf系列数据集记录_nerf数据集-CSDN博客</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CUDA和C++混合双打</title>
      <link href="/2025/01/08/CUDA%E5%92%8CC++%E6%B7%B7%E5%90%88%E5%8A%A0%E9%80%9F%E5%8A%9F%E8%83%BD%E6%A8%A1%E5%9D%97/"/>
      <url>/2025/01/08/CUDA%E5%92%8CC++%E6%B7%B7%E5%90%88%E5%8A%A0%E9%80%9F%E5%8A%9F%E8%83%BD%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<h1 id="gpu和cpu">GPU和CPU</h1><details><summary>参考链接</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_45264425/article/details/135830423&quot;&gt;【GPU】深入理解GPU硬件架构及运行机制&lt;/a&gt;&lt;/p&gt;</code></pre></details><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20241030151352542.png" alt="image-20241030151352542" /><figcaption aria-hidden="true">image-20241030151352542</figcaption></figure><p><span class="math inline">\(\colorbox{SpringGreen}{绿色方块}\)</span>​ 表示computational units(可计算单元) 或者称之为 cores(核心)。ALU是算数逻辑单元的缩写，是执行实际计算的中心，可以传输0和1的信号，实现对整数二进制变量的加减乘除等算数操作、以及AND-OR-NOT-XOR等逻辑操作。</p><p><span class="math inline">\(\colorbox{Orange}{橙色方块}\)</span> 代表memories(内存)，</p><p><span class="math inline">\(\colorbox{Yellow}{黄色方块}\)</span> 代表的是control units（控制单元）.</p><h2 id="计算单元core">计算单元(core)</h2><table><colgroup><col style="width: 26%" /><col style="width: 36%" /><col style="width: 36%" /></colgroup><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">CPU</th><th style="text-align: center;">GPU</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">计算能力</td><td style="text-align: center;">大（能执行更复杂的运算）、少（能执行计算的设备）<br />”一个老教授“</td><td style="text-align: center;">小、多<br />“2000个小学生”</td></tr><tr class="even"><td style="text-align: center;">乱序执行<br />“<strong>out-of-order exectutions</strong>”</td><td style="text-align: center;"><strong>能</strong>（可以用不同于输入指令的顺序执行指令，当遇到分支的时候，它可以预测在不久的将来哪一个指令最有可能被执行到（multiple branch prediction 多重分支预测）；通过这种方式，它可以预先准备好操作数，并且提前执行他们（soeculative execution 预测执行），以此<strong>节省了程序运行时间</strong>。）</td><td style="text-align: center;"><strong>不能</strong></td></tr><tr class="odd"><td style="text-align: center;">运算</td><td style="text-align: center;"></td><td style="text-align: center;"><strong>原始GPU只能做最简单的浮点运算</strong>，例如 multiply-add(MAD)或者 fused multiply-add(FMA)指令。<strong>现代GPU还可以执行更加复杂的运算操作</strong>，例如tensor张量(tensor core)或者光线追踪(ray tracing core)相关的操作。</td></tr><tr class="even"><td style="text-align: center;">编程方式</td><td style="text-align: center;"></td><td style="text-align: center;"><strong>SIMD(Single Instruction Multiple Data)</strong>，即所有Core的计算操作完全是在相同的时间内进行的，但是输入的数据有所不同。</td></tr><tr class="odd"><td style="text-align: center;">总结</td><td style="text-align: center;"><strong>核心处理能力强</strong></td><td style="text-align: center;"><strong>能大规模并行处理数据</strong></td></tr></tbody></table><h2 id="内存memory">内存(memory)</h2><blockquote><p>这部分论述乱乱的</p></blockquote><table><colgroup><col style="width: 9%" /><col style="width: 45%" /><col style="width: 45%" /></colgroup><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">CPU</th><th style="text-align: center;">GPU</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">内存大小</td><td style="text-align: center;">基于DRAM，桌面PC一般8G，服务器中能达到数百(256)G</td><td style="text-align: center;"><strong>GPU中有一大片橙色的内存，名称为DRAM</strong>，这一块被称为全局内存或者GMEM。<strong>GMEM的内存大小要比CPU的DRAM小的多</strong>，在最便宜的显卡中一般只有几个G的大小，在最好的显卡中GMEM可以达到24G。</td></tr><tr class="even"><td style="text-align: center;">高速缓存机制</td><td style="text-align: center;"><strong>cache，是用来减少CPU访问DRAM的时间。</strong>cache是一片小的内存区域，但是访问速度更快，更加靠近处理器核心的内存段，用来储存DRAM中的数据副本。cache一般分级，通常分为三个级别L1，L2，L3 cache，cache离核心越近就越小访问越快，例如 L1可以是64KB L2就是256KB L3是4MB。</td><td style="text-align: center;">与CPU略有差异。其 L1 Cache中每个SM都有一块专用的共享内存，作为Core需要协同工作、并且彼此交换结果时的中间站。</td></tr><tr class="odd"><td style="text-align: center;">数据访问</td><td style="text-align: center;"></td><td style="text-align: center;">存在全局的内存GMEM，但是访问较慢，Cores当需要访问GMEM的时候会首先访问L1,L2如果都miss了，那么才会花费大代价到GMEM中寻找数据。</td></tr></tbody></table><h2 id="gpu的底层结构sm">GPU的底层结构(SM)</h2><p>由上图，一个GPU显卡中绝大多数<strong>都是计算核心core组成的海洋</strong>。在一些例子（如图像缩放）中，core与core之间不需要任何协作，因为他们的任务是完全独立的；然而，GPU解决的问题不一定这么简单，如下例：</p><p>假设我们需要对一个数组里的数进行求和，这样的运算属于reductuin family类型，因为这样的运算试图将一个序列“reduce”简化为一个数。</p><p>我们可以将原本顺序相加的运算，在并行算法中进行转化。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20241030163417639.png" alt="image-20241030163417639" /><figcaption aria-hidden="true">image-20241030163417639</figcaption></figure><p>从GPU的角度来讲，只需要四个core就可以完成长度为8的数组求和算法，我们将四个core编号为0，1，2，3。那么第一个时钟下，两两相加的结果通过0号core计算，放入了0号core可以访问到的内存中，另外两两对分别由1号2号3号core来计算，第二个个时钟继续按照之前的算法计算，只需要0号和1号两个core即可完成。以此类推，最终的结果将在第三个时钟由0号core计算完成，并储存在0号core可以访问到的内存中。这样实际三次就能完成长度为8的数组求和计算。</p><p>如果GPU想要完成上述的推理计算过程，显然，多个core之间要可以共享一段内存空间以此来完成数据之间的交互，需要多个core可以在共享的内存空间中完成读/写的操作。虽然我们希望每个Cores都有交互数据的能力，但是不幸的是，一个GPU里面可以包含数以千计的core，如果使得这些core都可以访问共享的内存段是非常困难和昂贵的。</p><p>出于成本的考虑，折中的解决方案是将各类GPU的core分类为多个组，形成<strong>多个流处理器(Streaming Multiprocessors )或者简称为SMs</strong>。SMs是多个core的集合，一个SMs里面的cores可以通过该SM对应的 L1 Cache 共享内存块进行交互信息，完成使用GPU处理数组求和等问题的时候，多个核心共享数据的功能。</p><h2 id="gpu的架构">GPU的架构</h2><blockquote><p>图：单个SM的图灵架构</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/9fe3c7111745d0a8865fa45d5b1385c3.jpeg" alt="img" style="zoom:67%;" /></p><p><span class="math inline">\(\colorbox{Green}{绿色部分}\)</span> 是CORE相关的，我们进一步区分了不同类型的CORE：</p><ul><li><strong>Integer Cores，这些core执行一些对整数的操作</strong>，例如地址计算，可以和浮点运算同时执行指令。在前几代GPU中，执行这些整型操作指令都会使得浮点运算的管道停止工作。TU102总共由4608个Integer Cores，每个SM有64个ICs。</li><li><strong>FP32 Cores，执行单进度浮点运算</strong>，在TU102卡中，每个SM有64个FP32核，TU120由72个SMs。因此，FP32 Core的数量是 72 * 64。</li><li>FP64 Cores. 实际上每个SM都包含了2个64位浮点计算核心FP64 Cores，用来计算双精度浮点运算，虽然上图没有画出，但是实际是存在的。</li><li><strong>Tensor Cores，张量core是FP16单元的变种</strong>，认为是半精度单元，致力于张量积算加速常见的深度学习操作。张量Core还<strong>可以执行INT8和INT4精度的操作</strong>，用于可以接受量化而且不需要FP16精度的应用场景，在TU102中，我们每个SM有8个张量Cores，一共有8 * 72个Tensor Cores。</li></ul><p><span class="math inline">\(\colorbox{BlueGreen}{浅蓝色部分}\)</span> 是用于进行每个SM中核心的相互协作的 L1 Cache。这个cache段是允许各个Core都可以访问的段，<strong>在L1 Cache中每个SM都有一块专用的共享内存。</strong>实际上GPU中的L1 CACHE拥有两个功能：</p><ul><li>一个是用于SM上Core之间相互共享内存，当Core需要协同工作，并且彼此交换结果的时候，<strong>编译器编译后的指令会将部分结果储存在共享内存中，以便于不同的core获取到对应数据。</strong></li><li>另一个则是普通的cache功能。当core需要访问GMEM数据的时候，<ul><li>首先会在L1中查找，如果没找到，则回去L2 cache中寻找，</li><li>如果L2 cache也没有，则会从GMEM中获取数据，</li></ul></li></ul><p><strong>编程者有权决定L1 的内存多少是用作普通cache，多少是用作共享内存</strong>。Cache 缓存中的数据将会持续存在，除非出现新的数据做替换。如果Core需要从GMEM中多次访问数据，那么编程者应该将这块数据放入功能内存 Cache中，以加快他们的获取速度。<span style="background:#eef0f4;">(怎么放？？？AI 的解释是数据预加载到功能内存、动态内存分配等)</span></p><blockquote><p>最后，也是比较重要的是，可以储存各个core的计算中间结果、用于各个核心之间共享的内存段不仅仅可以是共享内存L1，也可以是寄存器，寄存器是离core最近的内存段，但是也非常小。<strong>最底层的思想是每个线程都可以拥有一个寄存器来储存中间结果</strong>，每个寄存器只能由相同的一个线程来访问，或者由相同的warp或者组的线程访问。</p></blockquote><h1 id="c">C++</h1><h2 id="命名空间">命名空间 ::</h2><p>在 C++ 中，两个冒号 <code>::</code> 被称为作用域解析运算符（scope resolution operator）。它用于指定标识符的作用域或所属的命名空间。如下面的一段代码：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// declare cuda forward function</span></span><br><span class="line"><span class="function">torch::Tensor <span class="title">sigmoid_cuda_forward</span><span class="params">(torch::Tensor input)</span></span>;</span><br></pre></td></tr></table></figure><p><span style="background:#FFCC99;"> <strong>torch::Tensor</strong> </span>：</p><ul><li><code>torch</code> 是一个命名空间（namespace），它包含了一系列与 PyTorch 相关的功能和类。</li><li><code>Tensor</code> 是 <code>torch</code> 命名空间下定义的一个类或类型。</li></ul><p>通过使用 <code>torch::Tensor</code>，我们明确地表示这个 <code>Tensor</code> 类型是属于 <code>torch</code> 命名空间的。这可以避免与其他命名空间或库中可能存在的同名类型或类发生冲突，从而指定变量、函数或类型的确切位置。</p><p>如果不想每次都加上<code>::</code>前缀，可以在开头加上 <font color=#ef042a><strong><code>using namespace spacename;</code></strong></font>。这样就可以直接使用该命名空间中的函数和对象。</p><h2 id="cuda线程索引">CUDA线程索引</h2><details><summary>参考链接</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/ambition_zhou/article/details/116498630?spm=1001.2101.3001.6650.5&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-5-116498630-blog-54867507.235%5Ev43%5Epc_blog_bottom_relevance_base1&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-5-116498630-blog-54867507.235%5Ev43%5Epc_blog_bottom_relevance_base1&amp;utm_relevant_index=10&quot;&gt;CUDA中grid、block、thread、warp与SM、SP的关系&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/dcrmg/article/details/54867507&quot;&gt;CUDA软件架构—网格（Grid）、线程块（Block）和线程（Thread）的组织关系以及线程索引的计算公式&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/fengye2two/article/details/83271904?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-83271904-blog-54867507.235^v43^pc_blog_bottom_relevance_base1&amp;spm=1001.2101.3001.4242.2&amp;utm_relevant_index=4&quot;&gt;【CUDA】grid、block、thread的关系及thread索引的计算&lt;/a&gt;&lt;/p&gt;</code></pre></details><h3 id="threadblock和grid的概念">thread、block和grid的概念</h3><p>首先，<strong>SM（Streaming Multiprocessor）</strong>和<strong>SP（streaming Processor）</strong>是硬件层次的，其中一个SM可以包含多个SP。而线程网格（Grid）、线程块（Block）和线程（Thread）是CUDA编程上的概念，属于软件层次，为了方便程序员软件设计，组织线程。</p><p><span style="background:#FFCC99;"><strong>thread</strong></span>：一个CUDA的并行程序会被以许多个threads来执行。</p><p><span style="background:#FFCC99;"><strong>block</strong></span>：每个block是一个thread群组。在<strong>同一个block中thread可以通过共享内存（shared memory）</strong>来通信同步。而不同block之间的thread是无法通信的。</p><p>GPU在实际执行过程中，<strong>会以block为单位</strong>，把一个个block分配给SM进行运算；而block中的thread又会被SM自动以<strong>warp（线程束）</strong>为单位进行分组计算，且SM以连续的方式来做分组。目前CUDA的warp大小都是32，也就是说连续的32个thread会被组成一个warp来一起执行。同一个warp中的thread执行的指令是相同的，只是处理的数据不同。如果有warp中的thread数小于32，相当于浪费了部分thread计算能力，这点是在设定block 中thread 数量一定要注意的事！</p><p>实际上，<strong>warp 也是CUDA 中，每一个SM 执行的最小单位</strong>；如果GPU 有16 组SM 的话，也就代表他真正在执行的thread 数目会是32*16 个。</p><p><span style="background:#FFCC99;"><strong>grid</strong></span>：多个blocks则会再构成grid。一个grid中包含的所有block执行相同程序，不同的grid可以执行不同的程序（kernel）。</p><h3 id="最大数量">最大数量</h3><p>CUDA中可以创建的网格数量跟GPU的计算能力有关，可创建的Grid、Block和Thread的最大数量参看以下表格：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20170204232629318" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>在单一维度上，程序的执行可以由多达3*65535*512=100661760（一亿）个线程并行执行，这对在CPU上创建并行线程来说是不可想象的。</p><h3 id="线程索引计算公式">线程索引计算公式</h3><blockquote><p>这里例子的问题背景是：对两个数组求和，并保存到另一个数组。详情见<a href="https://blog.csdn.net/fengye2two/article/details/83271904?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-83271904-blog-54867507.235%5Ev43%5Epc_blog_bottom_relevance_base1&amp;spm=1001.2101.3001.4242.2&amp;utm_relevant_index=4">这里</a>。</p></blockquote><p><strong>首先是grid和block的维度</strong>，下面举个例子：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">dim3 <span class="title">grid1</span><span class="params">(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span></span>; <span class="comment">// x=2, y=1, z=1</span></span><br><span class="line"><span class="function">dim3 <span class="title">grid2</span><span class="params">(<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span></span>; <span class="comment">// x=4, y=2, z=1</span></span><br><span class="line"><span class="function">dim3 <span class="title">grid3</span><span class="params">(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span></span>; <span class="comment">// x=2, y=3, z=4</span></span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> length = <span class="number">16</span>;    </span><br><span class="line"><span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span>, <span class="title">block</span><span class="params">(length, <span class="number">1</span>, <span class="number">1</span>)</span></span>;                  </span><br></pre></td></tr></table></figure><p>可以知道，grid1是一维的（因为y,z维度是1），grid2是二维的（因为z维度是1），grid3是三维的，且grid1,grid2,grid3中分别有2、8、24个block。同理，对于线程块（block），我们知道之前的代码中，block中存在16个线程，且该线程块维度是一维的，因为block(x,y,z)中x=length=16,y=1,z=1。画成图就是</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/e6a7fd29f77f3ddf33c5e46d0ff9fc7d.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>此时线程索引为 <code>int tid = threadIdx.x;</code>它的范围是[0,15]。因此，我们在计算线程索引是，只用这个內建变量threadIdx.x就行了（其他的为0，写了也不起作用）。</p><p><strong>然后再举个常用的复杂情况例子</strong>，此时grid和block都是2维，参数为：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">8</span>, <span class="number">4</span>, <span class="number">1</span>)</span>, <span class="title">block</span><span class="params">(<span class="number">8</span>, <span class="number">2</span>, <span class="number">1</span>)</span></span>;</span><br></pre></td></tr></table></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/f7d8de784bd1e6b5ce5113f1158771ad.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>那么其一维向量的计算方式如下：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">vector_add</span><span class="params">(<span class="type">float</span>* vec1, <span class="type">float</span>* vec2, <span class="type">float</span>* vecres, <span class="type">int</span> length)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 在第几个块中 * 块的大小 + 块中的x, y维度（几行几列）</span></span><br><span class="line">    <span class="type">int</span> tid = (blockIdx.y * gridDim.x + blockIdx.x) * (blockDim.x * blockDim.y) + threadIdx.y * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span> (tid &lt; length) &#123;</span><br><span class="line">        vecres[tid] = vec1[tid] + vec2[tid];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>再给出二维矩阵的：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">vector_add</span><span class="params">(<span class="type">float</span>** mat1, <span class="type">float</span>** mat2, <span class="type">float</span>** matres, <span class="type">int</span> width)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">    <span class="keyword">if</span> (x &lt; width &amp;&amp; y &lt; width) &#123;</span><br><span class="line">        matres[x][y] = mat1[x][y] + mat2[x][y];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>最后推广至多维情形，给出线程索引计算公式</strong>。CUDA中每一个线程都有一个唯一的标识ID—ThreadIdx，这个ID随着Grid和Block的划分维度的不同而变化：</p><blockquote><p>gridDim: 划分成多少block blockDim: 划分成多少thread</p><p>Dim.x: 一行有多少 xx Dim.y:一共有多少行</p><p>Idx.y: 在第几行 Idx.x: 在该行的第几个</p></blockquote><ul><li><p><strong>grid 划分成1维</strong></p><ol type="1"><li><strong>block划分成1维</strong>： int threadId = blockIdx.x * blockDim.x + threadIdx.x;<br /></li><li><strong>block划分成2维</strong>：int threadId = blockIdx.x * blockDim.x * blockDim.y+ threadIdx.y * blockDim.x + threadIdx.x;<br /></li><li><strong>block划分成3维</strong>：int threadId = blockIdx.x * blockDim.x * blockDim.y * blockDim.z<br />+ threadIdx.z * blockDim.y * blockDim.x<br />+ threadIdx.y * blockDim.x + threadIdx.x;</li></ol></li><li><p><strong>grid 划分成2维</strong></p><ol type="1"><li><p><strong>block划分成1维：</strong></p><p>int blockId = blockIdx.y * gridDim.x + blockIdx.x;<br />int threadId = blockId * blockDim.x + threadIdx.x;</p></li><li><p><strong>block划分成2维：</strong></p><p>int blockId = blockIdx.x + blockIdx.y * gridDim.x;<br />int threadId = blockId * (blockDim.x * blockDim.y)<br />+ (threadIdx.y * blockDim.x) + threadIdx.x;</p></li><li><p><strong>block划分成3维：</strong></p><p>int blockId = blockIdx.x + blockIdx.y * gridDim.x;<br />int threadId = blockId * (blockDim.x * blockDim.y * blockDim.z)<br />+ (threadIdx.z * (blockDim.x * blockDim.y))<br />+ (threadIdx.y * blockDim.x) + threadIdx.x;</p></li></ol></li><li><p><strong>grid 划分成3维</strong></p><ol type="1"><li><p><strong>block划分成1维：</strong></p><p>int blockId = blockIdx.x + blockIdx.y * gridDim.x<br />+ gridDim.x * gridDim.y * blockIdx.z;<br />int threadId = blockId * blockDim.x + threadIdx.x;</p></li><li><p><strong>block划分成2维：</strong></p><p>int blockId = blockIdx.x + blockIdx.y * gridDim.x<br />+ gridDim.x * gridDim.y * blockIdx.z;<br />int threadId = blockId * (blockDim.x * blockDim.y)<br />+ (threadIdx.y * blockDim.x) + threadIdx.x;</p></li><li><p><strong>block划分成3维：</strong></p><p>int blockId = blockIdx.x + blockIdx.y * gridDim.x<br />+ gridDim.x * gridDim.y * blockIdx.z;<br />int threadId = blockId * (blockDim.x * blockDim.y * blockDim.z)<br />+ (threadIdx.z * (blockDim.x * blockDim.y))<br />+ (threadIdx.y * blockDim.x) + threadIdx.x;</p></li></ol></li></ul><h2 id="宏">宏</h2><h3 id="宏定义-define">宏定义 #define</h3><p><a href="https://baike.baidu.com/item/%E5%AE%8F/2648286">宏（macro）是一种批量处理的称谓</a>。宏在编程中可作为用于简化和避免重复代码的一种预处理指令。它们可以接收传入参数，并通常在编译之前被代码替换为具体的值或代码段。</p><p>下面是C语言中宏的简单应用：</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> BOX_SIZE 1024  <span class="comment">//创建宏，允许在代码中使用 BOX_SIZE 来代替 1024，提高代码的清晰度和可维护性</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> __CUDACC__   <span class="comment">//标识正在编译的代码为 CUDA 程序，以使编译器可以启用与 CUDA 相关的特性和指令</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> SQUARE(x) ((x) * (x))  </span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> a = <span class="number">5</span>;  </span><br><span class="line"><span class="type">int</span> b = SQUARE(a); <span class="comment">// 此时 b 将被替换为 ((5) * (5))，即 25</span></span><br></pre></td></tr></table></figure><p>在使用CUDA和C++混合编译实现功能模块时，宏定义的主要作用是进行数据的形状检查或张量检查，防止运行过程中出现问题，如下：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// macro definition</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), <span class="string">&quot;data must be a CUDA Tensor.&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CONTIGUOUS(x) AT_ASSERTM(x.is_contiguous(), <span class="string">&quot;data must be contiguous.&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x) <span class="comment">//定义了一个名为 CHECK_INPUT 的宏。当使用这个宏时，它会依次检查传入参数 x 的两个条件：1.是否是GPU上的张量；2.是否是连续存储的张量</span></span></span><br></pre></td></tr></table></figure><h3 id="宏-at_dispatch_floating_types-和-at_dispatch_all_types">宏 AT_DISPATCH_FLOATING_TYPES 和 AT_DISPATCH_ALL_TYPES</h3><details><summary>参考链接</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/m0_63471305/article/details/140463952&quot;&gt;AT_DISPATCH_FLOATING_TYPES 和AT_DISPATCH_ALL_TYPES 的作用&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/48463543&quot;&gt;Pytorch-CUDA从入门到放弃（二）&lt;/a&gt;&lt;/p&gt;</code></pre></details><p>该宏一般在 .cu 文件中的定义函数里出现，用于解决global function (kernal function) 的输入输出问题，<strong>将 ATEN 声明的 Tensor 转化成global function可以接受的类型</strong>。这是因为ATEN 的数据（CUDA Tensor）和 global function 的计算结果都在 GPU 中。因此不涉及到拷贝或是公共内存的问题。唯一需要考虑的是，ATEN 的数据数据类型和 global function 不同。</p><p>假设我们有一个简单的 CUDA 内核函数 <code>example_kernel</code>，它对输入张量进行某种操作。</p><p>（一）AT_DISPATCH_FLOATING_TYPES</p><p>如果希望希望kernal function可以处理 <code>float</code> 和 <code>double</code> 类型的数据，ATEN 为我们提供了接口函数 AT_DISPATCH_FLOATING_TYPES。这个函数接收三个参数，<strong>第一个参数</strong>是输入数据的源类型，<strong>第二个参数</strong>是操作的标识符（用于报错显示），<strong>第三个参数</strong>是一个匿名函数。在匿名函数运行结束后，AT_DISPATCH_FLOATING_TYPES 会将 Float 数组转化为目标类型（运行中的实际类型）数组。</p><p>AT_DISPATCH_FLOATING_TYPES 中的匿名函数中可以使用 scalar_t 代指目标类型。而 ATEN 支持我们使用 Tensor.data<类型> 将 Tensor.data 转换为某个类型。因此，可以这样转换：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(input.<span class="built_in">type</span>(), <span class="string">&quot;example_forward_cuda&quot;</span>, ([&amp;] &#123;</span><br><span class="line">  example_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(</span><br><span class="line">      input.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(), size);</span><br><span class="line">&#125;));</span><br></pre></td></tr></table></figure><p>这里，如果只需要读取数据，使用 <code>.data()</code> 函数即可；如果涉及到修改数据，需使用 <code>.data_ptr()</code> 函数。</p><p>（二）AT_DISPATCH_FLOATING_TYPES_AND_HALF</p><p>如果希望希望kernal function可以处理<code>float</code>、<code>double</code> 和 <code>half</code>类型的数据，ATEN 为我们提供了接口函数 AT_DISPATCH_FLOATING_TYPES_AND_HALF。</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">AT_DISPATCH_FLOATING_TYPES_AND_HALF</span>(input.<span class="built_in">type</span>(), <span class="string">&quot;example_forward_cuda&quot;</span>, ([&amp;] &#123;</span><br><span class="line">  example_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(</span><br><span class="line">      input.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(), size);</span><br><span class="line">&#125;));</span><br></pre></td></tr></table></figure><p>（三）AT_DISPATCH_ALL_TYPES</p><p>如果希望希望kernal function可以处理所有基础数据类型（例如 <code>int</code>, <code>float</code>, <code>double</code> 等），ATEN 为我们提供了接口函数AT_DISPATCH_ALL_TYPES。</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">AT_DISPATCH_ALL_TYPES</span>(input.<span class="built_in">scalar_type</span>(), <span class="string">&quot;example_forward_cuda&quot;</span>, ([&amp;] &#123;</span><br><span class="line">  example_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(</span><br><span class="line">      input.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(), size);</span><br><span class="line">&#125;));</span><br></pre></td></tr></table></figure><p>写到完整的函数中就是：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// example.cpp</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">std::vector&lt;torch::Tensor&gt; <span class="title">example_forward</span><span class="params">(torch::Tensor input)</span> </span>&#123;</span><br><span class="line">  <span class="type">const</span> <span class="keyword">auto</span> size = input.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">  <span class="keyword">auto</span> output = torch::<span class="built_in">zeros_like</span>(input);</span><br><span class="line"></span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> threads = <span class="number">1024</span>;</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> blocks = (size + threads - <span class="number">1</span>) / threads;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">AT_DISPATCH_ALL_TYPES</span>(input.<span class="built_in">scalar_type</span>(), <span class="string">&quot;example_forward_cuda&quot;</span>, ([&amp;] &#123;</span><br><span class="line">    example_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(</span><br><span class="line">        input.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(), size);</span><br><span class="line">  &#125;));</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> &#123;output&#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//下面部分一般写在CPP文件中</span></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) &#123;</span><br><span class="line">  m.<span class="built_in">def</span>(<span class="string">&quot;forward&quot;</span>, &amp;example_forward, <span class="string">&quot;Example forward&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="数据类型">数据类型</h2><p>当不知道输入的数据是何类型时，可以使用“模板”。如下，定义一个模板，<code>scalar_t</code>是一个通用数据类型，可以是浮点数（如<code>float</code>或<code>double</code>）。</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">scalar_t</span> <span class="title">sigmoid</span><span class="params">(<span class="type">scalar_t</span> z)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + <span class="built_in">exp</span>(-z));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="cuda修饰符">CUDA修饰符</h2><details><summary>参考链接</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_39244242/article/details/135982330&quot;&gt;CUDA：修饰符_cuda global-CSDN博客&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/zdlnlhmj/article/details/104896470&quot;&gt;cuda 函数前缀 __host__ __device__ __global__ ____noinline__ 和 __forceinline__ 简介_force inline-CSDN博客&lt;/a&gt;&lt;/p&gt;</code></pre></details><p>CUDA 是建立在 NVIDIA GPU上的一个通用并行计算平台和编程模型，CUDA编程可以利用GPUs 的并行计算引擎来更加高效地解决比较复杂的计算难题。CUDA 的语法和 C++ 大多部分情况下是一致的，其默认文件名后缀是 .cu，默认头文件名后缀是 .cuh。CUDA 编程是异构的，即CPU负责处理逻辑复杂的串行程序，而 GPU 重点处理数据密集型的并行计算程序，从而发挥最大功效。其中 CPU 所在位置称为为主机端（host），而 GPU 所在位置称为设备端（device）。</p><p>因此当使用CUDA进行编程时，存在两种类型的函数：主机函数和设备函数。主机函数是在CPU上执行的，而设备函数是在GPU上执行的。我们可以在函数前加上修饰符，来表明函数是否可在设备上执行。</p><p>这些修饰符在官方的文档里被称为函数执行环境标识符Function execution space specifiers，也就是它们指明了这段函数是在哪里被调用的。</p><p><span style="background:#daf5e9;"><strong>global</strong></span></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">__global__  <span class="comment">//表示这是一个CUDA核函数，可以从主机代码调用，并在设备（GPU）上并行执行；在被调用时必须通过&lt;&lt;&lt;&gt;&gt;&gt;或cudaLaunchDevice等方式指定执行配置。</span></span><br></pre></td></tr></table></figure><ul><li>在 CPU 调用，GPU 执行。且修饰的函数是核函数。</li><li>修饰的函数必须采用<code>void</code>返回值，并且需要在调用时指定 <strong><em>运行的参数</em></strong> (也就是&lt;&lt;&lt;&gt;&gt;&gt;里的block数和线程数)</li></ul><p><span style="background:#daf5e9;"><strong>device</strong></span></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">__device__  <span class="comment">//表示这个函数在GPU设备上执行，但只能在其他__device__函数或者__global__函数中调用。</span></span><br></pre></td></tr></table></figure><p>修饰符使用示例：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">__device__ <span class="type">float</span> <span class="title">deviceFunction</span><span class="params">(<span class="type">float</span> a, <span class="type">float</span> b)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> a + b;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernelFunction</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    <span class="type">float</span> result = <span class="built_in">deviceFunction</span>(<span class="number">1.0f</span>, <span class="number">2.0f</span>);  </span><br><span class="line">    <span class="comment">// ... 其他计算 ...  </span></span><br><span class="line">&#125;</span><br><span class="line">``</span><br><span class="line"></span><br><span class="line">在上面的示例中，deviceFunction 是一个 __device__ 函数，只能在GPU上执行。它被包含在 kernelFunction 中，作为该核函数的一部分。</span><br></pre></td></tr></table></figure><ul><li>在 GPU 调用，GPU 执行。</li><li>使用 <code>__device__</code> 修饰符的函数只能被包含在 .cu 文件中，并且不能在主机代码中直接调用。</li><li><code>__global___</code>和<code>__device__</code>不能同时使用，但是 <code>__device__</code> 函数可以被包含在<code>__global___</code>函数中。</li></ul><p><span style="background:#daf5e9;"><strong>host</strong></span></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">__host__   <span class="comment">//用于标识某个函数只能在主机（CPU）上执行且只能在主机端调用。</span></span><br></pre></td></tr></table></figure><p><code>__host__</code>定义了主机端的函数，就是在主机端执行，在主机端调用的函数，也就是我们正常的c/c++的函数，如果不加任何的修饰符，默认就是<code>__host__</code>函数，这些函数只为主机端编译。</p><ul><li>在 CPU 调用，CPU 执行。</li><li><code>__device__</code>和<code>__host__</code>可以同时使用，这个函数会同时为主机端和设备端编译。</li><li><code>__global__</code>和<code>__host__</code>不能同时使用。</li></ul><h2 id="常见头文件">常见头文件</h2><p><strong>#include&lt;ATen/ATen.h&gt;</strong></p><p><code>ATen</code>是PyTorch的一个核心库，提供了许多用于张量操作和数学计算的功能。ATEN 是一个 Tensor 库，它将数组封装为一个 Tensor 类（就像 numpy 把数组封装成 nparray）。它在 CPU 和 GPU 上，为我们提供了创建数组和操作数组的方法（没错，和 Pytorch 中的 Tensor 一样）。</p><p>使用例子：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//使用其命名空间</span></span><br><span class="line"><span class="function">at::Tensor tensorX</span></span><br><span class="line"><span class="function"><span class="title">at::zeros_like</span><span class="params">(tensorX)</span></span></span><br><span class="line"><span class="function"><span class="title">at::ones_like</span><span class="params">(tensorX)</span></span></span><br><span class="line"><span class="function">    </span></span><br><span class="line"><span class="function"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;ATen/ATen.h&gt;</span></span></span></span><br><span class="line"><span class="function"><span class="comment">// 声明两个 Tensor 并相加</span></span></span><br><span class="line"><span class="function">at::Tensor a </span>= at::<span class="built_in">randn</span>(&#123;<span class="number">2</span>, <span class="number">2</span>&#125;, at::kInt);</span><br><span class="line">at::Tensor b = at::<span class="built_in">randn</span>(&#123;<span class="number">2</span>, <span class="number">2</span>&#125;, at::kInt);</span><br><span class="line"><span class="keyword">auto</span> c = a + b;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 GPU 声明两个 Tensor 并相加</span></span><br><span class="line">at::Tensor a = <span class="built_in">CUDA</span>(at::kFloat).<span class="built_in">ones</span>(&#123;<span class="number">3</span>, <span class="number">4</span>&#125;);</span><br><span class="line">at::Tensor b = <span class="built_in">CUDA</span>(at::kFloat).<span class="built_in">zeros</span>(&#123;<span class="number">3</span>, <span class="number">4</span>&#125;);</span><br><span class="line"><span class="keyword">auto</span> c = a + b;</span><br></pre></td></tr></table></figure><h1 id="cuda-编程">CUDA 编程</h1><details><summary>参考链接</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_42483745/article/details/127221340&quot;&gt;【CUDA和C++混合编译实现Python扩展】入门首推！&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/JIANGYINGH/article/details/124686645&quot;&gt;cuda基础（二）简单CUDA程序的基本框架和自定义设备函数&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/qq_42722197/article/details/134432550?spm=1001.2101.3001.6650.18&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-18-134432550-blog-127221340.235%5Ev43%5Epc_blog_bottom_relevance_base1&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-18-134432550-blog-127221340.235%5Ev43%5Epc_blog_bottom_relevance_base1&amp;utm_relevant_index=26&quot;&gt;PyTorch 源码解读之 ：揭秘 C++/CUDA 算子实现和调用全流程&lt;/a&gt;&lt;/p&gt;</code></pre></details><h2 id="程序设计流程">程序设计流程</h2><p>一般而言，CUDA 程序执行会依照如下流程：</p><ol type="1"><li>分配 host 内存，并进行数据初始化</li><li>分配 device 内存，并从 host 将数据拷贝到 device 上</li><li>调用 CUDA 的核函数在 device 上完成指定的运算</li><li>将 device 上的运算结果拷贝到 host 上</li><li>释放 device 和 host 上分配的内存</li></ol><p>而对 PyTorch 的 CUDA 扩展来说， CUDA 扩展传入和传出的 Tensor 都已经在 GPU 上，因此这里的 5 个步骤只有第 3 步了，这会为我们省下比较宝贵的时间而将更多注意力放到具体的程序实现上。</p><h2 id="文件结构">文件结构</h2><p>实现CUDA和C++的混合编译，并能够被Python调用，执行相应的功能主要需要4个文件：</p><p><strong>（一） CPP文件</strong> 在CPP文件中主要完成4个功能。</p><p>1）对要调用的CUDA函数进行声明，这里提到的CUDA函数的定义在cu文件中； 2）完成一些宏定义，这些宏定义主要作用是为了进行数据的形状检查或张量检查，防止运行过程中出现问题； 3）定义Python调用扩展功能的函数接口，也就是CPP文件中定义的C++函数； 4）PYBIND11_MODULE(TORCH_EXTENSION_NAME)函数封装Python调用扩展函数的C++接口。</p><p><strong>（二）CU文件</strong> 在CU文件中一般包括2个文件。</p><p>1）对CPP文件中的<strong>声明的CUDA函数</strong>进行定义；在该函数中一般会通过 <code>AT_DISPATCH_FLOATING_TYPES</code> 等宏调用真正实现模块逻辑功能的kernel函数； 2）定义实现模块真正逻辑功能的<strong>kernel函数</strong>，kernal函数中一般还会写有线程索引的计算。通过索引，利用CUDA的高效并行能力进行函数实现。此外，<strong>kernal 函数一般是global的</strong>。</p><p>注：在实际项目中，一般这种编译的扩展文件都会被集中放在一个名为src的文件夹中，在这个src文件夹中通常根据不同的功能包含很多的xxx文件夹，比如本文扩展想要实现sigmoid()函数的运算，那么CPP文件和CUDA文件就会放在sigmoid文件夹下。 那如何实现各个功能模块的编译呢，就是靠下面的setup.py文件啦~</p><p><strong>（三）setup.py文件</strong> 实现CPP文件和CUDA文件的混合编译。</p><p><strong>（四）test.py文件</strong> 在Python代码中实现对扩展的调用，并将相应功能封装成模块，定义相应的forward()函数和backward()函数，实现网络的前向和反向传播。</p><h2 id="核函数中能使用的数学函数">核函数中能使用的数学函数</h2><details><summary>参考链接</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_42034217/article/details/114108008?spm=1001.2101.3001.6650.18&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-18-114108008-blog-105394671.235%5Ev43%5Epc_blog_bottom_relevance_base1&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-18-114108008-blog-105394671.235%5Ev43%5Epc_blog_bottom_relevance_base1&amp;utm_relevant_index=23&quot;&gt;Cuda的数学函数最大值、最小值、次方、平方根&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/tuibianhuaisheng/article/details/110087798?spm=1001.2101.3001.6650.3&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-3-110087798-blog-127983389.235%5Ev43%5Epc_blog_bottom_relevance_base1&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-3-110087798-blog-127983389.235%5Ev43%5Epc_blog_bottom_relevance_base1&amp;utm_relevant_index=6&quot;&gt;CUDA中的数学函数&lt;/a&gt;&lt;/p&gt;</code></pre></details><p>cuda程序中核函数中肯定不能用math.h下的函数了，因为那些函数是CPU下函数，在核函数中，cuda有自己的数学运算接口，在#include &lt;thrust/extrema.h&gt;下。如果是对int操作，直接使用本名，如果操作long类型对数，在本名前加l，如果操作双精度浮点数，在本名前加f，如果操作单精度，在本名前后都加上f。</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">max</span><span class="params">(<span class="type">int</span>, <span class="type">int</span>)</span></span>; <span class="comment">//对int类型操作</span></span><br><span class="line"><span class="function"><span class="type">long</span> <span class="title">lmax</span><span class="params">(<span class="type">long</span>,<span class="type">long</span>)</span></span>;   <span class="comment">//对long类型操作</span></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">fmax</span><span class="params">(<span class="type">double</span>,<span class="type">double</span>)</span></span>; <span class="comment">//对double类型操作</span></span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">fmaxf</span><span class="params">(<span class="type">float</span>,<span class="type">float</span>)</span></span>;<span class="comment">//对float操作</span></span><br></pre></td></tr></table></figure><p>官方使用手册见：<a href="https://blog.csdn.net/qq_45779334/article/details/127983389">CUDA——可用于核函数中的CUDA基本数学运算函数总结_cuda math-CSDN博客</a></p><p>详细中文版说明见：<a href="https://zhuanlan.zhihu.com/p/678283126">【CUDA编程】数学函数（Mathematical Functions） - 知乎</a></p><h2 id="编译安装">编译安装</h2><p>setup.py 文件</p><p>test.py 文件</p><p>代码运行</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python setup.py install  <span class="comment">#编译安装 CUDA 扩展</span></span><br><span class="line">python test.py  <span class="comment">#测试扩展库是否运行良好</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 加速算法 </tag>
            
            <tag> 代码箱 </tag>
            
            <tag> 编码 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据预处理：归一化</title>
      <link href="/2025/01/08/%E5%BD%92%E4%B8%80%E5%8C%96%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
      <url>/2025/01/08/%E5%BD%92%E4%B8%80%E5%8C%96%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<details><summary>点击展开参考链接</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/m0_71212744/article/details/140457695&quot;&gt;一文搞懂什么是归一化，以及几种常用的归一化方法（超详细解读，其实也不太详细）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/Next_SummerAgain/article/details/127321209&quot;&gt;【机器学习】数据归一化全方法总结：Max-Min归一化、Z-score归一化、数据类型归一化、标准差归一化等&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/ytusdc/article/details/128504272&quot;&gt;归一化 （Normalization）、标准化 （Standardization）和中心/零均值化 （Zero-centered）（这链接太棒了）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/424518359&quot;&gt;如何理解归一化（normalization）?&lt;/a&gt;&lt;/p&gt;</code></pre></details><h3 id="定义">定义</h3><p>数据归一化处理是数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。数据归一化的手段一般是<strong>将特征数据按比例压缩到相同的尺度</strong>，即使之落入一个小的特定区间，通常是[0, 1]或[-1, 1]。</p><h3 id="归一化的作用">归一化的作用</h3><p>1.平衡各特征的贡献</p><p>一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。</p><p>如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。那么这时候我们可以通过最小-最大值归一化将其缩放到相同的尺度[0,1]之间，消除值域的影响。</p><p>2.提升模型的收敛速度</p><p>在使用梯度下降的方法求解最优化问题时， 如左图所示，未归一化/标准化时形成的等高线偏椭圆，迭代时很有可能走“之”字型路线（垂直长轴），从而导致迭代很多次才能收敛。而如右图对两个特征进行了零均值归一化，对应的等高线就会相对变圆，在梯度下降进行求解时能较快的收敛。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20241128105529814.png" alt="image-20241128105529814" /><figcaption aria-hidden="true">image-20241128105529814</figcaption></figure><h3 id="什么时候需要对数据归一化">什么时候需要对数据归一化</h3><p>1、涉及或隐含距离计算的算法，比如K-means、KNN、PCA、SVM等，一般需要进行归一化。 2、梯度下降算法，梯度下降的收敛速度取决于：参数的初始位置到local minima的距离，以及学习率η的大小，其实还是距离的计算。 3、采用sigmoid等有饱和区的激活函数，如果输入分布范围很广，参数初始化时没有适配好，很容易直接陷入饱和区，导致梯度消失，所以才会出现各种BN，LN等算法。</p><p>注：神经网络求解一般要进行归一化，否则很有可能导致数值不稳定或梯度爆炸/消失问题，详细原因见<a href="https://blog.csdn.net/ytusdc/article/details/128504272">链接</a>。</p><h3 id="什么时候不需要归一化">什么时候不需要归一化</h3><p>与距离计算无关的概率模型不需要，比如Naive Bayes。因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率；</p><p>与距离计算无关基于树的模型，比如决策树、随机森林等，树中节点的选择只关注当前特征在哪里切分对分类更好，即只在意特征内部的相对大小，而与特征间的相对大小无关。但是使用Z-Score归一化会提高模型的准确率。其实归一化的作用就是由绝对变为了相对，所以可以说归一化对于树型模型不那么重要，是一个可选项或者说可以作为一个超参数在训练时进行选择。</p><h4 id="线一min-max-归一化">线一、Min-Max 归一化</h4><p>最小-最大值归一化也称 <code>线性归一化</code> ，是最简单的归一化方法之一。它将数据缩放到 [0,1] 区间内。其公式如下: <span class="math display">\[X_{norm} = \frac{X-X_{min}}{X_{max} - X_{min}}\]</span> 该方法能实现对数据的等比例缩放，但是由于极值化方法在对变量无量纲化过程中仅仅与该变量的最大值和最小值这两个极端值有关，而与其他取值无关，这使得<u>该方法在改变各变量权重时过分依赖两个极端取值</u>。</p><p><span style="background:#daf5e9;"><strong>应用场景</strong></span>：最小-最大归一化方法适用于数值比较集中、不涉及距离度量的情况，实际使用中常用经验常量来替代max和min。比如图像处理中麻将RGB图像转为灰度图后将值限定在[0,255]范围内。</p><p>此外，最小-最大值归一化还有一个变种<span style="background:#fbd4d0;"><strong>mean normalization</strong></span> ，该变种将数据分布中心移到原点，其取值范围为 [-1,1]，公式如下： <span class="math display">\[X_{norm} = \frac{X-X_{mean}}{X_{max} - X_{min}}\]</span> mean 归一化的过程可以可视化如下：可以看到 mean 归一化先将数据分布移到原点，然后缩放，并不改变数据分布的形状。</p><p><img src="C:\Users\21218\AppData\Roaming\Typora\typora-user-images\image-20241128143734870.png" alt="image-20241128143734870" style="zoom:67%;" /></p><h4 id="线二z-score-归一化标准化">线二、Z-Score 归一化（标准化）</h4><blockquote><p><strong>中心化 / 零均值化（zero-centered）</strong></p><p>中心化，也叫零均值处理，就是将每个原始数据减去这些数据的均值。因此 中心化后的数据平均值为0，对标准差无要求。</p></blockquote><p>Z-Score 归一化也称 <code>零均值归一化</code> 或 <code>标准化</code>，它将数据转化为均值为0，标准差为1的分布（<strong>注意！转化后的分布并非一定是正态的</strong>；除非原始分布是正态分布，那么转换后为标准正态分布；如果希望数据分布服从正态分布，可以通过<a href="https://blog.csdn.net/WHYbeHERE/article/details/108049720">链接</a>中的方法先转化，然后再通过标准化得到标准正态分布）。归一化后数据的取值可以是(-∞，+∞)，其公式如下： <span class="math display">\[X_{norm} = \frac{X-\mu}{\sigma}\]</span> 虽然<u>该方法在无量纲化过程中利用了所有的数据信息</u>，但是该方法在无量纲化后不仅使得转换后的各特征变量均值相同，且标准差也相同，即无量纲化的同时还消除了各特征变量在变异程度上的差异，从而转换后的各特征在聚类分析中的重要性程度是同等看待的（权值的丢失）。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20241128103342397.png" alt="image-20241128103342397" /><figcaption aria-hidden="true">image-20241128103342397</figcaption></figure><p>上图中左图是原图可视化，中图是减均值后的可视化（将数据分布中心移到原点），右图是除以标准差后的可视化（使样本数据近似为某种分布，通常为近正态分布）。</p><p>对于离群点较多的数据，可以应用类似于标准化的变种<span style="background:#fbd4d0;"> <strong>Robust Scaling</strong> </span>来减少异常数据的影响。它使用中位数和四分位距来缩放，适用于对异常值较为敏感的情况。其计算公式为： <span class="math display">\[X_{norm}=\frac{X - median}{Q_3(X)-Q_1(X)}\]</span> <strong>应用场景</strong>：在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，标准化比最小最大归一化比表现更好。在带有的离群值较多的数据时，推荐使用RobustScaler。</p><p><strong>注意事项</strong>：最大的注意事项就是先拆分出test集，只在训练集上标准化，即均值和标准差是从训练集中计算出来的，不要在整个数据集上做标准化，因为那样会将test集的信息引入到训练集中，造成了数据信息泄露，这是一个非常容易犯的错误。</p><h5 id="附最小-最大归一化-vs-标准化">附：最小-最大归一化 vs 标准化</h5><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20241128104258910.png" alt="image-20241128104258910" /><figcaption aria-hidden="true">image-20241128104258910</figcaption></figure><p>来自<a href="https://blog.csdn.net/ytusdc/article/details/128504272">链接</a>博主的建议：</p><p>如果<span style="background:#FFCC99;">对输出结果范围有要求</span>，用最小-最大值归一化。 如果<span style="background:#FFCC99;">数据较为稳定，不存在极端的最大最小值</span>，用最小-最大值归一化。 如果<span style="background:#FFCC99;">数据存在异常值和较多噪音</span>，用标准化，可以间接通过中心化避免异常值和极端值的影响。 某知乎博主分享了他个人经验：一般来说，建议优先使用标准化。对于输出有要求时再尝试别的方法，如归一化或者更加复杂的方法。很多方法都可以将输出范围调整到[0, 1]，如果我们对于数据的分布有假设的话，更加有效的方法是使用相对应的概率密度函数来转换。</p><blockquote><p>标准化代码</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;x_standardized = (x-mean(x)) / std(x)</span><br></pre></td></tr></table></figure></blockquote><h4 id="线三l2范数归一化">线三、L2范数归一化</h4><p>顾名思义，即将特征向量中的每个元素除以该向量的L2范数 <span class="math inline">\(norm(X)\)</span>，公式为： <span class="math display">\[X_{norm} =(\frac{x_1}{norm(X)},\frac{x_2}{norm(X)},...,\frac{x_n}{norm(X)})^T\]</span> 在L2范数归一化后，特征向量的欧氏范数 / 模长为 1。</p><h4 id="非线一对数变换归一化">非线一、对数变换归一化</h4><p>在实际工程中，经常会有类似点击次数/浏览次数的特征，<u>这类特征是长尾/偏态分布的</u>，可以将其用对数函数进行压缩。特别地，在特征相除时，可以用对数压缩之后的特征相减得到。其公式如下： <span class="math display">\[X_{norm} = - \log (X+1)\]</span> 其实这也是将<a href="https://blog.csdn.net/liuerin/article/details/100154809">偏态分布</a>转换为正态分布的一种方法，更多说明可以参考<a href="https://blog.csdn.net/WHYbeHERE/article/details/108049720">链接</a>。</p><h4 id="非线二z-score-简单化">非线二、Z-Score 简单化</h4><p>Z-Score 简单化将数据规范在[0,1]之间，归一化后的数据与原数据成反比（原数据越小，归一化后越大）。其公式如下： <span class="math display">\[X_{norm} = \frac{1}{1+X}\]</span> #### 非线三、反正切函数归一化</p><p>使用这个方法需要注意的是如果想映射的区间为[0，1]，则数据都应该大于等于0，小于0的数据将被映射到[－1，0]区间上。公式为： <span class="math display">\[X_{norm} = \frac{2}{\pi}\arctan X\]</span></p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代码箱 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像的读取、保存和显示</title>
      <link href="/2024/12/08/%E5%9B%BE%E5%83%8F%E7%9A%84%E8%AF%BB%E5%8F%96%E3%80%81%E4%BF%9D%E5%AD%98%E5%92%8C%E6%98%BE%E7%A4%BA/"/>
      <url>/2024/12/08/%E5%9B%BE%E5%83%8F%E7%9A%84%E8%AF%BB%E5%8F%96%E3%80%81%E4%BF%9D%E5%AD%98%E5%92%8C%E6%98%BE%E7%A4%BA/</url>
      
        <content type="html"><![CDATA[<details><summary>参考链接</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/youcans/article/details/121169014&quot;&gt;【OpenCV 例程300篇】02. 图像的保存（cv2.imwrite）-CSDN博客&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_45954454/article/details/114707888&quot;&gt;四种方法完美解决plt.imshow显示cv2.imread读取的图像有色差发蓝的问题&lt;/a&gt;&lt;/p&gt;</code></pre></details><h2 id="cv库操作">cv库操作</h2><p>使用cv2.imread()读取图像时，默认彩色图像的三通道顺序为B、G、R。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取图片</span></span><br><span class="line">img = cv.imread(<span class="string">&#x27;path/to/img&#x27;</span>)</span><br><span class="line"><span class="comment">###############</span></span><br><span class="line"><span class="comment">#如果图片路径中有中文</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(pic_name,  <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    image_data = f.read()</span><br><span class="line">    img = cv2.imdecode(np.frombuffer(image_data, np.uint8), cv2.IMREAD_COLOR)</span><br><span class="line"><span class="keyword">if</span> img <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">raise</span> FileNotFoundError(<span class="string">f&quot;无法读取图像: <span class="subst">&#123;pic_name&#125;</span>，请检查路径是否正确&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">#显示图片</span></span><br><span class="line">cv.imshow(<span class="string">&#x27;img_window&#x27;</span>, img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#plt.imshow(img) 也可，但是要先调换通道顺序,否则图片会发蓝</span></span><br><span class="line">img = cv.cvtColor(img, cv.COLOR_BGR2RGB)</span><br><span class="line">plt.imshow(img)</span><br><span class="line"></span><br><span class="line"><span class="comment">#保存图片</span></span><br><span class="line">cv.imwrite(<span class="string">&#x27;path/to/img&#x27;</span>, img)</span><br></pre></td></tr></table></figure><blockquote><p><strong>调换BGR通道顺序为RGB通道顺序还有以下方法</strong></p><p>法1：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">&#x27;dog.jpg&#x27;</span>)<span class="comment">#读取通道顺序为B、G、R</span></span><br><span class="line">b,g,r = cv2.split(img)<span class="comment">#分别提取B、G、R通道</span></span><br><span class="line">img_new1 = cv2.merge([r,g,b])<span class="comment">#重新组合为R、G、B</span></span><br><span class="line"></span><br><span class="line">plt.xticks([]), plt.yticks([]) <span class="comment"># 隐藏x和y轴</span></span><br><span class="line"></span><br><span class="line">plt.imshow(img_new1)</span><br></pre></td></tr></table></figure><p>法2：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">&#x27;dog.jpg&#x27;</span>)<span class="comment">#读取通道顺序为B、G、R</span></span><br><span class="line"><span class="comment">#img[:,:,0]表示图片的蓝色通道，对一个字符串s进行翻转用的是s[::-1]，同样img[:,:,::-1]就表示BGR通道翻转，变成RGB</span></span><br><span class="line">img_new2 = img[:, :, ::-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">plt.xticks([]), plt.yticks([]) <span class="comment"># 隐藏x和y轴</span></span><br><span class="line"></span><br><span class="line">plt.imshow(img_new2)</span><br></pre></td></tr></table></figure></blockquote><h2 id="matplotlib库操作">matplotlib库操作</h2><p>使用plt.imshow()函数默认显示图像的通道顺序为R、G、B。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#显示图片</span></span><br><span class="line">plt.imshow(array, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#保存图片</span></span><br><span class="line">plt.savefig(<span class="string">&#x27;output_img.png&#x27;</span>)</span><br><span class="line">plt.close</span><br></pre></td></tr></table></figure><p>plt.imshow()函数用于显示图像,其数据格式可以是numpy数组、PIL图像或者是matplotlib的图像。如果是numpy数组,则需要满足以下条件:</p><ol type="1"><li>数据类型为 <strong>float</strong>（范围 0-1 ）或者 <strong>uint8</strong>（范围 0-255 ）</li><li>形状为(M,N)或者(M,N,3)或者(M,N,4),其中M和N分别表示图像的高和宽,3表示RGB三通道,4表示RGBA四通道。</li></ol><blockquote><p>如果用plt.imshow显示灰度图出现色差，可以用以下两个方法解决</p><p>法1：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">&#x27;dog.jpg&#x27;</span>)</span><br><span class="line">img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)<span class="comment">#转换为灰度图</span></span><br><span class="line"></span><br><span class="line">plt.xticks([]), plt.yticks([]) <span class="comment"># 隐藏x和y轴</span></span><br><span class="line"></span><br><span class="line">plt.imshow(img_gray,cmap=<span class="string">&#x27;gray&#x27;</span>) <span class="comment">#重点！设置cmap参数</span></span><br></pre></td></tr></table></figure><p>法2：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#cv2.imread()第二参数为0，直接将彩色图像转换为灰度图像</span></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;dog.jpg&#x27;</span>,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.xticks([]), plt.yticks([]) <span class="comment"># 隐藏x和y轴</span></span><br><span class="line"></span><br><span class="line">plt.imshow(img_gray,cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br></pre></td></tr></table></figure></blockquote><p>如果是PIL图像,则需要满足以下条件:</p><ol type="1"><li>图像模式为RGB或者RGBA</li><li>图像大小为(M,N)</li></ol><h2 id="pil库操作">PIL库操作</h2><blockquote><p>一个将PIL图像变量转换为张量的函数：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">PILtoTorch</span>(<span class="params">pil_image, resolution</span>):</span><br><span class="line">       resized_image_PIL = pil_image.resize(resolution)</span><br><span class="line">       <span class="comment">#resized_image = torch.from_numpy(np.array(resized_image_PIL)) / 255.0</span></span><br><span class="line">       resized_image_PIL =np.array(resized_image_PIL).astype(<span class="built_in">float</span>)  <span class="comment"># numpy强制类型转换,否则在 cuda 11.7 环境会报错</span></span><br><span class="line">       resized_image = torch.from_numpy(resized_image_PIL) / <span class="number">255.0</span> <span class="comment">#将值归一化（可选）</span></span><br><span class="line">        </span><br><span class="line">       <span class="keyword">if</span> <span class="built_in">len</span>(resized_image.shape) == <span class="number">3</span>:</span><br><span class="line">           <span class="keyword">return</span> resized_image.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>) <span class="comment">#令张量第一维是颜色通道</span></span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">           <span class="keyword">return</span> resized_image.unsqueeze(dim=-<span class="number">1</span>).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#读取图片</span></span><br><span class="line">PILimg = Image.<span class="built_in">open</span>(<span class="string">&#x27;path/to/img&#x27;</span>)</span><br><span class="line">tensor = PILtoTorch(PILimg, (<span class="number">1</span>,<span class="number">1</span>))  <span class="comment">#1.读取为张量</span></span><br><span class="line"></span><br><span class="line">array = np.array(Image.<span class="built_in">open</span>(<span class="string">&#x27;path/to/img&#x27;</span>)) <span class="comment">#2.读取为np数组</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#显示图片</span></span><br><span class="line">PILimg.show()</span><br><span class="line"><span class="built_in">print</span>(PILimg.<span class="built_in">format</span>, PILimg.size, PILimg.mode) <span class="comment"># 打印图片信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#保存图片</span></span><br><span class="line">array = array.astype(np.uint8)</span><br><span class="line">image = Image.fromarray(array)</span><br><span class="line">image.save(<span class="string">&#x27;output_img.png&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>附plt常见颜色表：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/614207746ddbca525b0b823b7beb1087.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><details><summary>linestyle可选参数</summary><p>'-' solid line style</p><p>'--' dashed line style</p><p>'-.' dash-dot line style</p><p>':' dotted line style</p></details><details><summary>marker可选参数</summary><p>'.'       point marker<br />','       pixel marker<br />'o'       circle marker<br />'v'       triangle_down marker<br />'^'       triangle_up marker<br />'&lt;'       triangle_left marker<br />'&gt;'       triangle_right marker<br />'1'       tri_down marker<br />'2'       tri_up marker<br />'3'       tri_left marker<br />'4'       tri_right marker<br />'s'       square marker<br />'p'       pentagon marker<br />'*'       star marker<br />'h'       hexagon1 marker<br />'H'       hexagon2 marker<br />'+'       plus marker<br />'x'       x marker<br />'D'       diamond marker<br />'d'       thin_diamond marker<br />'|'       vline marker<br />'_'       hline marker</p></details><details><summary>color名称与十六进制代码对应</summary><p>cnames = {<br />'aliceblue':            '#F0F8FF',<br />'antiquewhite':         '#FAEBD7',<br />'aqua':                 '#00FFFF',<br />'aquamarine':           '#7FFFD4',<br />'azure':                '#F0FFFF',<br />'beige':                '#F5F5DC',<br />'bisque':               '#FFE4C4',<br />'black':                '#000000',<br />'blanchedalmond':       '#FFEBCD',<br />'blue':                 '#0000FF',<br />'blueviolet':           '#8A2BE2',<br />'brown':                '#A52A2A',<br />'burlywood':            '#DEB887',<br />'cadetblue':            '#5F9EA0',<br />'chartreuse':           '#7FFF00',<br />'chocolate':            '#D2691E',<br />'coral':                '#FF7F50',<br />'cornflowerblue':       '#6495ED',<br />'cornsilk':             '#FFF8DC',<br />'crimson':              '#DC143C',<br />'cyan':                 '#00FFFF',<br />'darkblue':             '#00008B',<br />'darkcyan':             '#008B8B',<br />'darkgoldenrod':        '#B8860B',<br />'darkgray':             '#A9A9A9',<br />'darkgreen':            '#006400',<br />'darkkhaki':            '#BDB76B',<br />'darkmagenta':          '#8B008B',<br />'darkolivegreen':       '#556B2F',<br />'darkorange':           '#FF8C00',<br />'darkorchid':           '#9932CC',<br />'darkred':              '#8B0000',<br />'darksalmon':           '#E9967A',<br />'darkseagreen':         '#8FBC8F',<br />'darkslateblue':        '#483D8B',<br />'darkslategray':        '#2F4F4F',<br />'darkturquoise':        '#00CED1',<br />'darkviolet':           '#9400D3',<br />'deeppink':             '#FF1493',<br />'deepskyblue':          '#00BFFF',<br />'dimgray':              '#696969',<br />'dodgerblue':           '#1E90FF',<br />'firebrick':            '#B22222',<br />'floralwhite':          '#FFFAF0',<br />'forestgreen':          '#228B22',<br />'fuchsia':              '#FF00FF',<br />'gainsboro':            '#DCDCDC',<br />'ghostwhite':           '#F8F8FF',<br />'gold':                 '#FFD700',<br />'goldenrod':            '#DAA520',<br />'gray':                 '#808080',<br />'green':                '#008000',<br />'greenyellow':          '#ADFF2F',<br />'honeydew':             '#F0FFF0',<br />'hotpink':              '#FF69B4',<br />'indianred':            '#CD5C5C',<br />'indigo':               '#4B0082',<br />'ivory':                '#FFFFF0',<br />'khaki':                '#F0E68C',<br />'lavender':             '#E6E6FA',<br />'lavenderblush':        '#FFF0F5',<br />'lawngreen':            '#7CFC00',<br />'lemonchiffon':         '#FFFACD',<br />'lightblue':            '#ADD8E6',<br />'lightcoral':           '#F08080',<br />'lightcyan':            '#E0FFFF',<br />'lightgoldenrodyellow': '#FAFAD2',<br />'lightgreen':           '#90EE90',<br />'lightgray':            '#D3D3D3',<br />'lightpink':            '#FFB6C1',<br />'lightsalmon':          '#FFA07A',<br />'lightseagreen':        '#20B2AA',<br />'lightskyblue':         '#87CEFA',<br />'lightslategray':       '#778899',<br />'lightsteelblue':       '#B0C4DE',<br />'lightyellow':          '#FFFFE0',<br />'lime':                 '#00FF00',<br />'limegreen':            '#32CD32',<br />'linen':                '#FAF0E6',<br />'magenta':              '#FF00FF',<br />'maroon':               '#800000',<br />'mediumaquamarine':     '#66CDAA',<br />'mediumblue':           '#0000CD',<br />'mediumorchid':         '#BA55D3',<br />'mediumpurple':         '#9370DB',<br />'mediumseagreen':       '#3CB371',<br />'mediumslateblue':      '#7B68EE',<br />'mediumspringgreen':    '#00FA9A',<br />'mediumturquoise':      '#48D1CC',<br />'mediumvioletred':      '#C71585',<br />'midnightblue':         '#191970',<br />'mintcream':            '#F5FFFA',<br />'mistyrose':            '#FFE4E1',<br />'moccasin':             '#FFE4B5',<br />'navajowhite':          '#FFDEAD',<br />'navy':                 '#000080',<br />'oldlace':              '#FDF5E6',<br />'olive':                '#808000',<br />'olivedrab':            '#6B8E23',<br />'orange':               '#FFA500',<br />'orangered':            '#FF4500',<br />'orchid':               '#DA70D6',<br />'palegoldenrod':        '#EEE8AA',<br />'palegreen':            '#98FB98',<br />'paleturquoise':        '#AFEEEE',<br />'palevioletred':        '#DB7093',<br />'papayawhip':           '#FFEFD5',<br />'peachpuff':            '#FFDAB9',<br />'peru':                 '#CD853F',<br />'pink':                 '#FFC0CB',<br />'plum':                 '#DDA0DD',<br />'powderblue':           '#B0E0E6',<br />'purple':               '#800080',<br />'red':                  '#FF0000',<br />'rosybrown':            '#BC8F8F',<br />'royalblue':            '#4169E1',<br />'saddlebrown':          '#8B4513',<br />'salmon':               '#FA8072',<br />'sandybrown':           '#FAA460',<br />'seagreen':             '#2E8B57',<br />'seashell':             '#FFF5EE',<br />'sienna':               '#A0522D',<br />'silver':               '#C0C0C0',<br />'skyblue':              '#87CEEB',<br />'slateblue':            '#6A5ACD',<br />'slategray':            '#708090',<br />'snow':                 '#FFFAFA',<br />'springgreen':          '#00FF7F',<br />'steelblue':            '#4682B4',<br />'tan':                  '#D2B48C',<br />'teal':                 '#008080',<br />'thistle':              '#D8BFD8',<br />'tomato':               '#FF6347',<br />'turquoise':            '#40E0D0',<br />'violet':               '#EE82EE',<br />'wheat':                '#F5DEB3',<br />'white':                '#FFFFFF',<br />'whitesmoke':           '#F5F5F5',<br />'yellow':               '#FFFF00',<br />'yellowgreen':          '#9ACD32'}</p></details></blockquote><h2 id="torch库操作">torch库操作</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment">#保存图片</span></span><br><span class="line"><span class="comment">#1.张量在GPU上</span></span><br><span class="line">tensor = torch.load(<span class="string">&#x27;tensor.pt&#x27;</span>)</span><br><span class="line">array = tensor.cpu().numpy()</span><br><span class="line">array = array.astype(np.uint8)</span><br><span class="line">image = Image.fromarray(array)</span><br><span class="line">image.save(<span class="string">&#x27;output_img.png&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.张量在CPU上</span></span><br><span class="line">tensor = torch.load(<span class="string">&#x27;tensor.pt&#x27;</span>, map_location=torch.device(<span class="string">&#x27;cpu&#x27;</span>)) <span class="comment">#映射存储在CPU上</span></span><br><span class="line">array = tensor.detach().numpy()</span><br><span class="line">array = array.astype(np.uint8)</span><br><span class="line">image = Image.fromarray(array)</span><br><span class="line">image.save(<span class="string">&#x27;output_img.png&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八小工具 </category>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代码箱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>求不规则图形的几何参数</title>
      <link href="/2024/12/06/%E6%B1%82%E4%B8%8D%E8%A7%84%E5%88%99%E5%9B%BE%E5%BD%A2%E7%9A%84%E5%87%A0%E4%BD%95%E5%8F%82%E6%95%B0/"/>
      <url>/2024/12/06/%E6%B1%82%E4%B8%8D%E8%A7%84%E5%88%99%E5%9B%BE%E5%BD%A2%E7%9A%84%E5%87%A0%E4%BD%95%E5%8F%82%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>写在前面：下面用到的不规则图形图片例子（背景是白色的，其实一般是黑色的）：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/pen.png" alt="pen" /><figcaption aria-hidden="true">pen</figcaption></figure><h2 id="面积">面积</h2><details><summary>参考链接</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_46121540/article/details/135102329&quot;&gt;python中计算不规则图形面积的方法-CSDN博客&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_46121540/article/details/132040646&quot;&gt;使用蒙特卡洛算法计算不规则图形面积_算法 不规则图形面积计算-CSDN博客&lt;/a&gt;&lt;/p&gt;</code></pre></details><p>当图片的像素的物理长度可知时（通过拍摄已知尺寸的真实物体等比例获得比例尺），可通过求解被拍摄物在图片中占比求得被拍摄物面积。</p><h3 id="基于图像处理的方法像素点法">基于图像处理的方法（像素点法）</h3><p>对于复杂的不规则图形，可以使用图像处理的方法，通过将图形转换为二值图像，然后计算黑色区域的面积占比。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/pen_gray.png" alt="pen_gray" style="zoom: 33%;" /></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取图像并转换为灰度图像</span></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;不规则图形.png&#x27;</span>)</span><br><span class="line">gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) <span class="comment">#将彩色图像转化为灰度图像</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 二值化处理</span></span><br><span class="line">_, thresh = cv2.threshold(gray, <span class="number">127</span>, <span class="number">255</span>, cv2.THRESH_BINARY) <span class="comment">#像素值大于阈值127的部分被设置为255，小于127的部分被设置为0；返回值 thresh表示二值化后的图像</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算黑色区域的面积</span></span><br><span class="line">area = np.<span class="built_in">sum</span>(thresh == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算黑色区域面积/总面积</span></span><br><span class="line"><span class="built_in">print</span>(area / (thresh.shape[<span class="number">0</span>]*thresh.shape[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p><code>cv2.threshold()</code>中阈值的设置可以参考<a href="https://blog.csdn.net/m0_38068229/article/details/111769009">这里</a>。或许也可以考虑阈值通过神经网络学习？？在本例中，将阈值设置为 150 比设置为 127 有利于用opencv库进行实验。</p><blockquote><p>1.将阈值设置为 150 时的轮廓线（轮廓线数量为7）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20241012210402782.png" alt="image-20241012210402782" /><figcaption aria-hidden="true">image-20241012210402782</figcaption></figure><p>2.将阈值设置为 127 时的轮廓线（轮廓线数量为44）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20241012210510367.png" alt="image-20241012210510367" /><figcaption aria-hidden="true">image-20241012210510367</figcaption></figure></blockquote><h3 id="基于蒙特卡洛的方法">基于蒙特卡洛的方法</h3><p>蒙特卡洛算法通过在图形内随机生成大量的点，然后计算在图形内的点的比例来估算面积。</p><p>蒙特卡洛方法在计算不规则形状的面积时特别有用，因为它不需要知道形状的具体方程，而只需要生成足够多的点来精确估算面积。蒙特卡洛算法也有一些局限性。首先，它需要在图形内生成足够多的点，才能得到较为准确的面积估计值。这意味着它的计算时间可能很长，特别是对于复杂的形状。其次，蒙特卡洛算法的误差可能较大，因为它的计算结果取决于随机生成的点的分布情况。<strong>因此，如果生成的点数不够多，或者点的分布不均匀，算法得到的面积估计值可能会有很大误差。</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义图形的边界</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">domain</span>():</span><br><span class="line">    <span class="keyword">return</span> [(<span class="number">0</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算图形面积的蒙特卡洛近似</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">monte_carlo_area</span>(<span class="params">num_samples</span>):</span><br><span class="line">    points = np.random.rand(num_samples, <span class="number">2</span>) * np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>]]) * domain()</span><br><span class="line">    inside = points[np.linalg.norm(points - np.array([[<span class="number">0.5</span>, <span class="number">0.5</span>]]), axis=<span class="number">1</span>) &lt; <span class="number">0.5</span>]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(inside) / num_samples * np.linalg.norm(domain()[-<span class="number">1</span>] - domain()[<span class="number">0</span>]) * np.linalg.norm(domain()[-<span class="number">2</span>] - domain()[<span class="number">0</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>且该方法需要判断随机点是否落在图形内，较适合用于当轮廓线已知时的情况。</p><h3 id="matlab自带程序获取">matlab自带程序获取</h3><p><a href="https://blog.csdn.net/ccsss22/article/details/128432448?spm=1001.2101.3001.6650.16&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-16-128432448-blog-134120940.235%5Ev43%5Epc_blog_bottom_relevance_base8&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-16-128432448-blog-134120940.235%5Ev43%5Epc_blog_bottom_relevance_base8&amp;utm_relevant_index=22">博客</a>中介绍了运用MATLAB函数 <strong>bwarea</strong> 计算二值图像中对象面积、以及运用函数<strong>bwperim</strong>检测目标的边缘跟踪以计算周长的方法。</p><h3 id="opencv库获取">opencv库获取</h3><p>详细内容和解释点<a href="https://blog.csdn.net/Eric_Sober/article/details/126053385?spm=1001.2101.3001.6650.5&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-5-126053385-blog-128432448.235%5Ev43%5Epc_blog_bottom_relevance_base8&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-5-126053385-blog-128432448.235%5Ev43%5Epc_blog_bottom_relevance_base8&amp;utm_relevant_index=10">这个博客</a>（注：C++版本实现点<a href="https://gitcode.csdn.net/65ec51bc1a836825ed7983e6.html?dp_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpZCI6MTc3MjgwMywiZXhwIjoxNzI5MzQwNTMzLCJpYXQiOjE3Mjg3MzU3MzMsInVzZXJuYW1lIjoiYWhhX1lhbGkifQ.GHGjBtLPyRaO8J8xiOMop0XGGeLNJQTkghpY9T86rI0&amp;spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7Ebaidujs_baidulandingword%7Eactivity-1-125588721-blog-132740234.235%5Ev43%5Epc_blog_bottom_relevance_base8&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7Ebaidujs_baidulandingword%7Eactivity-1-125588721-blog-132740234.235%5Ev43%5Epc_blog_bottom_relevance_base8&amp;utm_relevant_index=2">这里</a>）。</p><p>博客中探讨了使用OpenCV库进行查找、绘制图像轮廓，轮廓的面积，周长，多边形逼近，多边形凸包，轮廓的外接矩形的方法。对于不规则图形的面积和周长求解，首先用<code>cv2.findContours(image, mode, method…)</code>查找轮廓，然后通过<code>contourArea(contour)</code>和<code>rcLengh(curve, closed)</code>分别得到轮廓面积和周长。主要代码为：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">cat = cv2.imread(<span class="string">&#x27;pen.png&#x27;</span>)</span><br><span class="line">img = cv2.cvtColor(cat, cv2.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line"><span class="comment">#二值化，有两个返回值，阈值，结果</span></span><br><span class="line">ret, binary = cv2.threshold(img, <span class="number">150</span>, <span class="number">255</span>, cv2.THRESH_BINARY)</span><br><span class="line">cat_cp = cat.copy()</span><br><span class="line"></span><br><span class="line">contours, hierarchy = cv2.findContours(binary, mode = cv2.RETR_TREE, method=cv2.CHAIN_APPROX_SIMPLE) <span class="comment">#mode = cv2.RETR_TREE 表示提取所有轮廓及其层次关系（树状结构）</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#cv2.drawContours(cat_cp, contours, -1, (0, 255, 0), 2)</span></span><br><span class="line">cv2.drawContours(cat_cp, contours, <span class="number">0</span>, (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">2</span>)  <span class="comment">#第一个轮廓用绿框画出</span></span><br><span class="line">cv2.drawContours(cat_cp, contours, <span class="number">1</span>, (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>), <span class="number">2</span>)  <span class="comment">#第二个轮廓用蓝框画出</span></span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">&#x27;cat&#x27;</span>, cat)</span><br><span class="line">cv2.imshow(<span class="string">&#x27;contours&#x27;</span>, cat_cp)</span><br><span class="line"></span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"><span class="comment">#一个轮廓的面积</span></span><br><span class="line">area = cv2.contourArea(contours[<span class="number">1</span>]) <span class="comment">#这里取1，因为第一个轮廓是图片方框（图片背景是白色的，因此 cv2.findContours 函数错误地将背景也识别为需要寻找轮廓的物体）</span></span><br><span class="line"><span class="built_in">print</span>(area)</span><br><span class="line"></span><br><span class="line"><span class="comment">#一个轮廓周长</span></span><br><span class="line">perimeter = cv2.arcLength(contours[<span class="number">1</span>], closed=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(perimeter)</span><br></pre></td></tr></table></figure><p><strong>注意！cv2.findContours</strong> 函数通常提取的是全白物体（值为255）的轮廓，背景为全黑（值为0）。因此，当使用 <code>cv2.findContours</code> 时，要将需要提取的物体设置为白色（255），背景设置为黑色（0），这样函数才能正确识别和提取轮廓。</p><p>如果图片中有多个物体轮廓，我们想找其中最大的轮廓，可以使用函数</p><h3 id="视觉分割大模型sam">视觉分割大模型SAM</h3><p>使用SAM图像分割算法进行图像分割，可以直接手动选择分割选区，简化前期的图像噪声清除、抠图工作，根据分割图像中的像素数计算实际面积。</p><p>SAM 安装及模型下载：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下载地址 https://github.com/facebookresearch/segment-anything?tab=readme-ov-file（手动离线下载 vit_h）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装</span></span><br><span class="line">pip install git+https://github.com/facebookresearch/segment-anything.git</span><br></pre></td></tr></table></figure><p>具体可参考这个<a href="https://enpeicv.com/forum.php?mod=viewthread&amp;tid=90&amp;page=1">链接</a>。</p><h2 id="周长">周长</h2><h3 id="matlab自带程序获取-1">matlab自带程序获取</h3><p>见上<strong>面积</strong>中的小节</p><h3 id="opencv库获取-1">opencv库获取</h3><p>见上<strong>面积</strong>中的小节</p><h2 id="拟合椭圆">拟合椭圆</h2><h3 id="opencv库获取-2">opencv库获取</h3><p>Python代码参考自<a href="https://blog.csdn.net/vivo01/article/details/132700714">这里</a>。C++版本代码见<a href="https://gitcode.csdn.net/65ec51bc1a836825ed7983e6.html?dp_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpZCI6MTc3MjgwMywiZXhwIjoxNzI5MzQwNTMzLCJpYXQiOjE3Mjg3MzU3MzMsInVzZXJuYW1lIjoiYWhhX1lhbGkifQ.GHGjBtLPyRaO8J8xiOMop0XGGeLNJQTkghpY9T86rI0&amp;spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7Ebaidujs_baidulandingword%7Eactivity-1-125588721-blog-132740234.235%5Ev43%5Epc_blog_bottom_relevance_base8&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7Ebaidujs_baidulandingword%7Eactivity-1-125588721-blog-132740234.235%5Ev43%5Epc_blog_bottom_relevance_base8&amp;utm_relevant_index=2">这里</a>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line">img = cv.imread(<span class="string">&quot;pen.png&quot;</span>)</span><br><span class="line">plt.imshow(img[:,:,::-<span class="number">1</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment">#轮廓检测</span></span><br><span class="line">img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)</span><br><span class="line">ret,thresh = cv.threshold(img_gray, <span class="number">150</span>, <span class="number">255</span>, <span class="number">0</span>)</span><br><span class="line">contours,hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_NONE)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">#显示轮廓</span></span><br><span class="line"><span class="string">img_contours = img.copy()</span></span><br><span class="line"><span class="string">img_contours = cv.drawContours(img_contours, contours, -1, (0,255,0), 20)  #用绿框画出所有轮廓</span></span><br><span class="line"><span class="string">plt.imshow(img_contours, cmap=plt.cm.gray)</span></span><br><span class="line"><span class="string">print(len(contours))</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#椭圆拟合</span></span><br><span class="line"><span class="comment">#ellips = cv.fitEllipse(cnt)</span></span><br><span class="line"><span class="comment">#ellipse: 椭圆信息((x,y),(a,b),angle) (x,y)椭圆中心点;(a,b) 椭圆长短轴的直径（注意：非半径）;angle中心旋转角度</span></span><br><span class="line">ellipse = cv.fitEllipse(contours[<span class="number">1</span>])</span><br><span class="line">((x,y),(a,b),angle) = ellipse</span><br><span class="line">cv.ellipse(img, ellipse, (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">20</span>) <span class="comment">#用绿框画出拟合椭圆</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;椭圆长轴直径：&quot;</span>,a)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;椭圆短轴直径：&quot;</span>,b)</span><br><span class="line"></span><br><span class="line">plt.imshow(img[:,:,::-<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>最终效果图：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20241012212342506.png" alt="image-20241012212342506" /><figcaption aria-hidden="true">image-20241012212342506</figcaption></figure><h2 id="最小外接圆">最小外接圆</h2><h3 id="opencv库获取-3">opencv库获取</h3><p>Python代码参考自<a href="https://blog.csdn.net/vivo01/article/details/132700714">这里</a>。C++版本代码见<a href="https://gitcode.csdn.net/65ec51bc1a836825ed7983e6.html?dp_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpZCI6MTc3MjgwMywiZXhwIjoxNzI5MzQwNTMzLCJpYXQiOjE3Mjg3MzU3MzMsInVzZXJuYW1lIjoiYWhhX1lhbGkifQ.GHGjBtLPyRaO8J8xiOMop0XGGeLNJQTkghpY9T86rI0&amp;spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7Ebaidujs_baidulandingword%7Eactivity-1-125588721-blog-132740234.235%5Ev43%5Epc_blog_bottom_relevance_base8&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7Ebaidujs_baidulandingword%7Eactivity-1-125588721-blog-132740234.235%5Ev43%5Epc_blog_bottom_relevance_base8&amp;utm_relevant_index=2">这里</a>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line">img = cv.imread(<span class="string">&quot;pen.png&quot;</span>)</span><br><span class="line">plt.imshow(img[:,:,::-<span class="number">1</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment">#轮廓检测</span></span><br><span class="line">img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)</span><br><span class="line">ret,thresh = cv.threshold(img_gray, <span class="number">150</span>, <span class="number">255</span>, <span class="number">0</span>)</span><br><span class="line">contours,hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_NONE)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">#显示轮廓</span></span><br><span class="line"><span class="string">img_contours = img.copy()</span></span><br><span class="line"><span class="string">img_contours = cv.drawContours(img_contours, contours, -1, (0,255,0), 20)  #用绿框画出所有轮廓</span></span><br><span class="line"><span class="string">plt.imshow(img_contours, cmap=plt.cm.gray)</span></span><br><span class="line"><span class="string">print(len(contours))</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#轮廓最小外接圆</span></span><br><span class="line"><span class="comment">#(x,y),radius = cv.minEnclosingCircle(cnt)</span></span><br><span class="line"><span class="comment">#cnt: 轮廓信息</span></span><br><span class="line"><span class="comment">#(x,y):最小外接圆的圆心</span></span><br><span class="line"><span class="comment">#radius: 最小外接圆的半径</span></span><br><span class="line">(x,y),radius = cv.minEnclosingCircle(contours[<span class="number">1</span>])</span><br><span class="line">center = (<span class="built_in">int</span>(x), <span class="built_in">int</span>(y))</span><br><span class="line">radius = <span class="built_in">int</span>(radius)</span><br><span class="line">cv.circle(img, center, radius, (<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>), <span class="number">20</span>) <span class="comment">#用蓝框画出最小外接圆</span></span><br><span class="line"></span><br><span class="line">plt.imshow(img[:,:,::-<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>最终效果图：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20241012212706720.png" alt="image-20241012212706720" /><figcaption aria-hidden="true">image-20241012212706720</figcaption></figure><h2 id="最大内切圆">最大内切圆</h2><h3 id="opencv库获取-4">opencv库获取</h3><p>Python实现 / C++ 实现请看：<a href="https://cloud.tencent.com/developer/article/2121753">1</a></p><p>Python的掩膜计算实现请看：<a href="https://zhuanlan.zhihu.com/p/315474495">2</a></p><p>C++ 同时实现最大内切和最小外接圆请看：<a href="https://gitcode.csdn.net/65ec51bc1a836825ed7983e6.html?dp_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpZCI6MTc3MjgwMywiZXhwIjoxNzI5MzQwNTMzLCJpYXQiOjE3Mjg3MzU3MzMsInVzZXJuYW1lIjoiYWhhX1lhbGkifQ.GHGjBtLPyRaO8J8xiOMop0XGGeLNJQTkghpY9T86rI0&amp;spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7Ebaidujs_baidulandingword%7Eactivity-1-125588721-blog-132740234.235%5Ev43%5Epc_blog_bottom_relevance_base8&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7Ebaidujs_baidulandingword%7Eactivity-1-125588721-blog-132740234.235%5Ev43%5Epc_blog_bottom_relevance_base8&amp;utm_relevant_index=2">3</a></p><p>1 和 2 都主要是通过距离变换函数<code>cv2.distanceTransform(mask, DIST_L2, 5, DIST_LABEL_PIXEL)</code>，计算给定二值图像中每个像素到最近对象像素的距离，获取距离变换结果最大值及其坐标；以距离变换结果最大值为半径，对应位置坐标为内切圆圆心绘制最大内切圆。</p><p>3 则是通过opencv中的点多边形测试函数<code>pointPolygonTest()</code>检测点是否在轮廓内：它可以准确的得到一个点距离多边形的距离，如果点是轮廓点或者属于轮廓多边形上的点，距离是零；如果是多边形内部的点，距离是正数；如果距离是负数返回表示点是外部。</p><p>下面的代码实现参考自法 1：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">src = cv2.imread(<span class="string">&#x27;pen.png&#x27;</span>)</span><br><span class="line"><span class="comment">#cv2.imshow(&#x27;src&#x27;,src)</span></span><br><span class="line">plt.subplot(<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plt.imshow(src[:,:,::-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 【1】将目标轮廓/区域提取出来，处理成二值图：目标区域黑色(0)，背景白色(255)，并将目标区域填充为黑色(0)。</span></span><br><span class="line"><span class="comment"># 即二值化 + 轮廓提取 + 填充绘制：</span></span><br><span class="line">gray = cv2.cvtColor(src,cv2.COLOR_BGR2GRAY)</span><br><span class="line">gray[gray &gt; <span class="number">150</span>] = <span class="number">255</span></span><br><span class="line">gray[gray != <span class="number">255</span>] = <span class="number">0</span></span><br><span class="line"><span class="comment">#cv2.imshow(&#x27;thres&#x27;,gray)</span></span><br><span class="line">plt.subplot(<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.imshow(gray,cmap=plt.cm.gray)</span><br><span class="line"></span><br><span class="line">contours,hierarchy = cv2.findContours(gray, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)</span><br><span class="line">mask = np.zeros(gray.shape,np.uint8)</span><br><span class="line"><span class="comment">#cv2.drawContours(mask,contours,0,(255,255,255),-1)</span></span><br><span class="line"><span class="comment">#cv2.imshow(&#x27;mask&#x27;,mask) </span></span><br><span class="line">img_contours = cv2.drawContours(mask,contours,<span class="number">1</span>,(<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>),-<span class="number">1</span>) <span class="comment">#-1：填充轮廓内部，-1表示将轮廓内部填充为指定颜色。</span></span><br><span class="line">plt.subplot(<span class="number">3</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">plt.imshow(img_contours, cmap=plt.cm.gray)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 【2】距离变换：获取距离变换结果最大值及其坐标。</span></span><br><span class="line">dt = cv2.distanceTransform(mask,cv2.DIST_L2,<span class="number">5</span>,cv2.DIST_LABEL_PIXEL)</span><br><span class="line">transImg = cv2.convertScaleAbs(dt)</span><br><span class="line">cv2.normalize(transImg, transImg, <span class="number">0</span>, <span class="number">255</span>, cv2.NORM_MINMAX) <span class="comment">#将transImg 中的像素值线性映射到 0 到 255 的范围内，以增强图像的对比度。</span></span><br><span class="line"><span class="comment">#cv2.imshow(&quot;distanceTransform&quot;, transImg)</span></span><br><span class="line">plt.subplot(<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">plt.imshow(transImg)</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20241012223802280.png" alt="image-20241012223802280" /><figcaption aria-hidden="true">image-20241012223802280</figcaption></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 【3】绘制结果：距离变换结果最大值为半径，对应位置为内切圆圆心。</span></span><br><span class="line">_, max_val, _, max_loc = cv2.minMaxLoc(dt)</span><br><span class="line">cv2.circle(src,max_loc, <span class="built_in">int</span>(max_val),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">20</span>) <span class="comment">#绘制绿色内切圆</span></span><br><span class="line"><span class="comment">#cv2.imshow(&#x27;result&#x27;,src)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#cv2.waitKey(0)</span></span><br><span class="line"><span class="comment">#cv2.destroyAllWindows()</span></span><br><span class="line">plt.imshow(src[:,:,::-<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>结果为：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20241012224826563.png" alt="image-20241012224826563" /><figcaption aria-hidden="true">image-20241012224826563</figcaption></figure><h2 id="轮廓">轮廓</h2><p><a href="https://cloud.tencent.com/developer/article/2151522">万字长文告诉新手如何学习Python图像处理（上篇完结 四十四） | 「Python」有奖征文-腾讯云开发者社区-腾讯云</a></p><p>halcon</p><p>cv库</p>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八小工具 </category>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代码箱 </tag>
            
            <tag> 编码 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>特征的距离度量</title>
      <link href="/2024/12/01/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E4%B8%8E%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
      <url>/2024/12/01/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E4%B8%8E%E7%9B%B8%E4%BC%BC%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<h2 id="一欧氏距离">一、欧氏距离</h2><p>欧氏距离(Euclidean Distance)是一种常用的距离度量方法，定义为两点之间的直线距离。它的定义公式为： <span class="math display">\[dist(\text{x},\text{y}) := || \text{x} - \text{y}|| = \sqrt{\sum_{i=1}^n(x_i - y_i)^2}\]</span></p><h2 id="二余弦距离">二、余弦距离</h2><details><summary>点击查看链接</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/hy592070616/article/details/122271927&quot;&gt;机器学习中的数学——距离定义（八）：余弦距离（Cosine Distance）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/dataanaly/p/12893276.html&quot;&gt;机器学习笔记-距离度量与相似度(三)余弦相似度（含数学推理与证明）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/qq_42257666/article/details/122143048?spm=1001.2101.3001.6650.12&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-12-122143048-blog-135808119.235%5Ev43%5Epc_blog_bottom_relevance_base1&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-12-122143048-blog-135808119.235%5Ev43%5Epc_blog_bottom_relevance_base1&amp;utm_relevant_index=20&quot;&gt;余弦距离和欧氏距离，知道原理和公式后真的很简单（通俗易懂）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/abella/p/11170592.html&quot;&gt;余弦相似度和余弦距离的推导与理解（深入浅出）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/universsky2015/article/details/135808119?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-135808119-blog-122271927.235^v43^pc_blog_bottom_relevance_base1&amp;spm=1001.2101.3001.4242.1&amp;utm_relevant_index=3&quot;&gt;实现高效的余弦距离计算方法&lt;/a&gt;&lt;/p&gt;</code></pre></details><h3 id="定义">定义</h3><p><strong>余弦距离(Cosine Distance)</strong>是<strong>余弦相似度</strong>的另一种度量形式。 几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本特征向量之间的差异。相比欧氏距离等度量，<strong>余弦相似度更加注重两个向量在方向上的差异</strong>，而非距离或长度上。n 维空间中的余弦相似度为： <span class="math display">\[similarity = cos(\text{x},\text{y}) =\frac{\text{x} \cdot \text{y}}{||x|| \cdot ||y||}= \frac{ \sum_{i=1}^n{x_i y_i} }{ \sqrt{\sum_{i=1}^n{x_i^2}}\sqrt{\sum_{i=1}^n{y_i^2}}}\]</span> 余弦取值范围为[ − 1 , 1 ] ，可以使用两个向量之间夹角的余弦值确定两个向量是否大致指向相同的方向。夹角越小，趋近于0度，余弦值越接近于1，它们的方向更加吻合，则越相似；当两个向量的方向完全相反,夹角余弦取最小值-1；当余弦值为0时，两向量正交，夹角为90度。</p><p><span style="background:#daf5e9;"><strong>余弦距离就是用 1 减去余弦相似度</strong></span>，取值范围是 [0,2]，这样也满足了非负性的性质。余弦相似度越大，余弦距离就越小。 <span class="math display">\[dist(\text{x},\text{y}) := 1-cos(\text{x},\text{y})\]</span> 余弦相似度的值是与向量的长度无关的，仅仅与向量的指向方向相关。换句话说，<span style="background:#daf5e9;"><strong>余弦相似度关注更多的是向量之间的角度关系，而不是它们的绝对大小</strong></span>。以余弦距离与欧氏距离的区别为例，总体来说，欧氏距离体现数值上的绝对差异，而余弦距离体现方向上的相对差异：</p><ul><li>统计两部剧的用户观看行为，用户A的观看向量为(0,1)，用户B为(1,0)；此时二者的余弦距很大，而欧氏距离很小；我们分析两个用户对于不同视频的偏好，更关注相对差异，显然应当使用<code>余弦距离</code>。</li><li>而当我们分析用户活跃度，以登陆次数(单位：次)和平均观看时长(单：分钟)作为特征时，余弦距离会认为(1,10)、(10,100)两个用户距离很近；但显然这两个用户活跃度是有着极大差异的，此时我们更关注数值绝对差异，应当使用<code>欧氏距离</code>。</li></ul><p>由于余弦距离的特性，它被应用于以下领域：</p><ol type="1"><li>文本分类：将文本映射为高维向量，计算余弦距离，将文档分类到最相似的类别；</li><li>图像识别：将图像特征映射为向量，计算余弦距离，识别最相似的图像；</li><li>数据挖掘：通过余弦相似度来度量集群内部的凝聚力。</li></ol><h3 id="代码实现">代码实现</h3><p>其python代码实现：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># x,y 为 numpy 数组时</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">CosineDistance</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> np.dot(x,y)/(np.linalg.norm(x)*np.linalg.norm(y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># x,y 为张量时</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">CosineDistance</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span>  torch.matmul(x, y) / (torch.norm(x) * torch.norm(y))  </span><br></pre></td></tr></table></figure><h3 id="高效的余弦距离计算方法">高效的余弦距离计算方法</h3><p>为了提高余弦距离计算的效率，可以采用以下方法：</p><ol type="1"><li>使用<strong>稀疏向量</strong>：将高维向量压缩为低维稀疏向量，减少计算量。</li><li>使用<strong>散列向量</strong>：将向量映射为哈希值，将计算问题转换为计算哈希值的问题。</li><li>使用<strong>特定数据结构</strong>：如 KD-Tree、Ball-Tree 等，加速高维空间中的近邻查找。</li></ol><h3 id="数学性质">数学性质</h3><p><span style="background:#FFCC99;"><strong>余弦相似度</strong></span></p><p>1.对称性：<span class="math inline">\(cos(\theta) = cos(-\theta)\)</span>​，因此 <span class="math inline">\(sim(A, B) = sim(B, A)\)</span>​​。 2.反身性：<span class="math inline">\(sim(A, A) = 1\)</span>​，<span class="math inline">\(sim(B, B) = 1\)</span>​​。 3.传递性：如果 <span class="math inline">\(sim(A, B) = 1\)</span>​ 且 <span class="math inline">\(sim(B, C) = 1\)</span>​，那么 <span class="math inline">\(sim(A, C) = 1\)</span>​。</p><p><span style="background:#FFCC99;"><strong>余弦距离不是严格定义上的距离</strong></span></p><p>1.非负性：余弦距离取值范围为 [0,2]。</p><p>2.同一性：若 <span class="math inline">\(dist(A,B)=0 &lt;—&gt; A=B\)</span>。</p><p>2.对称性：<span class="math inline">\(dist(A,B) = 1-\cos \theta = dist(B,A)\)</span>​ 。</p><p>3.三角不等式：不满足，我们举出一反例即可</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20241127211417197.png" alt="image-20241127211417197" style="zoom:67%;" /></p><p>根据数学上的定义，在一个集合中，如果一对元素可确定一个实数，使得非负性、对称性和三角不等式成立，则该实数可称为这对元素之间的距离。因此，虽然余弦距离并不是严格意义上的距离。</p><p><span style="background:#FFCC99;"><strong>余弦距离定义推导</strong></span></p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/03a5e59a4lefed76ea80299492c08977" alt="img" style="zoom:67%;" /></p><p><span style="background:#FFCC99;"><strong>余弦距离与欧氏距离存在单调的关系</strong></span></p><p><img src="C:\Users\21218\Downloads\20241127_210404_1.jpg" alt="20241127_210404_1" style="zoom:67%;" /></p><p>因此虽然余弦距离不是严格意义上的距离，但它仍然能体现一定的向量距离关系。</p><h3 id="q-a">Q &amp; A</h3><p>Q1：如何计算两个矩阵之间的余弦距离？ A1：可以将矩阵转换为向量，然后计算每行向量之间的余弦距离。</p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代码箱 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常见代码记录</title>
      <link href="/2024/12/01/%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81%E5%87%BD%E6%95%B0%E8%AE%B0%E5%BD%95/"/>
      <url>/2024/12/01/%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81%E5%87%BD%E6%95%B0%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="python基础">python基础</h1><h2 id="位运算">位运算</h2><p>在现代计算机中，所有数据都以二进制形式存储，即 0 和 1 两种状态。计算机对二进制数据进行的运算（如加、减、乘、除）被称为位运算，即对二进制数的每一位进行操作的运算。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20241009155539822.png" alt="图片来自菜鸟教程" /><figcaption aria-hidden="true">图片来自菜鸟教程</figcaption></figure><h3 id="异或">异或</h3><p>在二进制情况下，相同位消去，不同位保留。任何数与0异或，结果都是它本身。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">8</span> ^ <span class="number">2</span> <span class="comment">#10</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">8=（1000）   2=（0010）</span></span><br><span class="line"><span class="string">则8与2异或为 10=（1010）</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="按位与">按位与</h3><details><summary>参考链接</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/xindaxinda123/article/details/95617758&quot;&gt;为什么可以使用位运算(&amp;)来实现取模运算(%)呢？_&amp;运算为什么可以取模-CSDN博客&lt;/a&gt;&lt;/p&gt;</code></pre></details><p>只有当参与运算的两个数都为“1”时，结果才为1。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">3</span> &amp; <span class="number">5</span>   <span class="comment">#1</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">3=(011)   5=(101)</span></span><br><span class="line"><span class="string">则3按位与5为 1=(001)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>一个较重要的应用是，当进行2的幂的取模时，可以使用按位与运算(&amp;)来代替取模运算(%)，这里主要的考虑就是效率。<strong>位运算(&amp;)效率要比代替取模运算(%)高很多主要原因是位运算直接对内存数据进行操作，不需要转成十进制，因此处理速度非常快。</strong> <span class="math display">\[X \ \colorbox{pink}{\%} \ 2^n = X \ \colorbox{SpringGreen}{\&amp;} \ (2^n – 1)\]</span> 也就是说，一个数对 <span class="math inline">\(2^n\)</span> 取模 == 一个数和 <span class="math inline">\((2^n – 1)\)</span> 做按位与运算 。可以理解为，对 <span class="math inline">\(2^n\)</span> <span style="background:pink;">取模</span>相当于先将原数右移 n 位（得到 <span class="math inline">\(X/2^n\)</span> 的商），而被移掉的部分——那后 n 位，也就是取模结果（余数）；而对 <span class="math inline">\(2^n-1\)</span> 做<span style="background:yellowgreen;">按位与</span>则是取 X 的二进制的最后 n 位数；二者是等价的。</p><blockquote><p>参考链接里的例子：</p><p>假设n为3，则2^3 = 8，表示成2进制就是1000。2^3 = 7 ，即0111。</p><p>此时X &amp; (2^3 – 1) 就相当于取X的2进制的最后三位数。</p><p>从2进制角度来看，X / 8相当于 X &gt;&gt; 3，即把X右移3位，此时得到了X / 8的商，而被移掉的部分(后三位)，则是X % 8，也就是余数。</p><blockquote><p>6 % 8 = 6 ，6 &amp; 7 = 6</p><p>10 &amp; 8 = 2 ，10 &amp; 7 = 2</p></blockquote></blockquote><h3 id="位移操作">位移操作</h3><p>从二进制角度看，<span class="math inline">\(X &gt;&gt; n\)</span> 相当于 <span class="math inline">\(X/2^n\)</span> ，此时得到的是 <span class="math inline">\(X/2^n\)</span> 的商。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 左移相当于乘2,右边的数表示乘多少次</span></span><br><span class="line"><span class="number">1</span> &lt;&lt; <span class="number">3</span>    <span class="comment">#8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 右移相当于除2，右边的数表示除多少次</span></span><br><span class="line"><span class="number">8</span> &gt;&gt; <span class="number">1</span>    <span class="comment">#4</span></span><br><span class="line"></span><br><span class="line"><span class="number">2</span> &gt;&gt; <span class="number">2</span>    <span class="comment">#0，不会出现小数</span></span><br></pre></td></tr></table></figure><h1 id="configs-参数">configs 参数</h1><h4 id="white_bkgd"><code>white_bkgd</code>:</h4><blockquote><p><a href="https://blog.csdn.net/qq_41623632/article/details/126468034">Pytroch Nerf代码阅读笔记（LLFF 数据集pose 处理和Nerf 网络结构）-CSDN博客</a></p></blockquote><p>白色背景参数。使得渲染出的结果背景是白色滴</p><p>在Blender 的数据集图像有四个通道RGBA,其中A表示的是alpha通道，一般情况下就是两个取值【0,1】,当alpha=0 表示该处的pixel是透明的；当alpha=1 表示该处的pixel是不透明的。 而 white_bkgd 这个参数就是负责将透明像素的部分转化为白色的背景，转化的代码部分如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> args.white_bkgd:</span><br><span class="line">    images = images[...,:<span class="number">3</span>]*images[...,-<span class="number">1</span>:] + (<span class="number">1.</span>-images[...,-<span class="number">1</span>:])</span><br></pre></td></tr></table></figure><p>代码的解读： images是Normalize到【0,1】之间的图像，当alpha=0(也就是 images[…,-1:] = 0 )，那么images的像素将设置为1（纯白色)；当alpha=1的时候，那么images的像素的就是本来的RGB通道对应的颜色。</p><h1 id="checkpoint">checkpoint</h1><blockquote><p><a href="https://zhuanlan.zhihu.com/p/410548507">理解Checkpoint - 知乎 (zhihu.com)</a></p></blockquote><p>其实就是由于训练时间长，我们将中间产生的结果用 checkpoint 保存起来，在节省空间的同时防止丢失重要结果数据。类似游戏里的存档啦！只要有 checkpoint ，我们就能安心重新开启下一次训练，而不用担心此前工作白费！</p><h1 id="parser.add_argument">parser.add_argument()</h1><blockquote><p>参考链接：<a href="https://blog.csdn.net/qq_34243930/article/details/106517985">python之parser.add_argument()用法——命令行选项、参数和子命令解析器-CSDN博客</a></p></blockquote><h2 id="argparse-介绍">argparse 介绍</h2><blockquote><p>官方文档：<a href="https://docs.python.org/zh-cn/3/library/argparse.html">argparse --- 命令行选项、参数和子命令解析器 — Python 3.12.2 文档</a></p></blockquote><p>argparse 模块是 Python 内置的一个用于命令项选项与参数解析的模块。通过在程序中定义好我们需要的参数，然后 argparse 将会从 sys.argv 解析出这些参数，并在用户给程序传入无效参数时报出错误信息。</p><h2 id="argparse使用说明与示例">argparse使用说明与示例</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;test&#x27;</span>) <span class="comment">#Step 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--sparse&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>, <span class="built_in">help</span>=<span class="string">&#x27;GAT with sparse version or not.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">72</span>, <span class="built_in">help</span>=<span class="string">&#x27;Random seed.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10000</span>, <span class="built_in">help</span>=<span class="string">&#x27;Number of epochs to train.&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">ArgumentParser.add_argument(name or flags...[, action][, nargs][, const][, default][, type][, choices][, required][, help][, metavar][, dest])</span></span><br><span class="line"><span class="string">-name: 有&#x27;--&#x27;指定表示可选参数；不加&#x27;--&#x27;表示必选参数，需在命令行中手动指定，否则即使通过default设置默认参数，也还是会报错</span></span><br><span class="line"><span class="string">-action: 当参数在命令行中出现时使用的动作基本类型，如当其值为&#x27;store_true&#x27;时表示参数作用是存储True值，其默认值为&#x27;False&#x27;，const值为&#x27;True&#x27;(若在parse_args()的括号中被引用，使用&#x27;True&#x27;值，否则使用&#x27;False&#x27;值)</span></span><br><span class="line"><span class="string">-default: 不指定参数时其默认值</span></span><br><span class="line"><span class="string">-type: 参数应当被转换成的类型</span></span><br><span class="line"><span class="string">-help: 所添加参数的简要说明</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3</span></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"><span class="built_in">print</span>(args.sparse)</span><br><span class="line"><span class="built_in">print</span>(args.seed)</span><br><span class="line"><span class="built_in">print</span>(args.epochs)</span><br></pre></td></tr></table></figure><p>三个步骤：</p><ol type="1"><li>创建一个解析器——创建一个 <strong>argparse.ArgumentParser()</strong> 对象，名称为 <code>parser</code></li><li>添加参数——调用<code>parser</code>. <strong>add_argument()</strong>方法添加参数</li><li>解析参数——使用 <code>parser</code>. <strong>parse_args()</strong> 解析检查添加的参数集为<span style="background:#daf5e9;"><strong>args</strong></span>，调用时括号内通常不带参数</li></ol><h2 id="shell文件向py文件传参">shell文件向py文件传参</h2><p>方法一：<a href="https://blog.csdn.net/m0_45406092/article/details/115398020">【shell】 shell调用python脚本，并且向python脚本传递参数_shell脚本调用python脚本,并传参-CSDN博客</a></p><p>方法二(更推荐)：<a href="https://pythonjishu.com/python-sys-argv/">详解sys.argv（获取命令行参数）属性的使用方法 - Python技术站 (pythonjishu.com)</a></p><h1 id="tqdm-python进度条库">tqdm — python进度条库</h1><blockquote><p>参考链接：<a href="https://zhuanlan.zhihu.com/p/163613814">python进度条库tqdm详解 - 知乎 (zhihu.com)</a></p></blockquote><p><strong>tqdm模块是python进度条库，主要分为两种运行方式</strong></p><p>1.基于迭代对象运行：tqdm(iterator)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm, trange</span><br><span class="line"></span><br><span class="line"><span class="comment">#trange(i)是tqdm(range(i))的一种简单写法</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> trange(<span class="number">100</span>):</span><br><span class="line">    time.sleep(<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">100</span>), desc=<span class="string">&#x27;Processing&#x27;</span>):</span><br><span class="line">    time.sleep(<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">dic = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>]</span><br><span class="line">pbar = tqdm(dic)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> pbar:</span><br><span class="line">    pbar.set_description(<span class="string">&#x27;Processing &#x27;</span>+i)</span><br><span class="line">    time.sleep(<span class="number">0.2</span>)</span><br><span class="line"><span class="number">100</span>%|██████████| <span class="number">100</span>/<span class="number">100</span> [<span class="number">00</span>:06&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">16.04</span>it/s]</span><br><span class="line">Processing: <span class="number">100</span>%|██████████| <span class="number">100</span>/<span class="number">100</span> [<span class="number">00</span>:06&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">16.05</span>it/s]</span><br><span class="line">Processing e: <span class="number">100</span>%|██████████| <span class="number">5</span>/<span class="number">5</span> [<span class="number">00</span>:01&lt;<span class="number">00</span>:<span class="number">00</span>,  <span class="number">4.69</span>it/s]</span><br></pre></td></tr></table></figure><p>2.手动进行更新</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tqdm(total=<span class="number">200</span>) <span class="keyword">as</span> pbar:</span><br><span class="line">    pbar.set_description(<span class="string">&#x27;Processing:&#x27;</span>)</span><br><span class="line">    <span class="comment"># total表示总的项目, 循环的次数20*10(每次更新数目) = 200(total)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">        <span class="comment"># 进行动作, 这里是过0.1s</span></span><br><span class="line">        time.sleep(<span class="number">0.1</span>)</span><br><span class="line">        <span class="comment"># 进行进度更新, 这里设置10个</span></span><br><span class="line">        pbar.update(<span class="number">10</span>)</span><br><span class="line">Processing:: <span class="number">100</span>%|██████████| <span class="number">200</span>/<span class="number">200</span> [<span class="number">00</span>:02&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">91.94</span>it/s]</span><br></pre></td></tr></table></figure><h2 id="tqdm模块参数说明">tqdm模块参数说明</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">tqdm</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, iterable=<span class="literal">None</span>, desc=<span class="literal">None</span>, total=<span class="literal">None</span>, leave=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">               file=sys.stderr, ncols=<span class="literal">None</span>, mininterval=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">               maxinterval=<span class="number">10.0</span>, miniters=<span class="literal">None</span>, <span class="built_in">ascii</span>=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">               disable=<span class="literal">False</span>, unit=<span class="string">&#x27;it&#x27;</span>, unit_scale=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">               dynamic_ncols=<span class="literal">False</span>, smoothing=<span class="number">0.3</span>, nested=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">               bar_format=<span class="literal">None</span>, initial=<span class="number">0</span>, gui=<span class="literal">False</span></span>):</span><br></pre></td></tr></table></figure><ul><li>iterable: 可迭代的对象, 在手动更新时不需要进行设置</li><li>desc: 字符串, 左边进度条描述文字description</li><li>total: 总的项目数</li><li>leave: bool值, 迭代完成后是否保留进度条</li><li>file: 输出指向位置, 默认是终端, 一般不需要设置</li><li>ncols: 调整进度条宽度, 默认是根据环境自动调节长度, 如果设置为0, 就没有进度条, 只有输出的信息</li><li>unit: 描述处理项目的文字, 默认是'it', 例如: 100 it/s, 处理照片的话设置为'img' ,则为 100 img/s</li><li>unit_scale: 自动根据国际标准进行项目处理速度单位的换算, 例如 100 000 it/s &gt;&gt; 100k it/s</li></ul><h1 id="torch">torch</h1><h2 id="torch-基础">torch 基础</h2><p>文档位置：<a href="file:///C:/Users/LENOVO/Nutstore/2/这是一个文件夹/课程资料/深度学习/老师发的/1.实践基础.html">1.实践基础</a></p><p>下载链接：https://www.jianguoyun.com/p/DdmQaDwQorGMDBjGxtUFIAA</p><p>文件大纲如下（标红星处为需要在文档中重点查看的）：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240617_104011_00.jpg" alt="20240617_104011_0" /><figcaption aria-hidden="true">20240617_104011_0</figcaption></figure><h3 id="张量形状">张量形状</h3><p>有几个中括号"[]"就有几维。如果是最后的两维 m行×n列，m 表示一共有 m 个样本，n 表示一共有 n 个特征。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.Tensor([<span class="number">8</span>,<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(x.shape) <span class="comment">#torch.Size([3])</span></span><br><span class="line"></span><br><span class="line">x = torch.Tensor([[<span class="number">8</span>,<span class="number">3</span>,<span class="number">3</span>]])</span><br><span class="line"><span class="built_in">print</span>(x.shape) <span class="comment">#torch.Size([1, 3])</span></span><br><span class="line"></span><br><span class="line">x = torch.Tensor([[<span class="number">8</span>,<span class="number">3</span>,<span class="number">3</span>],[<span class="number">8</span>,<span class="number">3</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(x.shape) <span class="comment">#torch.Size([2, 3])</span></span><br></pre></td></tr></table></figure><h3 id="省略号的使用">省略号的使用</h3><p>省略号一般是“剩余所有维”的意思。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.Tensor([[<span class="number">8</span>,<span class="number">3</span>,<span class="number">3</span>],[<span class="number">8</span>,<span class="number">3</span>,<span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出所有维度，但对于最后一个维度只保留第一个通道值</span></span><br><span class="line"><span class="built_in">print</span>(x[...,<span class="number">0</span>]) <span class="comment">#tensor([8., 8.])</span></span><br><span class="line"><span class="comment">#创建一个与 coords 张量形状相同的全零张量，但只保留最后一个维度的第一个通道（即最后一个维度的索引为 0 的部分）</span></span><br><span class="line">xx = torch.zeros_like(x)[..., <span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(xx) <span class="comment">#tensor([0., 0.])</span></span><br></pre></td></tr></table></figure><h2 id="torch.randperm">torch.randperm</h2><p>torch.randperm(n)：将 <span class="math inline">\(0\)</span> ~ <span class="math inline">\(n-1\)</span>（包括 <span class="math inline">\(0\)</span> 和 <span class="math inline">\(n-1\)</span>）随机打乱后获得的数字序列，函数名是random permutation缩写。'permutation' 译为‘排列组合’。下面是一个例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.randperm(<span class="number">10</span>)</span><br><span class="line"><span class="comment">#输出 tensor([2, 3, 6, 7, 8, 9, 1, 5, 0, 4])</span></span><br></pre></td></tr></table></figure><h2 id="张量的梯度">张量的梯度</h2><p><span style="background:#FFCC99;"><strong>torch.no_grad</strong></span></p><blockquote><p>进阶内容请看：<a href="https://zhuanlan.zhihu.com/p/406823590">几行代码让你搞懂torch.no_grad - 知乎 (zhihu.com)</a></p></blockquote><p>一般是 <code>with torch.no_grad()</code> 和 <code>@torch.no_grad()</code> 两种用法。</p><ul><li><p><code>requires_grad=True</code> 要求计算梯度</p></li><li><p><code>requires_grad=False</code> 不要求计算梯度</p></li><li><p><code>with torch.no_grad()</code>或者<code>@torch.no_grad()</code>中的数据<strong>不需要计算梯度，也不会进行反向传播</strong>。</p><p>在该模块下，所有计算得出的tensor的requires_grad都自动设置为False。即使一个tensor（命名为x）的requires_grad = True，在with torch.no_grad计算，由x得到的新tensor（命名为w-标量）requires_grad也为False，且grad_fn也为None,即不会对w求导。</p></li></ul><p>两种用法如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()                                <span class="comment"># 测试模式</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">   <span class="keyword">pass</span></span><br><span class="line">   </span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">eval</span>():</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><span style="background:#FFCC99;"><strong>Tensor.detach()</strong></span></p><p><code>detach()</code>函数从计算图中分离出张量，返回一个新的张量，而<strong>这个张量不再与原始张量的梯度计算相关联</strong>。这通常用在某些情况下，我们希望在不影响模型训练的前提下使用这个张量的值，比如用于归一化或其他操作。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_patches = normalize(patchify(<span class="built_in">input</span>, patch_size), std = <span class="built_in">input</span>.std().detach())</span><br><span class="line"><span class="comment"># 对于这里的“input.std().detach()”,</span></span><br><span class="line"><span class="comment"># 计算input的标准差，并得到一个不需要计算梯度的新张量。这样可以避免在反向传播过程中计算该标准差的梯度，从而提高计算效率。</span></span><br></pre></td></tr></table></figure><h2 id="cv中常见张量形式">CV中常见张量形式</h2><p>在 2D 图像中，通常有形状为 <code>(高度h, 宽度w, 通道数c)</code> 的张量。这里的最后一个维度是通道数。</p><p>在 3D 数据中，如视频数据，张量形状可能是 <code>(时间, 高度h, 宽度w, 通道数c)</code>。最后一个维度仍然是通道。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(img.shape) <span class="comment">#(349, 322, 3)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">在这个例子中，</span></span><br><span class="line"><span class="string">349：表示图像的高度（行数），即图像在垂直方向上的像素数。</span></span><br><span class="line"><span class="string">322：表示图像的宽度（列数），即图像在水平方向上的像素数。</span></span><br><span class="line"><span class="string">3：表示图像的通道数，通常这个值为 3，代表图像采用 RGB 颜色模式，包含红色、绿色和蓝色三个颜色通道。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">综上，输出 (349, 322, 3) 表示该图像的尺寸为 349 像素高、322 像素宽，并且具有 3 个颜色通道。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>在训练过程中，2D图像往往有另一个形式，即形状为（N, C, H, W）的张量，其中N为batch_size，C是channel个数，H和W分别是channel的长宽。</p><h2 id="张量的各种变形函数">张量的各种变形函数</h2><blockquote><p><a href="https://blog.csdn.net/HUSTHY/article/details/101649012">pytorch中unsqueeze()、squeeze()、expand()、repeat()、view()、和cat()函数的总结_unsqueeze(2)-CSDN博客</a></p></blockquote><h3 id="增维">增维</h3><p><strong>tensor[None, ...]：</strong>给张量的最前面增加一个新的维度。省略号表示选择张量的所有剩余维度，可以简化表达。</p><p><strong>unsqueeze()</strong>：给张量增加维度。unsqueeze(dim)就是在维度序号为dim的地方给tensor增加一维，维度的扩展类似于直接在外面加"[]"括号。注意，输出张量与原张量共享内存。</p><p><strong>repeat()</strong>：对张量进行重复扩充。当参数有多个时，表示对对应位置上的维数重复扩充参数维度。如：</p><ul><li>当参数只有两个时：（列的重复倍数，行的重复倍数）。1表示不重复</li><li>当参数有三个时：（通道数的重复倍数，列的重复倍数，行的重复倍数）。</li></ul><h3 id="降维">降维</h3><p>可以参考数组降维的方法：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"></span><br><span class="line"><span class="comment">#  cnt 是原数组，维度是(461, 1, 2)，想要通过降维的方法减去中间的一维  </span></span><br><span class="line">cnt = np.array([[[<span class="number">123</span>, <span class="number">33</span>]], [[<span class="number">123</span>, <span class="number">34</span>]], [[<span class="number">122</span>, <span class="number">35</span>]], ...])  <span class="comment"># 省略其余部分 </span></span><br><span class="line"><span class="built_in">print</span>(cnt.shape) <span class="comment">#(461, 1, 2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 法1：使用 reshape() 函数</span></span><br><span class="line">new_cnt = cnt.reshape(-<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(new_cnt.shape) <span class="comment">#(461, 2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 法2：使用 squeeze() 函数，该函数能直接去除单维度</span></span><br><span class="line">new_cnt = cnt.squeeze()</span><br><span class="line"><span class="built_in">print</span>(new_cnt.shape) <span class="comment">#(461, 2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后得到的 new_cnt = [[123, 33], [123, 34], [122, 35], ...]</span></span><br></pre></td></tr></table></figure><h3 id="变维">变维</h3><p><strong>TensorX.permute(.., .., ..)</strong>: 通过维度索引值直接变换维度。如形状为 3*3024*4032 的张量 t，令 tt = t.permute(2,0,1) 后得到的是 3024*4032*3 的张量 tt。注意该函数可能会使张量变为非连续存储的布局。</p><p><strong>TensorX.view(.., .., ..)</strong>: 用法与 <code>reshape()</code>函数相似，但是不同的是<code>view()</code>要求张量是连续存储的，而 <code>reshape()</code>则不要求。可以通过 <code>is_contiguous()</code> 判断，然后使用<code>contiguous()</code> 将张量转变为连续存储的。</p><p><strong>TensorX.transpose(.., .., ..)</strong>: 对张量进行转置操作。如形状为 64*56*56 的张量 t，令 tt = t.transpose(0,1) 后得到是 56*64*56 的张量 tt。</p><h2 id="张量的运算">张量的运算</h2><p><strong>torch.matmul()：</strong>返回矩阵的点乘积，类似于 <code>np.dot(x,y)</code>。</p><p><strong>is_all_ones = torch.all(tensor == 1)：</strong>输出为 True 或 False，可以用来判断张量中的所有值是否全1。</p><h2 id="张量--ndarray">张量&lt;-&gt; ndarray</h2><p><span style="background:#daf5e9;"><strong>张量 -&gt; ndarray</strong></span></p><p>需要注意，用以下方法生成的 Numpy 数组与 Pytorch 张量是共享内存的，因此改变 NumPy 数组的值也会改变对应的张量，反之亦然。为了避免这种情况，可以创建 NumPy 数组的副本：<code>numpy_array = tensor.numpy().copy()</code></p><ul><li><strong>将CPU张量转换为 Numpy 数组：</strong><code>numpy_array = tensor.numpy()</code>;</li><li><strong>将GPU张量转换为 Numpy 数组：</strong><code>numpy_array = tensor_gpu.cpu().numpy()</code></li></ul><p><span style="background:#daf5e9;"><strong>ndarray -&gt; 张量</strong></span></p><p>转换语句：<code>tensor = torch.from_numpy(numpy_array)</code></p><h2 id="张量的存储设备">张量的存储设备</h2><p><strong>PyTorch 要求所有的张量都在同一个设备上进行计算</strong>，通常是 GPU（<code>cuda</code>）或 CPU。如果有张量在不同设备上，计算时就会报错。通过使用 <code>.to(device)</code> 或 <code>.cuda()</code> 和 <code>.cpu()</code> 方法，可以将所有张量和模型移动到同一设备上，确保计算顺利进行。<strong>注意使用方法后不修改原张量，而是创建一个新张量。</strong>下面是一个使用统一设备管理的例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义设备</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建张量时，直接分配到该设备</span></span><br><span class="line">tensor_a = torch.randn(<span class="number">3</span>, <span class="number">3</span>, device=device)</span><br><span class="line">tensor_b = torch.randn(<span class="number">3</span>, <span class="number">3</span>, device=device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确保模型也在相同的设备上</span></span><br><span class="line">model = model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入张量也在同一设备上</span></span><br><span class="line">tensor_input = torch.randn(<span class="number">3</span>, <span class="number">3</span>, device=device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查所有张量和模型是否在同一设备上</span></span><br><span class="line"><span class="built_in">print</span>(tensor_a.device)  <span class="comment"># 打印张量所在的设备</span></span><br><span class="line"><span class="built_in">print</span>(tensor_b.device)  <span class="comment"># 打印张量所在的设备</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(model.parameters()).device)  <span class="comment"># 打印模型参数所在的设备</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播计算</span></span><br><span class="line">output = model(tensor_input)</span><br></pre></td></tr></table></figure><h2 id="张量的保存与加载">张量的保存与加载</h2><p>1.<strong>使用 <code>torch.save()</code> 保存为 <code>.pt</code> 或 <code>.pth</code> 文件</strong>：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(tensor, <span class="string">&#x27;path/tensor.pt&#x27;</span>)</span><br><span class="line">torch.save(tensor, <span class="string">&#x27;path/tensor.pth&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>加载保存的张量：</strong> <code>loaded_tensor = torch.load('tensor.pt')</code></p><h1 id="numpy">Numpy</h1><h2 id="ndarray-的保存与加载">ndarray 的保存与加载</h2><p><strong>1.保存为 csv 文件：</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在代码运行中如果出现bug，常用以下代码调试</span></span><br><span class="line"><span class="comment"># 保存为 csv 文件，更方便查看</span></span><br><span class="line">array_2D = array_3D.reshape(array_3D.shape[<span class="number">0</span>],-<span class="number">1</span>) <span class="comment">#将多维数组转换为二维</span></span><br><span class="line">np.savetxt(<span class="string">&quot;array_2D.csv&quot;</span>, array_2D, delimiter=<span class="string">&quot;,&quot;</span>)</span><br></pre></td></tr></table></figure><p><strong>从 csv 文件中读取数据：</strong><code>loaded_numpy_array = np.loadtxt('array_2D.csv', delimiter=',')</code></p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>相机位姿估计</title>
      <link href="/2024/11/24/%E7%9B%B8%E6%9C%BA%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
      <url>/2024/11/24/%E7%9B%B8%E6%9C%BA%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="相机模型">相机模型</h1><details><summary>参考链接</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/450540827&quot;&gt;现代计算机图形学基础二：光栅化（Rasterization） - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1cz421872F/?spm_id_from=333.788&amp;vd_source=124ec79ebd3e16b0f454a3994a468f98&quot;&gt;较真系列】讲人话-3d gaussian splatting全解(原理+代码+公式)【2】 抛雪球_哔哩哔哩_bilibili&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/chentravelling/article/details/53558096&quot;&gt;计算机视觉：相机成像原理：世界坐标系、相机坐标系、图像坐标系、像素坐标系之间的转换 CSDN&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/wangmj_hdu/article/details/121252123?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522172160984216800185869228%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=172160984216800185869228&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-121252123-null-null.142^v100^pc_search_result_base5&amp;utm_term=%E4%B8%96%E7%95%8C%E5%9D%90%E6%A0%87%E7%B3%BB%E3%80%81%E7%9B%B8%E6%9C%BA%E5%9D%90%E6%A0%87%E7%B3%BB%E3%80%81%E5%9B%BE%E5%83%8F%E5%9D%90%E6%A0%87%E7%B3%BB%E3%80%81%E5%83%8F%E7%B4%A0%E5%9D%90%E6%A0%87%E7%B3%BB&amp;spm=1018.2226.3001.4187&quot;&gt;[计算机视觉] 相机成像模型之四个坐标系 CSDN&lt;/a&gt;&lt;/p&gt;</code></pre></details><p>(预备知识) 相机模型：从 物理世界坐标系（一般会化为极坐标系）——<font color=#0091ff>相机坐标系</font>——<font color=#4eb434>成像平面坐标系（图像坐标系、归一化坐标系NDC）</font>——<font color=#df8400>像素坐标系</font> 的转换。</p><ul><li>世界坐标系：描述相机位置，(Xw,Yw,Zw)</li><li>相机坐标系：相机坐标系是连接图像物理坐标系与世界坐标系的桥梁，镜头的光心为原点，(Xc,Yc,Zc)，也是投影中心</li><li>图像物理坐标系：相机坐标系的Z轴与图像的交点为原点，(x,y)</li><li>像素坐标系：图像的左上角为原点，单位pixel，（u,v）</li></ul><p>我们可以从拍摄的图片上一个点的坐标，利用相机内参外参，反推出世界中那个点的坐标，进行三维重建。相机模型的总体示意图如下：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240627105013793.png" alt="image-20240627105013793" /><figcaption aria-hidden="true">image-20240627105013793</figcaption></figure><p><strong>焦距<span class="math inline">\(f\)</span>​</strong> （Focal Length）：从相机镜头到成像平面（图像传感器或胶片）的距离。它通常以毫米（mm）为单位表示。焦距越大，相机拍摄的图像视野就越窄，物体看起来越远。</p><p><strong>主点</strong>（Principal Point）： 主点是图像传感器平面上的光学中心点，通常以像素坐标表示。它表示图像传感器的中心位置。</p><p><strong>像素尺寸</strong>（Pixel Size）： 像素尺寸表示图像传感器上相邻像素之间的物理距离，通常以毫米为单位。</p><p>Raster在德语中是屏幕的意思（Raster == screen in German），而<strong>光栅化</strong>就是<strong>把东西画在屏幕上</strong>的一个过程（Rasterize == drawing onto the screen ），也就是3D——&gt;2D。</p><p>屏幕是由一行行一列列像素组成的。我们先梳理下如何用相机拍摄物体，如苹果：</p><h2 id="世界坐标系">世界坐标系</h2><p><strong>第一步，摆放好苹果（model transformation）</strong>。此时苹果处在世界坐标系中，构建世界坐标系只是为了更好的描述相机的位置在哪里，在双目视觉中一般将世界坐标系原点定在左相机或者右相机或者二者X轴方向的中点。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240722092842428.png" alt="image-20240722092842428" style="zoom:67%;" /></p><h2 id="世界坐标系相机坐标系外参矩阵">世界坐标系—&gt;相机坐标系（外参矩阵）</h2><p><strong>第二步，找到一个好角度放相机（观测变换 view transformation）</strong>。即从不同的视角看物体有不同的形态，是从世界坐标系——&gt;相机坐标系的过程。该过程本质是仿射变换：<span class="math inline">\(X_c=RX_w+T\)</span>​ ，属于刚体变换——物体不会发生形变，只进行旋转和平移。一言以蔽之，“横看成岭侧成峰，远近高低各不同”。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240722093541877.png" alt="image-20240722093541877" style="zoom:67%;" /></p><p>首先明确旋转在二维中是绕着某一点进行旋转，三维中是绕着某一个轴进行旋转。二维中最简单的场景是绕着坐标原点进行旋转。这里绕着不同的坐标轴旋转不同的角度，得到相应的旋转矩阵，将三个方向轴的旋转矩阵相乘得到最终的旋转矩阵如下：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240722093735618.png" alt="image-20240722093735618" style="zoom:67%;" /></p><p>上侧图中将世界坐标系绕 z 轴顺时针旋转θ得到相机坐标系。其中 [ x ′ , y ′ ] 为世界坐标系下的坐标,将[ x ′ , y ′ ]在相机坐标系下的坐标记为[ x , y ]。z轴坐标不发生改变。相当于我们沿z轴做旋转。</p><h3 id="代码">代码</h3><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240826192724764.png" alt="image-20240826192724764" /><figcaption aria-hidden="true">image-20240826192724764</figcaption></figure><p>函数中输入的 <span class="math inline">\(R、T\)</span> 是相机外参，输出是从世界坐标系向相机坐标系的变换矩阵。</p><h2 id="相机坐标系图像坐标系3d-to-2d内参矩阵">相机坐标系—&gt;图像坐标系（3D to 2D、内参矩阵）</h2><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240622154536395.png" alt="image-20240622154536395" style="zoom:67%;" /></p><p><strong>第三步，进行投影变换（projection transformation），将空间中的苹果转换到单位立方体中。</strong>这步是相机坐标系3D——&gt;归一化坐标系/图像坐标系2D的过程，有两种选择，分别是正交投影（Orthographic projection）和透视投影（Perspective projection）。此时投影点p的单位还是mm，并不是pixel，需要进一步转换到像素坐标系。</p><p><span class="math inline">\(Z_c\)</span> 可以理解为“深度”。在从三维的相机坐标系—&gt;二维的成像坐标系时，一般会去除深度使其变为二维的。下图是透视投影的原理，利用<strong>相似三角形</strong>进行计算。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240722094248917.png" alt="image-20240722094248917" style="zoom:67%;" /></p><ul><li><p><strong>正交投影</strong>与 <span class="math inline">\(z\)</span> 方向无关，有缩放、平移操作，是仿射变换，属于平行投影中光线垂直于视平面时的特例。例如对于立方体 <span class="math inline">\([l,r]×[b,t]×[f,n]\)</span>,我们先将其平移到原点（Translate），然后再将其缩放为 <span class="math inline">\([-1,1]^3\)</span>​​ 的单位立方体（Scale）。整个 <span class="math inline">\(M_{ortho}\)</span> 矩阵由缩放矩阵和平移矩阵相乘而得。<img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240627164009813.png" alt="image-20240627164009813" style="zoom:80%;" /></p></li><li><p><strong>透视投影</strong>与 <span class="math inline">\(z\)</span>​ 方向相关，是非仿射变换（非线性），更符合人眼的成像原理，有“远小近大”的特点。透视投影先将视锥体压成立方体，然后再正交投影。它通过矩阵 <span class="math inline">\(M_{persp \rightarrow ortho}\)</span>​ 进行锥体变方体的操作。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240627164728931.png" alt="image-20240627164728931" style="zoom:80%;" /></p></li></ul><h2 id="图像坐标系像素坐标系内参矩阵">图像坐标系—&gt;像素坐标系（内参矩阵）</h2><p><strong>第四步，通过视口变换矩阵把单位立方体映射到屏幕空间（像素坐标系） ，得到屏幕空间中的很多三角形。</strong>这步是图像坐标系——&gt;像素坐标系的过程。这里的视口变换主要是进行“拉伸”操作，将第三步中产生的形变拉回原样，即将 <span class="math inline">\([-1,1]^2\)</span> 的正方形变换回 <span class="math inline">\([0,w]×[0,h]\)</span> 。<u>注意这一步也是与 <span class="math inline">\(z\)</span>​ 方向无关的。</u></p><p>图像坐标系和像素坐标系都在成像平面上，只是各自的原点和度量单位不一样。图像坐标系的原点为相机光轴与成像平面的交点，通常情况下是成像平面的中点或者叫主点 principal point。图像坐标系的单位是mm，属于物理单位，而像素坐标系的单位是pixel，我们平常描述一个像素点都是几行几列。所以这二者之间的转换如下：其中dx和dy表示x方向和y方向的一个像素分别占多少个mm，即1 pixel=dx*dy <span class="math inline">\(mm^2\)</span>（如 dx=3,dy=4 时，1 pixel=3×4 <span class="math inline">\(mm^2\)</span>），它是是反映现实中的图像物理坐标关系与像素坐标系转换的关键。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240722095242543.png" alt="image-20240722095242543" style="zoom:67%;" /></p><p><span class="math inline">\(u_0\)</span>，<span class="math inline">\(v_0\)</span>代表图像的中心像素坐标和图像原点像素坐标之间相差的横向和纵向像素数，<span class="math inline">\(u_0\)</span>和<span class="math inline">\(v_0\)</span>​一般不是正好是图像分辨率的一半，其是有偏差的，一般越好的摄像头则其越接近于分辨率的一半。</p><blockquote><p><strong>需要注意的是，坐标的原点（0, 0）通常位于图像的左上角</strong>，x 坐标向右增加，y 坐标向下增加。像素坐标直接对应于图像中的具体像素位置。</p></blockquote><h2 id="光栅化">光栅化</h2><p><strong>第五步，进行光栅化，把三角形打碎成像素，并给每个像素赋值，然后屏幕上就可以显示出苹果了</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240622155320689.png" alt="image-20240622155320689" style="zoom: 50%;" /></p><p>这个完整的过程称为“<strong>光栅化</strong>”。其中第一到三步可以统称为<u>空间变换</u>。</p><p>第五步中，三角形打碎成像素其实就是判断三角形包围盒覆盖的范围中有哪些像素中心在包围盒内，然后给中心在盒内的像素均匀上色。事实上，这一步会导致“锯齿（Jaggies）”。<strong>有两个原因导致锯齿：一是像素本身有一定的大小，二是采样的速度跟不上信号变化的速度（高频信号采样不足）。</strong> 我们将锯齿尽可能消除的方法就是<strong>光滑</strong>。</p><h2 id="相机内外参">相机内外参</h2><p>通过上面四个坐标系的转换就可以得到一个点从世界坐标系如何转换到像素坐标系的，下图的公式可以进一步化简，见总结。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240722100829693.png" alt="image-20240722100829693" /><figcaption aria-hidden="true">image-20240722100829693</figcaption></figure><p>其中相机的内参和外参可以通过张正友标定获取（戳<a href="http://blog.csdn.net/chentravelling/article/details/53282586">这里查看张正友标定的资料</a>）。通过最终的转换关系来看，一个三维中的坐标点，的确可以在图像中找到一个对应的像素点，但是反过来，通过图像中的一个点找到它在三维中对应的点就很成了一个问题，因为我们并不知道等式左边的深度Zc的值。</p><p><strong>相机外参：平移矩阵，旋转向量。世界坐标通过旋转平移到相机坐标</strong></p><p>MATLAB标定工具箱：标定的像素误差在0.1左右 opencv的StereoClibration标定函数：标定的像素误差在0.4以上</p><p><strong>相机内参：图片的宽和高，主点的像素坐标cx cy，焦距的像素坐标 fx fy，畸变参数</strong></p><ul><li>int <strong>width / height</strong>: 图片宽度 / 高度（以像素为单位，如1024*1024）</li><li>float <span class="math inline">\(f_x、f_y\)</span> ：表示水平和垂直方向上以像素为单位的焦距，<span class="math inline">\(f_x=f/dx\)</span> 且 $ f_y=f/dy $ .</li><li>float <span class="math inline">\(c_x、c_y\)</span>: 表示图片主点在水平或垂直方向上的坐标，以距左或上边缘的像素偏移量。其引入是为了解决成像平面原点（主点）与像素坐标原点位置不一致的问题。</li><li><strong>图像畸变参数</strong>（Distortion Parameters）： 这些参数用于校正相机镜头引起的畸变，包括径向畸变和切向畸变。在 "Simple Radial" 模型中，通常会包含一个或多个径向畸变参数，以纠正图像中的弯曲效果。</li></ul><h2 id="畸变情况下相机模型">畸变情况下相机模型</h2><p><a href="https://github.com/SHU-FLYMAN/CalibCamera/blob/master/docs/01%20理论部分：单目成像过程.md">CalibCamera/docs/01 理论部分：单目成像过程.md at master · SHU-FLYMAN/CalibCamera · GitHub</a></p><h2 id="相机模型总结">相机模型总结</h2><blockquote><p><a href="https://www.bilibili.com/video/BV1C84y1R7qE/?spm_id_from=333.337.search-card.all.click&amp;vd_source=124ec79ebd3e16b0f454a3994a468f98">多视图几何MVS简介及MVSNet如何高效入门？_哔哩哔哩_bilibili</a></p></blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240627105508668.png" alt="image-20240627105508668" /><figcaption aria-hidden="true">image-20240627105508668</figcaption></figure><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240627143733151.png" alt="image-20240627143733151" style="zoom:67%;" /></p><p>上面橘色的块就是“<strong>内参矩阵</strong>”，绿色块则是“<strong>外参矩阵</strong>”。内参矩阵中，最后一行通常保持不变，因为它是用于保持矩阵格式的。内参会在相机校准过程中获得，通过捕捉已知空间点在图像中的投影位置并使用数学方法进行计算。校准的结果将允许相机进行精确的三维-二维坐标转换，从而实现准确的图像捕捉和计算。</p><h2 id="colmap-原理">colmap 原理</h2><p>做配准等。该软件的算法主要从两篇文章中提炼出来：（1）Structure-from-Motion Revisited 和（2）Pixelwise View Selection for Unstructured Multi-View Stereo 。</p><h1 id="sfm与mvs">SFM与MVS</h1><details><summary>参考</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/Vpn_zc/article/details/124602747&quot;&gt;SFM和MVS&lt;/a&gt;&lt;/p&gt;</code></pre></details><p><strong>SFM</strong>：Structure From Motion 从运动恢复结构 <strong><a href="https://link.springer.com/content/pdf/10.1007/978-3-319-46487-9_31.pdf">MVS</a></strong>：Multi-View Stereo 多视图立体视觉</p><p>对于SFM，可以重建稀疏点云和标定相机的内外参数，输入是没有标定的图像； 对于MVS，可以重建稠密点云，是在SFM的基础上做的，输入是标定的图像（具有相机参数）。</p><p>关系总结：SFM给MVS算好了输入视角的位姿,内参,稀疏点云以及它们的共视关系,MVS再利用这些信息以及彩色图来估计深度图以及做最后的Fusion,还有点云过滤等等的。SFM是camera tracking, 而MVS是depth map estimation和depth fusion。在实际使用中，一般是SFM进行相机标定，然后采用MVS重建稠密点云。</p><h2 id="mvs">MVS</h2><p>具体看 <a href="https://blog.csdn.net/Yong_Qi2015/article/details/124358031">一文详解colmap中的多视图重建算法原理-CSDN博客</a></p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 位姿估计 </tag>
            
            <tag> 相机模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python os库的正确使用姿势</title>
      <link href="/2024/11/24/python%20os%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
      <url>/2024/11/24/python%20os%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<p>首先，导入库:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></table></figure><p>然后我们就可以愉快地使用了~</p><table><colgroup><col style="width: 23%" /><col style="width: 76%" /></colgroup><thead><tr class="header"><th>功能</th><th style="text-align: center;">指令</th></tr></thead><tbody><tr class="odd"><td>切换工作目录</td><td style="text-align: center;">os.chdir('filepath')</td></tr><tr class="even"><td>执行终端指令</td><td style="text-align: center;">os.system('command')<br /><font color=#df8400>如 os.system('rm -r' + output_path)</font></td></tr><tr class="odd"><td>创建文件夹</td><td style="text-align: center;">法1. os.mkdir('filepath') 如果文件夹已经存在，将报错<br />法2. os.makedirs('filepath', exist_ok=True) 如果文件夹已经存在，也不会报错<br />法3. os.system('mkdir -p filepath') 其中，<code>-p</code> 表示如果路径中的父目录不存在，则将先创建父目录</td></tr><tr class="even"><td>判断文件夹是否存在</td><td style="text-align: center;">os.path.exists('filepath')</td></tr><tr class="odd"><td>连接路径</td><td style="text-align: center;">os.path.join('path1','path2')</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代码箱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3D Gaussian Splatting for Real-Time Radiance Field Rendering 论文笔记</title>
      <link href="/2024/11/24/3DGS%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>/2024/11/24/3DGS%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>论文名称：3D Gaussian Splatting for Real-Time Radiance Field Rendering (实时辐射场渲染的 3D 高斯泼溅)</strong></p><p>论文代码：<a href="https://github.com/graphdeco-inria/gaussian-splatting">GitHub - graphdeco-inria/gaussian-splatting: Original reference implementation of "3D Gaussian Splatting for Real-Time Radiance Field Rendering"</a></p><p>论文主页：</p></blockquote><h1 id="论文基本信息">论文基本信息</h1><p>3D Gaussian Splatting for Real-Time Radiance Field Rendering<sup>[1]</sup>是由来自法国蔚蓝海岸大学的 Bernhard 等人发表在 <strong>2023 SIGGRAPH</strong> 会议上的SCI论文。</p><p>3DGS 之所以如此大受欢迎，原因主要在于以下几个亮点：</p><ul><li><u>SOTA级别的视觉质量</u>：优于 Mip-NeRF360[Barron et al. 2022]<sup>[2]</sup></li><li>以1080p的分辨率进行<u>实时渲染</u>(<span class="math inline">\(\ge30fps\)</span>)</li><li>competitive 的训练时长：比 InstantNGP[Müller et al. 2022]<sup>[3]</sup>、 Plenoxels[Fridovich-Keil and Yu et al. 2022]<sup>[5]</sup>快,差不多一个场景训练几分钟</li></ul><p><strong>3D Gaussian Splatting 主要工作：稠密输入的实时新视图合成(相当于 instant-ngp 的训练速度达到了mip-nerf 360 的渲染质量，甚至更好)</strong></p><ol type="1"><li><font color=#ef042a>使用 3D Gaussian 球进行场景表示，并通过可微的 tile-based Splatting 技术以1080p的质量实时渲染出来。</font></li><li>3DGS 证明了可以使用非连续的表示来训练快速、高质量的辐射场。</li><li>除了80%的 python 代码外，使用了 InstantNGP 中的优化 CUDA kernels 来加速<u>光栅化 rasterization / 新视角合成</u>。</li></ol><p><strong>数据集</strong>：Mip-NeRF360 数据集、Deep Blending 数据集[Hedman et al. 2018]<sup>[4]</sup>、Tanks&amp;Temples 数据集、Synthetic NeRF 数据集（在这个数据集下，甚至在随机初始化的情况下也达到了高质量）</p><p><strong>特点：</strong></p><ul><li>无深度学习，只有简单的机器学习</li><li>大量的CG知识，主要是关于光栅化</li><li>对GPU的高性能编程</li></ul><p><strong>sparkling</strong>：</p><ul><li>3DGS 是重建辐射场的，可以考虑把它扩展到重建三维表面 mesh reconstruction？</li><li>强依赖 SFM 生成的点云</li></ul><h1 id="预备知识">预备知识</h1><h2 id="original-splatting">original splatting</h2><blockquote><p><a href="https://zhuanlan.zhihu.com/p/660512916">Splatting 抛雪球法简介 - 知乎 (zhihu.com)</a></p><p><a href="https://www3.cs.stonybrook.edu/~qin/courses/visualization/visualization-splatting.pdf">Slide 1 (stonybrook.edu)</a></p></blockquote><h2 id="dgs-中的-splatting">3DGS 中的 splatting</h2><h3 id="初步定义">初步定义</h3><p>splatting 是一种体渲染方法，从 3D 物体渲染到 2D 平面。它于1990年[9]被首次提出，比体渲染快，但是效果一般。3DGS 是基于2001年的 EWA Volume Splatting ，不同于 被动的ray-casting 计算出每个像素点受到发光粒子的影响来生成图像，splatting 是主动的，它计算出每个发光粒子如何影响像素点。具体区别参见 <a href="https://zhuanlan.zhihu.com/p/660512916">Splatting 抛雪球法简介 - 知乎 (zhihu.com)</a>，简言之</p><ul><li>Ray-casting：合成沿着光线进行，在每次插值采样和每个像素上执行</li><li>Splatting：合成沿着表格进行，在特定一组体素的所有像素同时执行</li></ul><p>splatting 算法也称为抛雪球法、足迹法，splatting 也可译为“喷溅”。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240411203920419.png" alt="image-20240411203920419" /><figcaption aria-hidden="true">image-20240411203920419</figcaption></figure><p>之所以用“splatting”即“泼溅”来形容抛雪球算法，是因为它将椭球体的 3D 高斯投影投影到图像平面成为椭圆的过程，就像水珠溅到二维平面上留下印子，是一个从立体三维到平面二维的过程。</p><h3 id="第一步捏雪球3dgs-表示形状">第一步：捏雪球(3DGS 表示形状)</h3><p>我们选择高斯核来作为“雪球”，因为 Gaussian 有很多好的数学性质，如经过仿射变换仍为高斯、从3D降到2D后仍然是高斯等。在高维情况下，椭球高斯函数为： <span class="math display">\[G(x)=\frac{1}{\sqrt{(2 \pi)^k |\varSigma|}}e^{-\frac{1}{2}(x-\mu)^T\varSigma^{-1}(x-\mu)}\]</span> 其中，<span class="math inline">\(\varSigma\)</span> 为协方差矩阵 <span class="math inline">\(\boldsymbol{\Sigma}=\left[\begin{array}{ccc} \sigma_{x}^{2} &amp; \sigma_{x y} &amp; \sigma_{x z} \\ \sigma_{y x} &amp; \sigma_{y}^{2} &amp; \sigma_{y z} \\ \sigma_{z x} &amp; \sigma_{z y} &amp; \sigma_{z}^{2} \end{array}\right]\)</span> （如 <span class="math inline">\(\sigma_{xy}\)</span> 是x与y的相关程度），它控制椭球的形状，半正定，"|<span class="math inline">\(\varSigma\)</span>​|"是其行列式。其中，矩阵的对角线元素分别代表了沿x、y、z轴的方差，决定椭球沿这些轴的拉伸程度；非对角线元素代表不同轴之间的协方差，决定了椭球的倾斜或旋转程度。</p><p><strong>各向异性（Anisotropic）</strong>指的是在不同方向具有不同的梯度，即不同方向的投影各不相同。由于高斯核是个椭球，所以说它是各向异性的。如果是个圆球，那么梯度在不同方向都是相同的，这时就是<strong>各向同性（Isotropic）</strong>的。<strong>任意高斯 <span class="math inline">\(x \sim N(\mu,\varSigma)\)</span> 都可以看作是标准高斯 <span class="math inline">\(x \sim N(\vec{0},I)\)</span> 通过仿射变换得到</strong></p><ul><li>仿射变换：<span class="math inline">\(w=Ax+b\)</span> , <span class="math inline">\(b\)</span> 表示平移矩阵</li><li>一个均值为 <span class="math inline">\(\mu\)</span> 、方差为 <span class="math inline">\(\varSigma\)</span> 的高斯分布经过仿射变换后服从：<span class="math inline">\(w \sim N(A\mu+b,A\varSigma A^T)\)</span></li></ul><p>因此，我们可以得到 <span class="math inline">\(\varSigma =A\cdot I\cdot A^T\)</span> 。又由仿射变换的概念知，<span class="math inline">\(A=RS\)</span>，<span class="math inline">\(R\)</span> 是旋转矩阵，<span class="math inline">\(S\)</span> 是缩放矩阵。于是： <span class="math display">\[\begin{align}\varSigma &amp;= A \cdot I \cdot A^T \\&amp;= R\cdot S \cdot I \cdot (R \cdot S)^T \\&amp;= R\cdot S \cdot (S)^T \cdot (R)^T\end{align}\]</span></p><h3 id="第二步抛雪球">第二步：抛雪球</h3><p>这里的预备知识1是关于光栅化的操作过程。简单来说，主要是观测变换——投影变换——视口变换——光栅化的过程。预备知识2是雅可比矩阵，其主要思想是已知非线性函数中的某一点，对其附近去求导，利用一个很小邻域内的导数（<span class="math inline">\(\frac{dy}{dx}\)</span>）来近似该点附近的线性函数导数（<span class="math inline">\(y_x&#39;\)</span>）（在一个很小的区域内，非线性也会变成线性的），即“对非线性变换的局部线性近似”。<u>在3D高斯中是选取高斯中心点附近求雅可比矩阵</u>。</p><p>1.对于物理世界坐标系的一个3D高斯，设其</p><ul><li>均值：<span class="math inline">\(t_k\)</span></li><li>协方差矩阵：<span class="math inline">\(V^{&#39;&#39;}_k\)</span>​</li><li>高斯核中心：<span class="math inline">\(t_k=[t_0,t_1,t_2]^T\)</span> , <font color=#ef042a>认为高斯的中心点位置 <code>self._xyz</code> 就是高斯的均值</font></li><li>高斯核：<span class="math inline">\(r^{&#39;&#39;}_k(t)=G_{V^{&#39;&#39;}_k}(t-t_k)\)</span></li></ul><p>2.首先，我们对该3D高斯进行观测变换 <span class="math inline">\(u=\varphi(t)=Wt+d\)</span>, 实现物理世界坐标系—&gt;相机坐标系的转变。在相机坐标系中，3D高斯的</p><ul><li>均值：<span class="math inline">\(u_k=Wt_k+d\)</span></li><li>协方差矩阵：<span class="math inline">\(V^{&#39;}_k=WV^{&#39;&#39;}_kW^T\)</span></li><li>高斯核中心：<span class="math inline">\(u_k=[u_0,u_1,u_2]^T\)</span></li><li>高斯核：<span class="math inline">\(r^{&#39;}_k(u)=G_{V^{&#39;}_k}(u-u_k)\)</span></li></ul><p>3.然后，我们对3D高斯进行投影变换,实现相机坐标系——&gt;归一化坐标系NDC。<u>这里的投影变换使用的是非线性的透视投影</u> <span class="math inline">\(x=m(t)\)</span> ，这对于仅仅是一个点的均值可行（一个点不会发生形变），但是对于协方差矩阵不可行。因此需要<strong>用到雅可比矩阵 <span class="math inline">\(J=\frac{ \partial m(u_k)}{\partial u}\)</span> 将其模拟成线性的变换，此时雅可比矩阵就相当于仿射变换矩阵</strong>。</p><ul><li>均值：<font color=#ef042a><span class="math inline">\(x_k=m(u_k)\)</span></font></li><li>协方差矩阵：<font color=#ef042a><span class="math inline">\(V_k=JV^{&#39;}_kJ^T=JWV^{&#39;&#39;}_kW^TJ^T\)</span> </font></li><li>高斯核中心：<span class="math inline">\(x_k=[x_0,x_1,x_2]^T\)</span></li><li>高斯核：<span class="math inline">\(r_k(x)=G_{V_k}(x-x_k)\)</span>​</li></ul><blockquote><p><strong>投影变换的雅可比矩阵</strong></p><p>我们知道，在进行投影变换时，如果做透视投影，那么需要先用非仿射变换函数 <span class="math inline">\(x=m(t)\)</span> 将其转换为四四方方的立方体，再进行正交投影。</p><p>如对于视锥中的一个点 <span class="math inline">\([x,y,z,1]^T\)</span>，先乘该非仿射变换的转换矩阵 <span class="math inline">\(M_{persp \rightarrow ortho}\)</span>： <span class="math display">\[\left[ \begin{matrix}  n&amp;      0&amp;      0&amp;      0\\  0&amp;      n&amp;      0&amp;      0\\  0&amp;      0&amp;      n+f&amp;        -nf\\  0&amp;      0&amp;      1&amp;      0\\\end{matrix} \right] \left[ \begin{array}{l}  x\\  y\\  z\\  1\\\end{array} \right] =\left[ \begin{array}{c}  nx\\  ny\\  \left( n+f \right) z-nf\\  z\\\end{array} \right] =\left[ \begin{array}{c}  \frac{nx}{z}\\  \frac{ny}{z}\\  \left( n+f \right) -\frac{nf}{z}\\  1\\\end{array} \right] =\left[ \begin{array}{c}  f_1\left( x \right)\\  f_2\left( y \right)\\  f_3\left( z \right)\\  1\\\end{array} \right]\]</span> <span class="math inline">\(n\)</span> 和 <span class="math inline">\(f\)</span> 表示z方向的两个量 near 和 far，实际编写代码时 n 也分为 focal_x 和 focal_y 两种情况 。将右侧齐次坐标中的1去掉，进行求导即得上文中雅可比矩阵如下： <span class="math display">\[J=\left[\begin{array}{lll}\frac{d f_{1}}{d x} &amp; \frac{d f_{1}}{d y} &amp; \frac{d f_{1}}{d z} \\\frac{d f_{2}}{d x} &amp; \frac{d f_{2}}{d y} &amp; \frac{d f_{2}}{d z} \\\frac{d f_{3}}{d x} &amp; \frac{d f_{3}}{d y} &amp; \frac{d f_{3}}{d z}\end{array}\right]=\left[\begin{array}{lll}\frac{n}{z} &amp; 0 &amp; -\frac{n x}{z^{2}} \\0 &amp; \frac{n}{z} &amp; -\frac{n y}{z^{2}} \\0 &amp; 0 &amp; \frac{n f}{z^{2}}\end{array}\right]\]</span></p></blockquote><p>4.最后，将3D高斯变换到像素坐标系。我们对<strong>均值</strong>做完非线性的投影变换后，其变到了NDC坐标系，范围是<span class="math inline">\([-1,1]^3\)</span> 。而对于<strong>协方差矩阵</strong>，它没有进行非线性的操作。因此<strong>只有均值/高斯核中心需要进行视口变换</strong>，即进行平移+缩放操作，将 <span class="math inline">\([-1,1]^2\)</span> 的正方形变换回 <span class="math inline">\([0,w]×[0,h]\)</span>。<strong>对于协方差则利用其进行足迹渲染 <span class="math inline">\(G(\hat{x})=exp(-\frac{1}{2}(x-\mu)^TV_k^{-1}(x-\mu))\)</span></strong>, 点离均值越近，函数值就越大。</p><ul><li>均值：<span class="math inline">\(\mu=[\mu_1,\mu_2,\mu_3]^T\)</span></li><li>协方差矩阵：<span class="math inline">\(V_k=JV^{&#39;}_kJ^T=JWV^{&#39;&#39;}_kW^TJ^T\)</span></li><li>高斯核中心 ：<span class="math inline">\(\mu=[\mu_1,\mu_2,\mu_3]^T\)</span>​</li><li>高斯核（离散计算）：<span class="math inline">\(G(\hat{x})=exp(-\frac{1}{2}(x-\mu)^TV_k^{-1}(x-\mu))\)</span></li></ul><h3 id="雪球颜色球谐函数">雪球颜色(球谐函数)</h3><p>3DGS 中的 SH 参考的是 [Fridovich-Keil and Yu et al. 2022; Müller et al. 2022].</p><p><strong>球谐函数是一组基函数，任何一个球面坐标的函数 <span class="math inline">\(f(t)\)</span> 都可以用多个球谐函数来近似</strong>： <span class="math display">\[f(t)\approx \sum_l\sum_{m=-l}^lc_l^my_l^m(\theta,\phi)\]</span> 其中，<span class="math inline">\(c_l^m\)</span> 是各项系数，<span class="math inline">\(y_l^m\)</span> 是球谐基函数，方向<span class="math inline">\((\theta,\phi)\)</span>中的 <span class="math inline">\(\theta\)</span> 和 <span class="math inline">\(\phi\)</span> 分别表示仰角和方位角。那么对于任意球面函数，<strong>我们只需知道球谐系数和方向</strong>，再用系数去乘相应的球谐基即可得到函数在该方向的值了。具体计算过程如下：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240627202726854.png" alt="image-20240627202726854" /><figcaption aria-hidden="true">image-20240627202726854</figcaption></figure><p>在实际编程中，我们常用 $=<span class="math inline">\(、\)</span>=$ 等式子来简化计算，且提前计算出能预计算的常数进行保存。当使用球谐系数 <span class="math inline">\(c_l^m\)</span> 来表示三通道的RGB颜色时，通常取到下标阶数为 3 的球谐基。也就是，一共取了 3*(1+3+5+7)=3*16 个球谐系数。</p><h3 id="第三步足迹合成">第三步：足迹合成</h3><p>我们将捏好的雪球一个接一个抛在成像平面上，就能得到一个接一个的雪球印 (<strong>footprint</strong>)。这些足迹有的有重叠，有的没有，很容易想到后抛的雪球印会覆盖先抛的雪球印(<strong>深度排序</strong>)，而我们最后得到的图像就是这些雪球印的融合渲染结果(<strong>α-blending</strong>)。</p><p>实际上，虽然每抛一个高斯雪球就会留下一个足迹，但是在实际代码中对留下每个的足迹 —大小 H×W×3— 仍然是逐像素遍历进行赋值渲染的。<font color=#df8400><strong>这里作者利用到 GPU 的高性能进行渲染加速</strong>。用GPU的一个线程负责一个像素，所有线程并发运行进行渲染。此外，</font></p><ul><li>把整个图像划分为16*16个tiles, 每个tile视锥内挑选可视的3D高斯——每个视锥内只取置信度大于 99% 的高斯实例化为对象，并按所在视域深度进行排序</li><li>高斯按深度的从近到远，并行地在对应tile上splat</li><li>有像素的不透明度达到饱和就停止对应线程</li><li>反向传播误差时按tile对高斯进行索引</li></ul><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240629132954884.png" alt="image-20240629132954884" /><figcaption aria-hidden="true">image-20240629132954884</figcaption></figure><blockquote><p>GPU 和 CPU 好比 两千个小学生和一个老教授。当进行简单的小学计算时，这些小学生同时一人写一道题，能瞬间写完一份2000题的卷子，比孤身奋战的老教授快得多。但是在面对一道高数题时，老教授能慢慢悠悠写完，而这两千个小学生一道题都写不出。</p></blockquote><p>渲染雪球颜色用到的公式与NeRF中的体渲染公式大体类似： <span class="math display">\[\begin{align}&amp;C=\sum_{i=1}^{N} T_{i}\alpha_i \mathbf{c}_{i},\\&amp;\text { where } \  T_{i}=\exp \left(-\sum_{j=1}^{i-1} \sigma_{j} \delta_{j}\right)= \prod^{i-1}_{j=1}{(1-\alpha_i)},\\&amp; \ \ \ \ \ \ \ \ \ \ \ \ \ \alpha_i= 1-\exp \left(-\sigma_{i} \delta_{i}\right),\tag{1}\label{eq1}\end{align}\]</span> 其中</p><ul><li><span class="math inline">\(T_i\)</span> 是透射比，即光线不被遮挡的概率，初始值为1，需要通过 <span class="math inline">\(\alpha_i\)</span> 计算得到。<u>在按抛高斯雪球顺序计算像素颜色值时，如果前面的高斯雪球已经使 <span class="math inline">\(T\)</span> 值很小时，就不再计算后面雪球的影响；相反如果所有高斯雪球遍历完后 <span class="math inline">\(T\)</span> 值仍然不够大，就设置颜色为背景色——黑色</u>。</li><li><span class="math inline">\(\alpha_i\)</span> 是不透明度，需要计算。这里的不透明度有两层含义：1.高斯椭球本身的不透明度；2.像素离高斯椭球越远，高斯椭球对该像素影响越小。</li><li><span class="math inline">\(c_i\)</span> 是通过球谐函数算得的颜色值，是已知的。</li></ul><p>但<strong>不同的是</strong>，splatting 中没有用光线穿过逐个像素，对光线上的粒子颜色进行采样求和的过程。相反地，它根据抛雪球的顺序 即深度 <span class="math inline">\(z\)</span>​​ ，对高斯球进行深度排序。</p><p>首先，外循环遍历每一个像素，通过GPU对每个单一像素进行操作；然后内循环遍历每一个高斯，由深度的近到远对高斯颜色进行渲染得到指定像素的颜色。这样遍历后，就得到了最后的所有颜色。</p><h1 id="算法全貌">算法全貌</h1><p><strong>动机</strong>：目前，最有效的辐射场方案一般建模在通过插值<u>体素[Fridovich-Keil and Yu et al. 2022]<sup>[5]</sup>、哈希网格[Müller et al. 2022]<sup>[3]</sup>和点[Xu et al. 2022]<sup>[6]</sup></u>得到的连续表示，虽然这些模型的连续性有助于优化(Plenoxels 和 InstantNGP快，MipNeRF360 质量高)，然而渲染时需要的 stochastic sampling 是 costly 且可能导致噪声的。——具体见“3DGS 与 NeRF 的区别与联系”</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240413093014740.png" alt="image-20240413093014740" /><figcaption aria-hidden="true">image-20240413093014740</figcaption></figure><p><strong>总结</strong>：模型的输入是一组图片，首先通过 SfM 方法匹配这组场景图片的关键点从而给出该场景的点云，同时也估计了相机位姿。以该初始点云的各个点为中心，膨胀得到各个 3D 高斯椭球。（<span style="background:#eef0f4;">椭球的初始形状是各向同性的球(Isotropic Gaussians)。使用knn法，找到每个高斯椭球的3近邻，将高斯椭球的半径初始化为3近邻的距离的平均。</span>）</p><p>然后结合先前估计的相机位姿，通过GPU对3D高斯球实现高速的 tile-based splatting 渲染/光栅化得到图片，与真实图片比较得到损失。</p><p>最后通过SGD反向传播梯度优化，优化时改变的主要有中心点位置、不透明度 <span class="math inline">\(\alpha\)</span> 、协方差矩阵、球谐系数等3D Gaussians 属性。同时还根据传递的梯度进行自适应的密度控制，自适应的密度控制的判断标准是传递的梯度超过阈值时，认为重建不充分。</p><blockquote><p>参数：</p><ul><li>假设初始点云有10000个点</li><li>每个点膨胀成 3D高斯椭球</li><li>则每个椭球的参数包括：<ul><li>中心点位置：(x,y,z)</li><li>协方差矩阵：R、S</li><li>球谐函数系数：16×3</li><li>不透明度：α</li></ul></li></ul></blockquote><p><strong>算法</strong>：渲染+SGD+Sigmoid/Exponential激活函数。这里的Sigmoid激活函数是用来将不透明度 <span class="math inline">\(\alpha\)</span> 限制在[0,1]内。</p><p><strong>损失函数</strong>：<span class="math inline">\(L=(1-\lambda)L_1+\lambda L_{D-SSIM}, \ \lambda=0.2\)</span>。前者的L1损失是对渲染图片和真实图片做光度误差，后者是结构相似性误差。</p><p><strong>问题</strong>：<font color=#ef042a>强依赖 SFM 生成的初始点云</font></p><p><strong>解决方案</strong>：打补丁（Adaptive Control of Gaussians）</p><ul><li>重建不充分的区域往往有较大的梯度，即中心点 <code>self._xyz</code> 梯度大的高斯认为误差也大，需要 densify。根据高斯的尺度 <code>self._scaling</code>进行不同的 densify 操作<ul><li>scale 太大的高斯拆分—over-reconstruction</li><li>scale 太小的高斯复制—under-reconstruction</li></ul></li><li>存在感太低的剔除（每100次迭代移除不透明度小于阈值的点）</li><li>周期性将不透明度重置为0用于去除floaters、周期性移除较大的高斯用于避免重叠</li></ul><p><strong>第一部分：用 3D Gaussian 作为场景表示</strong></p><p>我们模型的输入是一组静态场景的图片，然后使用SfM方法 [Snavely et al. 2006]<sup>[7]</sup>，<u>通过匹配这组场景图片的关键点得到该场景的稀疏点云，同时也估计了相机位姿</u>。用 SfM 过程产生的稀疏点云初始化一组 3D Gaussians。（<span style="background:#eef0f4;">注意其他方法往往还需要多视图立体化 MVS 进行几何初始化，以此作为 SfM 的辅助。因此它们不可避免地继承了来自 MVS 的 artifacts ,在 featureless/shiny 的区域或者薄结构处出现 over 或 under 重建</span>）。</p><p><strong>第二部分：optimization 过程</strong></p><p>在这里将优化 3D 位置、不透明度 <span class="math inline">\(\alpha\)</span> 、各向异性协方差/三维协方差矩阵、颜色/球谐系数等3D Gaussians 属性。优化属性的步骤与自适应的密度控制步骤相交织，在这个过程中我们将添加和偶尔删除 3D Gaussians。Optimization 过程最后将产生所有被测试场景的 1-5 million 高斯的高质量表示（<span style="background:#eef0f4;">in part because highly anisotropic volumetric splats can be used to represent fine structures compactly</span>）。</p><p><strong>第三部分：使用快速GPU排序算法和 tile-based 的 Splatting 技术进行实时渲染</strong></p><p>这一部分是受启发于最近的一个<font color=#985fff> tile-based 的光栅化工作</font>[Lassner and Zollhofer 2021]<sup>[8]</sup>。这里作者的用法有所不同：However, thanks to our 3D Gaussian representation, we can perform anisotropic splatting that respects visibility ordering – <font color=#985fff>thanks to sorting and 𝛼-blending</font> – and enable a fast and accurate backward pass by tracking the traversal of as many sorted splats as required.</p><p>也就是说，作者在这一部分主要使用 <font color=#985fff>GPU 排序算法+tile-based 光栅化+<span class="math inline">\(\alpha\)</span>-blending</font> 来快速渲染高质量的新视图。且该过程是 visibility-aware 的。对应关系分别是：</p><ul><li>tile-based rasterizer——𝛼-blending of anisotropic splats</li><li>fast sorting——respecting visibility</li></ul><h1 id="实验">实验</h1><h1 id="代码">代码</h1><h2 id="方法梳理">方法梳理</h2><p>输入一系列图像，使用COLMAP从中构建点云，然后为点云中的每个点去初始化一个3D gaussian。每个3D gaussian都包含一组可学习的参数<font color=#ef042a>14×H×W</font>，包括中心点位置<code>self._xyz</code><font color=#ef042a>3</font>（3D gaussian的均值）、球谐系数<code>self._features_dc</code>和<code>self._features_rest</code>（用来表示颜色） <font color=#ef042a>假设为共3个</font>、尺度<code>self._scaling</code><font color=#ef042a>3</font>、旋转参数<code>self._rotation</code><font color=#ef042a>4</font>、不透明度<code>self._opacity</code><font color=#ef042a>1</font>。对<code>self._scaling</code>和<code>self._rotation</code>使用<code>build_covariance_from_scaling_rotation</code>，可以获得3D gaussian的协方差矩阵（表示了每个3D gaussian的大小、形状和方向）。这些可学习的参数通过<code>self.optimizer</code>进行优化。</p><p>给定一个相机视角，可以根据这些参数将这些3D gaussians投影到2D，得到对应的2D gaussian。通过将这些2D gaussian按照深度的顺序进行混合，就可以得到新视角2D图像上每个pixel的颜色，进而得到渲染好的2D图像。在渲染所得的2D图像和真实的2D图像之间进行loss的计算，从而可以优化每个3D gaussian对应的可学习参数。</p><p>为了应对欠重建和过重建，在训练期间还会每隔一定间隔，对那些均值<code>self._xyz</code>的梯度超过一定阈值且尺度<code>self._scaling</code>小于一定阈值的3D gaussian进行克隆操作，对于那些均值<code>self._xyz</code>的梯度超过一定阈值且尺度<code>self._scaling</code>大于一定阈值的3D gaussian进行分割操作，并将opacity小于一定阈值的3D gaussians进行删除。</p><h2 id="代码构成">代码构成</h2><ol type="1"><li><p>训练流程: <code>train.py</code>中的<code>training</code>函数;</p></li><li><p>数据加载 + 模型创建: <code>scene/__init__.py</code>中的<code>Scene</code>类的<code>__init__</code>函数;</p></li></ol><p>3DGS只有两个数据加载器一个是Blender,另一个是colmap数据加载器; Blender数据集用的是随机点云，不是从SFM生成的, blender数据集用的是box内随机生成点云。</p><ol start="3" type="1"><li>模型代码: <code>scene/gaussian_model.py</code>中的<code>GaussianModel</code>类;</li></ol><p>关于初始化的3D高斯：</p><p>~/gaussian-splatting/submodules</p><ul><li>diff-gaussian-rasterization：cuda加速渲染</li><li>simple-knn：通过knn法初始化椭球的半径</li></ul><ol start="4" type="1"><li><p>图像渲染 python 调用部分: <code>submodules/diff-gaussian-rasterization/diff_gaussian_rasterization/__init__.py</code>中的<code>_RasterizeGaussians</code>类;</p></li><li><p>图像渲染 CUDA 实现部分: <code>submodules/diff-gaussian-rasterization/rasterize_points.cu</code>中的<code>RasterizeGaussiansCUDA</code>类和<code>RasterizeGaussiansBackwardCUDA</code>类;</p></li><li><p>渲染前传 CUDA 实现部分: <code>submodules/diff-gaussian-rasterization/cuda_rasterizer/rasterizer_impl.cu</code>中的<code>int CudaRasterizer::Rasterizer::forward</code>函数;</p></li><li><p>渲染反传 CUDA 实现部分: <code>submodules/diff-gaussian-rasterization/cuda_rasterizer/rasterizer_impl.cu</code>中的<code>void CudaRasterizer::Rasterizer::backward</code>函数;</p></li></ol><h1 id="dgs-与-nerf-的区别与联系">3DGS 与 NeRF 的区别与联系</h1><h2 id="从-ray-marching-的体渲染到-point-based-alpha-blending-渲染">从 ray-marching 的体渲染到 point-based <span class="math inline">\(\alpha\)</span>-blending 渲染</h2><p>NeRF 中计算<strong>每个像素点</strong>颜色的体渲染公式： <span class="math display">\[\hat{C}_{c}(\mathbf{r})=\sum_{i=1}^{N_{c}} w_{i} c_{i}, \quad w_{i}=T_{i}\left(1-\exp \left(-\sigma_{i} \delta_{i}\right)\right) \tag{1} \\T_{i}=\exp \left(-\sum_{j=1}^{i-1} \sigma_{j} \delta_{j}\right)\]</span> 我们也可以将 NeRF 中这个 ray-marhing 的体渲染写成另一种形式，即： <span class="math display">\[\begin{align}&amp;C=\sum^N_{i=1}{T_i \alpha_i \mathbf{c}_i}, \tag{2} \\&amp;where \ \ \alpha_i=(1-exp(-\sigma_i\delta_i))\\&amp;\ \ \ \ \ \ \ \ \ \ \  \ \ T_i=\prod^{i-1}_{j=1}{(1-\alpha_i)}\end{align}\]</span> 通过上式，可计算得射线穿过的那个像素的颜色值。而一种经典的基于点的 neural 方法[Kopanas et al. 2022,2021]通过融合 <span class="math inline">\(\mathcal{N}\)</span> 个 ordered points 来覆盖该像素，从而得到该像素的颜色，计算公式如下： <span class="math display">\[C=\sum_{i \in \mathcal{N}}{c_i\alpha_i \prod^{i-1}_{j=1}{(1-\alpha_i})}，\tag{3}\]</span> 这里 <span class="math inline">\(\mathcal{c}_i\)</span> 是每个 point 的颜色，而不是每个采样点的颜色。<span class="math inline">\(\alpha_i\)</span> 是通过<u>用协方差 <span class="math inline">\(\varSigma\)</span> [Yifan et al. 2019]乘以学习到的 per-point 不透明度</u>来估计的 2D Gaussian 给出。</p><p><span style="background:#fbd4d0;">可以看出上面两种模型成像公式是一样的，但是这两种渲染算法是很不同的。</span></p><ol type="1"><li>NeRF 是连续表示，且通过隐式表达体现占据空间；而 points 是非结构化的离散表示，能更为灵活地构筑几何；</li><li>NeRF 需要 expensive 的随机抽样来采样 (2) 式中的样本点，由此产生噪声和很大的计算成本，因此无法达到 real-time。</li></ol><h2 id="nerf-vs.-3dgs"><strong>NeRF vs. 3DGS</strong></h2><p>在保持 <span class="math inline">\(\alpha\)</span>-blending 表示的体渲染优势的同时，结合 Pulsar[Lassner and Zollhofer 2021]<sup>[8]</sup> 的球体光栅化，3DGS 创造了一种 tile-based 排序渲染器。这种渲染器和 NeRF 中的渲染有着许多差别：训练时，NeRF 以<strong>每个像素</strong>为样本单位，而 3DGS 则以<strong>每张图片</strong>为样本单位，在训练完一张图片的颜色后才反向传播梯度，更新网络的参数。</p><p>此外，3DGS 还创新性地开创了用非连续的表达来进行快速高质量辐射场训练的先河：</p><ul><li>NeRF 的整体模型是连续可微、隐式表达的；</li><li>传统的 mesh 模型是纯离散（通过插值得到不同位置的值）、显式表达的；</li><li>3DGS 介于离散和连续之间：在高斯球内部连续可微、每个高斯球之间是离散的。</li></ul><h1 id="bib-tex">Bib Tex</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@Article&#123;kerbl3Dgaussians,</span><br><span class="line">      author       = &#123;Kerbl, Bernhard and Kopanas, Georgios and Leimk&#123;<span class="keyword">\&quot;</span>u&#125;hler, Thomas and Drettakis, George&#125;,</span><br><span class="line">      title        = &#123;3D Gaussian Splatting for Real-Time Radiance Field Rendering&#125;,</span><br><span class="line">      journal      = &#123;ACM Transactions on Graphics&#125;,</span><br><span class="line">      number       = &#123;4&#125;,</span><br><span class="line">      volume       = &#123;42&#125;,</span><br><span class="line">      month        = &#123;July&#125;,</span><br><span class="line">      year         = &#123;2023&#125;,</span><br><span class="line">      url          = &#123;https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="参考文献">参考文献</h1><blockquote><ul><li>适合小白阅读：<a href="https://zhuanlan.zhihu.com/p/688636817">NeRF与3DGS速通 - 知乎 (zhihu.com)</a></li><li>有很多基础知识：<a href="https://zhuanlan.zhihu.com/p/681085846">3d Gaussian splatting笔记 - 知乎 (zhihu.com)</a></li></ul></blockquote><p>[1] Kerbl B, Kopanas G, Leimkühler T, et al. 3d gaussian splatting for real-time radiance field rendering[J]. ACM Transactions on Graphics, 2023, 42(4): 1-14.</p><p>[2] Barron J T, Mildenhall B, Verbin D, et al. Mip-nerf 360: Unbounded anti-aliased neural radiance fields[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 5470-5479.</p><p>[3] Müller T, Evans A, Schied C, et al. Instant neural graphics primitives with a multiresolution hash encoding[J]. ACM transactions on graphics (TOG), 2022, 41(4): 1-15.</p><p>[4] Hedman P, Philip J, Price T, et al. Deep blending for free-viewpoint image-based rendering[J]. ACM Transactions on Graphics (ToG), 2018, 37(6): 1-15.</p><p>[5] Fridovich-Keil S, Yu A, Tancik M, et al. Plenoxels: Radiance fields without neural networks[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 5501-5510.</p><p>[6] Xu Q, Xu Z, Philip J, et al. Point-nerf: Point-based neural radiance fields[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 5438-5448.</p><p>[7] Snavely N, Seitz S M, Szeliski R. Photo tourism: exploring photo collections in 3D[M]//ACM siggraph 2006 papers. 2006: 835-846.</p><p>[8] Lassner C, Zollhofer M. Pulsar: Efficient sphere-based neural rendering[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 1440-1449.</p><p>[9] Westover, Lee Alan. <em>Splatting: a parallel, feed-forward volume rendering algorithm</em>. The University of North Carolina at Chapel Hill, 1991.</p>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SIGGRAPH 2023 </tag>
            
            <tag> 辐射场 </tag>
            
            <tag> splatting </tag>
            
            <tag> CUDA kernels 加速 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于selenium的爬虫笔记</title>
      <link href="/2024/11/16/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%8C%87%E5%8D%97/"/>
      <url>/2024/11/16/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<details><summary>参考链接</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/qq_16519957/article/details/128740502&quot;&gt;新版元素定位方法&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/qq_44728587/article/details/122997412&quot;&gt;强制等待、显式等待与隐式等待&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/huilan_same/article/details/52789954&quot;&gt;Python selenium —— 文件下载，不弹出窗口，直接下载到指定路径&lt;/a&gt;&lt;/p&gt;</code></pre></details><blockquote><p>搭建环境教程：<a href="https://blog.csdn.net/shiaohan/article/details/108834770">编写python代码实现打开并登录网页、对网页进行点击、输入信息等操作_python 网页登录-CSDN博客</a> (主要pip安装 <code>selenium</code> 和 <code>webdriver_manager</code>)</p><p>！！！selenium库无法与以下元素交互： 元素不可见、元素宽高为0、元素被其他元素遮挡</p><p><strong>以下笔记基于谷歌浏览器进行实验~</strong></p></blockquote><p>首先，初始化一个谷歌浏览器窗口 <code>driver</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver  </span><br><span class="line"><span class="keyword">from</span> webdriver_manager.chrome <span class="keyword">import</span> ChromeDriverManager  </span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.service <span class="keyword">import</span> Service  </span><br><span class="line"></span><br><span class="line"><span class="comment">#法1</span></span><br><span class="line">service = Service(ChromeDriverManager().install())  </span><br><span class="line">driver = webdriver.Chrome(service=service)</span><br><span class="line"></span><br><span class="line"><span class="comment">#法2（如果需要设置浏览器配置）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_driver</span>(<span class="params">options=<span class="literal">None</span></span>) -&gt; webdriver.Chrome:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    初始化浏览器驱动.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        options(Options): chrome配置选项</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        driver(WebDriver): 浏览器驱动对象</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> webdriver.Chrome(</span><br><span class="line">        service=Service(ChromeDriverManager().install()),</span><br><span class="line">        options=options</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">driver.get(<span class="string">&quot;要跳转的网址&quot;</span>)</span><br><span class="line">driver.switch_to.frame(<span class="string">&#x27;如果元素在frame里，还需要跳转进frame才能定位元素&#x27;</span>)</span><br></pre></td></tr></table></figure><p>然后，导入定位模块：<code>from selenium.webdriver.common.by import By</code>，根据以下八种元素定位方法进行定位操作。</p><ul><li><p>id 定位：</p><p><code>driver.find_element(By.ID,'ID')</code></p><p><code>driver.find_element('id','ID')</code></p></li><li><p>css 定位：</p><p><code>driver.find_element(By.CSS_SELECTOR,'[title="the title"]')</code></p><p><code>driver.find_element('css selector','[title="the title"]')</code></p></li><li><p>xpath 定位：</p><p><code>driver.find_element(By.XPATH,'the xpath')</code></p><p><code>driver.find_element('xpath','the xpath')</code></p></li><li><p>name 定位：</p><p><code>driver.find_element(By.NAME,'the name')</code></p><p><code>driver.find_element('name','the name')</code></p></li><li><p>tag_name 定位：</p><p><code>driver.find_element(By.TAG_NAME,'the tag name')</code></p><p><code>driver.find_element('tag name','the tag name')</code></p></li><li><p>class_name 定位：</p><p><code>driver.find_element(By.CLASS_NAME,'the class name')</code></p><p><code>driver.find_element('class name','the class name')</code></p></li><li><p>link_text 定位：</p><p><code>driver.find_element(By.LINK_TEXT,'the link text')</code></p><p><code>driver.find_element('link text','the link text')</code></p></li><li><p>partial_link_text 定位：</p><p><code>driver.find_element(By.PARTIAL_LINK_TEXT,'the partial link text')</code></p><p><code>driver.find_element('partial link text','the partial link text')</code></p></li></ul><p>最后，执行完所有操作后，关闭浏览器</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">driver.quit()  <span class="comment"># 关闭浏览器  </span></span><br></pre></td></tr></table></figure><h3 id="等待加载">等待加载</h3><h4 id="强制等待">强制等待</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">time.sleep(<span class="number">2</span>) <span class="comment"># 等待2s</span></span><br></pre></td></tr></table></figure><h4 id="显式等待常用">显式等待（常用）</h4><p>显式等待是有条件的等待，当条件满足时就执行下一步操作；如果在规定时间内没找到，就会报错。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"></span><br><span class="line"><span class="comment">#driver是webdriver对象，10是最长等待时间，0.5是每0.5秒去查询对应的元素。until后面跟的等待具体条件，EC是判断条件，检查元素是否存在于页面的 DOM 上。</span></span><br><span class="line">btn=WebDriverWait(driver,<span class="number">10</span>,<span class="number">0.5</span>).until(EC.presence_of_element_located((By.ID, <span class="string">&quot;the id&quot;</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">#或者当页面元素可见时点击</span></span><br><span class="line">wait = WebDriverWait(driver, <span class="number">10</span>)  </span><br><span class="line">btn = wait.until(EC.visibility_of_element_located((By.ID, <span class="string">&#x27;the id&#x27;</span>)))  </span><br><span class="line">btn.click()</span><br><span class="line"></span><br><span class="line"><span class="comment">#点击元素</span></span><br><span class="line">btn.click()</span><br></pre></td></tr></table></figure><h4 id="隐式等待">隐式等待</h4><p>隐式等待一次设置，全局生效，每个网页写一次即可。隐式等待设置一个网页中<strong>所有元素的最长等待时间</strong>，如果在规定时间内元素加载查找完成(也就是一般情况下你看到浏览器标签栏那个小圈不再转就代表加载完成)，则执行下一步；否则一直等到时间结束，然后抛出错误<code>NoSuchElementException</code>。</p><p>隐式等待只对<code>find_element</code> 和 <code>find_elements</code> 方法有效，对页面加载时间无影响。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">driver.get(<span class="string">&#x27;网址&#x27;</span>)</span><br><span class="line">driver.implicitly_wait(<span class="number">10</span>) <span class="comment">#等待10s</span></span><br></pre></td></tr></table></figure><h4 id="页面加载超时">页面加载超时</h4><p>如果仅设置页面最长加载时间，超过时间就进行报错：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">driver.set_page_load_timeout(<span class="number">10</span>)  <span class="comment"># 设置页面最大加载时间为10秒</span></span><br></pre></td></tr></table></figure><p>如果希望页面加载到规定时间后，若还未完成加载，直接执行下一步操作：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> <span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException  </span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:  </span><br><span class="line">    <span class="comment"># 设置页面加载超时时间为5秒  </span></span><br><span class="line">    driver.set_page_load_timeout(<span class="number">5</span>)  </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 尝试加载指定页面  </span></span><br><span class="line">    driver.get(<span class="string">&#x27;网址&#x27;</span>)  </span><br><span class="line"></span><br><span class="line"><span class="keyword">except</span> TimeoutException:  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;页面加载超过5秒，跳过等待，执行下一步操作。&quot;</span>)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 在这里执行下一步操作  </span></span><br></pre></td></tr></table></figure><h3 id="文件或图片上传">文件或图片上传</h3><h4 id="可见的input">可见的input</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># el = driver.find_element(By.CLASS_NAME, &#x27;the class name&#x27;)</span></span><br><span class="line"><span class="comment"># driver.execute_script(&#x27;arguments[0].style.visibility=\&#x27;visible\&#x27;&#x27;, el)</span></span><br><span class="line"><span class="comment"># el.send_keys(r&#x27;C:\Users\21218\Desktop\1.jpg&#x27;)</span></span><br><span class="line"></span><br><span class="line">driver.find_element(By.CLASS_NAME, <span class="string">&#x27;the class name&#x27;</span>).send_keys(<span class="string">r&#x27;C:\Users\21218\Desktop\1.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="上传窗口处理">上传窗口处理</h4><p>参考：<a href="https://blog.csdn.net/rednation_88/article/details/111588484">python selenium 文件上传窗口处理_python判断是否打开上传界面-CSDN博客</a></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pywinauto</span><br><span class="line"><span class="keyword">from</span> pywinauto.keyboard <span class="keyword">import</span> send_keys</span><br><span class="line"><span class="comment"># 使用pywinauto来选择文件</span></span><br><span class="line">app = pywinauto.Desktop()</span><br><span class="line"><span class="comment"># 选择文件上传的窗口</span></span><br><span class="line">dlg = app[<span class="string">&quot;打开&quot;</span>]</span><br><span class="line"><span class="comment"># 选择文件地址输入框，点击激活</span></span><br><span class="line">dlg[<span class="string">&quot;Toolbar3&quot;</span>].click()</span><br><span class="line"><span class="comment"># 键盘输入上传文件的路径</span></span><br><span class="line">send_keys(<span class="string">&quot;C:\课件\images&quot;</span>)</span><br><span class="line"><span class="comment"># 键盘输入回车，打开该路径</span></span><br><span class="line">send_keys(<span class="string">&quot;&#123;VK_RETURN&#125;&quot;</span>)</span><br><span class="line"><span class="comment"># 选中文件名输入框，输入文件名</span></span><br><span class="line">dlg[<span class="string">&quot;文件名(&amp;N):Edit&quot;</span>].type_keys(<span class="string">&quot;9.png&quot;</span>)</span><br><span class="line"><span class="comment"># 点击打开</span></span><br><span class="line">dlg[<span class="string">&quot;打开(&amp;O)&quot;</span>].click()</span><br></pre></td></tr></table></figure><h3 id="文件或图片下载">文件或图片下载</h3><p>在初始化<code>webdriver</code>对象前，配置其 options:</p><p>​ <code>download.default_directory</code>：设置下载路径</p><p>​ <code>profile.default_content_settings.popups</code>：设置为 <code>0</code> 禁止弹出窗口</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">options = webdriver.ChromeOptions()</span><br><span class="line">prefs = &#123;<span class="string">&#x27;profile.default_content_settings.popups&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;download.default_directory&#x27;</span>: <span class="string">&#x27;d:\\&#x27;</span>&#125;</span><br><span class="line">options.add_experimental_option(<span class="string">&#x27;prefs&#x27;</span>, prefs)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">#记得还需要将配置 options 加入到 webdriver 对象的初始化中，如：</span></span><br><span class="line"><span class="string">service = Service(ChromeDriverManager().install())  </span></span><br><span class="line"><span class="string">driver = webdriver.Chrome(service=service, options = options)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="报错及处理办法">报错及处理办法</h3><blockquote><p>文件路径名称书写方式，一般写为以下两种方式二选一：</p><ol type="1"><li>"盘符:\ +..." , 如 <code>C:\\Users\21218\Desktop\images</code>;</li><li>r"盘符:\ +..." , 如 <code>r"C:\Users\21218\Desktop\images"</code></li></ol></blockquote><blockquote><p><strong>找不到元素</strong>：<a href="https://blog.csdn.net/s_frozen/article/details/121136832">Selenium定位不到元素常见原因及解决办法_selenium frame中的元素定位不到-CSDN博客</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八小工具 </category>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代码箱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python实现计时器</title>
      <link href="/2024/11/16/%E8%AE%A1%E6%97%B6%E5%99%A8/"/>
      <url>/2024/11/16/%E8%AE%A1%E6%97%B6%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<p>最原始的计时器，点击开始计时，计时以秒为单位：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tkinter <span class="keyword">as</span> tk  </span><br><span class="line"><span class="keyword">from</span> tkinter <span class="keyword">import</span> ttk, messagebox  </span><br><span class="line"></span><br><span class="line">pass_time = <span class="number">0</span>  </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_time</span>():  </span><br><span class="line">    <span class="keyword">global</span> pass_time  </span><br><span class="line">    pass_time += <span class="number">1</span>  </span><br><span class="line">    label.config(text=<span class="string">&quot;时间已经过去 &quot;</span> + <span class="built_in">str</span>(pass_time) + <span class="string">&quot; 秒&quot;</span>)  </span><br><span class="line">    <span class="comment"># 每1000毫秒（1秒）调用一次count_time函数  </span></span><br><span class="line">    root.after(<span class="number">1000</span>, count_time)  </span><br><span class="line"></span><br><span class="line">root = tk.Tk()  </span><br><span class="line">root.title(<span class="string">&quot;time counter&quot;</span>)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置样式  </span></span><br><span class="line">style = ttk.Style()  </span><br><span class="line">style.theme_use(<span class="string">&#x27;clam&#x27;</span>)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义字体  </span></span><br><span class="line">style.configure(<span class="string">&#x27;TLabel&#x27;</span>, font=(<span class="string">&#x27;Helvetica&#x27;</span>, <span class="number">12</span>))  </span><br><span class="line">style.configure(<span class="string">&#x27;TButton&#x27;</span>, font=(<span class="string">&#x27;Helvetica&#x27;</span>, <span class="number">12</span>, <span class="string">&#x27;bold&#x27;</span>))  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义颜色  </span></span><br><span class="line">style.configure(<span class="string">&#x27;TLabel&#x27;</span>, background=<span class="string">&#x27;#f0f0f0&#x27;</span>)  </span><br><span class="line">style.configure(<span class="string">&#x27;TButton&#x27;</span>, background=<span class="string">&#x27;#4CAF50&#x27;</span>, foreground=<span class="string">&#x27;white&#x27;</span>)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建标签和按钮  </span></span><br><span class="line">label = ttk.Label(root, text=<span class="string">&quot;时间已经过去 &quot;</span> + <span class="built_in">str</span>(pass_time) + <span class="string">&quot; 秒&quot;</span>)  </span><br><span class="line">label.grid(column=<span class="number">0</span>, row=<span class="number">0</span>, padx=<span class="number">5</span>, pady=<span class="number">5</span>, sticky=<span class="string">&#x27;w&#x27;</span>)  </span><br><span class="line"></span><br><span class="line">start_button = ttk.Button(root, text=<span class="string">&quot;Count&quot;</span>, command=count_time)  </span><br><span class="line">start_button.grid(column=<span class="number">0</span>, row=<span class="number">2</span>, columnspan=<span class="number">2</span>, pady=<span class="number">10</span>)  </span><br><span class="line"></span><br><span class="line">root.mainloop()</span><br></pre></td></tr></table></figure><p>桌面计时器，可以配合壁纸使用，参考：<a href="https://blog.csdn.net/qq_43495412/article/details/113099677">桌面美化 Python tkinter倒计时工具_python 好看的倒计时插件-CSDN博客</a></p>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八小工具 </category>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代码箱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积神经网络</title>
      <link href="/2024/11/04/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/"/>
      <url>/2024/11/04/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/</url>
      
        <content type="html"><![CDATA[<blockquote><p>原文链接：<a href="https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/index.html">6. 卷积神经网络 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p></blockquote><h1 id="从全连接层到卷积">从全连接层到卷积</h1><blockquote><p>本节主要讲述卷积层的原理。卷积的本质是<strong>有效提取相邻像素间的相关特征</strong>。</p><p>之前我们将softmax回归模型和多层感知机模型应用于Fashion-MNIST数据集中的服装图片。 为了能够应用softmax回归和多层感知机，我们首先将大小为 k×28×28的图像集展平为一个[k,784]维的固定长度的二维向量，然后用全连接层对其进行处理。 而现在，若我们掌握了卷积层的处理方法，我们就<strong>可以在图像中保留空间结构</strong>。 同时，用卷积层代替全连接层的另一个好处是：<strong>模型更简洁、所需的参数更少</strong>。</p></blockquote><p>多层感知机十分适合处理表格数据，其中行对应样本，列对应特征。 对于表格数据，我们寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构。 此时，多层感知机可能是最好的选择，然而对于高维感知数据（如图片数据），这种缺少结构的网络可能会变得不实用。</p><p>图像中本就拥有丰富的结构，而这些结构可以被人类和机器学习模型使用。 <strong>卷积神经网络（convolutional neural networks，CNN）是机器学习利用自然图像中一些已知结构的创造性方法</strong>。</p><h2 id="不变性">不变性</h2><p>假设我们想从一张图片中找到某个物体。 合理的假设是：<strong>无论哪种方法找到这个物体，都应该和物体的位置无关</strong>。 不妨假设我们要在人群中寻找小明的位置，由于小明的外表不取决于他所处的位置，因此我们可以使用一个“小明检测器“扫描图像。该检测器将图像分割为多个区域，并为每个区域包含小明的可能性打分。卷积神经网络正是将<strong>空间不变性（spatial invariance）</strong>的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。</p><p>现在，我们将上述想法总结一下，从而帮助我们设计适合于计算机视觉的神经网络架构。</p><ol type="1"><li><p>平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</p></li><li><p>局部性（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</p></li></ol><h2 id="多层感知机的限制">多层感知机的限制</h2><h3 id="平移不变性">平移不变性</h3><h3 id="局部性">局部性</h3><h2 id="卷积">卷积</h2><p>在数学中，两个函数（如<span class="math inline">\(f,g:R^d\rightarrow R\)</span>）之间的“卷积”被定义为 <span class="math display">\[\left( f*g \right) \left( \mathbf{x} \right) =\int{f\left( \mathbf{z} \right)}g\left( \mathbf{x}-\mathbf{z} \right) d\mathbf{z}.\]</span> 也就是说，卷积是当把一个函数“翻转”并移位<span class="math inline">\(\mathbf{x}\)</span>时，测量<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>之间的重叠。 当为离散对象时，积分就变成求和。对于二维张量，则为<span class="math inline">\(f\)</span>的索引(<span class="math inline">\(a\)</span>,<span class="math inline">\(b\)</span>)和<span class="math inline">\(g\)</span>的索引(<span class="math inline">\(i-a\)</span>,<span class="math inline">\(j-b\)</span>)上的对应加和： <span class="math display">\[\left( f*g \right) \left( i,j \right) =\sum_a{\sum_b{f\left( a,b \right) g\left( i-a,j-b \right)}}.\]</span></p><h2 id="通道">通道</h2><p>回到上面的找小明游戏，让我们看看它到底是什么样子。卷积层根据<strong>滤波器</strong><span class="math inline">\(V\)</span> <span style="background:#daf5e9;">（filter, 即卷积核 convolution kernel, 亦或简单地称之为该卷积层的<em>权重</em>，通常该权重是可学习的参数）</span>选取给定大小的窗口，并加权处理图片。我们的目标是学习一个模型，以便探测出“小明”最可能出现的地方。</p><p>然而这种方法有一个问题：我们忽略了图像一般包含三个通道/三种原色（红色、绿色和蓝色）。 实际上，<strong>图像不是二维张量，而是一个由高度、宽度和颜色组成的三维张量</strong>，比如包含1024×1024×3个像素。 前两个轴与像素的空间位置有关，而第三个轴可以看作每个像素的多维表示。因此，我们将输入<span class="math inline">\(X\)</span>索引为<span class="math inline">\([X]_{i,j,k}\)</span>。由此卷积相应地调整为<span class="math inline">\([V]_{a,b,c}\)</span>，而不是<span class="math inline">\([V]_{a,b}\)</span>。</p><p>此外，由于输入图像是三维的，我们的隐藏表示<span class="math inline">\(H\)</span>也最好采用三维张量。 换句话说，对于每一个空间位置，我们想要采用一组而不是一个隐藏表示。这样一组隐藏表示可以想象成一些互相堆叠的二维网格。 因此，我们可以把隐藏表示想象为一系列具有二维张量的<strong>通道（channel）</strong>。 这些通道有时也被称为<strong>特征映射（feature maps）</strong>，因为每个通道都向后续层提供一组空间化的学习特征。 直观上可以想象在靠近输入的底层，一些通道专门识别边缘，而一些通道专门识别纹理。</p><p>为了支持输入<span class="math inline">\(X\)</span>和隐藏表示<span class="math inline">\(H\)</span>中的多个通道，我们可以在<span class="math inline">\(V\)</span>中添加第四个坐标，即<span class="math inline">\([V]_{a,b,c,d}\)</span>。综上所述， <span class="math display">\[[\mathsf{H} ]_{i,j,d}=\sum_{a=-\Delta}^{\Delta}{\sum_{b=-\Delta}^{\Delta}{\sum_c{[}}}\mathsf{V} ]_{a,b,c,d}[\mathsf{X} ]_{i+a,j+b,c},\]</span> 其中隐藏表示<span class="math inline">\(H\)</span>中的索引<span class="math inline">\(d\)</span>表示输出通道，而随后的输出将继续以三维张量<span class="math inline">\(H\)</span>作为输入进入下一个卷积层。 所以，上式可以定义具有多个通道的卷积层，而其中<span class="math inline">\(V\)</span>是该卷积层的权重。</p><h2 id="从全连接层到卷积总结">从全连接层到卷积总结</h2><ul><li>图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。</li><li>局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。</li><li>在图像处理中，卷积层通常比全连接层需要更少的参数，但依旧获得高效用的模型。</li><li>卷积神经网络（CNN）是一类特殊的神经网络，它可以包含多个卷积层。</li><li>多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。</li></ul><h1 id="图像卷积">图像卷积</h1><blockquote><p>由于卷积神经网络的设计是用于探索图像数据，本节以图像为例讲述卷积的实际应用。</p></blockquote><h2 id="互相关运算">互相关运算</h2><p>严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是<em>互相关运算</em>（cross-correlation），而不是卷积运算。 根据<u>第一节</u>中的描述，在卷积层中，输入张量和核张量通过互相关运算产生输出张量。</p><p>首先，我们暂时忽略通道（第三维）这一情况，看看如何处理二维图像数据和隐藏表示。在下图中，输入是高度为3、宽度为3的二维张量（即形状为3×3）。卷积核的高度和宽度都是2，而卷积核窗口（或卷积窗口）的形状由内核的高度和宽度决定（即2×2）。</p><figure><img src="https://zh-v2.d2l.ai/_images/correlation.svg" alt="zh-v2.d2l.ai/_images/correlation.svg" /><figcaption aria-hidden="true">zh-v2.d2l.ai/_images/correlation.svg</figcaption></figure><p>二维互相关运算。阴影部分是第一个输出元素，以及用于计算输出的输入张量元素和核张量元素： <span class="math display">\[0\times0+1\times1+3\times2+4\times3=19\]</span> 在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。 在如上例子中，输出张量的四个元素由二维互相关运算得到，输出高度为2、宽度为2。</p><blockquote><p>注意，输出大小略小于输入大小。这是因为卷积核的宽度和高度大于1， 而卷积核只与图像中每个大小完全适合的位置进行互相关运算。 所以，输出大小等于输入大小<span class="math inline">\(n_h×n_w\)</span>减去卷积核大小<span class="math inline">\(k_h×k_w\)</span>，即： <span class="math display">\[(n_h-k_h+1) \times (n_w-k_w+1).\]</span> 这是因为我们需要足够的空间在图像上“移动”卷积核。稍后，我们将看到如何通过在图像边界周围填充零来保证有足够的空间移动卷积核，从而保持输出大小不变。</p></blockquote><p>接下来，我们在<code>corr2d</code>函数中实现如上过程，该函数接受输入张量<code>X</code>和卷积核张量<code>K</code>，并返回输出张量<code>Y</code>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">X, K</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算二维互相关运算&quot;&quot;&quot;</span></span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i:i + h, j:j + w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;验证上述二维互相关运算的输出&#x27;&#x27;&#x27;</span></span><br><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">K = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">corr2d(X, K)</span><br></pre></td></tr></table></figure><p>tensor([[19., 25.], [37., 43.]])</p><h2 id="卷积层">卷积层</h2><blockquote><p>高度和宽度分别为<span class="math inline">\(ℎ\)</span>和<span class="math inline">\(w\)</span>的卷积核可以被称为<span class="math inline">\(h×w\)</span>卷积或<span class="math inline">\(h×w\)</span>卷积核。 我们也将带有<span class="math inline">\(h×w\)</span>卷积核的卷积层称为<span class="math inline">\(h×w\)</span>卷积层。</p></blockquote><p>卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。 所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。 就像我们之前随机初始化全连接层一样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。</p><p>基于上面定义的<code>corr2d</code>函数实现二维卷积层。在<code>__init__</code>构造函数中，将<code>weight</code>和<code>bias</code>声明为两个模型参数<code>Parameter</code>。前向传播函数调用<code>corr2d</code>函数并添加偏置。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, kernel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#赋初值，nn.Parameter为可更新的参数（requires_grad=True）</span></span><br><span class="line">        self.weight = nn.Parameter(torch.rand(kernel_size))  <span class="comment">#随机初始化</span></span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure><h2 id="图像中目标的边缘检测">图像中目标的边缘检测</h2><p>如下是卷积层的一个简单应用：通过找到像素变化的位置，来检测图像中不同颜色的边缘。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.首先，我们构造一个6×8像素的黑白图像。中间四列为黑色（0），其余像素为白色（1）。</span></span><br><span class="line">X = torch.ones((<span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">X</span></span><br><span class="line"><span class="string">tensor([[1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.接下来，我们构造一个高度为1、宽度为2的卷积核K。当进行互相关运算时，如果水平相邻的两元素相同，则输出为零，否则输出为非零。</span></span><br><span class="line">K = torch.tensor([[<span class="number">1.0</span>, -<span class="number">1.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.现在，我们对参数X（输入）和K（卷积核）执行互相关运算。 如下所示，输出Y中的1代表从白色到黑色的边缘，-1代表从黑色到白色的边缘，其他情况的输出为0。</span></span><br><span class="line">Y = corr2d(X, K)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Y</span></span><br><span class="line"><span class="string">tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.最后，我们将输入的二维图像转置，再进行如上的互相关运算。 其输出如下，之前检测到的垂直边缘消失了。 不出所料，这个卷积核K只可以检测垂直边缘，无法检测水平边缘。</span></span><br><span class="line">corr2d(X.t(), K)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="学习卷积核">学习卷积核</h2><p>如果我们只需寻找黑白边缘，那么以上<code>[1, -1]</code>的边缘检测器足以。然而，当有了更复杂数值的卷积核，或者连续的卷积层时，我们不可能手动设计滤波器。因此考虑是否可以通过<code>仅查看“输入-输出”对</code>来学习由<code>X</code>生成<code>Y</code>的卷积核？</p><p>我们先构造一个卷积层，并将其卷积核初始化为随机张量。接下来，在每次迭代中，我们比较<code>Y</code>与卷积层输出的平方误差，然后计算梯度来更新卷积核。为了简单起见，我们在此使用内置的二维卷积层<code>nn.Conv2d</code>，并忽略偏置。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造一个二维卷积层，它具有1个输入通道、1个输出通道和形状为（1，2）的卷积核</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），</span></span><br><span class="line"><span class="comment"># 其中批量大小和通道数都为1</span></span><br><span class="line">X = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">Y = Y.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>))</span><br><span class="line">lr = <span class="number">3e-2</span>  <span class="comment"># 学习率0.03</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    loss = (Y_hat - Y) ** <span class="number">2</span></span><br><span class="line">    conv2d.zero_grad()</span><br><span class="line">    loss.<span class="built_in">sum</span>().backward()</span><br><span class="line">    <span class="comment"># 迭代卷积核</span></span><br><span class="line">    conv2d.weight.data[:] -= lr * conv2d.weight.grad</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:<span class="comment">#偶数行输出</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l.<span class="built_in">sum</span>():<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>epoch 2, loss 6.422 epoch 4, loss 1.225 epoch 6, loss 0.266 epoch 8, loss 0.070 epoch 10, loss 0.022</p><p>在10次迭代之后，误差已经降到足够低。现在我们来看看我们所学的卷积核的权重张量。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv2d.weight.data.reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>tensor([[ 1.0010, -0.9739]])</p><p>发现我们学习到的卷积核权重非常接近我们之前定义的卷积核<code>K</code>。</p><h2 id="互相关和卷积">互相关和卷积</h2><blockquote><p>为了与深度学习文献中的标准术语保持一致，我们将继续把“互相关运算”称为卷积运算，尽管严格地说，它们略有不同。 此外，对于卷积核张量上的权重，我们称其为<em>元素</em>。</p></blockquote><h2 id="特征映射和感受野">特征映射和感受野</h2><p>如前文所述，输出卷积层有时被称为<strong>特征映射</strong>（feature map），因为它可以被视为一个输入映射到下一层的空间维度的转换器。 在卷积神经网络中，对于某一层的任意元素<span class="math inline">\(x\)</span>，其<strong>感受野</strong>（receptive field）是指在前向传播期间可能影响<span class="math inline">\(x\)</span>计算的所有元素（来自所有先前层）。</p><blockquote><p>注意，感受野可能大于输入的实际大小。</p><p>因此，当一个特征图中的任意元素需要检测更广区域的输入特征时，我们可以构建一个更深的网络。</p></blockquote><h2 id="图像卷积总结">图像卷积总结</h2><ul><li>二维卷积层的核心计算是二维互相关运算。最简单的形式是，对二维输入数据和卷积核执行互相关操作，然后添加一个偏置。</li><li>我们可以设计一个卷积核来检测图像的边缘。</li><li>我们可以从数据中学习卷积核的参数。</li><li>学习卷积核时，无论用严格卷积运算或互相关运算，卷积层的输出不会受太大影响。</li><li>当需要检测输入特征中更广区域时，我们可以构建一个更深的卷积网络。</li></ul><h1 id="填充padding-和步幅stride">填充padding 和步幅stride</h1><p>正如我们在第二节中所概括的那样，假设输入形状为<span class="math inline">\(n_h×n_w\)</span>，卷积核形状为<span class="math inline">\(k_h×k_w\)</span>，那么输出形状将是<span class="math inline">\((n_h-k_h+1)×(n_w-k_w+1)\)</span>。可见，<font color=#4eb434>卷积的输出形状取决于输入形状和卷积核的形状。</font></p><p>还有什么因素会影响输出的大小呢？本节我们将介绍<font color=#985fff><strong>填充</strong>（padding）</font>和<font color=#df8400><strong>步幅</strong>（stride）</font>。假设以下情景： <font color=#985fff>有时，在应用了连续的卷积之后，我们最终得到的输出远小于输入大小。这是由于卷积核的宽度和高度通常大于1所导致的。比如，一个240×240像素的图像，经过10层5×5的卷积后，将减少到200×200像素。如此一来，原始图像的边界丢失了许多有用信息。而<em>填充</em>是解决此问题最有效的方法</font>； <font color=#df8400>有时，我们可能希望大幅降低图像的宽度和高度。例如，如果我们发现原始的输入分辨率十分冗余。<em>步幅</em>则可以在这类情况下提供帮助。</font></p><h2 id="填充">填充</h2><p><strong>填充</strong>（padding）：在输入图像的边界填充元素（通常填充元素是0）。</p><p>例如，在下图中，我们将3×3输入填充到5×5，那么它的输出就增加为4×4。阴影部分是第一个输出元素以及用于输出计算的输入和核张量元素： 0×0+0×1+0×2+0×3=0。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/conv-pad.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">带填充的二维互相关</div></center><p>通常，如果我们添加<span class="math inline">\(p_h\)</span>行填充（大约一半在顶部，一半在底部）和<span class="math inline">\(p_w\)</span>列填充（大约左侧一半，右侧一半），则输出形状将为 <span class="math display">\[(n_h-k_h+1+p_h)\times(n_w-k_w+1+p_w)。\]</span> 这意味着输出的高度和宽度将分别增加<span class="math inline">\(p_h\)</span>和<span class="math inline">\(p_w\)</span>。</p><p><strong>在许多情况下，我们需要设置<span class="math inline">\(p_h=k_h-1\)</span>和<span class="math inline">\(p_w=k_w-1\)</span>，使输入和输出具有相同的高度和宽度。</strong> 这样可以在构建网络时更容易地预测每个图层的输出形状。假设<span class="math inline">\(k_h\)</span>是奇数，我们将在高度的两侧填充<span class="math inline">\(p_h/2\)</span>行。 如果<span class="math inline">\(k_h\)</span>是偶数，则一种可能性是在输入顶部填充<span class="math inline">\(\lceil p_h/2\rceil\)</span>行，在底部填充<span class="math inline">\(\lfloor p_h/2\rfloor\)</span>行。同理，我们填充宽度的两侧。</p><p><span style="background:#daf5e9;">卷积神经网络中卷积核的高度和宽度通常为奇数，例如1、3、5或7。 </span>选择奇数的好处是，保持空间维度的同时，我们可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。</p><p>此外，使用奇数的核大小和填充大小也提供了书写上的便利。对于任何二维张量<code>X</code>，当满足： 1. 卷积核的大小是奇数； 2. 所有边的填充行数和列数相同； 3. 输出与输入具有相同高度和宽度 则可以得出：输出<code>Y[i, j]</code>是通过以输入<code>X[i, j]</code>为中心，与卷积核进行互相关计算得到的。</p><p>在下面的例子中，我们创建一个高度和宽度为3的二维卷积层，并在所有侧边填充1个像素。给定高度和宽度为8的输入，则输出的高度和宽度也是8；<span style="background:#daf5e9;">当卷积核的高度和宽度不同时，我们可以填充不同的高度和宽度，使输出和输入具有相同的高度和宽度。</span>在如下示例中，我们使用高度为5，宽度为3的卷积核，高度和宽度两边的填充分别为2和1。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了方便起见，我们定义了一个计算卷积层的函数。</span></span><br><span class="line"><span class="comment"># 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span><br><span class="line">    <span class="comment"># 这里的（1，1）表示批量大小和通道数都是1</span></span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>) + X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="comment"># 省略前两个维度：批量大小和通道</span></span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([8, 8])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([8, 8])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="步幅">步幅</h2><p>在计算互相关时，卷积窗口从输入张量的左上角开始，向下、向右滑动。 在前面的例子中，我们默认每次滑动一个元素。 但是，有时候<span style="background:#daf5e9;">为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素</span>。</p><p>我们将每次滑动元素的数量称为<strong>步幅（stride）</strong>。到目前为止，我们只使用过高度或宽度为1的步幅，那么如何使用较大的步幅呢？下图是垂直步幅为3，水平步幅为2的二维互相关运算。 着色部分是输出元素以及用于输出计算的输入和内核张量元素：0×0+0×1+1×2+2×3=8、0×0+6×1+0×2+0×3=6。</p><p>可以看到，为了计算输出中第一列的第二个元素和第一行的第二个元素，卷积窗口分别向下滑动三行和向右滑动两列。但是，当卷积窗口继续向右滑动两列时，没有输出，因为输入元素无法填充窗口（除非我们添加另一列填充）。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/conv-stride.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">垂直步幅为3，水平步幅为2的二维互相关运算</div></center><p>通常，当垂直步幅为<span class="math inline">\(s_h\)</span>、水平步幅为<span class="math inline">\(s_w\)</span>时，输出形状为 <span class="math display">\[\lfloor(n_h-k_h+s_h+p_h)/s_h\rfloor \times \lfloor(n_w-k_w+s_w+p_w)/s_w\rfloor.\]</span> <strong>如果我们设置了<span class="math inline">\(p_h=k_h-1\)</span>和<span class="math inline">\(p_w=k_w-1\)</span>，则输出形状将简化为<span class="math inline">\(\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor\)</span>。 更进一步，如果输入的高度和宽度可以被垂直和水平步幅整除，则输出形状将为<span class="math inline">\((n_h/s_h) \times (n_w/s_w)\)</span>。</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们将高度和宽度的步幅设置为2，从而将输入的高度和宽度减半。</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([4, 4])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 看一个稍微复杂的例子</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), stride=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([2, 2])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>为了简洁起见，当输入高度和宽度两侧的填充数量分别为<span class="math inline">\(p_h\)</span>和<span class="math inline">\(p_w\)</span>时，我们称之为填充<span class="math inline">\((p_h, p_w)\)</span>。当<span class="math inline">\(p_h = p_w = p\)</span>时，填充是<span class="math inline">\(p\)</span>。同理，当高度和宽度上的步幅分别为<span class="math inline">\(s_h\)</span>和<span class="math inline">\(s_w\)</span>时，我们称之为步幅<span class="math inline">\((s_h, s_w)\)</span>。特别地，当<span class="math inline">\(s_h = s_w = s\)</span>时，我们称步幅为<span class="math inline">\(s\)</span>。默认情况下，填充为0，步幅为1。在实践中，我们很少使用不一致的步幅或填充，也就是说，我们通常有<span class="math inline">\(p_h = p_w\)</span>和<span class="math inline">\(s_h = s_w\)</span>。</p><h2 id="填充和步幅总结">填充和步幅总结</h2><ul><li>填充可以增加输出的高度和宽度。在许多情况下，我们需要设置<span class="math inline">\(p_h=k_h-1\)</span>和<span class="math inline">\(p_w=k_w-1\)</span>，用来使输出与输入具有相同的高和宽。</li><li>步幅可以减小输出的高和宽，例如在设置了填充的情况下，若输入的高度和宽度可以被步幅s整除，输出的高和宽仅为输入的高和宽的1/s（步幅s是一个大于1的整数）。</li><li>填充和步幅可用于有效地调整输出数据的维度。</li></ul><h1 id="多输入多输出通道">多输入多输出通道</h1><p>虽然我们在第一节中描述了构成每个图像的多个通道和多层卷积层。例如彩色图像具有标准的RGB通道来代表红、绿和蓝。 但是到目前为止，我们仅展示了单个输入和单个输出通道的简化例子。 这使得我们可以将输入、卷积核和输出看作二维张量。</p><p>当我们添加通道时，我们的输入和隐藏的表示都变成了三维张量。例如，每个RGB输入图像具有<span class="math inline">\(3×ℎ×w\)</span>的形状。我们将这个大小为3的轴称为<strong>通道（channel）</strong>维度。本节将更深入地研究具有多输入和多输出通道的卷积核。</p><h2 id="多输入通道">多输入通道</h2><p><span style="background:#daf5e9;">当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便与输入数据进行互相关运算。</span>若输入和卷积核都有<span class="math inline">\(c_i\)</span>个通道，我们可以对每个通道输入的二维张量和卷积核的二维张量进行互相关运算，再对通道求和（将<span class="math inline">\(c_i\)</span>的结果相加）得到二维张量。这是多通道输入和多输入通道卷积核之间进行二维互相关运算的结果。</p><p>在下图中，我们演示了一个具有两个输入通道的二维互相关运算的示例。阴影部分是第一个输出元素以及用于计算这个输出的输入和核张量元素：(1×1+2×2+4×3+5×4)+(0×0+1×1+3×2+4×3)=56。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/conv-multi-in.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">两个输入通道的互相关计算</div></center><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 再来看下具体实现，我们实现一下多输入通道互相关运算。 简而言之，我们所做的就是对每个通道执行互相关操作，然后将结果相加。</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 先遍历“X”和“K”的第0个维度（通道维度），再把它们加在一起</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(d2l.corr2d(x, k) <span class="keyword">for</span> x, k <span class="keyword">in</span> <span class="built_in">zip</span>(X, K))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们可以构造与上图中的值相对应的输入张量X和核张量K，以验证互相关运算的输出。</span></span><br><span class="line">X = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]],</span><br><span class="line">               [[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]]])</span><br><span class="line">K = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]], [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]]])</span><br><span class="line"></span><br><span class="line">corr2d_multi_in(X, K)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 56.,  72.],</span></span><br><span class="line"><span class="string">        [104., 120.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>zip()</strong> 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的zip对象： &gt;&gt;&gt; a = [1,2,3] &gt;&gt;&gt; b = [4,5,6] &gt;&gt;&gt; zipped = zip(a,b) # 返回一个对象 &gt;&gt;&gt; zipped &lt;zip object at 0x103abc288&gt; &gt;&gt;&gt; list(zipped) # list() 转换为列表 [(1, 4), (2, 5), (3, 6)]</p><p>对于zip(args)这个函数，Python还提供了一种逆操作： &gt;&gt;&gt;origin = zip(<em>result) #前面加</em>号，事实上*号也是一个特殊的运算符，叫解包运算符。</p></blockquote><h2 id="多输出通道">多输出通道</h2><p>到目前为止，不论有多少输入通道，我们还只有一个输出通道。然而，正如我们在第一节中所讨论的，每一层有多个输出通道是至关重要的。在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。直观地说，我们可以将每个通道看作对不同特征的响应。而现实可能更为复杂一些，因为<u>每个通道不是独立学习的，而是为了共同使用而优化的</u>。因此，<u>多输出通道并不仅是学习多个单通道的检测器</u>。</p><p>用<span class="math inline">\(c_i\)</span>和<span class="math inline">\(c_o\)</span>分别表示输入和输出通道的数目，并让<span class="math inline">\(k_h\)</span>和<span class="math inline">\(k_w\)</span>为卷积核的高度和宽度。为了获得多个通道的输出，我们可以<span style="background:#daf5e9;">为每个输出通道创建一个形状为<span class="math inline">\(c_i\times k_h\times k_w\)</span>的卷积核张量</span>，这样卷积核的形状是<span class="math inline">\(c_o\times c_i\times k_h\times k_w\)</span>。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如下，我们实现一个计算多个通道的输出的互相关函数。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。</span></span><br><span class="line">    <span class="comment"># 最后将所有结果都叠加在一起</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过将核张量K与K+1（K中每个元素加1）和K+2连接起来，构造了一个具有3个输出通道的卷积核。</span></span><br><span class="line">K = torch.stack((K, K + <span class="number">1</span>, K + <span class="number">2</span>), <span class="number">0</span>)</span><br><span class="line">K.shape</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([3, 2, 2, 2])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面，我们对输入张量X与卷积核张量K执行互相关运算。现在的输出包含3个通道，第一个通道的结果与先前输入张量X和多输入单输出通道的结果一致。</span></span><br><span class="line">corr2d_multi_in_out(X, K)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[ 56.,  72.],</span></span><br><span class="line"><span class="string">         [104., 120.]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 76., 100.],</span></span><br><span class="line"><span class="string">         [148., 172.]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 96., 128.],</span></span><br><span class="line"><span class="string">         [192., 224.]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>输入张量X与具有3个输出通道的卷积核K执行互相关运算，得到输出的过程如下图：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nnn.jpg" alt="nnn" /><figcaption aria-hidden="true">nnn</figcaption></figure></blockquote><h2 id="卷积层-1">1×1卷积层</h2><p>1×1卷积，即<span class="math inline">\(k_h = k_w = 1\)</span>，看起来似乎没有多大意义。 毕竟，<span style="background:#daf5e9;">卷积的本质是有效提取相邻像素间的相关特征</span>，而1×1卷积显然没有此作用。 尽管如此，1×1仍然十分流行，经常包含在复杂深层网络的设计中。下面，让我们详细地解读一下它的实际作用。</p><p>因为使用了最小窗口，1×1卷积失去了卷积层的特有能力——在高度和宽度维度上，识别相邻元素间相互作用的能力。 其实1×1卷积的唯一计算发生在<strong>通道</strong>上。</p><p>下图展示了使用1×1卷积核与3个输入通道和2个输出通道的互相关计算。 这里输入和输出具有相同的高度和宽度，输出中的每个元素都是从输入图像中同一位置的元素的线性组合。 我们可以将1×1卷积层看作在每个像素位置应用的全连接层，以<span class="math inline">\(c_i\)</span>个输入值转换为<span class="math inline">\(c_o\)</span>个输出值。 因为这仍然是一个卷积层，所以跨像素的权重是一致的。 同时，1×1卷积层需要的权重维度为<span class="math inline">\(c_o\times c_i\)</span>，再额外加上一个偏置。</p><blockquote><p>这里的“权重”理解为“卷积核”</p></blockquote><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/conv-1x1.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">互相关计算使用了具有3个输入通道和2个输出通道的1×1卷积核。其中，输入和输出具有相同的高度3和宽度3</div></center><p>下面，我们使用全连接层实现1×1卷积。 请注意，我们需要对输入和输出的数据形状进行调整。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out_1x1</span>(<span class="params">X, K</span>):</span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]</span><br><span class="line">    X = X.reshape((c_i, h * w))</span><br><span class="line">    K = K.reshape((c_o, c_i))</span><br><span class="line">    <span class="comment"># 全连接层中的矩阵乘法</span></span><br><span class="line">    Y = torch.matmul(K, X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape((c_o, h, w))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当执行1×1卷积运算时，上述函数相当于先前实现的互相关函数corr2d_multi_in_out。让我们用一些样本数据来验证这一点。</span></span><br><span class="line">X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">K = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">Y1 = corr2d_multi_in_out_1x1(X, K)</span><br><span class="line">Y2 = corr2d_multi_in_out(X, K)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">float</span>(torch.<span class="built_in">abs</span>(Y1 - Y2).<span class="built_in">sum</span>()) &lt; <span class="number">1e-6</span></span><br></pre></td></tr></table></figure><h2 id="多输入多输出通道总结">多输入多输出通道总结</h2><ul><li>多输入多输出通道可以用来扩展卷积层的模型。</li><li>当以每像素为基础应用时，1×1卷积层相当于全连接层。</li><li>1×1卷积层通常用于调整网络层的通道数量和控制模型复杂性。</li></ul><h1 id="池化层-pooling">池化层 pooling</h1><p>本节将介绍<em>池化</em>（pooling）层，它具有双重目的：<strong>1.降低对空间降采样表示的敏感性</strong>，<font color=#ef042a><strong>2.降低卷积层对位置的敏感性</strong></font>。</p><ol type="1"><li><p>我们的机器学习任务通常会跟<u>全局图像的问题</u>有关（例如，“图像是否包含一只猫呢？”），所以我们<u>最后一层的神经元应该对整个输入的全局敏感</u>。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。因此，当我们处理图像时，我们希望<u>逐渐降低隐藏表示的空间分辨率、聚集信息</u>，这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野（输入）就越大。</p></li><li><p>此外，当检测较底层的特征时（例如第二节中所讨论的边缘），我们通常<u>希望这些特征保持某种程度上的平移不变性</u>。例如，如果我们拍摄黑白之间轮廓清晰的图像<code>X</code>，并将整个图像向右移动一个像素，即<code>Z[i, j] = X[i, j + 1]</code>，则新图像<code>Z</code>的输出可能大不相同。而<u>在现实中，随着拍摄角度的移动，任何物体几乎不可能发生在同一像素上</u>。即使用三脚架拍摄一个静止的物体，由于快门的移动而引起的相机振动，可能会使所有物体左右移动一个像素（除了高端相机配备了特殊功能来解决这个问题）。</p></li></ol><h2 id="最大池化层和平均池化层">最大池化层和平均池化层</h2><p>与卷积层类似，池化层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口（有时称为<strong>池化窗口</strong>）遍历的每个位置计算一个输出。 然而，<span style="background:#daf5e9;">不同于卷积层中的输入与卷积核之间的互相关计算，池化层不包含参数</span>。 相反，池运算是确定性的，我们通常计算池化窗口中所有元素的最大值或平均值。这些操作分别称为<strong>最大池化层</strong>（maximum pooling）和<strong>平均池化层</strong>（average pooling）。</p><p>在这两种情况下，与互相关运算符一样，池化窗口从输入张量的左上角开始，从左往右、从上往下的在输入张量内滑动。在池化窗口到达的每个位置，它计算该窗口中输入子张量的最大值或平均值。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/pooling.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">池化窗口形状为2×2的最大池化层。着色部分是第一个输出元素，以及用于计算这个输出的输入元素:max(0,1,3,4)=4.</div></center><p>池化窗口形状为<span class="math inline">\(p \times q\)</span>的池化层称为<u><span class="math inline">\(p \times q\)</span>池化层</u>，池化操作称为<u><span class="math inline">\(p \times q\)</span>池化</u>。</p><p><span style="background:#daf5e9;">回到本节开头提到的对象边缘检测示例，现在我们将使用卷积层的输出作为2×2最大池化的输入。 设置卷积层输入为<code>X</code>，池化层输出为<code>Y</code>。 无论<code>X[i, j]</code>和<code>X[i, j + 1]</code>的值相同与否，池化层始终输出<code>Y[i, j] = 1</code>。 也就是说，<u>使用2×2最大池化层，即使在高度或宽度上移动一个元素，卷积层仍然可以识别到模式</u>。</span></p><p>在下面的代码中的<code>pool2d</code>函数，我们实现池化层的前向传播。 这类似于第二节中的<code>corr2d</code>函数。 然而，这里我们没有卷积核中的类似参数，输出为输入中每个区域的最大值或平均值。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们可以构建上图中的输入张量X，验证二维最大池化层的输出。</span></span><br><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[4., 5.],</span></span><br><span class="line"><span class="string">        [7., 8.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们还可以验证平均池化层。</span></span><br><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>), <span class="string">&#x27;avg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[2., 3.],</span></span><br><span class="line"><span class="string">        [5., 6.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="填充和步幅">填充和步幅</h2><p>与卷积层一样，池化层也可以改变输出形状。和以前一样，我们可以通过填充和步幅以获得所需的输出形状。 下面，我们用深度学习框架中内置的二维最大池化层，来演示池化层中填充和步幅的使用。 我们首先构造了一个输入张量<code>X</code>，它有四个维度，其中样本数和通道数都是1。</p><blockquote><p>注意！默认情况下，深度学习框架中的步幅与池化窗口的大小相同</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">X</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 0.,  1.,  2.,  3.],</span></span><br><span class="line"><span class="string">          [ 4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="string">          [ 8.,  9., 10., 11.],</span></span><br><span class="line"><span class="string">          [12., 13., 14., 15.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认情况下，深度学习框架中的步幅与池化窗口的大小相同。 因此，如果我们使用形状为(3, 3)的池化窗口，那么默认情况下，我们得到的步幅形状为(3, 3)。</span></span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[10.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 填充和步幅可以手动设定。</span></span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 5.,  7.],</span></span><br><span class="line"><span class="string">          [13., 15.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当然，我们可以设定一个任意大小的矩形池化窗口，并分别设定填充和步幅的高度和宽度。</span></span><br><span class="line">pool2d = nn.MaxPool2d((<span class="number">2</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 5.,  7.],</span></span><br><span class="line"><span class="string">          [13., 15.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="多个通道">多个通道</h2><p>在处理多通道输入数据时，池化层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总。 这意味着<span style="background:#daf5e9;">池化层的输出通道数与输入通道数相同</span>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们在通道维度上连结张量X和X + 1，以构建具有2个通道的输入。</span></span><br><span class="line">X = torch.cat((X, X + <span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">X</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 0.,  1.,  2.,  3.],</span></span><br><span class="line"><span class="string">          [ 4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="string">          [ 8.,  9., 10., 11.],</span></span><br><span class="line"><span class="string">          [12., 13., 14., 15.]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[ 1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">          [ 5.,  6.,  7.,  8.],</span></span><br><span class="line"><span class="string">          [ 9., 10., 11., 12.],</span></span><br><span class="line"><span class="string">          [13., 14., 15., 16.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如下，池化后输出通道的数量仍然是2。</span></span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 5.,  7.],</span></span><br><span class="line"><span class="string">          [13., 15.]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[ 6.,  8.],</span></span><br><span class="line"><span class="string">          [14., 16.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="池化层总结">池化层总结</h2><ul><li>对于给定输入元素，最大池化层会输出该窗口内的最大值，平均池化层会输出该窗口内的平均值。</li><li>池化层的主要优点之一是<strong>减轻卷积层对位置的过度敏感</strong>。</li><li>使用最大池化层以及大于1的步幅，可减少空间维度（如高度和宽度）。</li><li>我们可以指定池化层的填充和步幅。</li><li>池化层的输出通道数与输入通道数相同。</li></ul><h1 id="卷积神经网络-lenet">卷积神经网络 LeNet</h1><p>本节将介绍LeNet，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。 这个模型是由AT&amp;T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），目的是识别图像中的手写数字。 当时，Yann LeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果。</p><p>当时，LeNet取得了与支持向量机（support vector machines）性能相媲美的成果，成为监督学习的主流方法。 LeNet被广泛用于自动取款机（ATM）机中，帮助识别处理支票的数字。 时至今日，一些自动取款机仍在运行Yann LeCun和他的同事Leon Bottou在上世纪90年代写的代码呢！</p><h2 id="lenet">LeNet</h2><p>总体来看，LeNet（LeNet-5）由两个部分组成：</p><ul><li>卷积编码器：由两个卷积层组成;</li><li>全连接层密集块：由三个全连接层组成。</li></ul><p>该架构如下所示：</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/lenet.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">LeNet中的数据流。输入是手写数字，输出为10种可能结果的概率</div></center><p>每个卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均池化层。请注意，虽然ReLU和最大池化层更有效，但它们在20世纪90年代还没有出现。每个卷积层使用5×5卷积核和一个sigmoid激活函数。这些层将输入映射到多个二维特征输出，通常同时增加通道的数量。第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个2×2池操作（步幅2）通过空间下采样将维数即特征图大小减少4倍。卷积的输出形状由批量大小、通道数、高度、宽度决定。</p><p>为了将卷积块的输出传递给全连接层密集块即稠密块，我们必须在小批量中展平每个样本。<span style="background:#daf5e9;">换言之，我们将这个四维输入转换成全连接层所期望的二维输入。</span>这里的二维表示的<span style="background:#FFCC99;">第一个维度索引小批量中的样本，第二个维度给出每个样本的平面向量表示</span>。LeNet的稠密块有三个全连接层，分别有120、84和10个输出。因为我们在执行分类任务，所以输出层的10维对应于最后输出结果的数量。</p><p>通过下面的LeNet代码，可以看出用深度学习框架实现此类模型非常简单。我们只需要实例化一个<code>Sequential</code>块并将需要的层连接在一起。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>我们对原始模型做了一点小改动，去掉了最后一层的高斯激活。除此之外，这个网络与最初的LeNet-5一致。</p><p>下面，我们将一个大小为28×28的单通道（黑白）图像通过LeNet。通过在每一层打印输出的形状，我们可以检查模型，以确保其操作与我们期望的数据流图一致。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/lenet-vert.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">LeNet 的简化版</div></center><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape: \t&#x27;</span>,X.shape)</span><br><span class="line">    </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Conv2d output shape:         torch.Size([1, 6, 28, 28])</span></span><br><span class="line"><span class="string">Sigmoid output shape:        torch.Size([1, 6, 28, 28])</span></span><br><span class="line"><span class="string">AvgPool2d output shape:      torch.Size([1, 6, 14, 14])</span></span><br><span class="line"><span class="string">Conv2d output shape:         torch.Size([1, 16, 10, 10])</span></span><br><span class="line"><span class="string">Sigmoid output shape:        torch.Size([1, 16, 10, 10])</span></span><br><span class="line"><span class="string">AvgPool2d output shape:      torch.Size([1, 16, 5, 5])</span></span><br><span class="line"><span class="string">Flatten output shape:        torch.Size([1, 400])</span></span><br><span class="line"><span class="string">Linear output shape:         torch.Size([1, 120])</span></span><br><span class="line"><span class="string">Sigmoid output shape:        torch.Size([1, 120])</span></span><br><span class="line"><span class="string">Linear output shape:         torch.Size([1, 84])</span></span><br><span class="line"><span class="string">Sigmoid output shape:        torch.Size([1, 84])</span></span><br><span class="line"><span class="string">Linear output shape:         torch.Size([1, 10])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>注意，在整个卷积块中，与上一层相比，每一层特征的高度和宽度都减小了。 第一个卷积层使用2个像素的填充（上下/左右两边就是4个像素），来补偿5×5卷积核导致的特征减少。 相反，第二个卷积层没有填充，因此高度和宽度都减少了4个像素。 随着层叠的上升，通道的数量从输入时的1个，增加到第一个卷积层之后的6个，再到第二个卷积层之后的16个。 同时，每个池化层的高度和宽度都减半。最后，每个全连接层减少维数，最终输出一个维数与结果分类数相匹配的输出。</p><h2 id="模型训练">模型训练</h2><p>现在我们已经实现了LeNet，让我们看看LeNet在Fashion-MNIST数据集上的表现。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br></pre></td></tr></table></figure><p>虽然卷积神经网络的参数较少，但与深度的多层感知机相比，它们的计算成本仍然很高，因为每个参数都参与更多的乘法。 通过使用GPU，可以用它加快训练。</p><p>为了进行评估，我们需要对之前描述的<code>evaluate_accuracy</code>函数进行轻微的修改。 由于完整的数据集位于内存中，因此在模型使用GPU计算数据集之前，我们需要将其复制到显存中。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 设置为评估模式</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    <span class="comment"># 正确预测的数量，总预测的数量</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">                <span class="comment"># BERT微调所需的（之后将介绍）</span></span><br><span class="line">                X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                X = X.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            metric.add(d2l.accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>为了使用GPU，我们还需要一点小改动。 与之前定义的<code>train_epoch_ch3</code>不同，在进行正向和反向传播之前，我们需要将每一小批量数据移动到我们指定的设备（例如GPU）上。</p><p>如下所示，训练函数<code>train_ch6</code>也类似于之前定义的<code>train_ch3</code>。 由于我们将实现多层神经网络，因此我们将主要使用高级API。 以下训练函数假定从高级API创建的模型作为输入，并进行相应的优化。 我们使用在之前介绍的Xavier随机初始化模型参数。 与全连接层一样，我们使用交叉熵损失函数和小批量随机梯度下降。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch6</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用GPU训练模型(在第六章定义)&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)<span class="comment">#小批量随机梯度下降</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()<span class="comment">#交叉熵损失函数</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                            legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># 训练损失之和，训练准确率之和，样本数</span></span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()</span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>现在，我们训练和评估LeNet-5模型。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">loss 0.469, train acc 0.823, test acc 0.779</span></span><br><span class="line"><span class="string">55296.6 examples/sec on cuda:0</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><figure><img src="https://zh-v2.d2l.ai/_images/output_lenet_4a2e9e_67_1.svg" alt="zh-v2.d2l.ai/_images/output_lenet_4a2e9e_67_1.svg" /><figcaption aria-hidden="true">zh-v2.d2l.ai/_images/output_lenet_4a2e9e_67_1.svg</figcaption></figure><h2 id="卷积神经网络-lenet总结">卷积神经网络 LeNet总结</h2><ul><li>卷积神经网络（CNN）是一类使用卷积层的网络。</li><li>在卷积神经网络中，我们组合使用卷积层、非线性激活函数和池化层。</li><li>为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。</li><li>在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。</li><li>LeNet是最早发布的卷积神经网络之一。</li></ul><h1 id="高通低通滤波器">高通/低通滤波器</h1><blockquote><p><a href="https://blog.csdn.net/qq_44631615/article/details/134445747">OpenCV入门7——OpenCV中的滤波器（包括低通滤波与高通滤波，其中低通滤波用于降噪，而高通滤波用于边缘检测）_cv2滤波-CSDN博客</a></p></blockquote><h2 id="unfold">unfold</h2><p><a href="https://blog.csdn.net/qq_40714949/article/details/112836897">Pytorch nn.functional.unfold()的简单理解与用法-CSDN博客</a></p><p><a href="https://zhuanlan.zhihu.com/p/594006437">【pytorch】fold和unfold使用 - 知乎</a></p><figure><img src="C:\Users\21218\AppData\Roaming\Typora\typora-user-images\image-20241101151005838.png" alt="image-20241101151005838" /><figcaption aria-hidden="true">image-20241101151005838</figcaption></figure><figure><img src="C:\Users\21218\AppData\Roaming\Typora\typora-user-images\image-20241101151023759.png" alt="image-20241101151023759" /><figcaption aria-hidden="true">image-20241101151023759</figcaption></figure><figure><img src="C:\Users\21218\AppData\Roaming\Typora\typora-user-images\image-20241101153048348.png" alt="image-20241101153048348" /><figcaption aria-hidden="true">image-20241101153048348</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>打包 python 程序</title>
      <link href="/2024/10/28/%E6%89%93%E5%8C%85%20python%20%E7%A8%8B%E5%BA%8F/"/>
      <url>/2024/10/28/%E6%89%93%E5%8C%85%20python%20%E7%A8%8B%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[<p>参考：<a href="https://blog.csdn.net/libaineu2004/article/details/112612421">Python脚本打包成exe，看这一篇就够了！_python 打包-CSDN博客</a></p><h2 id="打包步骤">打包步骤</h2><p>1.准备<a href="https://www.ico51.cn/">图标ico文件</a>、脚本py文件，将它们放在一个文件夹下，cd到该文件；</p><p>2.新建环境(可选，还需要安装用到的额外库)，并安装 <code>pyinstaller</code> :</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda create -n dabao python==<span class="number">3.10</span></span><br><span class="line">conda activate dabao</span><br><span class="line">pip install pyinstaller -i https://pypi.douban.com/simple/ pyinstaller <span class="comment">#豆瓣源</span></span><br><span class="line"><span class="comment">#或</span></span><br><span class="line">pip install pyinstaller -i https://pypi.tuna.tsinghua.edu.cn/simple pyinstaller <span class="comment">#清华源</span></span><br></pre></td></tr></table></figure><p>3.执行命令打包：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pyinstaller -F py_word.py <span class="comment">#打包exe</span></span><br><span class="line">pyinstaller -F -w py_word.py <span class="comment">#不带控制台的打包</span></span><br><span class="line">pyinstaller -F -w -i chengzi.ico py_word.py <span class="comment">#打包指定exe图标打包（首推）</span></span><br></pre></td></tr></table></figure><p>其中，参数的解释如下<sub>AI</sub>~</p><ol type="1"><li><strong>-F</strong> 或 <strong>--onefile</strong>：表示将所有的依赖文件打包成一个单独的可执行文件。如果不使用这个参数，PyInstaller 会生成一个包含多个文件的输出文件夹。</li><li><strong>-w</strong> 或 <strong>--windowed</strong>：表示在 Windows 上运行时不显示命令行窗口。这个选项适用于 GUI 应用程序，通常与窗口界面程序一起使用，以避免命令行窗口的干扰。</li><li><strong>-i chengzi.ico</strong>：指定可执行文件的图标文件。<code>chengzi.ico</code> 是图标文件的名称，确保该图标文件位于与 <code>py_word.py</code> 相同的目录中，或者提供其完整路径。这个参数也可以写成<code>--icon=chengzi.ico</code>。</li><li><strong>py_word.py</strong>：要打包的 Python 脚本文件的名称。这是你的主程序文件，PyInstaller 会根据这个文件创建可执行文件。</li></ol><h2 id="报错解决办法">报错解决办法</h2><p>1.<strong>报错<code>IndexError: tuple index out of range</code></strong></p><p>参考 <a href="https://blog.csdn.net/weixin_41946088/article/details/121622392">python3.10.0+pyinstaller4.7打包，IndexError: tuple index out of range报错的解决方法_python 3.10.0 contains a bug making it unsupportab-CSDN博客</a>，亲测有用！</p>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八小工具 </category>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代码箱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>曲率、曲率半径与曲率圆</title>
      <link href="/2024/10/15/%E6%9B%B2%E7%8E%87%E3%80%81%E6%9B%B2%E7%8E%87%E5%8D%8A%E5%BE%84%E4%B8%8E%E6%9B%B2%E7%8E%87%E5%9C%86/"/>
      <url>/2024/10/15/%E6%9B%B2%E7%8E%87%E3%80%81%E6%9B%B2%E7%8E%87%E5%8D%8A%E5%BE%84%E4%B8%8E%E6%9B%B2%E7%8E%87%E5%9C%86/</url>
      
        <content type="html"><![CDATA[<h3 id="定义">定义</h3><h3 id="python-计算曲率曲率圆等">Python 计算曲率、曲率圆等</h3><h4 id="使用-numpy-模块计算">使用 numpy 模块计算</h4><p><span style="background:#FFCC99;"><strong>曲率</strong></span></p><p>点击<a href="https://blog.csdn.net/weixin_46713695/article/details/128706529">这里</a>跳转（主要是使用了一条含有7个坐标点的直线，并计算这7个点的曲率。计算曲率用到的一阶导数通过 <code>numpy.gradient()</code> 方法获得，该方法返回 N 维数组的梯度）。</p><p><span style="background:#FFCC99;"><strong>曲率圆</strong></span></p><p>点击<a href="https://blog.csdn.net/qq_65215974/article/details/141431869">这里</a>跳转</p><h4 id="解析方法计算">解析方法计算</h4><p>点击<a href="https://zhuanlan.zhihu.com/p/72083902">这里</a>跳转（这里的方法是在原曲线上取三个离散点，用方程组求解出拟合的二次曲线解析表达式，通过该解析表达式计算三点中中点的曲率作为估计结果）。</p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代码箱 </tag>
            
            <tag> 编码 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>三维重建中的哈希编码</title>
      <link href="/2024/10/14/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E4%B8%AD%E7%9A%84%E5%93%88%E5%B8%8C%E7%BC%96%E7%A0%81/"/>
      <url>/2024/10/14/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E4%B8%AD%E7%9A%84%E5%93%88%E5%B8%8C%E7%BC%96%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<details><summary>参考链接</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/leviopku/article/details/133931366&quot;&gt;Instant-NGP中的多分辨率哈希编码-CSDN博客&lt;/a&gt;&lt;/p&gt;</code></pre></details><h3 id="哈希编码">哈希编码</h3><p>哈希的定义：<strong>哈希算法是一种将任意长度的数据转换为固定长度值的算法</strong>。哈希算法天然就是一种<strong>紧凑表示</strong>。一般取哈希表容量为 <span class="math inline">\(2^{19}\)</span>。如下面的例子中，我们输入了两个相邻坐标(8,3,3)和(8,3,4)，得到的哈希值分别为93092和471887。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hash_computing</span>(<span class="params">coords, log2_hashmap_size=<span class="number">19</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    coords: this function can process upto 7 dim coordinates</span></span><br><span class="line"><span class="string">    log2T:  logarithm of T w.r.t 2</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    primes = [<span class="number">1</span>, <span class="number">2654435761</span>, <span class="number">805459861</span>, <span class="number">3674653429</span>, <span class="number">2097192037</span>, <span class="number">1434869437</span>, <span class="number">2165219737</span>] <span class="comment">#这个素数集是为了给不同维度的坐标赋予不同的权重，从而在哈希计算中减少冲突。</span></span><br><span class="line"></span><br><span class="line">    xor_result = torch.zeros_like(coords)[..., <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(coords.shape[-<span class="number">1</span>]): <span class="comment"># 3</span></span><br><span class="line">        xor_result ^= coords[..., i] * primes[i] <span class="comment"># ^ 表示异或操作，相同位消去，不同位保留</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> torch.tensor((<span class="number">1</span> &lt;&lt; log2_hashmap_size) - <span class="number">1</span>).to(xor_result.device) &amp; xor_result <span class="comment">#对哈希表容量求模，将输入值转化为索引值</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    x = torch.Tensor([[<span class="number">8</span>, <span class="number">3</span>, <span class="number">3</span>], [<span class="number">8</span>, <span class="number">3</span>, <span class="number">4</span>]]).<span class="built_in">int</span>()</span><br><span class="line">    y = hash_computing(x)</span><br><span class="line">    <span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure><h3 id="instant-ngp中的多分辨率哈希编码">Instant-NGP中的多分辨率哈希编码</h3><p>在神经隐式表达任务中，由于神经网络天然对尾数不敏感，会导致表达结果的高频信息丢失，引发“过平滑”的问题。于是位置编码显得尤为重要。</p><p>在NeRF中，采用<strong>频率位置编码</strong>。但频率位置编码的大问题就是不够紧凑。空间坐标本来是紧凑表示，但频率编码以后，会变得疏散。这种疏散的位置表示有两个问题：1，需要更大的神经网络去拟合；2，需要训练更多的轮数才能收敛。这两者都直接导致了NeRF训练和推理都很慢。于是，一种紧凑、通用的编码方式开始被研究者们寻找，包括参数编码和稀疏参数编码系列的编码方式，但要么就是通用性不够好，要么就是并行度不够高，总达不到理想的编码效果。</p><p>哈希编码也是一种位置编码。Instant-NGP提出的<strong>多分辨率哈希编码</strong>的诞生，解决了以上的问题。 <span class="math display">\[h(\mathbf{x})=\left(\bigoplus_{i=1}^d x_i \pi_i \right) \mod  T,\]</span> 先理解这个公式：假如我输入的坐标是<strong>x</strong>=(8,6,7)，有一组包含1的质数集<strong>π</strong> = ( 1 , 2654435761 , 805459861 ) ，质数集的维度与坐标维度一致即可。这里圆圈中间有个加号的符号就是“<strong>异或</strong>”计算，这是一种位运算：相同为0，不同为1。mod 表示模除，即取余数。T 表示哈希表的容量，典型值为<span class="math inline">\(2^{19}\)</span>。</p><p>这个公式的输出结果依然是一个整数，表达为一个索引值（即哈希值）。这个索引值可以直接在具体的哈希表上查找内容。将一个坐标化成一个索引值，<strong>相邻的坐标的哈希值会有明显的差异</strong>。</p><h3 id="dngaussian中的哈希网格编码">DNGaussian中的哈希网格编码</h3><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/1.jpg" alt="1" /><figcaption aria-hidden="true">1</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 加速算法 </tag>
            
            <tag> 编码 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高斯核函数 RBF</title>
      <link href="/2024/10/08/%E9%AB%98%E6%96%AF%E6%A0%B8%E5%87%BD%E6%95%B0RBF/"/>
      <url>/2024/10/08/%E9%AB%98%E6%96%AF%E6%A0%B8%E5%87%BD%E6%95%B0RBF/</url>
      
        <content type="html"><![CDATA[<h3 id="径向基函数rbf">径向基函数RBF</h3><details><summary>参考链接们点这里展开</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_53118224/article/details/124333490&quot;&gt;一文彻底理解机器学习高斯核函数和基函数_高斯基函数-CSDN博客（形象有图解!）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://ghx0x0.github.io/2015/06/11/ML-RBFnet/&quot;&gt;ML学习笔记之————RBF径向基网络 | GH‘s blog (ghx0x0.github.io)&lt;/a&gt;&lt;/p&gt;</code></pre></details><p><strong>径向基（Radical Basis Function, RBF）函数</strong>，是某种沿径向对称的标量函数，用于将有限维数据映射到高维空间。它由 Powell 在1985年提出。通常也定义为空间中任一点 <span class="math inline">\(x\)</span> 到某一中心 <span class="math inline">\(c\)</span> 之间欧氏距离的单调函数，可记作 <span class="math inline">\(k(||x-c||)\)</span> ，往往作用于围绕中心的一个局部区域，当 <span class="math inline">\(x\)</span> 距离中心很远时作用小到可忽略不计。<strong>高斯核函数（Gaussian kernel）</strong>是最常用的一种径向基函数，其基函数一般形式为： <span class="math display">\[R_i(x)= \exp(- \frac{||x-c_i||^2}{2 \sigma_i^2}),\]</span> 可发现，该公式与高斯分布概率密度函数 <span class="math inline">\(f(x)=\frac{1}{\sqrt{2 \pi} \sigma} \exp(- \frac{(x-\mu)^2}{2 \sigma^2})\)</span> 大致相同。其中，<span class="math inline">\(i=1,2,...,m\)</span> 表示一共有 <span class="math inline">\(m\)</span> 个样本点，每个样本点对应一个以样本点为中心的基函数 <span class="math inline">\(R_i(x)\)</span> （如二维样本 <span class="math inline">\((x_i,y_i)\)</span> 对应基函数 <span class="math inline">\(R_i(x)=k(||x-x_i||)\)</span>）。函数 <span class="math inline">\(R_i(x)\)</span> 在中心 <span class="math inline">\(c_i\)</span> 处取到唯一最大值。由基函数表示的高斯核函数为： <span class="math display">\[R(x)=\sum_{i=1}^m \omega_i R_i(x) = \sum_{i=1}^m \omega_i \cdot k(||x-x_i||)\]</span> 其中，当被应用于多变量插值问题时，基函数权重 <span class="math inline">\(\omega_i\)</span> 可以通过学习获得。<font color=#0091ff>对于既定输入 <span class="math inline">\(x\)</span> ，唯有 <span class="math inline">\(x\)</span> 中心的附近少部分基函数被激活，因为 <span class="math inline">\(x\)</span>​ 距离这些被激活基函数的中心足够近吗？？</font></p><blockquote><p><strong>径向基函数的由来</strong></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20241008163448622.png" alt="image-20241008163448622" /><figcaption aria-hidden="true">image-20241008163448622</figcaption></figure><p>事实上，通过实验可知，当权重都视为 1 时，样本点越密集的地方得到的总曲线函数值越大。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20241008163705259.png" alt="image-20241008163705259" style="zoom:67%;" /></p><p>实验代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> re <span class="keyword">import</span> X</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> numpy.linalg <span class="keyword">as</span> la</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">factorial</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span> <span class="keyword">or</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> (n*factorial(n-<span class="number">1</span>))</span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gaussiankernel</span>(<span class="params">sigma,x,x0</span>):</span><br><span class="line">    y=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> x:</span><br><span class="line">        y0=np.exp(-(la.norm(i-x0)**<span class="number">2</span>) / (<span class="number">2</span> * sigma ** <span class="number">2</span>))</span><br><span class="line">        y.append(y0)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gaussianji</span>(<span class="params">sigma,x,n,x_di</span>):</span><br><span class="line">    y=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,n):</span><br><span class="line">        x_new=(x-x_di[i])</span><br><span class="line">        y0=(np.exp(-(x_new**<span class="number">2</span>)))<span class="comment">#*((x_new/sigma)**i)*np.sqrt((2**i)/factorial(i))</span></span><br><span class="line">        y.append(y0)</span><br><span class="line">    y=np.array(y)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"> </span><br><span class="line">N=<span class="number">11</span></span><br><span class="line"><span class="comment">#x_di=np.linspace(-10,10,N)</span></span><br><span class="line">x_di=[random.uniform(-<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">x=np.linspace(-<span class="number">10</span>,<span class="number">10</span>,N)</span><br><span class="line">j=-<span class="number">10</span></span><br><span class="line">color=[<span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;black&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;brown&#x27;</span>, <span class="string">&#x27;green&#x27;</span>,<span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;black&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;brown&#x27;</span>, <span class="string">&#x27;green&#x27;</span>,<span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;black&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;brown&#x27;</span>, <span class="string">&#x27;green&#x27;</span>]</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">sum_y = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> x:</span><br><span class="line">    y=gaussianji(<span class="number">1</span>,k,N,x_di)</span><br><span class="line">    sum_y.append(<span class="built_in">sum</span>(y))</span><br><span class="line">    plt.plot(x_di,y,linewidth=<span class="number">0.5</span>,color=color[<span class="built_in">abs</span>(j)%<span class="number">10</span>])</span><br><span class="line">    plt.plot(x_di,y,<span class="string">&#x27;*&#x27;</span>)</span><br><span class="line">    j=j+<span class="number">1</span></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">#y=gaussiankernel(1,x,0) 画出标准正态分布</span></span><br><span class="line"><span class="comment">#plt.plot(x,y,linewidth=0.5,color=&#x27;blue&#x27;)</span></span><br><span class="line">plt.plot(x,sum_y,linewidth=<span class="number">0.5</span>,color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line"> </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></blockquote><h3 id="rbf神经网络">RBF神经网络</h3><details><summary>参考链接们点这里展开</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_42555080/article/details/94661762&quot;&gt;径向基(Radial Basis Function:RBF)神经网络学习笔记_radial basis function neural networks-CSDN博客&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_44603934/article/details/123649337&quot;&gt;RBF神经网络算法分析与应用（适合快速入门实战）_rbf需要更多的径向基神经元-CSDN博客&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://ghx0x0.github.io/2015/06/11/ML-RBFnet/&quot;&gt;ML学习笔记之————RBF径向基网络 | GH‘s blog (ghx0x0.github.io)&lt;/a&gt;&lt;/p&gt;</code></pre></details><p><strong>RBF 网络的优点</strong>：网络学习速度快，且能避免局部极小问题。</p><p><strong>RBF 网络的局限性（“方仲永”之者）</strong>：RBF 网络目前最多只有三层，因此拟合能力虽然前期优于传统的前向网络，但是却无法与深层神经网络相比。</p><p>1988年， Moody和 Darken提出了RBF神经网络，属于前向神经网络类型，它能够以任意精度逼近任意连续函数，<strong>特别适合于解决分类问题</strong>。</p><p><img src="https://ghx0x0.github.io/img/QQ截图20150614122027.jpg" alt="Edge Image Viewer (ghx0x0.github.io)" style="zoom:80%;" /></p><p>RBF网络的结构与多层前向网络类似，它是一种三层前向网络。<strong>输入层</strong>由信号源结点组成；<strong>第二层</strong>为隐含层，隐单元数视所描述问题的需要而定，隐单元的变换函数是RBF径向基函数，它是对中心点径向对称且衰减的非负非线性函数；<strong>第三层</strong>为输出层，它对输入模式的作用作出响应。从输人空间到隐含层空间的变换是非线性的，而从隐含层空间到输出层空间变换是线性的。</p><figure><img src="https://ghx0x0.github.io/img/QQ截图20150614122035.jpg" alt="Edge Image Viewer (ghx0x0.github.io)" /><figcaption aria-hidden="true">Edge Image Viewer (ghx0x0.github.io)</figcaption></figure><p>RBF网络的基本思想是：用RBF作为隐单元的“基”构成隐含层空间，这样就可将输入矢量直接（即不需要通过权连接）映射到隐空间。<span style="background:#daf5e9;">换句话来说，RBF网络的隐层的功能就是将低维空间的输入通过非线性函数映射到一个高维空间，这样低维度线性不可分的情况到高维度就可以变得线性可分了，主要就是核函数的思想。</span>然后再在这个高维空间进行曲线的拟合。它等价于在一个隐含的高维空间寻找一个能最佳拟合训练数据的表面。这点与普通的多层感知机MLP是不同的。</p><p>当RBF的中心点确定以后，这种映射关系也就确定了。而隐含层空间到输出空间的映射是线性的，即<strong>网络的输出是隐单元输出的线性加权和，此处的权即为网络可调参数</strong>。由此可见，从总体上看，网络由输人到输出的映射是非线性的，而网络输出对可调参数而言却又是线性的。这样<strong>网络的权就可由线性方程组直接解出，从而大大加快学习速度并避免局部极小问题</strong>。</p><p>由上，我们知道 RBF 的结构是三层，在实际应用中常从两个方面来设计一个 RBF 神经网络。一是结构设计，也就是说隐藏层含有几个节点合适。二是参数设计，也就是对网络各参数进行求解。RBF的可学习参数有3个：<strong>基函数的中心 <span class="math inline">\(c\)</span></strong>、<strong>方差 <span class="math inline">\(\sigma\)</span> </strong>以及<strong>隐含层到输出层的权值 <span class="math inline">\(\omega\)</span> </strong>。</p><h3 id="rbf-神经网络的训练与参数学习">RBF 神经网络的训练与参数学习</h3><details><summary>参考链接们点这里展开</summary><pre><code>&lt;p&gt;&lt;a href=&quot;https://ghx0x0.github.io/2015/06/11/ML-RBFnet/&quot;&gt;ML学习笔记之————RBF径向基网络 | GH‘s blog (ghx0x0.github.io)&lt;/a&gt;&lt;/p&gt; &lt;/details&gt;</code></pre><p>方法1</p><p>方法2</p><h3 id="rbf-神经网络与-bp-神经网络和-svm-的区别">RBF 神经网络与 BP 神经网络和 SVM 的区别</h3><p>请参考：<a href="https://blog.csdn.net/weixin_42555080/article/details/94661762">径向基(Radial Basis Function:RBF)神经网络学习笔记_radial basis function neural networks-CSDN博客</a></p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Diffusion Model（二）</title>
      <link href="/2024/10/05/Diffusion%20Model%20%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2024/10/05/Diffusion%20Model%20%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="引入">引入</h1><p><strong>模型网站：</strong></p><p>DALL·E2： <a href="https://openai.com/dall-e-2">DALL·E 2 (openai.com)</a></p><p>DALL·E3：<a href="https://openai.com/dall-e-3">DALL·E 3 (openai.com)</a></p><p>Sora: <a href="https://openai.com/sora?ref=aihub.cn">Sora (openai.com)</a></p><h2 id="发展简介">发展简介</h2><blockquote><p><a href="https://zhuanlan.zhihu.com/p/593896912">李沐论文精读系列——由DALL·E 2看图像生成模型 - 知乎 (zhihu.com)</a></p><p><a href="https://swarma.org/?p=37227">AI艺术的背后：详解文本生成图像模型 | 集智俱乐部 (swarma.org)</a></p></blockquote><p>近几年图像生成模型的工作：</p><figure><img src="https://pic1.zhimg.com/80/v2-6350528949be8a46b618e7fb84c28e04_720w.webp" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="基于vq-vae">基于VQ-VAE</h2><p>顺序：<code>AE</code>(20世纪80年代)——<code>VAE</code>（Kingma et al, 2014<sup>[2]</sup>）——<code>VQ-VAE</code>(Van et al, 2017<sup>[3]</sup>)——<code>DALL·E</code>（Ramesh et al, 2021[4]）</p><h3 id="ae">AE</h3><p>自编码器（Auto Encoder）主要由编码器 encoder 和解码器 decoder 组成，目标是重建出输入图像。首先，输入图像 x 通过编码器被压缩，提取出高维特征 latent feature，然后这些高维特征再经过解码器重建出图像 x' 。AE 希望重建出的图像和输入图像越接近越好，因此损失函数为 <span class="math inline">\(loss=||x-x&#39;||^2\)</span> 。通常 latent feature 的维度比输入、输出的维度小，因此称之为 bottleneck。</p><figure><img src="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-d690623495c81ce7adb97e3eab397475.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>由于 AE 是一个自重建的过程，因此被称为自编码器，实际常用于降维，去噪，异常检测或者神经风格迁移中。</p><h3 id="vae">VAE</h3><p>VAE（Variational Auto Encoder）仍然由一个编码器和一个解码器构成，并且目标仍然是重建原始输入。与 AE 不同的是，VAE 的中间表征是通过采样高斯分布得到的 <span class="math inline">\(z\)</span> 。首先，VAE 将原图 <span class="math inline">\(x\)</span> 通过编码器网络映射为高斯分布 $( _x,_x ) $ 。然后通过重参数技巧在该分布中采样中间表征 $z=_x+_x$ ，其中 <span class="math inline">\(\epsilon\thicksim \mathcal{N}\left( 0,\mathbf{I} \right)\)</span> 。最后，解码器解码 <span class="math inline">\(z\)</span>​ 重建出图像。其损失函数定义为： <span class="math display">\[loss=||x-\hat{x}||^2+D_{KL}\left[ \mathcal{N}\left( \mu _x,\sigma _x \right) ,\mathcal{N}\left( 0,\mathbf{I} \right) \right]\]</span> 在 VAE 的损失函数中，第二项是之所以让编码器输出的分布尽可能接近标准正态分布，是因为这样在生成时可以直接从正态分布中采样，然后通过解码器重建图像（怪怪的）。</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/574959469">事实上，编码器拟合的是后验概率 <span class="math inline">\(p(z|x)\)</span> ，解码器拟合的是似然函数 <span class="math inline">\(p(x|z)\)</span>。</a>似然函数理解为，在模型参数 <span class="math inline">\(z\)</span> 下，使得模型输出为 <span class="math inline">\(x\)</span> 的概率最大化的函数。</p></blockquote><figure><img src="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-249c281b4571a5b27ac531512e7c1cc0.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>VAE 提高了生成结果的多样性（引入了符合高斯分布的随机变量）。对于同一个输入，由于 latent vector 不再是固定的，而是采样得到，我们可以得到不同的但类似的任意多个输出。</p><h3 id="vq-vae">VQ-VAE</h3><p>VAE 具有一个最大的问题就是使用了固定的先验（正态分布），其次是使用了连续的中间表征，这样会导致图片生成的多样性并不是很好以及模型的可控性差。为了解决这个问题，VQ-VAE（ Vector Quantized Variational Auto Encoder） 选择使用<strong>离散的中间表征</strong>，同时，通常会使用一个自回归模型来学习先验（例如 PixelCNN 或者 Transformer），在训练完成后，直接用来采样得到 <span class="math inline">\(z_q\)</span>​ ，然后通过匹配 Codebook, 使用解码器进行图片生成。</p><blockquote><p>VAE的目的是训练完成后, 丢掉 encoder, 在 prior 上直接采样, 加上 decoder 就能生成. 如果我们现在独立地采 <span class="math inline">\(H×W\)</span> 个 <span class="math inline">\(z\)</span> 组成 <span class="math inline">\(z_e(x)\)</span> , 然后查表得到维度为 <span class="math inline">\(H×W×D\)</span> 的 <span class="math inline">\(z_q(x)\)</span>，那么生成的图片在空间上的每块区域之间几乎就是独立的。<strong>因此我们需要让各个 <span class="math inline">\(z\)</span> 之间有关系</strong>。用 PixelCNN, 对这些 <span class="math inline">\(z\)</span> 建立一个自回归模型： <span class="math inline">\(p(z_1,z_2,z_3,...)=p(z_1)p(z_2|z_1)p(z_3|z_1,z_2)...\)</span> 这样就可以进行 <a href="https://hyper.ai/wiki/2986">ancestral sampling</a> 生成 <span class="math inline">\(x\)</span>, 得到一个互相之间有关联的 <span class="math inline">\(H×W\)</span> 的整数矩阵。 <span class="math inline">\(p(z_1,z_2,z_3,...)\)</span> 这个联合概率即为我们想要的 prior。——<a href="https://zhuanlan.zhihu.com/p/91434658">Elijha：VQ-VAE解读</a></p><p>The prior distribution over the discrete latents <span class="math inline">\(p(z)\)</span> is a categorical distribution, and can be made autoregressive by depending on other <span class="math inline">\(z\)</span> in the feature map. Whilst training the VQ-VAE, the prior is kept constant and uniform. After training, we fit an autoregressive distribution over <span class="math inline">\(z\)</span>, <span class="math inline">\(p(z)\)</span>, so that we can generate <span class="math inline">\(x\)</span> via ancestral sampling. We use a PixelCNN over the discrete latents for images, and a WaveNet for raw audio. Training the prior and the VQ-VAE jointly, which could strengthen our results, is left as future research. ——来自<a href="https://arxiv.org/pdf/1711.00937.pdf">原论文</a></p></blockquote><figure><img src="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-b761643dc19adceb1734df82210eda76.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p><strong>VQ-VAE 的算法流程为：</strong></p><ol type="1"><li><span style="background:#dad5e9;">（Embedding Space）</span> 首先设置 <span class="math inline">\(K\)</span> 个 <span class="math inline">\(D\)</span> 维向量 <span class="math inline">\(e_1,e_2...e_k\)</span> 作为可查询的 Codebook。</li><li><span style="background:#daf5e9;">（绿方块）</span> 输入图片通过编码器 CNN 来得到中间表征 <span class="math inline">\(z_e(x)\)</span>，<span class="math inline">\(z_e(x)\)</span> 是 <span class="math inline">\(H×W\)</span> 个 <span class="math inline">\(D\)</span> 维向量。</li><li><span style="background:#dcffff;">（蓝方块）</span> 通过最邻近算法，在 Codebook 中查询与 <span class="math inline">\(z_e(x)\)</span> 中每个 <span class="math inline">\(D\)</span> 维向量最相似的向量 <span class="math inline">\(e_i^{(1)},e_i^{(2)}...e_i^{(H×W)}\)</span> ，用 index 表示，就得到了 <span class="math inline">\(q(z|x)\)</span> 。</li><li><span style="background:#dad5e9;">（紫方块）</span>根据 <span class="math inline">\(q(z|x)\)</span> 将 Codebook 中查询的相似向量放到对应 <span class="math inline">\(z_e(x)\)</span> 的位置上，得到 <span class="math inline">\(z_q(x)\)</span>​ 。</li><li>解码器通过得到的中间表征 <span class="math inline">\(z_q(x)\)</span> 重建图片。</li></ol><p>一般 <span class="math inline">\(K=8192\)</span>，<span class="math inline">\(D=512\)</span> or <span class="math inline">\(768\)</span>。</p><p>VQ-VAE 最核心的部分就是 <strong>Codebook 查询操作</strong>，通过使用具有高度一致性的 Codebook 来代替混乱的中间表征，可以有效的提高图像生成的可控性。初代的DALL·E 模型就是基于 VQ-VAE 的架构实现的。</p><h3 id="delle">DELL·E</h3><blockquote><p>Project page: <a href="https://openai.com/research/dall-e">DALL·E: Creating images from text (openai.com)</a></p></blockquote><p>文本生成图像模型 DALL·E 由 OpenAI 开发，其第一代版本使用的是在 VQ-VAE 自回归生成的基础上加上文本条件，实现了 <a href="https://zhuanlan.zhihu.com/p/624793654">zero-shot</a> 的 text-to-image 生成。</p><p>在 DELL·E 中使用了 Transformer 来作为自回归模型。在生成过程中，输入文本通过 Transformer 预测中间表征 <span class="math inline">\(z\)</span> ,然后匹配 <span class="math inline">\(K=8192\)</span> 的 Codebook得到 <span class="math inline">\(z_q\)</span> ，最后通过 Decoder 模块将 <span class="math inline">\(z_q\)</span>​ 解码为图像。 具体过程见<a href="https://zhuanlan.zhihu.com/p/683882116">DALL-E 系列 (1-3) - 知乎 (zhihu.com)</a>。在 DALL-E 的论文中，作者还提出了很多技术上的细节，例如，在最后挑选图片的时候，可以使用 CLIP 模型来选择与文本相似度最高的模型，以及分布式训练，混合精度训练等，具体细节可以查看<a href="https://arxiv.org/pdf/2102.12092.pdf">原论文</a>。</p><blockquote><p>CLIP (Contrastive Language-Image Pre-training，对比图文预训练) 是一种 zero-shot 的视觉分类模型，它是和 DALL·E 一起被发布的。</p><p>CLIP 是一种神经网络，为输入的图像返回最佳的标题。它所做的事情与 DALL-E 所做的相反 —— 它是将图像转换为文本，而 DALL-E 是将文本转换为图像。引入 CLIP 的目的是为了学习物体的视觉和文字表示之间的联系。</p></blockquote><figure><img src="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-5bb80123f487ae06090f48943cfa8322.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="基于-gan">基于 GAN</h2><p>顺序： <code>GAN</code>（Creswell et al, 2014<sup>[5]</sup>）——<code>VQGAN</code>（Esser et al, 2021<sup>[6]</sup>）——<code>VQGAN-CLIP</code>（Crowson et al, 2022<sup>[7]</sup>）——<code>DELL·E Mini</code>——<code>Parti</code>（Google, 2022 在 Imagen 之后一个月推出, AI 绘图）——<code>NUWA-Infinity</code>（微软亚洲研究院， 2022， 无限创作）</p><h3 id="gan">GAN</h3><p><strong>生成对抗网络</strong>（<strong>GAN</strong>，Generative Adversarial Networks）由两个主要的模块构成：生成器和判别器。生成器负责生成一张图片，而判别器则负责判断这张图片质量，也就是判断是真实样本还是生成的虚假样本，通过逐步的迭代，左右互博，最终生成器可以生成越来越逼真的图像，而判别器则可以更加精准的判断图片的真假。GAN 的最大优势是其<strong>不依赖于先验假设，而是通过迭代的方式逐渐学到数据的分布</strong> [6]。</p><p>其训练流程如下：</p><ol type="1"><li>初始化一个生成器 <span class="math inline">\(G\)</span> 和一个判别器 <span class="math inline">\(D\)</span> ;</li><li>固定生成器 <span class="math inline">\(G\)</span> 的参数， 只更新判别器 <span class="math inline">\(D\)</span> 的参数。具体过程为：选择一部分真实样本，以及从生成器 <span class="math inline">\(G\)</span> 得到一些生成的样本，送入到判别器 <span class="math inline">\(D\)</span> 中，判别器需要判断哪些样本为真实的，哪些样本为生成的，通过与真实结果的误差来优化判别器;</li><li>固定判别器 <span class="math inline">\(D\)</span> 的参数, 只更新生成器 <span class="math inline">\(G\)</span> 的参数。具体过程为：使用生成器生成一部分样本， 将生成的样本喂入到判别器 <span class="math inline">\(D\)</span> 中，判别器会对其进行判断，优化生成器 <span class="math inline">\(G\)</span> 的参数，使得判别器将其判断为更加偏向于真实样本。</li></ol><h3 id="vqgan">VQGAN</h3><p>VQGAN （ Vector Quantized Generative Adversarial Networks ） 是一种 GAN 的变种，一种视觉生成模型,来自德国海德堡大学IWR研究团队受到 VQ-VAE 的启发，<strong>使用了 Codebook 来进行离散表征</strong>。</p><p>具体来说，预先定义 <span class="math inline">\(K\)</span> 个向量作为离散的特征查询表 Codebook。当一张图片被送入到 CNN Encoder 中后，会得到 <span class="math inline">\(h×w×n_z\)</span> 维的中间表征 <span class="math inline">\(\hat{z}\)</span> ,之后与 VQ-VAE 类似地查询 Codebook 得到 <span class="math inline">\(z_q\)</span> 。最后，CNN Decoder 也就是生成器 <span class="math inline">\(G\)</span> 根据得到的表征 <span class="math inline">\(z_q\)</span> 重建出图像。其中，通过 <span class="math inline">\(\hat{z}\)</span> 得到 <span class="math inline">\(z_q\)</span> 的公式为： <span class="math display">\[z_q=q(\hat{z}):=\left( \underset{z_k\in \mathcal{Z}}{arg\ \min}||\hat{z}_{ij}-z_k|| \right) \in \mathbb{R}^{h×w×n_z}\]</span> 同样地，这里的对于生成器部分的优化，需要使用与 VQ-VAE 一样的方法去进行：</p><p>在训练好 VQGAN 之后，在生成的时候，可以直接初始化一个 <span class="math inline">\(z_q\)</span> 去生成图像，然而，为了能够得到稳定的 <span class="math inline">\(z_q\)</span> ，需要使用一个模型对先验进行学习。这里使用了 Transformer 模型来学习 <span class="math inline">\(z_q\)</span> 中离散表征的序列，可以简单的将其建模为自回归模型 <span class="math inline">\(p(s)=\prod_i{p(s_i|s_{&lt;i})}\)</span> ，这样，我们只需要给定一个初始的随机向量，就可以通过 Transfomer 模型生成完整的 <span class="math inline">\(z_q\)</span> ，从而可以通过 CNN Decoder 模块生成最终的图像。</p><figure><img src="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-dfb34aff6b13b8f38a0ed05e4c0b0de1.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><blockquote><p>这里的判别器不是对每张整体图片进行判断，而是对图片的每一小块进行判断。</p></blockquote><h2 id="基于-diffusion">基于 Diffusion</h2><p>顺序: Diffusion（Sohl-Dickstein et al, 2015<sup>[12]</sup>）——Diffusion DDPM（ Ho et al, 2020<sup>[1]</sup>）——GLIDE（Nichol et al, 2021.12<sup>[8]</sup>）——DALL·E2（Ramesh, 2022.4<sup>[9]</sup>）——Imagen（Saharia, 2022.5<sup>[10]</sup>）——Stable Diffusion(Rombach et al, 2022<sup>[11]</sup>)——ChatGPT（2022.11）——ChatGPT-4（2023.3）——DALL·E3（2023.10）——Sora（2024.2）</p><h3 id="diffusion">Diffusion</h3><p>回忆上文提到的 VQ-VAE 以及 VQ-GAN，都是先通过编码器将图像映射到中间潜变量，然后解码器在通过中间潜变量进行还原。实际上，扩散模型做的事情本质上是一样的，不同的是，扩散模型完全使用了全新的思路来实现这个目标。</p><blockquote><p><strong>为什么叫“扩散”模型呢？</strong> 扩散这个名词来自于热力学，某个区域物质密度很高，那么它就会向低密度的区域扩散，最终达到平衡；就好比香水的味道会扩散到整个房间。这里的平衡指的是标准高斯分布的噪声。图片由原来的有序，逐渐变为无序的噪声，可以认为是“无序度”的扩散。</p></blockquote><p>在传统的扩散模型中，主要有两个过程组成，<strong>前向扩散过程，反向去噪过程</strong>，前向扩散过程主要是在一张图片上随机添加高斯噪声，而逆向去噪过程则是将一张随机噪音的图片还原为一张完整的图片。</p><p>最初的 Diffusion 在2015 年甚至更早就被提出了，当时有人基于非平衡热力学提出了一个纯数学的生成模型（Sohl-Dickstein et al, 2015<sup>[12]</sup>）。这种生成模型通过迭代正向扩散破坏数据分布结构，并通过学习反向扩散过程恢复数据结构。最初的模型并没有用代码实现。同时由于 forward 和 backward 训练采样和推理都需要很长时间，所以扩散模型一直不如 GAN 受关注，直到2020年 DDPM 的出现。DDPM 主要有两个贡献：</p><ol type="1"><li>不需要推测 <span class="math inline">\(x_t\)</span> ，只需要推测 <span class="math inline">\(\epsilon_\theta\)</span> 就可以了。这样大大降低了推理的难度。这里计算梯度的网络是 U-net。</li><li>由于每步都是高斯噪声，只需要预测它的均值和方差即可。并且作者提出，固定方差 <span class="math inline">\(\sigma\)</span>​ ，仅仅预测噪声的均值就可以达到很好的效果。</li></ol><p>DDPM 中 Diffusion 的损失函数如下（具体推导见下文）： <span class="math display">\[L=\lVert \boldsymbol{\epsilon }_t-\boldsymbol{\epsilon }_{\theta}\left( \sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon }_t,t \right) \rVert ^2\]</span> 其算法流程为：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240320200738823.png" alt="image-20240320200738823" /><figcaption aria-hidden="true">image-20240320200738823</figcaption></figure><p>在完成训练之后，只需要通过重参数化技巧，进行采样操作即可，具体流程如上边右图所示，通过不断的「减去」模型预测的噪音，可以逐渐生成一张完整的图片。</p><p><strong>Diffusion 的初步改进： classifier guided diffusion</strong></p><p>DDPM的成功引起了大家的兴趣。2020年底， <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2102.09672.pdf">improved DDPM</a> 就出炉了。它在DDPM的基础上，不仅仅预测均值，而且预测方差；并且在更大模型上尝试了DDPM，发现大模型会带来很大的效果提升。于是紧接着有一篇 <a href="https://arxiv.org/pdf/2105.05233.pdf">Diffusion Models Beat GANs on Image Synthesis</a>（2021.5, OpenAI），进一步把模型做大、做复杂，取得了很好的效果；并且，作者在这篇论文里使用了 <code>classifier guidance</code> 的方法，引导模型做采样和生成。这不仅让生成的图像更逼真，而且加速了反向采样的速度，只需要25次采样就能从噪声还原出高质量的图片。</p><p>简单来说，<code>classifier guided diffusion</code>使用一个额外训练好的图像分类器 classifier ，每次判别 <span class="math inline">\(x_t\)</span> 的类别，得到交叉熵损失函数，进一步可以得到梯度 gradient ，用这个梯度去引导 <span class="math inline">\(f_\theta\)</span> （这里的 <span class="math inline">\(f_\theta\)</span>​ 是如上文的 U-net）做预测。这里的梯度大概暗含了当前生成的图像有没有某个物体，以及生成的物体真不真实。通过引导，扩散模型的逼真度提升了很多，击败了一些GAN模型。</p><figure><img src="https://pic3.zhimg.com/80/v2-9d9e7d2039c35f9f82056b3c180cd4c2_720w.webp" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>这和 conditional GAN 的思路差不多，提供更多的 condition，也就是引导，辅助它完成任务。这里的引导不一定非得是一个分类器，可以是文本，也可以是CLIP模型。</p><p><strong>Diffusion 的再次改进： Classifier-Free Guidance Diffusion</strong></p><p>后续又有一些改进操作，这些改进操作使得扩散模型被广泛的应用于文本生成图像任务中。其中，最常用的改进版本为 Classifier-Free Guidance Diffusion。OpenAI 后续的 <strong>GLIDE</strong> 模型和 <strong>DALL·E 2</strong> ，以及谷歌的 <strong>Imagen</strong>，在推理过程中抛弃了分类器的引导——Classifier free guidance。</p><p>这种扩散模型在每个时间步 <span class="math inline">\(t\)</span>， 除了原有的无引导情况下的输出 <span class="math inline">\(\epsilon_\theta (x_t)\)</span> 外，还增加了在有引导情况下的输出 <span class="math inline">\(\epsilon_\theta (x_t,y)\)</span> ，由此 Classifier-Free Guidance Diffusion 结合了条件和无条件噪声估计模型，定义为： <span class="math display">\[\hat{\epsilon}_\theta(x_t|y)=\epsilon_\theta(x_t)+s·(\epsilon_\theta(x_t,y)-\epsilon_\theta(x_t))\]</span> 通过有引导时的输出与无引导时的输出作差，就得到了一个方向，告诉网络如何从无引导的输出到达有引导的输出。这样在训练完成后，就能达到抛开引导的目的。这种改进优点是<strong>训练过程非常稳定，且摆脱了分类器的限制</strong>（实际上等价于学习了一个隐含的分类器），缺点是，训练成本比较高，相当于每次要生成两个输出，尽管如此，后面的大部份知名文本生成图像模型，都是基于这个方法进行的。</p><p>在使用了很多技巧之后，基于扩散模型的 <strong>GLIDE</strong> 用了35亿参数，效果就直逼基于VQ-VAE 的、用了120亿参数的<strong>DALL·E</strong> 模型。OpenAI 看到扩散模型确实靠谱，所以顺着 GLIDE 的思路，孕育出了现在的 <strong>DALL·E 2</strong>。</p><h3 id="glide">GLIDE</h3><blockquote><p>代码已开源</p></blockquote><p>GLIDE 使用了文本作为 condition 来引导扩散模型，主要用了两种策略: Classifier-Free Guidance Diffusion 和 CLIP 来作为条件监督。GLIDE 的扩散模型中的 <span class="math inline">\(\hat{\epsilon}_\theta(x_t|y)\)</span> 中的 <span class="math inline">\(y\)</span> 是一段文本描述。</p><p>GLIDE的生成效果。下图是GLIDE基于不同的文本提示生成的16个图像集，例如“使用计算器的刺猬”、“戴着红色领带和紫色帽子的柯基”等等，如图所示，生成的图像基本符合文本描述。</p><figure><img src="https://img-blog.csdnimg.cn/img_convert/d2e902a630e984757eb922d0b5662bdc.png" alt="d2e902a630e984757eb922d0b5662bdc.png" /><figcaption aria-hidden="true">d2e902a630e984757eb922d0b5662bdc.png</figcaption></figure><p>除了图文转换，该论文还包括一个交互式系统的原型，支持通过 <strong>选取区域+文本Prompt</strong> 来对图像进行编辑操作，用于逐步细化图像的选定部分。使用过程中，只需要将遮蔽区域进行 mask，以及剩下的图片一起送入到网络中，即可产生补全之后的图片。</p><figure><img src="https://img-blog.csdnimg.cn/img_convert/da853db5c9f392a658cf03922989c6c4.png" alt="da853db5c9f392a658cf03922989c6c4.png" /><figcaption aria-hidden="true">da853db5c9f392a658cf03922989c6c4.png</figcaption></figure><p>此外，<strong>GLIDE 的语义理解能力并不是很强，在一些少见的文本描述下（如八条腿的猫），很难产生合乎逻辑的图像，而 DALL-E2 在这方面的能力上，要远超 GLIDE</strong></p><figure><img src="https://img-blog.csdnimg.cn/img_convert/65b3e3049c10e65525e3e5e92f57b60b.png" alt="65b3e3049c10e65525e3e5e92f57b60b.png" /><figcaption aria-hidden="true">65b3e3049c10e65525e3e5e92f57b60b.png</figcaption></figure><h3 id="dalle2">DALL·E2</h3><p>DALL-E2 是 OpenAI 2022年4月推出的 AI 生成图像模型，其最大的特色是<strong>模型具有惊人的理解力和创造力</strong>，它可以根据给定的概念、特性以及风格来生成原创性的图片。除此之外，DALL·E 2 还能根据描述，对已有的图片进行二次编辑，比如移除或添加某个物体，并且把阴影、反射、纹理考虑在内。还有，就算不给定语言描述，DALL·E 2 也能根据已有的图片，生成一系列风格相似的新的图片。 其参数大约 3.5B , 相对于上一代版本，DALL-E2 可以生成4倍分倍率的图片，且非常贴合语义信息。作者使用了人工评测方法，让志愿者看1000张图，71.7% 的人认为其更加匹配文本描述 ，88.8% 认为画的图相对于上一代版本更加好看。</p><figure><img src="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-0544da18e16e0b011f1d4d85d5b1cf07.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p><strong>DALL-E2 由三个模块组成</strong>（相当于两阶段生成，第一阶段用 prior 模型从文本生成图像特征；第二阶段用 decoder 生成图像）：</p><ul><li>CLIP模型，对齐图片文本表征</li><li>先验模型，接收文本信息，将其转换成 CLIP 图像表征</li><li>扩散模型，接受图像表征，解码来生成完整图像</li></ul><figure><img src="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-0fce8bc8d73882ce798bb7e4b18763df.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p><strong>DALL-E2 的训练过程</strong>为：</p><ul><li>训练一个 CLIP 模型，使其能够对齐文本和图片特征。</li><li>训练一个先验模型，由自回归模型或者一个扩散先验模型（实验证明，扩散先验模型表现更好。这里的扩散模型是用 sequence (text feature) 预测 sequence (image feature)，没有要求前后尺寸不变。作者这里没有用 U-net，而是用了 Transformer——将CLIP的文本编码、加入噪声的CLIP的图像编码、扩散时间步的time embedding等输入，去预测未加噪声的CLIP图像编码。），其功能是将文本表征映射为图片表征。</li><li>训练一个扩散解码模型，其目标是根据图片表征，还原原始图片。</li></ul><p>在训练完成之后，推理过程就比较直接了，首先使用CLIP 文本编码器，获得文本编码，之后使用先验模型将文本编码映射为图片编码，最后使用扩散解码器用图片编码生成完整图片。注意这里扩散解码模型使用的是经过修改的 GLIDE 扩散模型，其生成的图像尺寸为 64×64，然后使用两个上采样扩散模型将其上采样至 256×256，以及 1024×1024。</p><p>DALL·E2 原论文中也提到了其许多不足，例如容易将物体和属性混淆，无法精确的将文本放置到图像中，以及社会伦理道德等方面的问题。</p><p>谷歌的 Imagen 模型是继 DALL·E2 的后续工作，它没有用两阶段的生成，直接用了一个 U-net 搞定，更简单，效果也好。另外谷歌2022年6月份最新的基于 GAN 的 <a href="https://sites.research.google/parti/">Pathways Autoregressive Text-to-Image Model</a>（Parti）模型，用 200亿参数的 Pathways 模型做自回归的图像生成，效果直接超越了 DALL·E 2 和 Imagen。</p><h3 id="imagen">Imagen</h3><p>在 DALL-E2 提出没多久，Google 就提出了一个新的文本生成图像模型 Imagen，论文中提到，<strong>其生成的图片相对于 DALL-E2 真实感和语言理解能力都更加强大</strong>（使用一种新的评测方法 DrawBench）。</p><figure><img src="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-67fa2ff5f289e376304e905a8c87eff7.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>Imagen 的模型结构与 DALL·E2 十分类似，首先将文本进行编码表征，然后使用扩散模型将表征解码为完整图像，最后使用两个 text-conditional super-resolution 的扩散模型将其上采样至 256×256，以及 1024×1024。不同的是，<strong>Imagen 使用了 T5-XXL 模型直接编码文本信息，然后使用条件扩散模型，直接用文本编码生成图像。因此，在 Imagen 中，无需学习先验模型。</strong></p><figure><img src="https://pic2.zhimg.com/80/v2-be42b33c6d55d5fece5b8ae9866e42a9_720w.webp" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>由于直接使用 T5-XXL 模型，其语义知识相对于 CLIP 要丰富很多（图文匹配数据集数量要远远少于纯文本数据集数量），因此 Imagen 相对于 DALL-E2 在语义保真度上做的更好。同时，作者也发现，增大语言模型，可以有效的提高样本的语义保真度。</p><h3 id="stable-diffusion">Stable Diffusion</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/593896912">李沐论文精读系列——由DALL·E 2看图像生成模型 - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/560645985">图像生成：VQGAN，CLIP，DALLE，DIFFUSION - 知乎 (zhihu.com)</a></p><p><a href="https://swarma.org/?p=37227">AI艺术的背后：详解文本生成图像模型 | 集智俱乐部 (swarma.org)</a></p><p><a href="https://zhuanlan.zhihu.com/p/640545463">一文读懂Stable Diffusion 论文原理+代码超详细解读 - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/imwaters/article/details/127269368">【论文简介】Stable Diffusion的基础论文:2112.High-Resolution Image Synthesis with Latent Diffusion Models_stable diffusion论文-CSDN博客</a></p><p><a href="https://blog.csdn.net/sinat_31711799/article/details/130874306">【AI绘图】一、stable diffusion的发展史_stable diffusion研究现状-CSDN博客</a></p></blockquote><p>DALL·E3</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/660733999#:~:text=DALL-E%203%20将于%2010,月初向%20ChatGPT%20Plus%20和企业客户推出。">OpenAI发布DALL-E 3 | 原理简介 - 知乎 (zhihu.com)</a></p></blockquote><h3 id="sora">Sora</h3><p>Sora 是由 OpenAI 于 2024.2.15 发布的文生视频大模型。</p><h2 id="各生成模型对比">各生成模型对比</h2><p>翻译自 Lil' Log:</p><p>GAN、VAE 和流动模型等生成模型在生成高质量 samples 方面取得了巨大成功，但它们都有其自身的局限性。GAN模型因对抗性训练性质而具有潜在的不稳定训练和较少的生成多样性（需要精心选择的超参数和正则化器）。VAE依赖于替代损失 surrogate loss 。流动模型必须使用专门的架构来构建可逆转换。</p><p>扩散模型的灵感来自非平衡热力学。他们定义了一个扩散步骤的马尔可夫链，以缓慢地将随机噪声添加到数据中，然后学习反向扩散过程以从噪声中构建所需的数据样本。与VAE或流动模型不同，扩散模型是通过固定程序学习的，并且潜在变量 latent code（z）具有高维数（与原图同尺寸大小）。</p><figure><img src="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png" alt="Edge Image Viewer (lilianweng.github.io)" /><figcaption aria-hidden="true">Edge Image Viewer (lilianweng.github.io)</figcaption></figure><blockquote><p>各种生成模型的对比图，来自<a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models? | Lil'Log (lilianweng.github.io)</a></p></blockquote><h1 id="ddpm">DDPM</h1><details><summary>参考链接</summary><p><a href="https://zhuanlan.zhihu.com/p/525106459">由浅入深了解Diffusion Model - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/576475987">扩散模型 (Diffusion Model) 简要介绍与源码分析 - 知乎 (zhihu.com)</a></p><p><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models? | Lil'Log (lilianweng.github.io)</a></p><p><a href="https://zhuanlan.zhihu.com/p/601641045">Diffusion Model学习笔记(1)——DDPM - 知乎 (zhihu.com)</a></p></details><p>本文主要基于 <strong>DDPM：Denoising Diffusion Probabilistic Models</strong><sup>[1]</sup> [Ho et al. 2020] 展开 Diffusion 模型的原理推导。</p><h1 id="forward-与-backward-过程">forward 与 backward 过程</h1><p>如下，存在一系列高斯噪声（ <span class="math inline">\(T\)</span> 轮），将输入图片 <span class="math inline">\(x_0\)</span> 变为纯高斯噪声 <span class="math inline">\(x_T\)</span> 。而我们的模型则负责将 <span class="math inline">\(x_T\)</span> 复原回图片 <span class="math inline">\(x_0\)</span> 。这样一来其实 diffusion model 和 GAN 很像，都是给定噪声 <span class="math inline">\(x_t\)</span> 生成图片 <span class="math inline">\(x_0\)</span> ，但是要强调的是，这里噪声 <span class="math inline">\(x_T\)</span> 与图片 <span class="math inline">\(x_0\)</span> 是<strong>同维度</strong>的。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240315_145856_0.jpg" alt="20240315_145856_0" /><figcaption aria-hidden="true">20240315_145856_0</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240315_145951_1.jpg" alt="20240315_145951_1" /><figcaption aria-hidden="true">20240315_145951_1</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/diff222.jpg" alt="20240315_150015_2" /><figcaption aria-hidden="true">20240315_150015_2</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240315_150043_3.jpg" alt="20240315_150043_3" /><figcaption aria-hidden="true">20240315_150043_3</figcaption></figure><h1 id="模型训练">模型训练</h1><h2 id="预备知识">预备知识</h2><blockquote><p>关于变分下界与KL散度参考： <a href="https://bluefisher.github.io/2020/02/06/理解-Variational-Lower-Bound/">理解 Variational Lower Bound | Fisher's Blog (bluefisher.github.io)</a></p></blockquote><h3 id="名词概念">名词概念</h3><h4 id="重参数技巧">重参数技巧</h4><p>重参数（reparameterization trick）通过将随机性保存在一个独立的随机变量 <span class="math inline">\(\epsilon\)</span> 中，解决了从某个分布中随机采样导致的无法反传梯度问题。在 diffusion 模型中，我们有很多通过高斯噪声采样得到 <span class="math inline">\(x_t\)</span> 的过程，这些过程都用到了重参数技巧使其可微。例如，从高斯分布 <span class="math inline">\(z \sim \mathcal{N}(z;\mu_{\theta},\sigma^2_{\theta} \bold{I})\)</span> 中采样 <span class="math inline">\(z\)</span>, 可以写成： <span class="math display">\[z = \mu_{\theta} + \sigma_{\theta}\odot \epsilon, \ \epsilon\sim\mathcal{N}(0,\bold{I})\]</span> 通过重参数，采样得到的 <span class="math inline">\(z\)</span> 仍然具有随机性，其随机性保存在 <span class="math inline">\(\epsilon\)</span> 中。且 <span class="math inline">\(z\)</span> 满足均值为 <span class="math inline">\(\mu_{\theta}\)</span>、方差为 <span class="math inline">\(\sigma^2_{\theta}\)</span> 的高斯分布，这里的 <span class="math inline">\(\mu_{\theta}、\sigma^2_{\theta}\)</span> 可以是由参数 <span class="math inline">\(\theta\)</span> 的神经网络推断得到，整个”采样”过程依然梯度可导。</p><h4 id="后验分布">后验分布</h4><p>在贝叶斯推断中，我们通常需要计算后验分布 <span class="math inline">\(p(z| x)\)</span>，即在给定观测数据 <span class="math inline">\(x\)</span> 的情况下推断潜在变量 <span class="math inline">\(z\)</span> 的分布。<strong>贝叶斯定理中的后验分布公式为：</strong> <span class="math display">\[\colorbox{RedOrange}{p(z | x)} = \frac{\colorbox{Apricot}{p(x| z)}\colorbox{pink}{p(z)}}{\colorbox{SpringGreen}{p(x)}}\]</span> 其中，</p><p><font color=#ef042a><strong>后验分布</strong><span class="math inline">\(p(z | x)\)</span></font>：在给定观测数据 <span class="math inline">\(x\)</span> 时，潜在变量 <span class="math inline">\(z\)</span> 的分布。</p><p><font color=#df8400><strong>似然函数</strong> <span class="math inline">\(p(x| z)\)</span></font>：在给定潜在变量 <span class="math inline">\(z\)</span> 时，数据样本 <span class="math inline">\(x\)</span> 出现的概率。</p><p><font color=#985fff><strong>先验分布</strong> <span class="math inline">\(p(z)\)</span></font>：在没有观测数据时，我们对潜在变量的先验知识。</p><p><font color=#4eb434><strong>证据</strong> <span class="math inline">\(p(x)\)</span></font>：Evidence，也称<strong>归一化常数</strong>（<span style="background:#eef0f4;">证据 <span class="math inline">\(p(x)\)</span> 之所以被称为“归一化常数”，是因为它能确保后验分布 <span class="math inline">\(p(z | x)\)</span> 是一个有效的概率分布，使得其积分为 1 </span>），表示观测到数据 <span class="math inline">\(x\)</span> 的总概率。它是通过对所有可能的潜在变量 <span class="math inline">\(z\)</span> 进行积分得到的，即： <span class="math display">\[\colorbox{SpringGreen}{p(x)} = \int \colorbox{Apricot}{p(x| z)} \colorbox{pink}{p(z)}dz\]</span> 因为证据 <span class="math inline">\(p(x)\)</span> 涉及对所有潜在变量 <span class="math inline">\(z\)</span> 进行积分，且这个积分往往没有解析解（即无法通过常规的数学计算直接得到）。尤其是在<strong>复杂的高维模型</strong>中，积分操作会变得非常复杂和难以计算。在<strong>复杂的非线性模型</strong>中，似然函数 <span class="math inline">\(p(x|z)\)</span> 本身也可能非常复杂，使得积分更加难以求解。</p><p>因此在实际的贝叶斯推断中，我们往往使用近似推断方法（如变分推断、马尔可夫链蒙特卡洛方法）来避开直接计算证据的难题。</p><h4 id="什么是变分">什么是“变分”</h4><blockquote><p>详细内容可点击<a href="https://www.bilibili.com/video/BV1TR4y1c7ns/?spm_id_from=333.337.search-card.all.click&amp;vd_source=124ec79ebd3e16b0f454a3994a468f98">这里</a></p></blockquote><p><strong>变分（Variational）</strong>在数学和物理中指的是研究函数变化如何影响其关联量（如积分、函数值等）的过程。变分的概念源于<strong>变分法（Calculus of Variations）</strong>，变分法是一种用于研究如何在某些约束下优化函数或泛函的数学技术。<span style="background:#daf5e9;"><strong>注意！与传统优化问题不同的是，变分法优化函数本身，而不是对单个变量进行优化。</strong></span>其主要原理如下：</p><p><strong>1.构造泛函</strong>：在变分法中，我们先定义一个与问题相关的<strong>泛函（Functional）</strong><span class="math inline">\(J[y]\)</span>，它是一个<u>将函数映射到实数</u>的对象。该泛函常可表示为一个积分： <span class="math display">\[J[y] = \int^b_a F(x,y(x),y&#39;(x)) \ dx\]</span> 问题目标是找到一个函数 <span class="math inline">\(y(x)\)</span> ，使该泛函极大化或极小化。</p><p><strong>2.变分 / 引入微小变化</strong>：<strong>变分</strong>表示我们通过对函数 <span class="math inline">\(y(x)\)</span> 进行微小的变化，得到 <span class="math inline">\(Y(x)=y(x) + \epsilon \eta (x)\)</span>。其中，<span class="math inline">\(\epsilon\)</span> 是一个非常小的常数，<span class="math inline">\(\eta(x)\)</span>​ 是一个<strong>任意</strong>扰动函数（有时也把 <span class="math inline">\(\eta(x)\)</span> 称为“变分”）。</p><p><u>值得注意的是，一般要求 <span class="math inline">\(\eta(a) = \eta(b) = 0\)</span></u> ，<u>即要求积分曲线的初始位置和终止位置不能上下移动（定端变分 / 固定边界的变分）</u>。否则则为自由变分 / 自由边界变分。</p><p>我们有： <span class="math display">\[Y&#39; = y&#39; + \epsilon \eta&#39;, \ \frac{dY}{d\epsilon}= \eta(x), \ \frac{dY&#39;}{d\epsilon} = \eta&#39;(x)\]</span> <strong>3.计算泛函变化</strong>：将变化后的函数 <span class="math inline">\(Y(x)\)</span> 代入泛函，得到新的泛函值： <span class="math display">\[\colorbox{Apricot}{J[Y]}=\int^b_a F(x,Y(x),Y&#39;(x)) \ dx=J[y+\epsilon \eta]\\J[y + \epsilon \eta] = \int^b_a F(x,\ y(x)+\epsilon \eta(x),\ y&#39;(x)+\epsilon \eta &#39;(x)) \ dx = \colorbox{Apricot}{J(epsilon)}\]</span> 此时，若假设 <span class="math inline">\(y(x),\eta(x)\)</span>均取定（注意这是对 <span class="math inline">\(x\)</span> 的积分，因此 <span class="math inline">\(x\)</span> 无需处理），泛函的值随着 <span class="math inline">\(\epsilon\)</span> 的变化而变化，即此时<span style="background:#FFCC99;"> <span class="math inline">\(J\)</span> 可视为 <span class="math inline">\(\epsilon\)</span> 的函数</span>（上面橙色块）。</p><hr /><p>4.<strong>极小化条件</strong></p><p>特别地，当 <span class="math inline">\(\epsilon=0\)</span>，有 <span class="math inline">\(Y=y\)</span> ,且： <span class="math display">\[J&#39;_{\epsilon}(\epsilon)|_{\epsilon =0}=0 \tag{*}\]</span> 导数为0的原因是此时 <span class="math inline">\(J\)</span> 的取值与 <span class="math inline">\(\epsilon\)</span> 无关。由导数为0，知在点 <span class="math inline">\(\epsilon = 0\)</span> 处泛函 <span class="math inline">\(J[\epsilon]\)</span> 的变化量为0，故点 <span class="math inline">\(\epsilon = 0\)</span> 是极值点（类似导数为0）。那么我们只需利用极值情况下的（*）式得到一个不含 <span class="math inline">\(\epsilon 、\eta(x)\)</span>的方程，满足该方程的 <span class="math inline">\(y(x)\)</span> 即可使得泛函最大化或最小化。求解 <span class="math inline">\(J&#39;_{\epsilon}(\epsilon)\)</span> 的公式如下：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/f811426a956e419245f70d9b74dcc99.jpg" alt="f811426a956e419245f70d9b74dcc99" /><figcaption aria-hidden="true">f811426a956e419245f70d9b74dcc99</figcaption></figure><p>将 <span class="math inline">\(\epsilon=0\)</span> 和（*）代入，有 :</p><p><span class="math display">\[J&#39;(\epsilon)|_{\epsilon=0} = \int_a^b \eta(x) \cdot \left[\frac{dF}{dy}-\frac{d}{dx} [\frac{dF}{dy&#39;}]\right]dx = 0 \tag{**}\]</span></p><p>由于变分 <span class="math inline">\(\eta(x)\)</span> 可以取定为任意的扰动函数，且无论它如何变化，(**) 式始终都为0，因此： <span class="math display">\[\frac{dF}{dy}-\frac{d}{dx} [\frac{dF}{dy&#39;}] = 0 \tag{***}\]</span> 最终得到的(***) 式即为<strong>欧拉-拉格朗日方程</strong>，这是变分法中的核心方程。我们找到满足该方程的 <span class="math inline">\(y(x)\)</span> 即可。</p><hr /><blockquote><p>注：欧拉-拉格朗日方程的正确写法应该为： <span class="math display">\[\frac{\partial F}{\partial y} - \frac{d}{dx}\left(\frac{\partial F}{\partial y&#39;} \right) = 0\]</span> 上面的偏分和微分符号写错了，对 <span class="math inline">\(F\)</span> 的偏导符号都应该写成“<span class="math inline">\(\partial\)</span>”。<sub>不过这个也问题不大啦，懒得改了</sub> ~</p></blockquote><h4 id="变分推断">变分推断</h4><p>变分推断（Variational Inference, VI）是一种用于复杂概率模型（如贝叶斯网络、变分自编码器）中近似推断的技术，尤其在后验分布 <span class="math inline">\(p(z| x)\)</span> 难以直接计算时，VI 提供了一种高效的近似方法。它通过将概率推断问题转化为一个优化问题: <span style="background:#daf5e9;">找到一个易于处理的<strong>变分分布</strong> <span class="math inline">\(q(z)\)</span> （如高斯分布、拉普拉斯分布）来逼近复杂的后验分布 <span class="math inline">\(p(z| x)\)</span> </span>。</p><p>这个“逼近”可以通过最小化两者之间的差异来实现，通常使用<strong>Kullback-Leibler 散度</strong>（KL 散度）作为损失函数来度量差异： <span class="math display">\[KL(q(z)||p(z| x)) = \int q(z) \log \frac{q(z)}{p(z| x)} dz\]</span> KL 散度越小，表示 <span class="math inline">\(q(z)\)</span> 越接近 <span class="math inline">\(p(z|x)\)</span> 。因此我们的目标是通过优化方法最小化 KL 散度，从而找到最优的变分分布 <span class="math inline">\(q(z)\)</span>。<strong>因为直接计算KL散度时，归一化常数（即证据 <span class="math inline">\(p(z| x)\)</span>）通常难以计算，因此我们引入一个可以优化的变分下界 （Variational Lower Bound，VLB）/ 证据下界（Evidence Lower Bound, ELBO）：</strong> <span class="math display">\[\text{ELBO} = \mathbb{E}_{q(z)}[\log p(x,z)] - \mathbb{E}_{q(z)}[\log q(z)]\]</span> <strong>通过最大化证据下界来间接最小化 KL 散度</strong>，最大化 ELBO 等价于最小化 KL 散度 （证明见下）。那么最后只需通过梯度下降或其他优化方法来最大化 ELBO，即可找到最优的 <span class="math inline">\(q(z)\)</span>​​ 。</p><blockquote><p><strong>变分推断的变分体现在哪里?</strong> <sub>AI的解释，没看懂</sub>~</p><p>1.求解优化问题的过程是通过最小化 <span class="math inline">\(q(z)\)</span> 和 <span class="math inline">\(p(z∣x)\)</span>​ 之间的 KL 散度，或等价地最大化证据下界（ELBO）。这一过程本质上是一个<strong>基于函数的优化问题</strong>，与传统的变分法非常相似。</p><p>2.变分推断的优化过程可以看作是应用了变分法的思想：我们引入一个变分分布（即函数的变化），并通过调整这个分布的参数来最小化某种目标函数（这里是 KL 散度）。这种通过优化近似分布的方式，体现了变分法中对函数变化的处理。</p></blockquote><p>变分推断有很多应用，如应用于拥有复杂后验分布的大型贝叶斯网络、无向图模型中的复杂概率分布（马尔科夫随机场）和变分自编码器（Variational Autoencoder, VAE）等。</p><h4 id="变分贝叶斯">变分贝叶斯</h4><p><strong>变分贝叶斯</strong>（Variational Bayesian, VB）是<strong>变分推断</strong>在贝叶斯推断中的一种具体应用。VB 非常有用的一个特性是推断优化的二元性：<u>我们可以将统计推断问题（从一个随机变量的值推断出另一种随机变量的值）作为优化问题（找到参变量的值来最小化某些目标函数）</u>。另外，<strong>变分下界 variational lower bound</strong> ，也被称作 <strong>evidence lower bound</strong> (ELBO)，在 VB 的推导中起了非常重要的作用。</p><h3 id="推导变分下界-l">推导变分下界 <span class="math inline">\(L\)</span></h3><p><strong>变分下界（variational lower bound 或 evidence lower bound, ELBO）</strong>是一种用于估计概率分布参数的方法。它通过最大化似然函数的下界来逼近真实的后验概率分布。这种方法可以用于在无法直接计算后验概率分布的情况下，近似地推断参数的取值。</p><p>已知变分贝叶斯（VB）将统计推断问题，即从一种随机变量推断出另一种随机变量的值，作为优化问题。</p><p>这里，将问题设置成从观测值（observed variable） <span class="math inline">\(X\)</span> 推断隐变量（hidden/latent variable） <span class="math inline">\(Z\)</span>​ ,由于两者之间存在关系：</p><blockquote><p>隐变量 <span class="math inline">\(Z\)</span> 可能包含参数 <span class="math inline">\(\theta\)</span> 。</p></blockquote><figure><img src="https://bluefisher.github.io/images/2020-02-06-%E7%90%86%E8%A7%A3-Variational-Lower-Bound/image-20200206195028885.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>因此，<span class="math inline">\(X \rightarrow Z\)</span> 可视为求隐变量的后验概率： <span class="math display">\[P\left( Z|X \right) =\frac{P\left( X|Z \right) P\left( Z \right)}{P\left( X \right)}=\frac{P\left( X|Z \right) P\left( Z \right)}{\int_Z{p\left( X,Z \right)}}\]</span> 其中，大写的 <span class="math inline">\(P(X)\)</span> 表示某个变量的概率分布，小写的 <span class="math inline">\(p(X)\)</span> 表示 <span class="math inline">\(X\)</span> 分布的概率密度函数。</p><p>从观测值 <span class="math inline">\(X\)</span> 的边缘分布出发，设 <span class="math inline">\(q(Z)\)</span> 为VB中后验概率 <span class="math inline">\(p(Z|X)\)</span> 的估计概率，则： <span class="math display">\[\begin{align}\log P(X) &amp;=\log \int_{Z} p(X, Z) \tag{1}\\&amp;=\log \int_{Z} p(X, Z) \frac{q(Z)}{q(Z)} \tag{2}\\&amp;=\log \left(\mathbb{E}_{q}\left[\frac{p(X, Z)}{q(Z)}\right]\right) \tag{3}\\&amp; \geq \mathbb{E}_{q}\left[\log \frac{p(X, Z)}{q(Z)}\right] \tag{4}\\&amp;=\mathbb{E}_{q}[\log p(X, Z)]+H[Z] \tag{5}\\&amp;=L\ \ --\ variational\ lower\ bound\end{align}\]</span> 从公式（2）到公式（3）运用到期望函数的定义： <span class="math inline">\(E\left( f\left( x \right) \right) =\int{xf\left( x \right) dx}\)</span> 。公式（4）对凸函数 log 运用了<strong>琴生不等式</strong>： <span class="math inline">\(f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)]\)</span> 。公式（5）中 <span class="math inline">\(H[Z]=-\mathbb{E}_{q}[\log q(Z)]\)</span> 是香农熵。</p><p>我们做标记： <span class="math display">\[L=\mathbb{E}_{q}[\log p(X, Z)]+H[Z]\]</span> 很明显 <span class="math inline">\(L\)</span> 就是观测变量的 log 概率的一个 lower bound。<strong>也就是说，如果我们想要去最大化边缘分布，我们可以转而最大化它的 variational lower bound <span class="math inline">\(L\)</span></strong> 。</p><h3 id="推导-相对熵-kl-散度">推导 相对熵 / KL 散度</h3><p>在很多情况下，后验概率 <span class="math inline">\(P(Z|X)\)</span> 的计算是十分困难的，比如我们可能需要对所有的隐变量做积分（求和）来计算分母。</p><p>变分方法的主要思想就是找一个估计的概率分布 <span class="math inline">\(q(Z)\)</span> 来尽可能地接近后验概率 <span class="math inline">\(p(Z|X)\)</span> 。这些估计的概率分布可以有它们独有的<em>变分参数（variational parameters）</em>：<span class="math inline">\(q(Z|\theta)\)</span> ，所以我们想要去寻找这些参数来使 <span class="math inline">\(q(Z)\)</span> 尽可能接近后验概率。当然 <span class="math inline">\(q(Z)\)</span> 的分布肯定要在推断中相对来说更加简单好求一些。</p><p>为了衡量两个概率分布 <span class="math inline">\(q(Z)\)</span> 和 <span class="math inline">\(p(Z|X)\)</span> 的相似程度，一个常用的标准就是就是 <strong>Kullback-Leibler (KL) 散度</strong>。其计算如下：</p><p><span class="math display">\[\begin{align} K L[q(Z) \| p(Z | X)] &amp;=\int_{Z} q(Z) \log \frac{q(Z)}{p(Z | X)} \tag{6}\\ &amp;=-\int_{Z} q(Z) \log \frac{p(Z | X)}{q(Z)} \tag{7}\\ &amp;=-\left(\int_{Z} q(Z) \log \frac{p(X, Z)}{q(Z)}-\int_{Z} q(Z) \log p(X)\right) \tag{8}\\ &amp;=-\int_{Z} q(Z) \log \frac{p(X, Z)}{q(Z)}+\log p(X) \int_{Z} q(Z) \tag{9}\\ &amp;=-L+\log p(X) \tag{10}\end{align}\]</span></p><p>这里 <span class="math inline">\(L\)</span> 是变分下界 variational lower bound 。公式 (10) 是归一化常量 $ _{Z} q(Z) =1$ 而推导得来。整理可得： <span class="math display">\[L=\log p(X)-K L[q(Z) \| p(Z | X)]\]</span> 因为 KL 散度永远是 ≥0 的，所以，再一次，我们得到了 <span class="math inline">\(L \le log\ p(X)\)</span> 是观测变量的分布的一个 lower bound。同时我们也知道了它们之间的区别就在于估计分布和真实分布之间的 KL 散度。换句话说，如果估计分布与真实后验分布完美接近，那么 lower bound <span class="math inline">\(L\)</span>​ 就等于 log 概率。</p><h3 id="推导交叉熵">推导交叉熵</h3><blockquote><p>参考自：<a href="https://blog.csdn.net/tsyccnh/article/details/79163834">一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉-CSDN博客</a></p><p>简洁版介绍看：<a href="https://zhuanlan.zhihu.com/p/115277553">损失函数：交叉熵详解 - 知乎 (zhihu.com)</a></p></blockquote><p>在离散情况下 <span class="math inline">\(P(x)、Q(x)\)</span> 两种概率分布的相对熵计算公式： <span class="math display">\[D_{KL}\left( p||q \right) =\sum_{i=1}^n{p\left( x_i \right) \log \frac{p\left( x_i \right)}{q\left( x_i \right)}}\]</span> 对该式变形可得： <span class="math display">\[\begin{aligned}    D_{KL}\left( p||q \right) &amp;=\sum_{i=1}^n{p}\left( x_i \right) \log \left( p\left( x_i \right) \right) -\sum_{i=1}^n{p}\left( x_i \right) \log \left( q\left( x_i \right) \right)\\    &amp;=-H\left( p\left( x \right) \right) +\left[ -\sum_{i=1}^n{p}\left( x_i \right) \log \left( q\left( x_i \right) \right) \right]\\    &amp;=-H\left( p\left( x \right) \right) +E_p\left[ -\log \left( q\left( x_i \right) \right) \right]\\    &amp;=-H\left( p\left( x \right) \right) +H\left( p,q \right)\\\end{aligned}\]</span> 即 KL 散度等式的前一部分恰巧就是 <span class="math inline">\(p\)</span> 的熵，后一部分正是交叉熵 <span class="math inline">\(H(p,q)\)</span> （<u>交叉熵 <span class="math inline">\(H(p,q)\)</span> 可理解为</u>）： <span class="math display">\[H\left( p,q \right) =E_p\left[ -\log \left( q\left( x_i \right) \right) \right] =-\sum_{i=1}^n{p}\left( x_i \right) \log \left( q\left( x_i \right) \right)\]</span> 在机器学习中，如果 <span class="math inline">\(P(x)\)</span> 用来表示样本的真实分布，而 <span class="math inline">\(Q(x)\)</span> 用来表示模型所预测的分布。如果我们需要评估 label 和 predicts 之间的差距，可以使用 KL 散度，即 $D_{KL}( p||q ) $ 。<strong>由于 KL 散度中的前一部分 <span class="math inline">\(-H(p)\)</span> 只与真值有关，是不变的，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用交叉熵做 loss，评估模型</strong>。</p><h2 id="损失函数">损失函数</h2><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240320_194222_0.jpg" alt="20240320_194222_0" /><figcaption aria-hidden="true">20240320_194222_0</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/diff22111.jpg" alt="20240320_194348_1" /><figcaption aria-hidden="true">20240320_194348_1</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240320_194446_2.jpg" alt="20240320_194446_2" /><figcaption aria-hidden="true">20240320_194446_2</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/diff22333.jpg" alt="20240320_194530_3" /><figcaption aria-hidden="true">20240320_194530_3</figcaption></figure><p><strong>注3</strong>： 上述&lt;8&gt;式去掉权重系数是因为 DDPM 发现这样能使得 diffusion 模型工作得更好。</p><p>最后的简化形式： <span class="math display">\[L_{simple}=L^{simple}_t+C\]</span> 其中， <span class="math inline">\(C\)</span> 是与模型参数 <span class="math inline">\(\theta\)</span> 无关的常量。</p><h1 id="最终算法流程">最终算法流程</h1><p><span style="background:#dad5e9;"><strong>总结：</strong> DDPM（Denoising Diffusion Probabilistic Models）作为一种生成模型，它通过逐步添加噪声的方式将数据转化为噪声，然后通过学习逆过程来生成数据。</span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240320200738823.png" alt="image-20240320200738823" /><figcaption aria-hidden="true">image-20240320200738823</figcaption></figure><blockquote><p>The training and sampling algorithms in DDPM (Image source: <a href="https://arxiv.org/abs/2006.11239">Ho et al. 2020</a>)</p></blockquote><p><span style="background:#FFCC99;"><strong>训练阶段</strong>重复如下步骤:</span></p><ul><li><p>从数据集中采样 <span class="math inline">\(x_0\)</span></p></li><li><p>从 <span class="math inline">\(1...T\)</span> 随机采样 time step <span class="math inline">\(t\)</span> ：对该 batch 中的每个图像随机取 <span class="math inline">\(t\)</span> 进行训练</p></li><li><p>从标准高斯分布采样噪声 $_t( 0, ) $</p></li><li><p>调用模型预估 $_{}( _0+_t,t ) $：这里使用的模型主要是 U-net 模型</p></li><li><p>计算<strong>最小化噪声之间的 MSE Loss</strong>: $<em>t-</em>{}( _0+_t,t ) ^2 $, 并利用反向传播算法训练模型.</p></li></ul><blockquote><p>DDPM（Denoising Diffusion Probabilistic Models）是一种生成模型，它通过逐步添加噪声的方式将数据转化为噪声，然后通过学习逆过程来生成数据。DDPM的训练过程包括以下几个关键步骤： 1. 初始化：首先，从数据集中随机抽取一个样本 <span class="math inline">\(x_0\)</span>，并随机选择一个时间步 t。 2. 加噪：在选定的时间步 t，根据预先定义的噪声方差计划 <span class="math inline">\(\beta_t\)</span>，向样本 <span class="math inline">\(x_0\)</span> 添加噪声，生成带噪声的样本 <span class="math inline">\(x_t\)</span>。这一步模拟了前向扩散过程。 3. 模型预测：将带噪声的样本 <span class="math inline">\(x_t\)</span> 和时间步 t 输入到训练的神经网络（通常是U-Net结构）中，网络预测在时间步 t 添加的噪声$ _(x_t, t)$。 4. 计算损失：计算网络预测的噪声和实际添加的噪声之间的均方误差（MSE），作为损失函数。 5. 优化：通过反向传播和梯度下降等优化算法更新网络的参数，以最小化损失函数。 6. 重复：重复以上步骤，直到网络训练完成。 在训练过程中，DDPM使用了一个称为U-Net的神经网络结构，该结构包含多个残差块和注意力机制，以提高模型的学习能力和效率。U-Net的输入是带噪声的图像和时间步 t，输出是预测的噪声。通过训练，U-Net学习如何逐步去除噪声，最终恢复出清晰的图像。 此外，DDPM的训练还涉及到一个噪声调度器（DDPM_Scheduler），它根据时间步 t 来确定噪声的方差。这个调度器是固定的，不参与训练，它定义了从数据样本到纯噪声的整个扩散过程。 在实际的训练代码中，还会使用一些技术来提高模型的性能，例如指数移动平均（EMA）来平滑模型参数，以及在训练过程中保存检查点（checkpoints）以便恢复训练。 总的来说，DDPM的训练过程是通过模拟数据的扩散过程，并训练一个神经网络来逆转这个过程，从而学习生成与原始数据相似的新样本。这个过程涉及到复杂的数学推导和深度学习技术，但最终的目标是生成高质量、高逼真度的图像。</p></blockquote><p><strong>逆向阶段</strong>采用如下步骤进行采样:</p><ul><li>从高斯分布采样 <span class="math inline">\(x_T\)</span></li><li>按照 <span class="math inline">\(T,...,1\)</span> 的顺序进行迭代:<ul><li>如果 <span class="math inline">\(t=1\)</span> , 令 <span class="math inline">\(\mathbf{z}=0\)</span> ; 如果 <span class="math inline">\(t&gt;1\)</span> , 从高斯分布中采样 $( 0, ) $ <span style="background:#FFCC99;">（最后一步不加噪音）</span></li><li>利用式 &lt;6&gt; 学习出均值 $_{}( _t,t ) =( <em>t-</em>{}( _t,t ) ) $ , 并计算均方差 <span class="math inline">\(\sigma _t=\sqrt{\tilde{\beta}_t}=\sqrt{\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\cdot \beta _t}\)</span></li><li>通过重参数技巧采样 <span class="math inline">\(x_{t-1}=\mu _{\theta}\left( x_t,t \right) +\sigma _t\mathbf{z}\)</span></li></ul></li><li>经过以上过程的迭代, 最终恢复 <span class="math inline">\(x_0\)</span> .</li></ul><blockquote><p>DDPM（Denoising Diffusion Probabilistic Models）的采样过程是一个逐步的去噪过程，它是训练过程的逆过程。以下是DDPM采样过程的详细步骤： 1. 初始化：从一个标准正态分布开始，采样一个纯噪声图像 <span class="math inline">\(x_T\)</span>。 2. 逆扩散过程：这个过程涉及多个时间步，通常从 T 到 1 递减。在每一步 t，执行以下操作： 使用预测模型（通常是训练好的U-Net）预测在时间步 t 的噪声 <span class="math inline">\(\epsilon_{\theta}(x_t, t)\)</span>​。 根据预测的噪声和当前噪声图像 x_t，计算下一个时间步 t-1 的图像 <span class="math inline">\(x_{t-1}\)</span>。这通常通过以下公式完成：_ <span class="math display">\[    x_{t-1} = f_\theta(x_t) + \sqrt{\alpha_t} \cdot \epsilon  \]</span></p><p>​ 其中 <span class="math inline">\(\alpha_t\)</span> 是预先定义的方差调度计划中的一个因子，<span class="math inline">\(\epsilon\)</span> 是从标准正态分布中采样的噪声。 3. 迭代：重复步骤2，直到时间步 t 减少到1。 4. 输出：最终的 <span class="math inline">\(x_0\)</span> 是生成的图像，它经过了从纯噪声到数据分布的逆扩散过程。 在采样过程中，DDPM模型的网络部分 <span class="math inline">\(f_\theta\)</span> 被用来预测在每一步添加到图像中的噪声。这个预测是基于当前的噪声图像和时间步的信息。通过逐步减少噪声，模型最终能够生成一幅与训练数据相似的图像。 这个过程可以被看作是一个确定性的、逐步的逆向过程，它能够生成高质量的图像样本。然而，由于每一步都需要模型的一次前向传播，因此整个采样过程可能需要大量的计算步骤，特别是当时间步 T 很大时。为了提高效率，一些改进的模型如 DDIM（Denoising Diffusion Implicit Models）被提出，它们通过减少所需的采样步骤来加速这个过程 。</p></blockquote><h1 id="参考文献">参考文献</h1><p>[1] <a href="https://arxiv.org/pdf/2006.11239.pdf">Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.</a></p><p>[2] <a href="https://arxiv.org/abs/1312.6114">Kingma D P, Welling M. Auto-encoding variational bayes[J]. arxiv preprint arxiv:1312.6114, 2013.</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/684456268">Van Den Oord A, Vinyals O. Neural discrete representation learning[J]. Advances in neural information processing systems, 2017, 30.</a></p><p>[4] <a href="https://arxiv.org/pdf/2102.12092.pdf">Ramesh A, Pavlov M, Goh G, et al. Zero-shot text-to-image generation[C]International conference on machine learning. Pmlr, 2021: 8821-8831.</a></p><p>[5] <a href="https://arxiv.org/pdf/1406.2661.pdf">Creswell A, White T, Dumoulin V, et al. Generative adversarial networks: An overview[J]. IEEE signal processing magazine, 2018, 35(1): 53-65.</a></p><p>[6] <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html">Esser P, Rombach R, Ommer B. Taming transformers for high-resolution image synthesis[C]Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 12873-12883.</a></p><p>[7] <a href="https://arxiv.org/abs/2204.08583">Crowson K, Biderman S, Kornis D, et al. Vqgan-clip: Open domain image generation and editing with natural language guidance[C]European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022: 88-105.</a></p><p>[8] <a href="https://arxiv.org/pdf/2112.10741.pdf">Nichol A, Dhariwal P, Ramesh A, et al. Glide: Towards photorealistic image generation and editing with text-guided diffusion models[J]. arixv preprint arixv:2112.10741, 2021.</a></p><p>[9] <a href="https://arxiv.org/pdf/2204.06125.pdf">Ramesh A, Dhariwal P, Nichol A, et al. Hierarchical text-conditional image generation with clip latents[J]. arixv preprint arixv:2204.06125, 2022, 1(2): 3.</a></p><p>[10] <a href="https://arxiv.org/pdf/2205.11487.pdf">Saharia C, Chan W, Saxena S, et al. Photorealistic text-to-image diffusion models with deep language understanding[J]. Advances in neural information processing systems, 2022, 35: 36479-36494.</a></p><p>[11] <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf">Rombach R, Blattmann A, Lorenz D, et al. High-resolution image synthesis with latent diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 10684-10695.</a></p><p>[12] <a href="https://arxiv.org/abs/1503.03585">Sohl-Dickstein J, Weiss E, Maheswaranathan N, et al. Deep unsupervised learning using nonequilibrium thermodynamics[C]//International conference on machine learning. PMLR, 2015: 2256-2265.</a></p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>专业名词索引</title>
      <link href="/2024/09/21/%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D%E7%B4%A2%E5%BC%95/"/>
      <url>/2024/09/21/%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D%E7%B4%A2%E5%BC%95/</url>
      
        <content type="html"><![CDATA[<h1 id="瓶颈层-bottleneck-layer">瓶颈层 bottleneck layer</h1><p>瓶颈层是指网络模型中<strong>输出维度明显小于输入维度的层</strong>，由此它极大地限制了网络的性能与表达，造出瓶颈。瓶颈层常用于降低网络维度，并提取高维特征。</p><h1 id="相机模型">相机模型</h1><blockquote><p><a href="https://www.bilibili.com/video/BV1C84y1R7qE/?spm_id_from=333.337.search-card.all.click&amp;vd_source=124ec79ebd3e16b0f454a3994a468f98">多视图几何MVS简介及MVSNet如何高效入门？_哔哩哔哩_bilibili</a></p></blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240627105013793.png" alt="image-20240627105013793" /><figcaption aria-hidden="true">image-20240627105013793</figcaption></figure><p>物理世界坐标系（一般会化为极坐标系）——<font color=#0091ff>相机坐标系</font>——<font color=#4eb434>成像平面坐标系</font>——<font color=#df8400>像素坐标系</font></p><p><strong>焦距<span class="math inline">\(f\)</span>​</strong> （Focal Length）：从相机镜头到成像平面（图像传感器或胶片）的距离。它通常以毫米（mm）为单位表示。焦距越大，相机拍摄的图像视野就越窄，物体看起来越远。</p><p><strong>主点</strong>（Principal Point）： 主点是图像传感器平面上的光学中心点，通常以像素坐标表示。它表示图像传感器的中心位置。</p><p><strong>像素尺寸</strong>（Pixel Size）： 像素尺寸表示图像传感器上相邻像素之间的物理距离，通常以毫米为单位。</p><p><strong>图像畸变参数</strong>（Distortion Parameters）： 这些参数用于校正相机镜头引起的畸变，包括径向畸变和切向畸变。在 "Simple Radial" 模型中，通常会包含一个或多个径向畸变参数，以纠正图像中的弯曲效果。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240627105508668.png" alt="image-20240627105508668" /><figcaption aria-hidden="true">image-20240627105508668</figcaption></figure><p><span class="math inline">\(Z_c\)</span>: 可以理解为“深度”。在从三维的相机坐标系—&gt;二维的成像坐标系时，一般会去除深度使其变为二维的。</p><p><span class="math inline">\(f_x、f_y\)</span> ：表示水平和垂直方向上的像素的物理宽度。/ 水平和垂直方向上的焦距。/ 表示相机的焦距在图像坐标系中的不同方向上的缩放因子。它决定了图像中相同物体的像素之间的距离和物理世界中的实际距离之间的关系。</p><p><span class="math inline">\(c_x、c_y\)</span>: 表示像素二分之一的长和宽。其引入是为了解决成像平面原点（主点）与像素坐标原点位置不一致的问题。/ 主点在图像平面上的坐标。/ 表示图像平面中心在水平或垂直方向上的偏移量。它表示图像的中心点在 x 轴或 y 轴方向上的像素位置。</p><p><strong>总结变换如下：</strong></p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240627143733151.png" alt="image-20240627143733151" style="zoom:67%;" /></p><p>上面橘色的块就是“<strong>内参矩阵</strong>”，绿色块则是“<strong>外参矩阵</strong>”。内参矩阵中，最后一行通常保持不变，因为它是用于保持矩阵格式的。内参会在相机校准过程中获得，通过捕捉已知空间点在图像中的投影位置并使用数学方法进行计算。校准的结果将允许相机进行精确的三维-二维坐标转换，从而实现准确的图像捕捉和计算。</p><h1 id="光栅化-rasterization">光栅化 rasterization</h1><blockquote><p>参考自：</p><ul><li><p><a href="https://zhuanlan.zhihu.com/p/450540827">现代计算机图形学基础二：光栅化（Rasterization） - 知乎 (zhihu.com)</a></p></li><li><p><a href="https://www.bilibili.com/video/BV1cz421872F/?spm_id_from=333.788&amp;vd_source=124ec79ebd3e16b0f454a3994a468f98">【较真系列】讲人话-3d gaussian splatting全解(原理+代码+公式)【2】 抛雪球_哔哩哔哩_bilibili</a></p></li></ul></blockquote><p>(预备知识)相机模型：从世界坐标系——相机坐标系——归一化坐标系(NDC)——像素坐标系的变换。</p><p>Raster在德语中是屏幕的意思（Raster == screen in German），而<strong>光栅化</strong>就是<strong>把东西画在屏幕上</strong>的一个过程（Rasterize == drawing onto the screen ），也就是3D——&gt;2D。</p><p>屏幕是由一行行一列列像素组成的。我们先梳理下如何用相机拍摄物体，如苹果：</p><p><strong>第一步，摆放好苹果（model transformation）</strong>。</p><p><strong>第二步，找到一个好角度放相机（观测变换 view transformation）</strong>。即从不同的视角看物体有不同的形态，是从世界坐标系——&gt;相机坐标系的过程。该过程本质是仿射变换：<span class="math inline">\(w=Ax+b\)</span> 。一言以蔽之，“横看成岭侧成峰，远近高低各不同”。</p><p><strong>第三步，进行投影变换（projection transformation），将空间中的苹果转换到单位立方体中。</strong>这步是相机坐标系——&gt;归一化坐标系的过程，有两种选择，分别是正交投影（Orthographic projection）和透视投影（Perspective projection）。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240622154536395.png" alt="image-20240622154536395" style="zoom:67%;" /></p><ul><li><p><strong>正交投影</strong>与 <span class="math inline">\(z\)</span> 方向无关，有缩放、平移操作，是仿射变换，属于平行投影中光线垂直于视平面时的特例。例如对于立方体 <span class="math inline">\([l,r]×[b,t]×[f,n]\)</span>,我们先将其平移到原点（Translate），然后再将其缩放为 <span class="math inline">\([-1,1]^3\)</span>​​ 的单位立方体（Scale）。整个 <span class="math inline">\(M_{ortho}\)</span> 矩阵由缩放矩阵和平移矩阵相乘而得。<img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240627164009813.png" alt="image-20240627164009813" style="zoom:80%;" /></p></li><li><p><strong>透视投影</strong>与 <span class="math inline">\(z\)</span>​ 方向相关，是非仿射变换（非线性），更符合人眼的成像原理，有“远小近大”的特点。透视投影先将视锥体压成立方体，然后再正交投影。它通过矩阵 <span class="math inline">\(M_{persp \rightarrow ortho}\)</span>​ 进行锥体变方体的操作。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240627164728931.png" alt="image-20240627164728931" style="zoom:80%;" /></p></li></ul><p><strong>第四步，通过视口变换矩阵把单位立方体映射到屏幕空间（像素坐标系） ，得到屏幕空间中的很多三角形。</strong>这步是归一化坐标系——&gt;像素坐标系的过程。这里的视口变换主要是进行“拉伸”操作，将第三步中产生的形变拉回原样，即将 <span class="math inline">\([-1,1]^2\)</span> 的正方形变换回 <span class="math inline">\([0,w]×[0,h]\)</span> 。<u>注意这一步也是与 <span class="math inline">\(z\)</span> 方向无关的。</u></p><p><strong>第五步，进行光栅化，把三角形打碎成像素，并给每个像素赋值，然后屏幕上就可以显示出苹果了</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240622155320689.png" alt="image-20240622155320689" style="zoom: 50%;" /></p><p>这个完整的过程称为“<strong>光栅化</strong>”。其中第一到三步可以统称为<u>空间变换</u>。</p><p>第五步中，三角形打碎成像素其实就是判断三角形包围盒覆盖的范围中有哪些像素中心在包围盒内，然后给中心在盒内的像素均匀上色。事实上，这一步会导致“锯齿（Jaggies）”。<strong>有两个原因导致锯齿：一是像素本身有一定的大小，二是采样的速度更不上信号变化的速度（高频信号采样不足）。</strong> 我们将锯齿尽可能消除的方法就是<strong>光滑</strong>。</p><h1 id="数学用语">数学用语</h1><h2 id="kl散度与变分下界">KL散度与变分下界</h2><blockquote><p>出处：<a href="https://bluefisher.github.io/2020/02/06/理解-Variational-Lower-Bound/">理解 Variational Lower Bound | Fisher's Blog (bluefisher.github.io)</a></p></blockquote><h1 id="衡量指标">衡量指标</h1><h2 id="nvs-常用指标">NVS 常用指标</h2><h3 id="psnr-uparrow">PSNR <span class="math inline">\(\uparrow\)</span></h3><p><strong>峰值信噪比（Peak Signal-to-Noise Ratio，PSNR）</strong>是一种用于衡量图像或视频质量的指标。<strong>PSNR</strong> 是通过 MSE （均方误差，Mean Squared Error，详细解释见下）计算得出的，<strong>MSE</strong> 衡量的是原始图像与重建图像的差异。<strong>PSNR</strong> 表示的是信号（图像）中的噪声水平，数值越高，表示图像质量越好。</p><p>PSNR的计算公式如下： <span class="math display">\[\text{PSNR} = 10 \cdot \log_{10}(\frac{R^2}{\text{MSE}})\]</span> 其中，R表示像素值的最大可能值（比如对于 8-bit 图像，R=255; 对于 16-bit 图像，R=65535）。<span style="background:#daf5e9;">PSNR的值通常以分贝（dB）为单位，数值越高表示质量越好</span>。常见的PSNR阈值为30dB以上，30-40dB为较好的质量，40-50dB为很好的质量，50dB以上为极好的质量。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240903162626687.png" alt="image-20240903162626687" /><figcaption aria-hidden="true">image-20240903162626687</figcaption></figure><h3 id="ssim-uparrow">SSIM <span class="math inline">\(\uparrow\)</span></h3><p>结构相似性（Structural Similarity，简称SSIM）是一种用于衡量两幅图像相似程度的指标。与传统的均方误差（Mean Squared Error，MSE）相比，SSIM更能反映人眼对图像质量的感知。</p><p>SSIM指标结合了亮度、对比度和结构三个方面的信息，通过计算这些信息的相似性来评估图像的相似度。<span style="background:#daf5e9;">SSIM的取值范围在[0, 1]之间，值越接近1表示两幅图像越相似</span>。</p><p>SSIM的计算公式如下：</p><p><span class="math inline">\(SSIM(x, y)\)</span> = (2 * <span class="math inline">\(\mu_x\)</span> <em><span class="math inline">\(\mu_y\)</span> + <span class="math inline">\(c_1\)</span>) </em> (2 * <span class="math inline">\(\sigma_{xy}\)</span> + <span class="math inline">\(c_2\)</span>) / (<span class="math inline">\({\mu_x}^2\)</span> + <span class="math inline">\({\mu_y}^2\)</span> + c1) * (<span class="math inline">\({σ_x}^2\)</span> + <span class="math inline">\({σ_y}^2\)</span> + <span class="math inline">\(c_2\)</span>)</p><p>其中，x和y分别表示两幅图像，<span class="math inline">\(\mu\)</span> 表示图像的均值，<span class="math inline">\(\sigma\)</span> 表示图像的标准差，<span class="math inline">\(\sigma_{xy}\)</span> 表示图像的协方差，<span class="math inline">\(c_1\)</span> 和 <span class="math inline">\(c_2\)</span> 是常数，用于避免分母为0。</p><h3 id="lpips-downarrow">LPIPS <span class="math inline">\(\downarrow\)</span></h3><p>学习感知图像块相似度 （Learned Perceptual Image Patch Similarity，简称LPIPS）是一种用于计算图像质量和相似度的指标。它基于深度学习模型，通过学习人类感知图像质量的方式来评估图像之间的相似性。</p><p>LPIPS使用预训练的卷积神经网络（如VGG网络）来提取图像的特征表示，然后通过比较这些特征表示之间的差异来计算图像之间的相似度。与传统的像素差异度量方法（如均方误差）相比，LPIPS更能反映人类感知的差异。<span style="background:#daf5e9;">LPIPS 值越小越好</span>。</p><h2 id="其他指标">其他指标</h2><h3 id="avge-downarrow">AVGE <span class="math inline">\(\downarrow\)</span></h3><p>广义的 AVGE 公式如下： <span class="math display">\[\text{AVGE} = \frac{1}{N} \sum^N_{i=1}|y_i - \hat{y_i}|\]</span> <strong>新视图合成中</strong>，Average Error (AVGE)<span class="math inline">\(^{[1]}\)</span> 是 <span class="math inline">\(\text{MSE } = 10^{- \text{PSNR} /10}、 \sqrt{1- \text{SSIM}}、 \text{LPIPS}\)</span> 的几何平均。<strong>几何平均数</strong> 是将多个值相乘后再开方（对应的次方数是参与运算的值的个数）。它在这里被用来组合这三个指标，得到一个综合的误差度量，即 <strong>AVGE</strong>： <span class="math display">\[\text{AVGE}= \sqrt[3]{\text{MSE} \times \sqrt{1- \text{SSIM}} \times \text{LPIPS}}\]</span></p><h3 id="mse-downarrow">MSE <span class="math inline">\(\downarrow\)</span></h3><p>MSE，即均方误差（Mean Squared Error），表示原始图像与重建图像之间的平均误差，计算公式为： <span class="math display">\[\text{MSE}= \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\]</span></p><ul><li><span class="math inline">\(y_i\)</span> 和 <span class="math inline">\(\hat{y_i}\)</span> 分别表示原始图像和重建图像的第 <span class="math inline">\(i\)</span> 个像素值；</li><li><span class="math inline">\(n\)</span>​ 表示图像中像素的总数。</li></ul><p>MSE 是 PSNR 计算的一个关键部分，且 MSE 与 PSNR 成反比。MSE 越小，PSNR 越高，表示重建图像质量越好。</p><h3 id="rmse-downarrow">RMSE <span class="math inline">\(\downarrow\)</span></h3><p><strong>RMSE</strong> (<strong>Root Mean Squared Error</strong> ，“均方根误差”或“均方根偏差”) 是 MSE 的开方，其公式为： <span class="math display">\[\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}\]</span> 相比与 MSE ，RMSE 具有单位一致性，这使得它比 MSE 更容易解释。比如，如果数据表示温度（摄氏度），那么 RMSE 的单位也是摄氏度。</p><h3 id="fitness-uparrow">Fitness <span class="math inline">\(\uparrow\)</span>​</h3><p>在点云配准（例如 ICP 算法）中，fitness 通常表示源点云与目标点云之间匹配点的比例。一个高的 fitness 值表明两组点云对齐得很好，即大多数点在空间中紧密匹配。在典型的点云配准任务中，fitness 通常是根据以下公式计算的： <span class="math display">\[\text{Fitness}=\frac{N_{\text{inliers}}}{N_{\text{total}}}\]</span></p><ul><li><span class="math inline">\(N_{\text{inliers}}\)</span> : 配准后源点云中与目标点云中某点匹配的点的数量。这些点通常满足某个距离阈值（例如，配准后源点到目标点的距离小于某个指定的距离）;</li><li><span class="math inline">\(N_{\text{total}}\)</span> : 源点云中所有点的数量。</li></ul><h3 id="abserrorrel-rae-downarrow">absErrorRel / RAE <span class="math inline">\(\downarrow\)</span>​</h3><p><strong>Relative Absolute Error (absErrorRel / RAE)</strong> 是深度图评估中常用的一种误差度量，可用于量化预测深度与真实深度之间的相对差异，尤其是在深度差异较大的情况下。其计算公式为： <span class="math display">\[\text{RAE} = \frac{1}{N} \sum_{i=1}^N \frac{|d^{pred}_i - d^{true}_i|}{d^{true}_i}\]</span></p><ul><li><span class="math inline">\(N\)</span> : 表示深度图中总的像素数。</li><li><span class="math inline">\(d^{true}_i\)</span> : 第 <span class="math inline">\(i\)</span> 个像素的真实深度值。</li><li><span class="math inline">\(d^{pred}_i\)</span> : 第 <span class="math inline">\(i\)</span> 个像素的预测深度值。</li><li><span class="math inline">\(|d^{pred}_i - d^{true}_i|\)</span> : 表示第 <span class="math inline">\(i\)</span>​ 个像素的深度预测值与真实值之间的绝对误差。</li></ul><p>从公式中可看出，RAE 的值通常为非负数，值越小表示预测与真实深度的相对误差越小，模型的深度估计越准确。<strong>RAE 相比于绝对误差（Absolute Error）</strong>，对不同深度范围的误差具有更好的鲁棒性，特别是当深度值范围变化较大时（例如远近物体的深度差异较大）。<strong>RAE 相比于相对误差（Relative Error）</strong>，相对误差一般定义为 <span class="math inline">\(\frac{|d^{pred}_i - d^{true}_i|}{d^{true}_i}\)</span>​ 而不是平均误差，这是两者的主要区别。</p><h3 id="ssim_sk-和-avge_sk">SSIM_sk 和 AVGE_sk</h3><p>以 SSIM_sk（SSIM with a specific kernel）为例，它表示 SSIM 的一种扩展或变体，涉及使用不同的卷积核或窗口方法来计算相似性，以适应特定的应用场景或提高性能。</p><h1 id="数据集">数据集</h1><h2 id="inward-facing"><strong>inward-facing</strong></h2><blockquote><p><a href="https://immortalqx.github.io/2022/08/22/nerf-notes-3/#!">(ฅ&gt;ω&lt;*ฅ) 噫？又好了~ (immortalqx.github.io)</a></p></blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240101144152110.png" alt="image-20240101144152110" /><figcaption aria-hidden="true">image-20240101144152110</figcaption></figure><h1 id="sota-benchmark-和-baseline">SOTA, benchmark 和 baseline</h1><blockquote><p>参考链接：https://www.zhihu.com/question/433986039/answer/1618236738</p></blockquote><p>当在浩瀚无边的论文海洋中畅游时，最需要的找到的是SOTA论文。 SOTA，全称「state-of-the-art」，用于描述特定任务中取得当前最优效果的模型。 例如在图像分类任务上，某个模型在常用的数据集（如 ImageNet）上取得了当前最优的性能表现，我们就可以说这个模型达到了SOTA。</p><p>Benchmark和baseline都是指最基础的比较对象。你论文的motivation来自于想超越现有的baseline/benchmark，你的实验数据都需要以baseline/benckmark为基准来判断是否有提高。唯一的区别就是baseline讲究一套方法，而benchmark更偏向于一个目前最高的指标，比如precision，recall等等可量化的指标。举个例子，NLP任务中BERT是目前的SOTA，你有idea可以超过BERT。那在论文中的实验部分你的方法需要比较的baseline就是BERT，而需要比较的benchmark就是BERT具体的各项指标。</p><h1 id="batch-iteration-和-epoch">Batch, iteration 和 epoch</h1><p><a href="https://blog.csdn.net/aha_Yali/article/details/128173662">【深度学习训练之Batch】_深度学习batch-CSDN博客</a></p><h1 id="归一化互相关-ncc">归一化互相关 NCC</h1><p>NCC（Normalized cross-correlation）即归一化互相关，常作为相似性度量指标。取值范围是【-1,1】，-1 表示两个向量负相关，0 表示不相关，1 表示相关。</p><p>NCC 的几何意义可以视为“两个向量的夹角余弦”。</p><p>NCC公式与详情见：<a href="https://blog.csdn.net/fb_help/article/details/104162770">NCC归一化互相关(详解)-CSDN博客</a></p><h1 id="平滑系数">平滑系数</h1><p>Smooth 系数，即平滑系数。以指数平滑法为例，平滑常数决定了平滑水平以及对预测值与实际结果之间差异的响应速度。平滑常数越接近于1，远期实际值对本期平滑值的影响程度下降越迅速；平滑常数越接近于0，远期实际值对本期平滑值影响程度的下降越缓慢。由此，当时间序列相对平稳时，可取较小的平滑系数；当时间序列波动较大时，应取较大的平滑系数，以不忽略远期实际值的影响。</p><p><img src="https://bkimg.cdn.bcebos.com/pic/622762d0f703918faf551c3c5a3d269758eec4bb?x-bce-process=image/format,f_auto/watermark,image_d2F0ZXIvYmFpa2UyNzI,g_7,xp_5,yp_5,P_20/resize,m_lfit,limit_1,h_1080" alt="img" style="zoom:50%;" /></p><h1 id="重投影误差">重投影误差</h1><p><a href="https://blog.csdn.net/weixin_49804978/article/details/121922128">一文读懂重投影误差-CSDN博客</a></p><p>重投影的意思就是第二次投影。有第二次投影，就有第一次投影。</p><p>第一次投影指相机拍照时三维空间点投影到图像上。如下图，也就是世界坐标系下的三维空间点Ｐ映射到相机的图像坐标系中，再转换到像素坐标系下得到像素点p1和p2的坐标（p1为左相机的像素点，p2为右相机的像素点）。</p><p>然后我们利用这些图像对一些特征点进行三角定位（triangulation），利用几何信息（对极几何） 构建三角形反过来估计<u>原来三维空间点的位置（空间三维坐标）</u>，而且也能得到<u>空间点到像素点的坐标转换关系[R,t]</u>。</p><p>第二次投影指利用上一步估计的三维点坐标和估算得到的相机位姿[R,t]在右相机进行重投影，得到新的投影图。重投影误差其实就是两次投影过程得到的像素点的差值。</p><p><strong>重投影误差</strong>：指的真实三维空间点在图像平面上的投影（也就是图像上的像素点）和重投影（其实是用我们的计算值得到的虚拟的像素点）的差值。因为种种原因计算得到的值和实际情况不会完全相符，也就是这个差值不可能恰好为0，此时也就需要将这些差值的和最小化获取最优的相机位姿参数及三维空间点的坐标。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240731101523735.png" alt="image-20240731101523735" style="zoom:67%;" /></p><h1 id="参考文献">参考文献</h1><ol type="1"><li>Niemeyer, M., Barron, J.T., Mildenhall, B., Sajjadi, M.S., Geiger, A., Radwan, N.: Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5480–5490 (2022)</li></ol>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> implicit rendering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Typora 搜狗自定义短语设置</title>
      <link href="/2024/09/07/Typora%20%E6%90%9C%E7%8B%97%E5%BF%AB%E6%8D%B7%E9%94%AE%E8%AE%BE%E7%BD%AE/"/>
      <url>/2024/09/07/Typora%20%E6%90%9C%E7%8B%97%E5%BF%AB%E6%8D%B7%E9%94%AE%E8%AE%BE%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考网址：</p><p><a href="https://blog.csdn.net/kllo__/article/details/122494151">Typora：改变字体的背景颜色_typora文字背景色_kllo__的博客-CSDN博客</a></p><p><a href="https://www.cnblogs.com/USTHzhanglu/p/16073072.html">MD两种折叠方法对比（Typora/Jupyter设置代码折叠） - USTHzhanglu - 博客园 (cnblogs.com)</a></p><p><a href="https://nickxu.me/2022/02/20/Hexo-Butterfly-建站指南（五）日常写作/">Hexo + Butterfly 建站指南（五）日常写作 | NX の 博客 (nickxu.me)</a></p></blockquote><blockquote><p>在Typora中没有快捷设置字体颜色的方式，因此我们想到在搜狗输入法的自定义短语中定义快捷输入html代码来设置字体颜色。本文给出了一些常用的自定义短语。</p></blockquote><p>[TOC]</p><h2 id="设置方法">设置方法</h2><p>打开<code>搜狗-属性设置-高级-候选扩展-自定义短语</code>，点击<code>添加新定义</code>，即可定义新的自定义短语。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231206093545264.png" alt="image-20231206093545264" /><figcaption aria-hidden="true">image-20231206093545264</figcaption></figure><h2 id="设置字体颜色">设置字体颜色</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#常见颜色</span><br><span class="line">&lt;font color=red&gt;红色字体&lt;/font&gt;  #缩写red</span><br><span class="line">&lt;font color=blue&gt;蓝色字体&lt;/font&gt;  #缩写blu</span><br><span class="line">&lt;font color=green&gt;绿色字体&lt;/font&gt;  #缩写gree</span><br><span class="line">&lt;font color=orange&gt;橙色字体&lt;/font&gt;#缩写oran</span><br><span class="line">&lt;font color=purple&gt;紫色字体&lt;/font&gt;#缩写purp</span><br><span class="line">&lt;font color=yellow&gt;黄色字体&lt;/font&gt;#缩写yel</span><br><span class="line"></span><br><span class="line">&lt;font color=#1ec4d3&gt;藻蓝色字体&lt;/font&gt;</span><br><span class="line">&lt;font color=#orange&gt;草绿色字体&lt;/font&gt;</span><br></pre></td></tr></table></figure><h4 id="样式预览">样式预览：</h4><p><font color=red>红色字体</font><br /><font color=blue>蓝色字体</font><br /><font color=green>绿色字体</font><br /><font color=orange>橙色字体</font><br /><font color=purple>紫色字体</font><br /><font color=yellow>黄色字体</font></p><p><font color=#1ec4d3>藻蓝色字体</font> <font color=#orange>草绿色字体</font></p><p>另外附上我从mubu上扒下来的字体颜色设置：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;font color=#ef042a&gt;红色字体&lt;/font&gt;  #缩写red</span><br><span class="line">&lt;font color=#0091ff&gt;蓝色字体&lt;/font&gt;  #缩写blu</span><br><span class="line">&lt;font color=#4eb434&gt;绿色字体&lt;/font&gt;  #缩写gree</span><br><span class="line">&lt;font color=#df8400&gt;橙色字体&lt;/font&gt;#缩写oran</span><br><span class="line">&lt;font color=#985fff&gt;紫色字体&lt;/font&gt;#缩写purp</span><br></pre></td></tr></table></figure><h4 id="样式预览-1">样式预览：</h4><p><font color=#df8400>这是橙色</font> <font color=#ef042a>这是红色</font> <font color=#4eb434>这是绿色</font> <font color=#0091ff>这是蓝色</font> <font color=#985fff>这是紫色</font></p><h2 id="给字体加底色">给字体加底色</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;span style=&quot;background:#eef0f4;&quot;&gt;浅灰色背景&lt;/span&gt;#缩写bggrey</span><br><span class="line">&lt;span style=&quot;background:#fbd4d0;&quot;&gt;浅红色背景&lt;/span&gt;#缩写bgred</span><br><span class="line">&lt;span style=&quot;background:#d4e9d5;&quot;&gt;浅绿色背景&lt;/span&gt;#缩写bggre</span><br><span class="line">&lt;span style=&quot;background:#dad5e9;&quot;&gt;浅紫色背景&lt;/span&gt;#缩写bgpur</span><br><span class="line">&lt;span style=&quot;background:#f9eda6;&quot;&gt;浅黄色背景&lt;/span&gt;#缩写bgyel</span><br><span class="line">&lt;span style=&quot;background:#FFCC99;&quot;&gt;浅橘色背景&lt;/span&gt;#缩写bgora</span><br><span class="line"></span><br><span class="line">&lt;span style=&quot;background:#daf5e9;&quot;&gt;亮绿色背景&lt;/span&gt;</span><br><span class="line">&lt;span style=&quot;background:#fff1c9;&quot;&gt;亮黄色背景&lt;/span&gt;  </span><br><span class="line">&lt;span style=&quot;background:#dcffff;&quot;&gt;亮蓝色背景&lt;/span&gt; </span><br></pre></td></tr></table></figure><h4 id="样式预览-2">样式预览</h4><p><span style="background:#eef0f4;">浅灰色背景</span> <span style="background:#fbd4d0;">浅红色背景</span> <span style="background:#d4e9d5;">浅绿色背景</span> <span style="background:#dad5e9;">浅紫色背景</span> <span style="background:#f9eda6;">浅黄色背景</span> <span style="background:#FFCC99;">浅橘色背景</span></p><p><span style="background:#daf5e9;">亮绿色背景</span> <span style="background:#fff1c9;">亮黄色背景</span> <span style="background:#dcffff;">亮蓝色背景</span></p><blockquote><p>配色参考网站：</p><p><a href="https://likexia.gitee.io/tools/peise/index.html">网页设计常用色彩搭配表 - 配色表 (gitee.io)</a></p><p><a href="https://encycolorpedia.cn/">十六进制颜色代码表，图表及调色板 - Encycolorpedia</a></p><p><a href="https://encycolorpedia.cn/e0ffff">Lightcyan／亮青／淺藍／淺藍色／#e0ffff十六进制颜色代码表，图表，调色板，绘图&amp;油漆 (encycolorpedia.cn)</a></p></blockquote><p>再附上latex中预定义的一些颜色（<a href="https://en.wikibooks.org/wiki/LaTeX/Colors">参考链接</a>）：</p><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\colorbox</span>&#123;Apricot&#125;&#123;近橙色背景&#125;</span><br><span class="line"><span class="keyword">\colorbox</span>&#123;SpringGreen&#125;&#123;近绿色背景&#125;</span><br><span class="line"><span class="keyword">\colorbox</span>&#123;pink&#125;&#123;粉色背景&#125;</span><br><span class="line"><span class="keyword">\colorbox</span>&#123;CornflowerBlue&#125;&#123;近蓝色背景&#125;</span><br><span class="line"><span class="keyword">\colorbox</span>&#123;RedOrange&#125;&#123;近红色背景&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[\colorbox{Apricot}{近橙色背景}\colorbox{SpringGreen}{近绿色背景}\colorbox{pink}{粉色背景}\colorbox{CornflowerBlue}{近蓝色背景}\colorbox{RedOrange}{近红色背景}\]</span></p><h2 id="给字体加底色且改颜色">给字体加底色且改颜色</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;span style=&quot;background:#f9eda6;color:red&quot;&gt;浅黄色背景红字&lt;/span&gt;</span><br></pre></td></tr></table></figure><p>如：<span style="background:#f9eda6;color:red">浅黄色背景</span></p><h2 id="设置图片题注tuzh">设置图片题注tuzh</h2><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">center</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">img</span> <span class="attr">style</span>=<span class="string">&quot;border-radius: 0.3125em;</span></span></span><br><span class="line"><span class="string"><span class="tag">    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);&quot;</span> </span></span><br><span class="line"><span class="tag">    <span class="attr">src</span>=<span class="string">&quot;这里输入图片地址&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">&quot;color:orange; border-bottom: 1px solid #d9d9d9;</span></span></span><br><span class="line"><span class="string"><span class="tag">    display: inline-block;</span></span></span><br><span class="line"><span class="string"><span class="tag">    color: #999;</span></span></span><br><span class="line"><span class="string"><span class="tag">    padding: 2px;&quot;</span>&gt;</span>这里输入题注<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">center</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="设置-html-超链接">设置 html 超链接</h2><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;链接地址&quot;</span>&gt;</span>链接名称<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这种形式的超链接在 Typora 中相较于原版，可以实现点击跳转页面！而且可以用于嵌入到折叠栏中，跳转功能也不受影响！</p><h2 id="设置内容折叠">设置内容折叠</h2><h3 id="普通内容折叠zhed">普通内容折叠zhed</h3><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">details</span>&gt;</span> <span class="tag">&lt;<span class="name">summary</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">summary</span>&gt;</span></span><br><span class="line">contents ...</span><br><span class="line"><span class="tag">&lt;/<span class="name">details</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在<code>content</code>中可以嵌套使用 Markdown 语法和 HTML 语法。</p><h4 id="效果如下">效果如下：</h4><details><summary>Title</summary>contents ...</details><h3 id="代码内容折叠czhed">代码内容折叠czhed</h3><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">details</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">summary</span>&gt;</span>点击时的区域标题：点击查看详细内容<span class="tag">&lt;/<span class="name">summary</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">pre</span>&gt;</span><span class="tag">&lt;<span class="name">code</span>&gt;</span>#define A B</span><br><span class="line">#endif</span><br><span class="line">void init(void)<span class="tag">&lt;/<span class="name">code</span>&gt;</span><span class="tag">&lt;/<span class="name">pre</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">details</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在Typora和网站上均可以折叠代码，但是代码不能高亮。<code>&lt;pre&gt;&lt;/pre&gt;</code>用于显示多行代码，若去除则渲染为单行代码。</p><h4 id="效果如下-1">效果如下：</h4><details><summary>点击时的区域标题：点击查看详细内容</summary><pre><code>#define A B#endifvoid init(void)</code></pre></details><h2 id="butterfly-标签外挂">Butterfly 标签外挂</h2><blockquote><p>官方文档：<a href="https://butterfly.js.org/posts/4aa8abbe/#標籤外掛（Tag-Plugins）">Butterfly 安裝文檔(三) 主題配置-1 | Butterfly</a></p></blockquote><h3 id="折叠栏-toggle类似内容折叠">折叠栏 Toggle（类似内容折叠）</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">&#123;% hideToggle 点击以打开 %&#125;</span><br><span class="line"></span><br><span class="line">内容</span><br><span class="line"></span><br><span class="line">&#123;% endhideToggle %&#125;</span><br></pre></td></tr></table></figure><h4 id="效果如下-2">效果如下：</h4><details class="toggle" ><summary class="toggle-button" style="">点击以打开</summary><div class="toggle-content"><p>内容</p></div></details><h3 id="选项卡-tabs">选项卡 Tabs</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">&#123;% tabs 样例 %&#125;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab 代码  --&gt;</span><br><span class="line"></span><br><span class="line">这里是代码</span><br><span class="line"></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab 预览  --&gt;</span><br><span class="line"></span><br><span class="line">这里是预览</span><br><span class="line"></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&#123;% endtabs %&#125;</span><br></pre></td></tr></table></figure><h4 id="效果如下-3">效果如下：</h4><div class="tabs" id="样例"><ul class="nav-tabs"><button type="button" class="tab  active" data-href="样例-1">代码</button><button type="button" class="tab " data-href="样例-2">预览</button></ul><div class="tab-contents"><div class="tab-item-content active" id="样例-1"><p>这里是代码</p></div><div class="tab-item-content" id="样例-2"><p>这里是预览</p></div></div><div class="tab-to-top"><button type="button" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div><h3 id="时间轴-timeline">时间轴 timeline</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">&#123;% timeline 2024 %&#125;</span><br><span class="line">&lt;!-- timeline 01-02 --&gt;</span><br><span class="line">这是测试文本</span><br><span class="line">&lt;!-- endtimeline --&gt;</span><br><span class="line">&#123;% endtimeline %&#125;</span><br></pre></td></tr></table></figure><h4 id="效果如下-4">效果如下：</h4><div class="timeline undefined"><div class='timeline-item headline'><div class='timeline-item-title'><div class='item-circle'><p>2024</p></div></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>01-02</p></div></div><div class='timeline-item-content'><p>这是测试页面</p></div></div></div>]]></content>
      
      
      <categories>
          
          <category> blog搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CoRGS 论文笔记</title>
      <link href="/2024/09/01/CoRGS%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>/2024/09/01/CoRGS%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="cor-gs-基本信息">CoR-GS 基本信息</h1><p>论文名称：CoR-GS: Sparse-View 3D Gaussian Splatting via Co-Regularization</p><p>发表期刊：ECCV 2024（本文偏计算机类，重效果）</p><h1 id="论文思想和整体流程">论文思想和整体流程</h1><p>由于<strong>致密化操作的随机性</strong>（文中已通过曲线图佐证），当训练两个高斯辐射场时，他们的点云和渲染图常存在不一致的现象，我们可以利用这种现象来无监督地优化重建质量。我们提出了CoR-GS, 它主要从两个方面来利用这种不一致，识别并抑制不准确的重建：</p><ol type="1"><li>认为在不准确重建的地方往往有<strong>大的点云不一致性</strong>（文中已通过曲线图佐证），我们通过 Co-pruning 将这些点云剪去；</li><li>认为在不准确重建的地方的<strong>渲染像素有大的不一致性</strong>（文中已通过曲线图佐证），我们通过 Pseudo-view Co-regularization 惩罚抑制。</li></ol><p><strong>下面是主要流程：</strong></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240822094652262.png" alt="image-20240822094652262" /><figcaption aria-hidden="true">image-20240822094652262</figcaption></figure><p>我们同时训练两个高斯辐射场，并通过抑制点云不一致性和渲染不一致性正则化辐射场。首先，由于在稀疏视角下的点云往往受限，我们与FSGS一样，选择使用 stereo-fusion-based initialization。然后，通过不同的指标衡量点云不一致性和渲染像素不一致性：</p><ol type="1"><li>对于点云不一致性：<strong>Fitness</strong> （计算高斯场中的重叠区域，取最大距离 <span class="math inline">\(\tau = 5\)</span>）和 <strong>RMSE</strong>（计算相同位置点云的平均距离）。</li><li>对于渲染像素不一致性：<strong>PSNR</strong>（渲染图）和 <strong>absErrorRel</strong> （深度图）。</li></ol><p>通过 <strong>Co-pruning</strong> 和 <strong>Pseudo-view Co-regularization</strong> 分别抑制两种不一致性。最后，为了衡量重建质量，我们选取 Fitness 、RMSE 和 absErrorRel 作为指标对比？？？。</p><ul><li>在 LLFF、DTU、Blender 数据集上训练 10 K 次，在 MipNeRF 数据集上和 3DGS 一样训练 30 K 次。</li></ul><p>在训练结束后，保存其中一个高斯辐射场用于推理。</p><h1 id="主要原理">主要原理</h1><h2 id="co-pruning-抑制点云不一致">Co-pruning 抑制点云不一致</h2><p>在致密化控制操作中，每间隔5次进行剪枝。设置最大容忍距离 <span class="math inline">\(\tau = 5\)</span>，将超过这个距离的高斯通过掩膜消去。此外这个过程减少了高斯数，因此使得速度也得到提升。</p><figure><img src="C:\Users\21218\AppData\Roaming\Typora\typora-user-images\image-20240822162806052.png" alt="image-20240822162806052" /><figcaption aria-hidden="true">image-20240822162806052</figcaption></figure><h2 id="pseudo-view-co-regularization-抑制渲染不一致">Pseudo-view Co-regularization 抑制渲染不一致</h2><p><strong>伪视角采样</strong>：从欧氏空间中最近的两个训练视角进行采样，取其平均旋转角 <span class="math inline">\(q\)</span>。相机中心为 <span class="math inline">\(t\in P\)</span> ，令采样视角为 <span class="math inline">\(P&#39;=(t+ \epsilon,q)\)</span>，其中 <span class="math inline">\(\epsilon\)</span> 表示服从正态分布的随机噪声。我们对该采样视角下的两个高斯点云进行渲染，得到两个伪视图，然后对两个伪视图求正则化损失使其差异尽量小。</p><p><strong>渲染损失</strong>：我们对训练视角下的模型高斯点云进行渲染，对渲染图和真实图求颜色损失。</p><p>最终，Pseudo-view Co-regularization 的损失项由<u>无监督的伪视图正则化损失</u>和<u>真实图像监督的损失</u>两项组成。注意这一步的两个正则化损失并没有用到深度图，原因在补充材料B.2。</p><h1 id="实验">实验</h1><h2 id="实验细节">实验细节</h2><ul><li>输入稀疏图片：3张（llff）</li><li>总训练迭代数：10 000次（llff）</li><li>测试迭代数：[500, 2000, 3000, 5000, 7000, 10000, 15000, 30000]</li><li>保存迭代数：[10000, 30000]</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gaussian splatting </tag>
            
            <tag> 稀疏重建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自定义相机教程笔记</title>
      <link href="/2024/08/30/%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9B%B8%E6%9C%BA%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
      <url>/2024/08/30/%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9B%B8%E6%9C%BA%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<blockquote><p>搬运链接：<a href="https://www.bilibili.com/video/BV1uspre7EDZ/?spm_id_from=333.999.0.0&amp;vd_source=124ec79ebd3e16b0f454a3994a468f98">【3DGS】自定义相机教程（含代码）（三维高斯泼溅）_哔哩哔哩_bilibili</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八小工具 </category>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代码箱 </tag>
            
            <tag> gaussian splatting </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GaussianPro 论文笔记</title>
      <link href="/2024/07/30/GaussionPro/"/>
      <url>/2024/07/30/GaussionPro/</url>
      
        <content type="html"><![CDATA[<p>论文提出了一种渐进式传播策略：</p><ol type="1"><li>将三维高斯投影到二维空间，生成渲染深度图和法线图，引导高斯生成；（<code>/gaussian_renderer/init.py</code>，通过rasterizer得到一个返回字典，字典里有[<code>render_depth</code>, <code>render_normal</code>, <code>render_opacity</code>]）</li><li>基于邻近的四个像素，迭代传播更新每个像素的深度和法线，那些更新后的深度与原来深度差别很大的高斯会被消除，在原有三维位置上生成新高斯；<ol type="1"><li>传播的区域在不透明度<code>opacity</code>小于阈值的区域，<code>viewpoint_cam.sky_mask</code></li><li></li></ol></li><li>提出一种平面约束来正则化场景几何，生成更加精确的几何。</li></ol><h2 id="多视图几何一致性过滤器">多视图几何一致性过滤器</h2>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 新视图合成 </tag>
            
            <tag> 3DGS </tag>
            
            <tag> ECCV2022 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>球谐函数</title>
      <link href="/2024/07/26/%E7%90%83%E8%B0%90%E5%87%BD%E6%95%B0/"/>
      <url>/2024/07/26/%E7%90%83%E8%B0%90%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="球坐标系">球坐标系</h2><p>根据<a href="https://baike.baidu.com/item/%E7%90%83%E5%9D%90%E6%A0%87%E7%B3%BB/8315363">百度百科的定义</a>，球坐标系（Spherical coordinate system）属于三维坐标系的一种，<strong>它利用球坐标 <span class="math inline">\((r,\theta,\varphi)\)</span> 表示一个点 P 在三维空间的位置</strong>。下图显示了球坐标的几何意义：<strong>原点到 P 点的距离</strong> <span class="math inline">\(r\)</span> ，原点到点 P 的连线与正 z-轴之间的<strong>天顶角/仰角</strong> <span class="math inline">\(\theta\)</span> 以及原点到点 P 的连线，在 xy-平面的<a href="https://baike.baidu.com/item/投影/7565?fromModule=lemma_inlink">投影</a>线，与正 x-轴之间的<strong>方位角</strong> <span class="math inline">\(\varphi\)</span> 。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240605154048335.png" alt="image-20240605154048335" /><figcaption aria-hidden="true">image-20240605154048335</figcaption></figure><h2 id="球谐函数">球谐函数</h2><p>类似地，球面坐标系的基函数可以表示为：<span class="math inline">\(r_k=f_k(\theta,\varphi)，k=0,1,...\)</span>​ 的形式。最有名的球面基函数就是球面谐波/球谐函数SH。 <span class="math display">\[f(t)\approx \sum_l\sum_{m=-l}^lc_l^my_l^m(\theta,\phi)\]</span> 球谐函数是单位圆上傅里叶基的球面模拟，由于球谐函数形成了一组完整的正交函数，形成了正交基，因此<strong>定义在球面上的每个函数 <span class="math inline">\(f(t)\)</span> 都可以写成这些球谐函数的总和的近似</strong>，如上式。与信号处理中使用的傅立叶基一样，在截断序列时必须小心，以尽量减少可能发生的“振铃”伪影。</p><p>球谐函数有很多好的性质：</p><ul><li><strong>正交性</strong>：每个基函数都是独立的，不能由其他基函数加权得到。即球谐函数是球面上的一组正交基。正交基函数有两个重要特性：<ol type="1"><li>任意函数在某个基函数下的投影系数就等于这个函数与基函数相乘之后的积分结果。</li><li>两个函数相乘之后的积分结果就等于这两个函数的SH系数组成的向量的点积。</li></ol></li><li><strong>旋转不变性</strong>：类似于傅里叶变换中的平移不变性。输入光源旋转一定角度，其通过SH表达的近似解也是同样旋转一个相同的角度，这也就说明，光照结果在旋转后的SH表达是连续的稳定的，不会因为光照的旋转而导致输出光效的抖动。因此，环境光照变化之后只需要简单的计算就可以得到光源旋转之后的结果。</li></ul><p>用图像来直观地观察SH的基函数：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240605155732186.png" alt="image-20240605155732186" /><figcaption aria-hidden="true">image-20240605155732186</figcaption></figure><blockquote><p>图注：<span class="math inline">\(y_l^m\)</span> 的下标 <span class="math inline">\(l\)</span> 表示阶数 "band level/degree" ，在第 <span class="math inline">\(l\)</span> 个阶有 <span class="math inline">\(2l+1\)</span> 个基函数，前 <span class="math inline">\(D\)</span>阶(degree)的基函数个数为 <span class="math inline">\(n=D^2\)</span> 个。上标 <span class="math inline">\(m\)</span> 表示在该波段的函数序号 <span class="math inline">\([-l,l]\)</span> 。</p></blockquote><p>在复数空间中，球谐函数基函数的定义为： <span class="math display">\[ Y_ {l}^ {m}  (  \theta  ,  \varphi  )=  K_ {l}^ {m}   e^ {im\varphi }   P_ {l}^ {|m|}  (  \cos   \theta  ),l  \in  N,-l  \leqslant  m  \leqslant  l\]</span> 其中，</p><ul><li><span class="math inline">\(K_l^m\)</span> 是正则化常数：<span class="math inline">\(K_{l}^{m}=\sqrt{\frac{(2l+1)}{4\pi}\frac{(l-|m|)!}{(l+|m|)!}}\)</span> ；</li><li><span class="math inline">\(P_l^m\)</span> 是 Legendre 多项式：$P_{l}<sup>{m}(x)=(-1)</sup>m( 1-x^2 ) ^{m/2}( P_l(x) ) $ （注：$P_n(x)=$）</li></ul><p>在图形学中，我们用得更广泛的是实数空间的基： <span class="math display">\[y_{l}^{m}(\theta ,\varphi )= \begin{cases}\sqrt{2}Re(Y^{m}_l),&amp; m&gt;0\\\sqrt{2}Im(Y_{l}^{m}),&amp;m&lt;0\\Y_{l}^{0},&amp; m=0\end{cases} =\left\{\begin{matrix}    \sqrt{2}K_{l}^{m}\cos\mathrm{(}m\varphi )P_{l}^{m}(\cos \theta ),&amp;      m&gt;0\\    \sqrt{2}K_{l}^{m}\sin\mathrm{(}-m\varphi )P_{l}^{-m}(\cos \theta ),&amp;        m&lt;0\\    K_{l}^{0}P_{l}^{0}(\cos \theta ),&amp;      m=0\\\end{matrix} \right.\]</span> 代入图中就是：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240606151631384.png" alt="image-20240606151631384" /><figcaption aria-hidden="true">image-20240606151631384</figcaption></figure><p>这种表达方式在进行符号运算求解析解时很方便，但是不易编代码。因此又给出了在笛卡尔坐标系下的表示方式，此时，SH就是简单的二次多项式。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240606151834756.png" alt="image-20240606151834756" /><figcaption aria-hidden="true">image-20240606151834756</figcaption></figure><h2 id="sh系数">SH系数</h2><p>前面了解到，球谐函数是一种球面基函数，那么“任何球相关函数都可以用这组基的算术组合来近似”。即给定任意球相关函数 <span class="math inline">\(f(x)\)</span> ,与一组 SH 基函数 <span class="math inline">\([e_1(x),e_2(x),e_3(x),e_4(x)]\)</span> ,我们都可以找到一组系数 <span class="math inline">\([a_1,a_2,a_3,a_4]\)</span>，使得： <span class="math display">\[f(x)=a_1e_1(x)+a_2e_2(x)+a_3e_3(x)+a_4e_4(x)\]</span> 这组系数其实就是“SH系数”。SH系数用得越多，那么基函数就用得越多，进而近似函数的表达能力就越强，与原始函数越接近。如下图，<span class="math inline">\(n\)</span> 表示球面谐波基的个数。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240606154628750.png" alt="image-20240606154628750" /><figcaption aria-hidden="true">image-20240606154628750</figcaption></figure><h2 id="nerf中rgb描述光照">NeRF中RGB描述光照</h2><p>NeRF 模型的表达式为： <span class="math display">\[f_{\varTheta}(x,y,z,\theta,\phi) = (c,\sigma)\]</span> 这里的 <span class="math inline">\(f_{\varTheta}\)</span>​ 表示 MLP。即 NeRF 用到位置和方向来求颜色RGB值。事实上，作者针对当训练模型时不使用视觉方向 d，只使用空间位置 x 的情况（理想的、完全漫反射的朗伯定律情况）做了实验如下图，此时很难表示镜面反射（specularities）的效果：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/8289c107dd374add889457754ae8ec2f.png" alt="img" style="zoom:67%;" /></p><p>因此NeRF在求颜色RGB时，还使用到观察角度 d 来表达“非朗伯效应”。“非朗伯效应”（non-Lambertian effects）是指物体表面的反射特性不遵循朗伯定律（Lambert’s Law）。</p><p><strong>朗伯定律</strong>是光学中的一个基本原理，它描述了当光线照射在理想的散射表面或者完全漫反射表面上时，出射光强度只与入射光强度和入射角有关，与观察角无关。然而，在现实生活中，许多物体并不符合这一规律。例如，镜子、金属等具有镜面反射特性的物体就会产生非朗伯效应。</p><p><strong>镜面反射</strong>是指光线照在平滑物体表面（如镜子或光滑金属）时，按照入射角等于反射角的规律进行反射的现象。这种现象通常会使物体表面产生亮点或高光区域。如果一个模型不能很好地处理这种效果，那么它就可能无法准确地再现真实世界中具有镜面特性的物体所展示出来的视觉效果。</p><h3 id="tip-朗伯体与非朗伯体">tip: 朗伯体与非朗伯体</h3><p><strong>百度百科中的定义</strong>：朗伯体是指当入射能量在所有方向均匀反射，即入射能量以入射点为中心，在整个半球空间内向四周各向同性地反射能量的现象，称为漫反射，也称各向同性反射，<u>一个完全的漫射体称为<strong>朗伯体</strong></u>。常见的接近理想情况的朗伯体有积雪、刷粉的白墙与十分粗糙的白纸表面等。</p><p><strong>非朗伯体</strong>即物体表面存在镜面反射（玻璃、镜子等有反光的物体），通常属于较具有挑战性的重建情况。</p><h2 id="球谐函数描述光照">球谐函数描述光照</h2><p>在三维重建中，我们往往认为：空间中点的颜色 <span class="math inline">\(c\)</span> 受观测角度/光线方向 <span class="math inline">\((\theta,\phi)\)</span> 的影响，即 <span class="math inline">\(c=f(\theta,\phi)\)</span> 。我们用球谐基 <span class="math inline">\(y_l^m\)</span> 来分解 <span class="math inline">\(f\)</span> ，在 NeRF 中一般用到二阶或三阶的球谐基，而 3DGS 中则默认采用三阶球谐基。通常，颜色 RGB 的三个通道都会用一组球谐基，因此：</p><ul><li>degree = 2 时，二阶球谐基 <span class="math inline">\(l=0,1,2\)</span>：共9=1+3+5个球谐基，对应9*3个SH系数；</li><li>degree = 3 时，三阶球谐基 <span class="math inline">\(l=0,1,2,3\)</span>：共16=1+3+5+7个球谐基，对应16*3个SH系数。</li></ul><p>在实现中，常把能预计算的常数计算出来先保存。</p><blockquote><p>为啥不用更高阶的SH？一方面是因为更多的系数会带来更大的存储压力、计算压力，而一般描述变化比较平滑的环境漫反射部分，用3阶SH就足够了；另一方面则是因为SH的物理含义不是特别好理解，高阶SH容易出现各种花式Artifact。</p></blockquote><p>首次将SH引入NeRF的工作是 <a href="https://arxiv.org/pdf/2103.14024">PlenOctrees</a>，论文中比较详细地描述了SH在三维重建中的应用。NeRF 模型原本的数学表达为： <span class="math display">\[f_{\varTheta}(x,y,z,\theta,\phi) = (c,\sigma)\]</span> 这里的 <span class="math inline">\(f_{\varTheta}\)</span> 表示 MLP。经过SH编码后的表达式变为： <span class="math display">\[f_{\varTheta}(x,y,z)=(k,\sigma)\]</span> 这里的 <span class="math inline">\(k\)</span> 表示SH系数。颜色 <span class="math inline">\(c\)</span> 可以由SH系数进一步得到：<span class="math inline">\(c(\mathbf{d} ; \mathbf{k})=S\left(\sum_{\ell=0}^{\ell_{\max }} \sum_{m=-\ell}^{\ell} k_{\ell}^{m} Y_{\ell}^{m}(\mathbf{d})\right)\)</span> 。这里的 <span class="math inline">\(\mathbf{d}\)</span> 表示视角方向，<span class="math inline">\(S\)</span> 表示用来归一化颜色的 sigmoid 函数 <span class="math inline">\(S:x \rightarrow (1+exp(-x))^{-1}\)</span>​​。</p><h2 id="为什么球谐函数表达颜色好">为什么球谐函数表达颜色好</h2><p>直觉原因是球谐函数的参数数量多（相比于RGB只有三个参数）、维度高，因此能存储的信息也更多。</p><p>具体原因可以深溯到CG中的<strong>球形环境贴图</strong>。反射包括漫反射和镜面反射。当物体表面越光滑时，其漫反射就越少、而镜面反射越多。如下图，一个绝对光滑的球没有漫反射，全是镜面反射，其面上将记录外部环境的所有光源。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240629120203703.png" alt="image-20240629120203703" /><figcaption aria-hidden="true">image-20240629120203703</figcaption></figure><p>我们将该记录了环境光的球设法展开。先找到球的最小外接距，然后将球六等分，每个等分球面与最小外接距作一一映射，从而将球面映射成六个矩形。在渲染中，我们常用球谐函数来重建光，随着球谐函数阶数的增加，它能更加根据方向展示不同的颜色，从而使环境贴图的重建效果越来越好。当阶数为1时，球谐函数就相当于RGB，不与方向有关，只有一个颜色。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240629121159224.png" alt="image-20240629121159224" /><figcaption aria-hidden="true">image-20240629121159224</figcaption></figure><h2 id="代码">代码</h2><p><a href="https://blog.csdn.net/qq_39300235/article/details/106834471">球谐函数的概念与应用：球谐函数-CSDN博客</a></p><h2 id="总结">总结</h2><p>在NeRF或者3DGS等三维重建技术的背后，需要对空间点（或者高斯球）的空间位置<span class="math inline">\((x,y,z)\)</span>和观测方向<span class="math inline">\((\theta,\phi)\)</span>进行<strong>合理</strong>表示。我们发现，空间位置<span class="math inline">\((x,y,z)\)</span>是笛卡尔坐标系下，而观测方向是<span class="math inline">\((\theta,\phi)\)</span>球形坐标系下，不同坐标系可以选择不同的编码方式。 对空间坐标系的编码方式常见的有<u>多分辨率哈希编码、频率编码</u>； 对球形坐标系的编码方式最常见的就是<u>球谐函数编码</u>了～（频率编码不实用的原因是编码不紧凑）；</p><p>球面谐波函数是最紧凑的球面坐标编码方式（至少是之一），任何关于<span class="math inline">\((\theta,\phi)\)</span>的函数都可以由球谐基的组合来表示。</p><h3 id="参考链接">参考链接：</h3><ul><li><a href="https://puye.blog/posts/SH-Introduction-CN/">球谐函数介绍（Spherical Harmonics） | Puye's Blog</a></li><li><a href="https://blog.csdn.net/leviopku/article/details/135136978">【AI数学】NeRF中的球面谐波函数（Spherical Harmonics）-CSDN博客</a></li><li><a href="https://www.jianshu.com/p/16f8a84bcc38">球面调和 Spherical Harmonics (SH) - 简书 (jianshu.com)</a></li><li><a href="https://www.bilibili.com/video/BV1oM4m1U7qP/?p=3&amp;spm_id_from=pageDriver">【较真系列】讲人话-3d gaussian splatting全解(原理+代码+公式)【3】 雪球颜色_哔哩哔哩_bilibili</a></li><li><a href="https://blog.csdn.net/jiaoyangwm/article/details/133804918">【NeRF】2、NeRF 首篇经典论文介绍（ECCV2020）_nerf第一篇论文-CSDN博客</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>长江后浪扑前浪，NeRF现在沙滩上</title>
      <link href="/2024/07/26/NeRF/"/>
      <url>/2024/07/26/NeRF/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文主页： <a href="https://www.matthewtancik.com/nerf">NeRF: Neural Radiance Fields (matthewtancik.com)</a></p><p>参考链接：</p><p><a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODA3MDkyMA==&amp;mid=2247485459&amp;idx=1&amp;sn=3228ed101ae9ece2f65021cfb3aeccd4&amp;chksm=cf81fe74f8f67762b2d02998a6d9f76c862b13b78ff808655639f27441f567721f7ccf72e24d&amp;cur_album_id=2737490560171376640&amp;scene=189#wechat_redirect">NeRF入门之神经辐射场全貌 (qq.com)</a> （本文大多内容直接抄的这里的，建议自己亲自看一遍，强烈推荐！！！本文仅做记录）</p><p>[<a href="https://blog.csdn.net/g11d111/article/details/118959540">NeRF]代码+逻辑详细分析_nerf代码解读 csdn-CSDN博客</a></p><p><a href="https://blog.csdn.net/jiaoyangwm/article/details/133804918">【NeRF】2、NeRF 首篇经典论文介绍（ECCV2020）_nerf第一篇论文-CSDN博客</a> (写的很详细深入，有不懂的可以查这里)</p></blockquote><h1 id="引入">引入</h1><p>NeRF 全称是 Neural Radiance Field (神经辐射场)，它是想做这样一件事情：</p><p><strong>给定一个场景，输入相机 (或者观察者) 的位置和朝向后，输出这个视角下的视图——新视图合成</strong></p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/640" /></p><p>更详细地说，NeRF 是一种用神经网络来渲染三维场景的技术。它可以在提供某几个角度的图像的前提下，生成其他角度下的图像，间接重建出场景的三维信息。</p><p>为什么要学习这个东西呢？</p><p>已故计算机视觉先驱 David Marr 曾经把计算机视觉终极问题定义为：<strong>从二维图像重建出三维物体的位置和形状。</strong></p><p>在 Marr 看来，只有当计算机可以学习到三维信息，才能说明计算机已经学会了视觉。</p><p>按照他的理论，CV 中的分类、检测、分割等任务，只能称为「模式识别」问题，而像 NeRF 这类三维重建任务，才能称为真正的计算机视觉。</p><p>NeRF 早在 2020 年就出现了，并在当年的 ECCV 会议上被提名最佳论文。由于 NeRF 优秀的 3D 建模能力，它可以做很多事情。</p><p>计算景深：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf1x.gif" /></p><p>设置对焦：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf2x.gif" /></p><p>重建三维场景（不是真正意义上，仅仅提供新视图）：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf3.gif" /></p><p>设置光影变换：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf4.gif" /></p><h1 id="ray-marching-体渲染">ray-marching 体渲染</h1><blockquote><p>参考链接：<a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODA3MDkyMA==&amp;mid=2247485148&amp;idx=1&amp;sn=4e6acc1b358e624ac2301729d35cc89d&amp;chksm=cf81f0bbf8f679ad3f990f91d5c84fcc67cc6948c3cbc0c34c59874f7795e9263f2382979414&amp;token=400584181&amp;lang=zh_CN#rd">NeRF入门之体渲染 (Volume Rendering) (qq.com)</a></p></blockquote><p><strong>体渲染（ Volume Rendering）能够对光线进行建模，计算出像素的光线强度/颜色值。</strong></p><p>不管是相机还是人眼，像素的诞生都是光线打在感光器件 (视锥细胞) 上的结果。以最经典的小孔成像为例，光线穿过孔洞后，打在屏幕上形成像素：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf5.png" /></p><p>因此，只要对光线渲染过程进行建模，计算出最终的像素值就可以了。当然啦，真实的相机中为了获得更高的信噪比，用的是镜头成像，不过，在 NeRF 中，小孔成像模型就足够建模整个渲染过程了。</p><p>而对光线进行建模，正好是<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODA3MDkyMA==&amp;mid=2247485148&amp;idx=1&amp;sn=4e6acc1b358e624ac2301729d35cc89d&amp;chksm=cf81f0bbf8f679ad3f990f91d5c84fcc67cc6948c3cbc0c34c59874f7795e9263f2382979414&amp;scene=21#wechat_redirect">体渲染</a>的拿手好戏。</p><p>ray-marching 体渲染把每一条光路建模成由一个个粒子构成。光子在光路传播中，可能因为粒子的遮挡损失能量，导致入射光减弱 (比如气体会遮挡部分光线，不透明固体则会遮挡所有光线)。光路中的粒子也可能本身会发光，抑或是周围光路的光子被弹射到当前光路，导致光线增强。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf6.png" /></p><p>在计算机中，由于计算资源限制，不可能建模出所有粒子的状态，因此通常会在每条光路上采样一些点，由这些离散采样点上的粒子来近似整条连续的光线。</p><h2 id="透射比-transmittance">透射比 transmittance</h2><p><strong>透射比</strong>（transmittance）的公式如下： <span class="math display">\[T(s)=\frac{I_{o}}{I_{i}}=\exp \left(-\int_{i}^{o} \tau_{a}(t) d t\right)\]</span> 其中，<span class="math inline">\(I_o\)</span>表示入射光辐射强度/颜色，<span class="math inline">\(I_i\)</span>表示出射光辐射强度/颜色。由上述公式知，<strong>透射比 <span class="math inline">\(T(s)\)</span> 表示粒子群中 <span class="math inline">\(s\)</span> 点的<u>透明度</u>——<font color=#ef042a>在 <span class="math inline">\(s\)</span> 点之前，光线没有被阻碍的概率</font>，数值越大，说明粒子群越透明，光线衰减的幅度就越小</strong>。</p><p>而透明度本身是关于光学厚度 $_a( t ) $的方程， $_a( t ) <span class="math inline">\(越大，\)</span>T(s)$就越小。由公式 <span class="math inline">\(\tau _a\left( t \right) =\rho \left( t \right) A\)</span> 可以知道，它是由粒子密度<span class="math inline">\(\rho \left( t \right)\)</span>和粒子垂直光线方向的投影面积<span class="math inline">\(A\)</span>决定的。这在直觉上也很好理解，如果粒子密度大，粒子本身也比较大，那么遮住光线的概率也会相应提升，自然透明度也就下降了。</p><blockquote><p><span class="math inline">\(\tau_a(t)\)</span>被称为<strong>光学厚度 (optical depth)</strong>。下图为不同光学厚度下，光线透射度对比。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/bdpnCavfx2pcmvCruJHc0c3l8penibibHHg7jWW7W01Vb3hOHbB7UCjBJgQSdGX4j6GiaclqUQ0aIMsqibMhKNZ0lw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt="不同光学厚度下，光线透射度对比" style="zoom:30%;" /></p></blockquote><h2 id="体渲染公式">体渲染公式</h2><p><strong>最终每条光线渲染出来的颜色值都可以用下面的公式表示</strong> (该公式的详细说明与推导见：<a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODA3MDkyMA==&amp;mid=2247485148&amp;idx=1&amp;sn=4e6acc1b358e624ac2301729d35cc89d&amp;chksm=cf81f0bbf8f679ad3f990f91d5c84fcc67cc6948c3cbc0c34c59874f7795e9263f2382979414&amp;token=400584181&amp;lang=zh_CN#rd">NeRF入门之体渲染 (Volume Rendering) (qq.com)</a> / <a href="https://www.bilibili.com/video/BV1oM4m1U7qP/?spm_id_from=333.788&amp;vd_source=124ec79ebd3e16b0f454a3994a468f98">【较真系列】讲人话-3d gaussian splatting全解(原理+代码+公式)【3】 雪球颜色_哔哩哔哩_bilibili</a>： <span class="math display">\[\begin{align}&amp;\hat{C}(\mathbf{r})=\sum_{i=1}^{N} T_{i}\alpha_i \mathbf{c}_{i},\\&amp;\text { where } \  T_{i}=\exp \left(-\sum_{j=1}^{i-1} \sigma_{j} \delta_{j}\right)= \prod^{i-1}_{j=1}{(1-\alpha_i)},\\&amp; \ \ \ \ \ \ \ \ \ \ \ \ \ \alpha_i= 1-\exp \left(-\sigma_{i} \delta_{i}\right),\tag{1}\label{eq1}\end{align}\]</span> 这里的</p><ul><li><span class="math inline">\(\sigma_i\)</span>可以理解为是第 i 个采样点处粒子的密度，或光线碰击粒子（光线被粒子阻碍）的概率密度</li><li><span class="math inline">\(\delta_i\)</span>是相邻采样点的距离</li><li><span class="math inline">\(c_i\)</span>表示采样点处粒子发射的光线强度 (也可以理解为颜色)</li><li><span class="math inline">\(T_i\)</span>则表示从第 i 个采样点到光线终点的透射比，或在该点前光线没有被阻碍的概率</li></ul><p>从公式可知一共采样了<span class="math inline">\(N\)</span>个点。公式计算结果<span class="math inline">\(\hat{C}(\mathbf{r})\)</span>就是这条光线最终渲染得到的像素值。</p><p>以上就是体渲染的精髓了。<strong>在传统的计算机图形学中，我们需要先知道整个场景中，每条光线上的每个采样点的粒子状态</strong> (即<span class="math inline">\(\sigma_i\)</span>这些数值，在NeRF中是颜色<span class="math inline">\(RGB\)</span>和密度<span class="math inline">\(\sigma\)</span>)，<strong>才能渲染出整个画面</strong>。但如何计算这些粒子的状态，本身是一件很复杂的事情。</p><p>那 NeRF 做了什么呢？</p><p>没错，它就是用来计算这些数值的。</p><h1 id="神经辐射场">神经辐射场</h1><p>在 NeRF 中，作者做了一个这样的尝试，既然体渲染中这些粒子状态这么复杂，那有没有可能让神经网络自己去学出来呢？</p><p>事实证明，这是可以的，而且效果相当好。</p><p>这个过程是这样的：</p><p><strong>给定相机位置和朝向后，我们可以确定出当前的成像平面。然后，将相机的位置坐标和平面上的某个像素相连，就确定了一条光线 (也即确定了光线的方向)。接着用网络预测出光线上每个采样点的粒子信息，就可以确定像素颜色。这个过程重复下去，直到每个像素都渲染完为止。</strong></p><p>这些排列整齐的光线，构成了类似磁场一样的东西，而光线本身就是一种辐射，因此叫辐射场。而每条光线上的粒子信息又都是由神经网络预测的，因此作者又给整个过程命名为<strong>神经辐射场</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf7.gif" /></p><h1 id="nerf-全貌">NeRF 全貌</h1><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf8.png" /></p><blockquote><p>NeRF 的训练流程图</p></blockquote><p><span style="background:#dad5e9;"><strong>NeRF 的整个模型可以被概括为以下函数：</strong></span> <span class="math display">\[f:(x,y,z,\theta,\phi) \rightarrow (c,\sigma)\]</span> 首先，选择一个特定的场景 (比如上图的乐高玩具)。</p><p>然后，在这个场景的四周摆放一些相机，并确定好相机位置<span class="math inline">\((x,y,z)\)</span>和光线方向 <span class="math inline">\((\theta,\phi)\)</span>。</p><p>沿着光线方向乘以不同的采样距离，可以确定光线上每个采样点的位置。<strong>注意，此时每个采样点对应的方向向量会转换成笛卡尔坐标系 (x', y', z')</strong>。</p><p>将相机位置以及方向向量送入网络后，让网络预测出光线上采样点的粒子信息<span class="math inline">\((RGB,\sigma)\)</span>，根据公式 <span class="math inline">\(\eqref{eq1}\)</span> 渲染出整个画面。</p><p>将模型渲染的结果和相机捕捉的真实结果计算损失 (均方误差)，由于公式<span class="math inline">\(\eqref{eq1}\)</span>是可导的，因此梯度可以正常回传，从而训练整个网络。</p><p>在预测的时候，我们直接将相机位置等参数输入网络，让网络计算出每根光线上的粒子信息后，便可以渲染出任意视角下的画面了。</p><p>以上便是 NeRF 的整个算法流程，是一种很经典的<strong>神经渲染</strong>方法。</p><blockquote><p><span style="background:#daf5e9;"><strong>需要注意一点，原始的 NeRF 只能针对一个三维场景进行重建</strong></span>，之后有不少论文对这一点做了改进，我们以后再聊。</p></blockquote><p>下面再介绍一下 NeRF 所使用的网络结构，以及其中用到的两个技巧。</p><h1 id="网络结构">网络结构</h1><p>NeRF 的网络结构是一个很简单的全连接网络。</p><p>作者把密度<span class="math inline">\(\sigma\)</span>和颜色<span class="math inline">\(RGB\)</span>分为两部分输出，这么做的考量在于，粒子密度是跟三维场景本身更强相关的属性，不管观察的方向怎么变，它都不会有太大的变化 (即论文中提到的 <u>multiview consistent</u>)，而颜色值在不同观察方向下，受光照影响，可能会发生大的变动，即它受相机位置和观察方向的影响都更大。</p><p>基于此，论文先把相机位置<span class="math inline">\((x,y,z)\)</span>经过位置编码 Positional encoding (下文会提及) 后拓展成 60 维的向量，送入一堆全连接网络 (FC 层, Full Connect) 后，分叉出两路，其中一路经过一个 FC 层和 ReLU 激活后输出<span class="math inline">\(\sigma\)</span>值；另一路和方向向量经过位置编码的向量 concat 后，一起送入之后的全连接网络，最后经过 Sigmoid 激活输出颜色值 <span class="math inline">\(RGB\)</span>。</p><blockquote><p>相机位置向量 <span class="math inline">\(\gamma(x)\)</span> 和方向向量 <span class="math inline">\(\gamma(d)\)</span> 都是三维。由于位置编码选择的参数<span class="math inline">\(L\)</span>不同，因此位置向量 <span class="math inline">\(\gamma(x)\)</span> 经过 positional encoding 后是 60 维，方向向量 <span class="math inline">\(\gamma(d)\)</span> 在 encoding 后则是 24 维。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf9.png" /></p><p>网络输出的<span class="math inline">\(\sigma\)</span>和<span class="math inline">\(RGB\)</span>就是一条光线上一个采样点对应的粒子密度和颜色值。在整个训练过程中，我们会收集所有光线上所有采样点的粒子密度和颜色，再根据体渲染得到预测的投影视图，然后和真实的投影视图计算 loss 来训练网络。</p><p>不过，仅靠这个网络训练的结果并不够逼真，论文使用了两个 trick 进一步优化。</p><h1 id="trick-1-位置编码-positional-encoding">trick 1: 位置编码 positional encoding</h1><p>第一个 trick 是位置编码 (positional encoding)。</p><p>它来自之前一些研究的实验发现：<strong>如果给网络输入的数据维度越高 (越高频)，那网络也能输出更加高频的信号 (即图像清晰度越好)</strong>。所谓“高频”可以理解为<u>参数的微小变化可导致编码结果的巨大差异</u>。</p><p><strong>由于<span class="math inline">\((x,y,z)\)</span>这类位置和方向向量只有三维</strong>，如果直接将它们投喂给网络，那网络也只能回馈给你低维度的信号，而如果能把输入拓展成更高维，那网络的输出信号会包含更高维的信息，图像内容会更丰富。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf10.png" /></p><blockquote><p>图：使用 positional encoding 的结果对比</p></blockquote><p>论文使用三角函数对输入向量的每个元素进行扩展 <span class="math display">\[\gamma(p)=\left(\sin \left(2^{0} \pi p\right), \cos \left(2^{0} \pi p\right), \cdots, \sin \left(2^{L-1} \pi p\right), \cos \left(2^{L-1} \pi p\right)\right)\tag{2}\]</span> 这个公式可以使用以下 pytorch 代码实现：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">positional_encoding</span>(<span class="params">inputs, L=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    inputs: 输入向量，包含(x,y,z)三个坐标</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    L = <span class="number">10</span></span><br><span class="line">    <span class="comment"># freq_bands: [2^0, 2^1, ..., 2^(L-1)]</span></span><br><span class="line">    freq_bands = <span class="number">2</span> ** torch.linespace(<span class="number">0</span>, L-<span class="number">1</span>, L)</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> freq <span class="keyword">in</span> freq_bands:</span><br><span class="line">        <span class="comment"># [sin(2^f \pi x), sin(2^f \pi y), sin(2^f \pi z)]</span></span><br><span class="line">        outputs += [torch.sin(freq * inputs)]</span><br><span class="line">        <span class="comment"># [cos(2^f \pi x), cos(2^f \pi y), cos(2^f \pi z)]</span></span><br><span class="line">        outputs += [torch.cos(freq * inputs)]</span><br><span class="line">    <span class="comment"># [sin(2^0 \pi x), sin(2^0 \pi y), sin(2^0 \pi z), cos(2^0 \pi x), cos(2^0 \pi y), cos(2^0 \pi z), ...]</span></span><br><span class="line">    outputs = torch.cat(outputs, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><p>对于位置向量<span class="math inline">\((x,y,z)\)</span>，论文选取的<span class="math inline">\(L=10\)</span>，即每个元素会拓展成 20 维，所以在之前的网络输入中，输入是 3*20 即 60 维的向量。而方向向量的<span class="math inline">\(L=4\)</span>，每个元素会拓展为 8 维，对应的位置编码 positional encoding 则是 24 维 (3*8)。</p><p>NeRF 采用的位置编码和 transformer 中的位置编码几乎一样，不过二者的目的不同：transformer 的位置编码是为了给模型提供序列中每个元素的位置信息，而 NeRF 纯粹是为了帮助网络拟合更高清的图像。</p><h1 id="trick-2-分层采样-hierarchical-sampling">trick 2: 分层采样 hierarchical sampling</h1><p>分层采样 (hierarchical sampling) 是另一个重要的 trick。</p><p>前面提到，NeRF 会对每根光线进行采样，然后用网络对每个采样点进行预测。而由于资源的限制，采样不可能做到很密集。<strong>在实际情况中，粒子在空间中的分布也是不均匀的，有些采样点可能粒子密度很高，有些则密度几乎为 0</strong>。</p><p>因此，在密度高的地方多放一些采样点是比较合适的，即做<strong>重要性采样 importance sampling</strong>。论文为此设计了一种由粗到细的分层采样方法：</p><ol type="1"><li>先在每条光线上均匀采样 64 个点，让网络预测其体密度；</li><li>然后根据“密度大的地方多采点”的想法，再次采样，并计算预测密度；</li><li>两轮预测的密度结果分别计算 loss.</li></ol><p>具体实施流程：</p><p>首先是<strong>粗粒度采样</strong>。论文在每根光线上<u>均匀采样</u> 64 个点，然后让网络预测出每个点的信息，包括颜色值 <span class="math inline">\(c_i\)</span> 、粒子密度 <span class="math inline">\(\sigma_i\)</span> 和透射比 <span class="math inline">\(T_i\)</span> 。其中，<span class="math inline">\(\sigma_i\)</span> 和 <span class="math inline">\(T_i\)</span> 可以整合成一个权重 <span class="math inline">\(w_i\)</span> : <span class="math display">\[\hat{C}_{c}(\mathbf{r})=\sum_{i=1}^{N_{c}} w_{i} c_{i}, \quad w_{i}=T_{i}\left(1-\exp \left(-\sigma_{i} \delta_{i}\right)\right)\tag{3}\]</span></p><blockquote><p>体渲染公式可以理解为采样点颜色的累积。</p><p><strong><span class="math inline">\(w_i\)</span>的值也是先大后小</strong>：在靠近物体表面时，透射比 <span class="math inline">\(T_i\)</span>逐渐变小，<span class="math inline">\(\sigma_i\)</span>逐渐变大，但是整体看 <span class="math inline">\(w_i\)</span>是增大的，<strong>即越靠近表面越不被遮挡的点贡献越大</strong>；远离物体时同理。</p></blockquote><p>怎么理解这里的 <span class="math inline">\(w_i\)</span> 呢？光线最终的辐射强度 (或者说颜色) 其实是由光线上每个点的粒子辐射<strong>累加</strong>而成的。既然是累加，那就可以简单地把 <span class="math inline">\(w_i\)</span> 理解为是每个采样点的粒子对最终成像的贡献度，或者说粒子的浓度。 <span class="math inline">\(w_i\)</span> 大的地方，证明这个区域粒子很浓，信息量大，需要重点采样。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf11.png" /></p><blockquote><p>图：粗粒度均匀采样</p></blockquote><p>那下面就是在第一轮预测的基础上，找出 <span class="math inline">\(w_i\)</span> 比较大的地方再重点采样，也即<strong>细粒度采样</strong>。</p><p>这个过程是这样，先将 <span class="math inline">\(w_i\)</span> 归一化：$<em>{i}=w</em>{i} / <em>{j=1}^{N</em>{c}} w_{j} $ （<span class="math inline">\(N_c\)</span> 表示粗粒度采样的点数），这样 <span class="math inline">\(\hat{w}_{i}\)</span> 就可以视为一个概率分布了。然后，为了在 <span class="math inline">\(\hat{w}_{i}\)</span> 数值更大的地方重采样，我们可以把 <span class="math inline">\(\hat{w}_{i}\)</span> 累加起来，求出<code>概率累积直方图 (cdf)</code>，然后<u>在 cdf 的<strong>纵坐标</strong>上再均匀采样</u>，这样，大部分采样点会对应到 <span class="math inline">\(\hat{w}_{i}\)</span> 更大的直方图，这个直方图的横坐标就是细粒度采样点。这个过程也称为<code>逆变换采样 (inverse transform sampling)</code>。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf12.png" /></p><blockquote><p>图：逆变换采样</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf13.png" /></p><blockquote><p>在粗粒度采样的基础上进行细粒度采样</p></blockquote><h2 id="实现整个分层采样的过程">实现整个分层采样的过程</h2><h1 id="实验效果">实验效果</h1><p>NeRF 这类新视图生成任务的主要评价标准，就是看生成的图片逼真度如何。因此评价指标采用的是图像复原中常用的 PSNR 、 SSIM 和 LPIPS 等衡量方法。</p><p>下图是 NeRF 和之前几种方法的对比结果：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf14.png" /></p><p>NeRF 之所以诞生后受到很大的关注，就在于它相比之前的方法提升太多了，对于 PSNR 这类指标来说，能提升两三个点证明方法本身已经取得质的突破，更何况 NeRF 在有些数据集上提升了五六个点！</p><p>对比方法中的 LLFF 和 NeRF 是同一个作者，所以人家也是在这个领域默默耕耘了很久。</p><p>效果对比图就不用说了，秒杀般的存在：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf15.png" /></p><h2 id="深度图">深度图</h2><p>NeRF 有一个副产品是，它可以输出场景的深度图。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf1x.gif" /></p><p>深度图代表的是场景中的物体离相机的距离。NeRF 输出深度图的方法是把光线上每个采样点的权重 <span class="math inline">\(w_i\)</span> 根据距离的远近累加起来：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># weights是光线上各个采样点的权重，z_vals是采样点离相机的距离</span></span><br><span class="line">depth_map = torch.<span class="built_in">sum</span>(weights*z_vals, -<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="为什么会有这种效果">为什么会有这种效果？</h3><h1 id="nerf与三维重建">NeRF与三维重建</h1><p>NeRF 可以预测三维场景中每个视角的二维投影信息，因此，它本身也具备了整个三维场景的信息，可以认为是一种三维重建算法。</p><p>但它跟我们以往认知的三维重建又有所区别。</p><p>最关键的一点，<strong>它并没有显示地学习出整个三维场景的结构信息，而是把整个三维场景的信息编码进了神经网络中</strong>。这个神经网络建立了观察视角与三维模型之间的映射，它就像一个查找表，通过输入观察视角，找出对应视角下三维场景的粒子和反射光等信息。</p><p>如果想通过 NeRF 直接获得三维模型，抱歉，办不到，NeRF 只能给你二维投影，需要你自己用这些投影图像去合成真正的三维模型。</p><p>在 NeRF 中存储的是一种很像点云的「雾」，只有在你观察它的时候，它才会给你具体某个投影面的信息，从这个角度想，这团雾又很像是「量子态」的。</p><h1 id="nerf的缺陷">NeRF的缺陷</h1><p>NeRF 相比之前的工作，最大的优势就是渲染出来的图像清晰度更高，更真实。但它也存在几个严重的命门。</p><p><strong>首先，NeRF 的渲染速度极其慢</strong>。假设要渲染一张 1024x1024 的图片，且每根光线的采样点为 128，那总共需要跑 1024x1024x128=134217728 次网络。</p><p><strong>其次，NeRF 只适用于一个场景</strong>，如果换了别的场景，就得重新训练了，这泛化能力约等于没有。</p><p>不过，作为奠基之作，NeRF 已经开了个好头，剩下的问题自然有追随者帮忙解决。在之后的文章中，我们会逐一看到那些精妙的破解之法。</p><h1 id="代码">代码</h1><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf9.png" /></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NeRF</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, D=<span class="number">8</span>, W=<span class="number">256</span>, input_ch=<span class="number">3</span>, input_ch_views=<span class="number">3</span>, output_ch=<span class="number">4</span>, skips=[<span class="number">4</span>], use_viewdirs=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(NeRF, self).__init__()</span><br><span class="line">        self.D = D</span><br><span class="line">        self.W = W</span><br><span class="line">        <span class="comment">## Position Encoding之后的 位置vector通道数（63）</span></span><br><span class="line">        self.input_ch = input_ch  </span><br><span class="line">        <span class="comment">## Position Encoding之后的 direction的vector通道数（27）</span></span><br><span class="line">        self.input_ch_views = input_ch_views</span><br><span class="line">        self.skips = skips   <span class="comment">## 在第4层有跳跃连接</span></span><br><span class="line">        self.use_viewdirs = use_viewdirs</span><br><span class="line">        </span><br><span class="line">        <span class="comment">## 前8层的MLP实现：输入为63，输出为 256</span></span><br><span class="line">        self.pts_linears = nn.ModuleList(</span><br><span class="line">            [nn.Linear(input_ch, W)] + [nn.Linear(W, W) <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> self.skips <span class="keyword">else</span> nn.Linear(W + input_ch, W) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(D-<span class="number">1</span>)])</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### 构建了第9层的输入为 第8层的输出 和 direction 进行concat,输出为128 维</span></span><br><span class="line">        self.views_linears = nn.ModuleList([nn.Linear(input_ch_views + W, W//<span class="number">2</span>)])</span><br><span class="line">   </span><br><span class="line">        <span class="keyword">if</span> use_viewdirs:</span><br><span class="line">            self.feature_linear = nn.Linear(W, W) <span class="comment"># 第9层 输出256维的向量</span></span><br><span class="line">            self.alpha_linear = nn.Linear(W, <span class="number">1</span>) <span class="comment"># 第9层输出 density alpha(1维)</span></span><br><span class="line">            self.rgb_linear = nn.Linear(W//<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.output_linear = nn.Linear(W, output_ch)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        input_pts, input_views = torch.split(x, [self.input_ch, self.input_ch_views], dim=-<span class="number">1</span>)</span><br><span class="line">        h = input_pts</span><br><span class="line">        <span class="keyword">for</span> i, l <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.pts_linears):</span><br><span class="line">            h = self.pts_linears[i](h)</span><br><span class="line">            h = F.relu(h)</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> self.skips:</span><br><span class="line">                h = torch.cat([input_pts, h], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.use_viewdirs:</span><br><span class="line">            alpha = self.alpha_linear(h)<span class="comment">#256—&gt;1</span></span><br><span class="line">            feature = self.feature_linear(h)<span class="comment">## 256—&gt;256</span></span><br><span class="line">            h = torch.cat([feature, input_views], -<span class="number">1</span>) <span class="comment">#第9层concat direction 向量</span></span><br><span class="line">        </span><br><span class="line">            <span class="keyword">for</span> i, l <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.views_linears):</span><br><span class="line">                h = self.views_linears[i](h)</span><br><span class="line">                h = F.relu(h)</span><br><span class="line"></span><br><span class="line">            rgb = self.rgb_linear(h)  <span class="comment">## 输出rgb 3维度向量 #128—&gt;3</span></span><br><span class="line">            outputs = torch.cat([rgb, alpha], -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            outputs = self.output_linear(h)<span class="comment">#256—&gt;4</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs   </span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="bib-citation">Bib Citation</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;mildenhall2020nerf,</span><br><span class="line"> title=&#123;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&#125;,</span><br><span class="line"> author=&#123;Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng&#125;,</span><br><span class="line"> year=&#123;2020&#125;,</span><br><span class="line"> booktitle=&#123;ECCV&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 新视图合成 </tag>
            
            <tag> implicit rendering </tag>
            
            <tag> ECCV2020 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>学术英语写作与沟通 笔记</title>
      <link href="/2024/07/26/%E5%AD%A6%E5%A0%82%E5%9C%A8%E7%BA%BF%20%E5%AD%A6%E6%9C%AF%E8%8B%B1%E8%AF%AD%E5%86%99%E4%BD%9C%E4%B8%8E%E6%B2%9F%E9%80%9A/"/>
      <url>/2024/07/26/%E5%AD%A6%E5%A0%82%E5%9C%A8%E7%BA%BF%20%E5%AD%A6%E6%9C%AF%E8%8B%B1%E8%AF%AD%E5%86%99%E4%BD%9C%E4%B8%8E%E6%B2%9F%E9%80%9A/</url>
      
        <content type="html"><![CDATA[<p>课程链接：https://www.xuetangx.com/learn/hfut05021002478/hfut05021002478/16907237/video/36265667</p><p><strong>本文只作学习交流使用！</strong></p><h1 id="ch1-title-author-affiliation-论文题目-作者姓名-单位">Ch1 Title + Author + Affiliation 论文题目 + 作者姓名 + 单位</h1><h2 id="how-to-write-the-title-of-academic-paper">1.1 How to Write the Title of Academic Paper</h2><p>Titles are succinct descriptive labels of texts and are meant to fulfifil different purposes, such as to individualize a publication, summarize its content and appeal to its audience among others. They are ideally relevant to present the content of a study and, in general, they are self-explanatory to their readers. This topic will cover 5 sections; they are function, basic requirements, classification, syntax and tips.</p><h2 id="ex-1.1">ex 1.1</h2><p>1.Which type does this title belong to?</p><p>Effect of non-pharmaceutical interventions to contain COVID-19 in China</p><p><font color=red>Noun phrase title </font></p><p>2.Analyze the grammatical construction of the two following titles from top academic journals?</p><p>An investigation of transmission control measures during the first 50 days of the covid-19 epidemic in China ------ From <em>Science</em></p><p><font color=red>Nominal group construction</font></p><h2 id="author-affiliation">1.2 Author + Affiliation</h2><p>Author and affiliation provide important information of a published paper so we need to write them correctly in English. This section will talk about how to write author and affiliation for an academic paper from four aspects, namely definition, function, layout and writing tips.</p><h2 id="ex-1.2">ex 1.2</h2><p>1.Affiliations lie just above authors’ names in published papers, which usually contain such information as authors’ institutions or addresses.</p><p><font color=red>false</font></p><p>2.Corresponding author has the authority to act on behalf of all authors and is the contact person for the research paper.</p><p><font color=red>true</font></p><p>3.In terms of author’s affiliated information, we tend to put bigger unit first then followed by smaller one.</p><p><font color=red>false</font></p><p>4.We usually use the word “and” or the sign “&amp;” to connect the last two authors’ names.</p><p><font color=red>true</font></p><h1 id="ch2-outline-大纲">Ch2 Outline 大纲</h1><h2 id="ex-2">ex 2</h2><p>1.Which of the following is not true about the reverse outlining?</p><p><font color=red>It is useful only when your paper focuses on complex issues in detail.</font></p><p>2.Analyze the sample outline, and try to tell the type of the sample outline.</p><p><strong>Thesis:</strong> Explorers who went to conquer Mt. Everest have achieved much in many ways, but their expeditions also have exerted impacts, negative as well as positive, on Mt. Everest and the local community.</p><ol type="I"><li><pre><code>Background Information</code></pre></li></ol><p>​ A. Location of Mt. Everest</p><p>​ B. Geography of the Surrounding Area</p><p>​ C. Facts about Mt. Everest</p><p>​ 1. Height of the Mountain</p><p>​ 2. How the Mountain Was Named</p><p>​ a. Peak XV</p><p>​ b. Jomolungma (Tibetan name)</p><p>​ c. Sagarmatha (Nepalese name)</p><p>​ 3. The Number of People Who Have Climbed Everest to Date</p><ol start="2" type="I"><li><pre><code>Major Explorers Covered in this Paper</code></pre></li></ol><p>​ A. Sir Edmund Hillary</p><p>​ 1. First to Reach the Summit (1953)</p><p>​ 2. Leader of a Team of Experienced Mountain Climbers Who Worked Together</p><p>​ B. Tenzing Norgay and the Sherpas</p><p>​ 1. Norgay the Experienced Climber and Guide Who Accompanied Hillary</p><p>​ 2. Sherpas Still Used to Guide Expeditions</p><p>​ C. Rob Hall</p><p>​ 1. Leader of the Failed 1996 Expedition</p><p>​ 2. Leading Group of Tourists with Little Mountain Climbing Experience</p><ol start="3" type="I"><li><pre><code>The Impact Expeditions have had on Mt. Everest and Local Community</code></pre></li></ol><p>​ A. Ecological Effects</p><p>​ 1. Loss of Trees Due to High Demand for Wood for Cooking and Heating for Tourists.</p><p>​ 2. Piles of Trash Left by Climbing Expeditions</p><p>​ B. Economic Effects</p><p>​ 1. Expedition Fees Providing Income for the Country</p><p>​ 2. Expeditions Providing Work for the Sherpas, Contributing to the Local Economy.</p><p>​ C. Cultural Effects</p><p>​ 1. Introduction of Motor Vehicles</p><p>​ 2. Introduction of Electricity</p><p><font color=red>The topic outline</font></p><h2 id="importantce-of-an-outline-in-reading-and-writing">Importantce of an outline in reading and writing</h2><p><em>Outline can help the paper readers sort out the important and unimportant details, so that they can read more effectively.</em></p><p><em>When writing, the authors tend to use brainstorming, a useful tool for generating ideas in a free thinking way. Then outlining can help organize all the generated ideas, and help the authors prioritize the important information and eliminate the trivial details.</em></p><p><em>Therefore, although outline is not a part of your paper, it is an important tool to facilitate your academic career.</em></p><h2 id="outline-巩固题">outline 巩固题</h2><p>1.A thesis statement should be _____.</p><p><font color=red>a sentence</font></p><p>2.What is the role of thesis statement in an outline?</p><p><font color=red>the writer’s position about the topic</font></p><h1 id="ch3-abstract-摘要">Ch3 Abstract 摘要</h1><h2 id="ex-3.1">ex 3.1</h2><p>1.An abstract is a ____ summary of your published or unpublished research paper, usually about a paragraph long.</p><p><font color=red>short</font></p><p>2.An abstract lets readers get the gist or essence of your paper or article quickly, in order todecide whether to read the ____ paper.</p><p><font color=red>full</font></p><p>3.An abstract is a short statement about a paper designed to give a complete yet concise understanding of its research and finding.</p><p><font color=red>true</font></p><p>4.An abstract should include a ____ but ____ statement of the problem or issue, a description of the research method and design, the major findings and their significance, and the conclusion.</p><p><font color=red>brief</font> <font color=red>precise</font></p><p>5.The function of an abstract is to ___ .</p><p><font color=red>let the reader understand the main content of the paper </font></p><p><font color=red>provide convenience for the construction and maintenance of scientific and technological literature retrieval database</font></p><p>6.The features of an abstract is ___ .</p><p><font color=red>concise、objective、consistent、complete</font></p><h2 id="ex-3.2">ex 3.2</h2><p>1.An abstract prepares readers to follow the ____ information, analyses, and arguments in your full paper.</p><p><font color=red>detailed</font></p><p>2.An abstract helps readers remember ____ from your paper.</p><p><font color=red>key points</font></p><p>3.An abstract is often composed of____.</p><p><font color=red>Background or purpose、Methods、Results、Conclusion</font></p><p>4.The premises, objectives and tasks of the research work, and the scope of the topics covered will be introduced in the abstract.</p><p><font color=red>true</font></p><p>5.Methods are going to define how do you get answers to your research question, so ____, conditions, ____, means, ____, procedures employed in the paper will be illustrated define what material, what method, and what procedures are to be used.</p><p><font color=red>theories、materials、equipment</font></p><p>6.You may choose ____ as keywords.</p><p><font color=red>3-5 highly relevant terms</font></p><h2 id="samples-of-abstract">Samples of Abstract</h2><h3 id="descriptive-abstract"><strong>Descriptive abstract</strong></h3><p>Translation is not only a linguistic transference, but also an intercultural communication. For quite a long time, translation studies have been concentrated on the prescription of translation methods, with scant attention paid to the description of macro-cultural factors involved in the translating. In this paper, the writer contends that the study of the macro-cultural factors will surely enlarge the scope and enrich the content of the translation studies. The paper is largely a rudimentary step, both in theory and in practice, to expose some of the factors influencing Mr. Fu’s translation of <em>Gone with the Wind</em>, with a historic and descriptive approach employed.</p><h3 id="informative-abstract"><strong>Informative abstract</strong></h3><p>This study investigated the role of "signaling" in helping good readers comprehend expository text. As the existing literature on signaling, reviewed in the last issue of the Journal, pointed to deficiencies in previous studies' methodologies, one goal of this study was to refine prose research methods. Two passages were designed in one of eight signaled versions each. The design was constructed to assess the individual and combined effect of headings, previews, and logical connectives. The study also assessed the effect of passage length, familiarity and difficulty. The results showed that signals do improve a reader's comprehension, particularly comprehension two weeks after the reading of a passage and comprehension of subordinate and superordinate inferential information. This study supports the hypothesis that signals can influence retention of text-based information, particularly with long, unfamiliar, or difficult passages.</p><h1 id="ch4-data-collection-and-evaluation-数据收集与评价">Ch4 Data Collection and Evaluation 数据收集与评价</h1><h2 id="ex-4">ex 4</h2><p>1.There are ____ formats of data collection and evaluation.</p><p><font color=red>3</font></p><p>2.When introducing background information, if the content of a sentence is a general fact that is not affected by time, apply_____.</p><p><font color=red>the present tense</font></p><p>3.Data evaluation refers to the process of ____.</p><p><font color=red>critically reading and evaluating the sources</font></p><p>4.The titles of tables or diagrams are usually expressed in ____.</p><p><font color=red>phrases</font></p><h2 id="tips-for-data-collection-and-evaluation">Tips for Data Collection and Evaluation</h2><h3 id="i.-data-collection"><strong>I</strong>. <strong>Data Collection</strong></h3><p><strong>Collecting primary sources</strong></p><p><strong>Texts</strong>: Once the main arguments are in mind, the text should be re-read while highlighting, and underlining, scribbling in the margins, or using sticky notes to pick out what is needed.</p><p><strong>Interviews</strong>: Locate someone through friends and family networks.</p><p><strong>Collecting secondary sources</strong></p><p>Searching the Web for government documents. Government records may be helpful but in most cases secondary-source research begins at the library.</p><p>The library is the main source of data collection, where readers can access the relevant literature, books, periodicals, audio and video recording, radio, television and the Internet.</p><p><strong>As to the types of information</strong>, in the library there are data in print and electronic resources, including encyclopedias, almanacs (年鉴)，indexes (索引), abstracts, dictionaries, reference literature, compilations, bibliographic catalogs, online databases, web sites, online communities, search engines and so on.</p><p>Researchers often follow the following four steps in collecting data from the library:</p><p>l Search encyclopedias.</p><p>l Search biographical references, yearbooks, atlases（地图册），gazettes（公报，报纸）and professional dictionaries relevant to the topic.</p><p>l Search library catalogs to find appropriate books. Search by author, title or subject.</p><p>l Retrieve journal articles citation from journals or newspapers index with the relevant terms found in encyclopedias, dictionaries and other reference directories (目录).</p><h3 id="ii.-data-evaluation"><strong>II. Data Evaluation</strong></h3><p>Data evaluation refers to the process of critically reading and evaluating the sources. The gist of being critical is not just to criticize, but to question and, not take anything read at face value.</p><p>Structure, purpose, audience and author are four important dimensions of the text to pay close attention to in critical thinking and reading.</p><p><strong>Structure</strong></p><p>If starting with a book, look at the table of contents. See the shape of what is to come and identify places where the thesis or question might be most directly addressed. Notice the subsections. Is there anything very obviously missing?</p><p>Glance at any appendices, diagrams, tables, or figures and see what kinds of things make it into the Endnotes section if there is one.</p><p>Look at the topics listed in the Index at the back. Which of the entries has the most page numbers listed next to it? This will give you an indication of the subjects that contribute to the real scope of the book.</p><p><strong>Purpose</strong></p><p>Examine the title and first few paragraphs. What is the author trying to do? What is his or her bias? Any assumptions to be challenged?</p><p><strong>Audience</strong></p><p>Who is the intended audience? How narrow or broad is it? To answer this, look at stylistic choices such as diction and tone. Who is the target audience?</p><p><strong>Author</strong></p><p>Who is the author? Is it someone a professor has mentioned or one that you come across in the course of other reading? Has the person been mentioned in other texts or bibliographies of other texts? Is the person a teacher or researcher from a reputable academic institution?</p><p>Does the person have considerable knowledge of what he or she is talking about? Is the author respected and well-received（深受好评的）?</p><p><strong>Evaluating Web Pages</strong></p><p>Authorship</p><p>Who wrote this?</p><p>The publishing body</p><p>Is the name of any organization given on the document? Are there headers (页眉) , footers, or a distinctive watermark that show the document to be part of an official academic or scholarly web site?</p><p>Point of view or bias</p><p>Referral to or display of knowledge of the literature</p><p>Accuracy or verifiability of details</p><p>Currency: the date of publication</p><p>All information needs to be evaluated by readers for authority, appropriateness and other personal criteria for value. Never use information that cannot be verified. Establishing and learning criteria to filter information found on the Internet is a good beginning for becoming a critical consumer of information in all forms.</p><p>Learn to be skeptical and then learn to trust your instincts.</p><h2 id="useful-expressions-and-sentence-patterns">Useful Expressions and Sentence Patterns</h2><ol type="1"><li><h3 id="useful-expressions">Useful Expressions</h3></li></ol><ol type="1"><li><p>开头 图表类型：table（表格）、chart（图表）、graph（多指曲线图）、diagram（图标）、column chart（柱状图）、pie graph（饼图）、tree diagram（树形图） 描述：show, describe, illustrate, can be seen from, clear, apparent, reveal, represent 内容：figure, statistic, number, percentage, proportion</p></li><li><p>表示数据变化的单词或者词组 rapid/rapidly 迅速的/地，飞快的/地，险峻的/地 dramatic/dramatically 戏剧性的/地</p></li></ol><p>significant/significantly 有意义的/地，重大的/地 sharp/sharply 锐利的/地，急剧的/地 steep/steeply 急剧升降的/地</p><p>gradual/gradually 渐进的/地，逐渐的/地 slow/slowly 缓慢的/地 slight/slightly 稍微的/略微地</p><p>stable/stably 稳定的/地</p><p>steady/steadily 稳固的/地，坚定不移的/地</p><ol start="3" type="1"><li>其它在描述中的常用到的词</li></ol><p>grow 增长 distribute 分布 unequally 不相等地</p><p>measure n. 方法，措施 v. 估量，调节 forecast n.先见，预见 v. 猜测</p><p>significant changes 图中一些较大变化 noticeable trend 明显趋势 during the same period 在同一时期 in the case of 在……的情况下 in terms of/in respect of/regarding 在……方面 in contrast 相反，大不相同</p><ol start="2" type="1"><li><h3 id="useful-sentence-patterns">Useful Sentence Patterns</h3></li></ol><ol type="1"><li><p>The table shows the changes in the number of…over the period from…to…. 该表格描述了在……年至……年间……数量的变化。</p></li><li><p>The bar chart illustrates that…. 该柱状图展示了……</p></li><li><p>The graph provides some interesting data regarding…. 该图为我们提供了有关……有趣数据。</p></li><li><p>The diagram shows（that）…. 该图向我们展示了……</p></li><li><p>The pie graph depicts （that）…. 该圆形图揭示了……</p></li><li><p>This is a cure graph which describes the trend of…. 这个曲线图描述了……的趋势。</p></li><li><p>The figures/statistics show （that）…. 数据（字）表明……</p></li><li><p>The tree diagram reveals how…. 该树型图向我们揭示了如何……</p></li><li><p>The data/statistics show （that）…. 该数据（字）可以这样理解……</p></li><li><p>The data/statistics/figures lead us to the conclusion that….</p></li><li><p>From the table/chart/diagram/figure，we can see clearly that……or it is clear/apparent from the chart that…. 从图表我们可以很清楚（明显）看到……</p></li><li><p>This is a graph which illustrates…. 这个图表向我们展示了……</p></li><li><p>This table shows the changing proportion of a and b from……to…. 该表格描述了……年到……年间a与b的比例关系。</p></li><li><p>The graph，presented in a pie chart，shows the general trend in…. 该图以圆形图形式描述了……总的趋势。</p></li><li><p>This is a column chart showing…. 这是一个柱型图，描述了……</p></li><li><p>As can be seen from the graph，the two curves show the fluctuation of…. 如图所示，两条曲线描述了……的波动情况。</p></li></ol><h1 id="ch5-summary-概要">Ch5 Summary 概要</h1><h2 id="ex-5">ex 5</h2><p>1.A good summary is an accurate reflection of the author’s viewpoint.</p><p><font color=red>true</font></p><p>2.When you write a summary, you need to________.</p><p><font color=red>Write in your own words</font></p><p>3.How would you avoid plagiarism when you want to cite source materials?</p><p><font color=red>Summarize、Quote、Paraphrase</font></p><p>4.Most summaries start with a sentence containing two elements:____ and ____.</p><p><font color=red>the source、the main idea</font></p><h1 id="ch6-literature-review-文献综述">Ch6 Literature Review 文献综述</h1><h2 id="ex-6.1">ex 6.1</h2><p>1.Is it plagiarism if I use big parts of someone’s literature review for the introduction section of my paper?</p><p><font color=red>Yes</font></p><p>2.The most important reason for you to write literature review in an article is to ____.</p><p><font color=red>find a gap or something contradictory in the previous studies to justify your research</font></p><p>3.Which of the following title looks like a literature review?</p><p><font color=red>Background subtraction techniques: a review.</font></p><h2 id="summarize-vs-synthesize">Summarize VS Synthesize</h2><p><strong>Summarizing and synthesizing information from multiple sources is an indispensable step for writing a literature review.</strong> <strong>But are you really clear about their differences?</strong></p><ul><li><p><strong>A summary reiterates what the study is about.</strong></p><p><strong>Summary is important in a literature review because some research may not be familiar to the reader. It helps the reader develop an understanding of the subject matter. But a literature review goes beyond retelling what the data points out.</strong></p></li><li><p><strong>Synthesis pulls several sources together and explains through the writer’s words what his interpretation of the data means to the writer in his own voice.</strong></p><p><strong>For example, you have five research studies and they all point to the same conclusion.</strong></p></li></ul><h2 id="ex-6.2">ex 6.2</h2><p>1.We can use past tense when describing an action in a research beginning in the past and continuing to the present.</p><p><font color=red>false</font></p><p>2.The literature review is a synthesis and analysis of research on your topic in your own words. Most ideas can be and should be paraphrased. Paraphrase is a preferred choice over direct quotations on most occasions.</p><p><font color=red>true</font></p><h1 id="ch7-proposal-开题报告">Ch7 Proposal 开题报告</h1><h2 id="ex-7.1">ex 7.1</h2><p>1.The research proposal can __________.</p><ul><li><font color=red> clearly and systematically present the research problem and objective</font></li><li><font color=red>indicate the significance and introduce the specific methodology and research procedures synthesize current knowledge, seek gaps and formulate a plan to address the problem</font></li><li><font color=red>synthesize current knowledge, seek gaps and formulate a plan to address the problem</font></li><li><font color=red>provide a timetable for the study and a budget of the investigation or experiments</font></li></ul><p>2.A research proposal is intended to_________.</p><ul><li><font color=red>convince the readers that you are ready to do a research</font></li><li><font color=red>demonstrate that you have the knowledge, full understanding and the expertise to complete the project</font></li><li><font color=red>show your competency in a particular area of study</font></li><li><font color=red>serve as a planning tool</font></li></ul><p>3.The main elements of a research proposal include __________.</p><p><font color=red>what、why、how、expected result</font></p><p>4.Examine carefully the following to determine to what extent the topic chosen meets the criteria for a proposal:</p><ul><li><font color=red>It must be interesting to you.</font></li><li><font color=red>It must be within your competence.</font></li><li><font color=red> It must be feasible.</font></li><li><font color=red>It must be sufficiently delimited.</font></li><li><font color=red>It must have the potential to make a contribution to knowledge or practice in the appropriate area.</font></li></ul><p>5.<strong>The potential supervisors use research proposals to assess</strong></p><ul><li><font color=red>The quality and originality of your ideas</font></li><li><font color=red>Your skills in critical thinking</font></li><li><font color=red>The feasibility of the research project</font></li></ul><h2 id="ex-7.2">ex 7.2</h2><p>1.The budget of a research proposal consists of __________.</p><p><font color=red>a section details the amount of cost of equipment and service</font></p><p><font color=red>a section provides the justification for the funding requested for reviewers to check and see if it is reasonable</font></p><p>2.A thesis statement is a simple sentence that formulates both your topic and _________ toward it.</p><p><font color=red>point of view</font></p><p>3.In exploring data for a research proposal, sources can be divided into ____________.</p><p><font color=red>primary source and secondary source</font></p><p>4.Sometimes the literature review section is incorporated into ( ) section.</p><p><font color=red> Introduction</font></p><p>5.Most students’ literature reviews suffer from the following problems:</p><ul><li><font color=red>Lacking organization and structure</font></li><li><font color=red>Lacking focus, unity and coherence</font></li><li><font color=red>Failing to cite influential papers</font></li><li><font color=red>Failing to critically evaluate cited papers</font></li><li><font color=red>Depending too much on secondary sources</font></li></ul><h2 id="ex-7.3">ex 7.3</h2><p>1.In a successful writing of a research proposal, we should __________.</p><ul><li><font color=red>use accepted scientific terms</font></li><li><font color=red> try to use full forms instead of abbreviations and avoid contractions</font></li><li><font color=red>use formal word and phrases instead of nonstandard or informal expressions</font></li><li><font color=red>use impersonal expressions and passive voice in a proper manner</font></li></ul><p>2.In proposal, the discussion section will include:</p><ul><li><font color=red>An analysis of sources of error in the data</font></li><li><font color=red>Integration with what was previously known</font></li><li><font color=red>Implications for future study</font></li></ul><h1 id="ch8-how-to-write-a-research-paper-introduction">Ch8 How to Write a Research Paper Introduction</h1><h2 id="ex-8">ex 8</h2><p>1.Generally speaking, what will be covered in Introduction Part?</p><ul><li><font color=red>Significance and necessity of the study</font></li><li><font color=red>Background, scope of the issue being study </font></li><li><font color=red>Clear definitions of key term involved in the study</font></li><li><font color=red>Theoretical foundations for the study</font></li><li><font color=red>Objectives of the present study</font></li><li><font color=red>Brief literature review and comment on the previous studies</font></li></ul><h1 id="ch9-material-and-methods-材料与方法">Ch9 Material and Methods 材料与方法</h1><h2 id="ex-9">ex 9</h2><p>1.Among the following research methods, which one is cheaper and easier?</p><p><font color=red>Opinion-based research method.</font></p><p>2.Compared with qualitative research method, quantitative research method_________.</p><p><font color=red>focuses on specific and narrow area.</font></p><p>3.In the materials and methods section, the ____ tense is more natural since you are describing work that is already completed at the time of writing, and the____ voice is preferable since this section focuses more on research than on researcher.</p><p><font color=red>past、passive</font></p><h1 id="ch10-resultsfindings-figures-tables-结果图表">Ch10 Results/Findings + Figures &amp; Tables 结果+图表</h1><p>The Results/Findings describes the statistical results/ findings of a research, which directly answers the research questions raised previously. It is important as it is the section where authors present new information and make new knowledge claims. The results/ findings can be given in the form of numerical data, verbal description or the combination of the above two.</p><h2 id="ex-10.1">ex 10.1</h2><p>1.In which form can the Results/ Findings section be given? ( )</p><p><font color=red>numerical data、verbal description、the combination of B and C</font></p><p>2.What are the three main functions of Results/ Findings section? ( )</p><p><font color=red>locating results、reporting results、explaining results</font></p><p>3.What are the three writing principles for the Results/ Findings section?</p><p><font color=red>faithfulness、innovation、generalization</font></p><p>4.How can the main findings be presented in the Results/ Findings section?</p><ul><li><font color=red>in a certain logical order</font></li><li><font color=red>in chronological order</font></li><li><font color=red>in order of importance</font></li></ul><h2 id="tips-for-writing-the-results-findings">Tips for Writing the Results/ Findings</h2><h3 id="structure-of-the-resultsfindings-section"><strong>1) structure of the Results/Findings section</strong></h3><p><strong>The first subdivision</strong> in this section is to provide preparatory information for the presentation of the results and it functions as a reminder and connector between the Methods section and the Results section. Authors often introduce source of data such as the type of data, the size of data, and the data collection method to prepare for the presentation of the significant results.</p><p><strong>The second subdivision</strong> in this section is to present results. Authors present the results of the study with relevant evidence such as statistics and examples. To report results is obligatory while to locate results and to explain results are optional.</p><h3 id="writing-principles-for-the-results-findings"><strong>2) writing principles for the Results/ Findings</strong></h3><p>There are some writing principles to follow here. They are faithfulness, innovation and generalization, or FIG.</p><p><strong>Principle of faithfulness.</strong> Whether research results can hold water depends on whether they can be tested repeatedly. Never add or delete the research results subjectively. Conflicting results sometimes lead to more meaningful further hypotheses and even more scientific conclusions. So be faithful in presenting your results.</p><p><strong>Principle of innovation.</strong> It is not necessary to cite too much of others' research results here. Therefore, when this section is written, the content of authors’ own original findings should be highlighted.</p><p><strong>Principle of generalization.</strong> Authors need to generalize essential facts and summarize key results in this section. No need to report all specific raw data here. State the main findings in a certain logical order, say, in chronological order or in order of importance.</p><h2 id="ex-10.2">ex 10.2</h2><p><em>Tables and figures are very important in academic writing and communication. They provide visual ways of presenting data and each type has its own advantages and disadvantages.</em></p><p>1.Tables and figures provide visual ways of presenting data and each type has its own advantages and disadvantages.</p><p><font color=red>true</font></p><p>2.Figures can display exact data or statistical information.</p><p><font color=red>false</font></p><p>3.There is no need to classify, process or select data in tables or figures.</p><p><font color=red>false</font></p><p>4.Which one is a flow chart?</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/ba2ccc22-193a-4164-8a21-f41d0087b960.png" alt="chart 4.png" /><figcaption aria-hidden="true">chart 4.png</figcaption></figure><h2 id="tips-for-various-tables-and-figures">Tips for Various Tables and Figures</h2><p>A table is an arrangement of data in rows and columns, or possibly in a more complex structure. Tables are widely used in communication, research and data analysis. They can display exact data or statistical information.</p><p>Figures have many various types. Here we just focus on pie chart, bar chart, line chart and flow chart.</p><ol type="1"><li>A pie chart is a circular statistical graphic which is divided into slices to illustrate numerical proportions. But sometimes it is not easy to compare different sections of a given pie chart or compare data across different pie charts.</li><li>A bar chart is a chart to present grouped data with rectangular bars whose lengths are proportional to the values they present. It can show comparisons among various categories.</li><li>A line chart displays information as a series of data points connected by straight line segments. It is often used to visualize a trend in data changing with the time or condition.</li><li>A flow chart is a diagram to show the order of operations or sequence of tasks for solving a problem or managing a complex project.</li></ol><h1 id="ch11-discussion-and-conclusion">Ch11 Discussion and Conclusion</h1><h2 id="ex-11.1">ex 11.1</h2><p>1.The results show that those who reported driving unsupervised 1-12 times were 80% more likely to be involved in crash than those who reported never having done this.</p><p><font color=red>Reporting results</font></p><p>2.The research has some potential limitations.</p><p><font color=red>Indicating limitations</font></p><p>3.We bring everything together in this part by discussing the ______ of our findings and its relationship to previous research in the area.</p><p><font color=red>significance</font></p><p>4.Each result reported should be followed by a proper ______.</p><p><font color=red>discussion</font></p><h2 id="tips-for-discussion">Tips for Discussion</h2><ol type="1"><li>summarized the main results.</li><li>interpreted (not described) the results.</li><li>discussed the significance of the results.</li><li>explained whether the results prove or disprove the hypothesis.</li><li>discussed the results in the light of previous research.</li><li>explained the wider implications of the work.</li><li>discussed any problems with or limitations of the study.</li><li>made suggestions for improvements.</li><li>suggested directions for future research.</li></ol><h2 id="ex-11.2">ex 11.2</h2><p>1.Given that a significant portion of learner license holders report driving unsupervised and those that violate this condition the most are more likely to crash, improving compliance with learner license supervised driving conditions varrants increased attention.</p><p><font color=red>Interpreting results</font></p><p>2.Evaluations of the initiatives designed to increase compliance supervised driving condition should be a research priority.</p><p><font color=red>Recommending further research</font></p><p>3.Though components of a conclusion may vary, a conclusion generally contains a restatement of the thesis in the ________, a summary of key points in the body, and a broad statement.</p><p><font color=red>introduction </font></p><p>4.This exploratory case study ________the stated beliefs and actual instructional practices of two experienced teachers of English language in a primary school in Singapore.</p><p><font color=red>investigated </font></p><h2 id="tips-for-conclusion">Tips for Conclusion</h2><p>The structure of a conclusion generally follows a pattern, moving from specific to general.</p><ol type="1"><li>Restatement of the main premise.</li><li>Summary of key points in the essay.</li><li>Broad statement.</li></ol><h1 id="ch12-how-to-write-acknowledgements-致谢">Ch12 How to Write Acknowledgements 致谢</h1><h2 id="ex-12">ex 12</h2><p>1.请翻译：国家自然科学基金</p><p><font color=red>National Natural Science Foundation of China</font></p><p>2.Whom will we thank in Acknowledgement Part?</p><ul><li><font color=red>Those who have helped the author's scientific research</font></li><li><font color=red>Reviewers and editors </font></li><li><font color=red>Funding / program</font></li></ul><h1 id="ch13-references-参考文献">Ch13 References 参考文献</h1><h2 id="ex-13">ex 13</h2><p>1.The functions of the references section include the following points except:</p><p><font color=red>It concludes the whole researc</font></p><p>2.The following two samples are documented in ____ style.</p><p>Wysocki, Anne Frances, et al. <em>Writing New Media: Theory and Applications for Expanding the Teaching of Composition</em>. Logan, UT: Utah State UP, 2004. Print.</p><p>Foucault, Michel. <em>Madness and Civilization: A History of Insanity in the Age of Reason</em>. Trans. Richard Howard. New York: Vintage-Random House, 1988. Print.</p><p><font color=red>MLA</font></p><p>3.Which of the following is a documentation tool?</p><p><font color=red>Latex、Endnote、Bibtex（Trados 是翻译软件）</font></p><h2 id="samples-of-apa-and-mla-style">Samples of APA and MLA style</h2><p>Anderson, A. K., Christoff, K., Panitz, D., De Rosa, E., &amp; Gabrieli, J. D. E. (2003). Neural correlates of the automatic processing of threat facial signals. <em>Journal of Neuroscience, 23,5627-5633.</em></p><p>Chow T. W., &amp; Cummings, J. L. (2000). The amygdala and Alzheimer<em>’</em>s disease. In J. P. Aggleton (Ed.), <em>The amygdala: A functional analysis</em> (pp. 656–680). Oxford,England: Oxford University Press.</p><p>Shipley, W. C. (1986). <em>Shipley Institute of Living Scale</em>. Los Angeles, CA: Western Psychological Services.</p><p>Wheeler, D.P., &amp; Bragin, M. (2007). Bringing it all back home: Social work and the challenge of returning veterans. Health and Social Wor, 32, 297-300. Retrieved from <a href="http://www/">http://www</a>. naswpressonline. org</p><p>Samples of MLA style</p><p>Wysocki, Anne Frances, et al. <em>Writing New Media: Theory and Applications for Expanding the Teaching of Composition</em>. Logan, UT: Utah State UP, 2004. Print.</p><p>Foucault, Michel. <em>Madness and Civilization: A History of Insanity in the Age of Reason</em>. Trans. Richard Howard. New York: Vintage-Random House, 1988. Print.</p><p>Crowley, Sharon, and Debra Hawhee. <em>Ancient Rhetorics for Contemporary Students</em>. 3rd ed. New York: Pearson/Longman, 2004. Print.</p><p><strong>The New Jerusalem Bible</strong>. Ed. Susan Jones. New York: Doubleday, 1985. Print.</p><p>Bishop, Karen Lynn. <em>Documenting Institutional Identity: Strategic Writing in the IUPUI Comprehensive Campaign</em>. Diss. Purdue University, 2002. Ann Arbor: UMI, 2004. Print.</p><p>Poniewozik, James. "TV Makes a Too-Close Call." <em>Time</em> 20 Nov. 2000: 70-71. Print.</p><p>Buchman, Dana. "A Special Education." <em>Good Housekeeping</em> Mar. 2006: 143-48. Print.</p><p>"Of Mines and Men." Editorial. <em>Wall Street Journal</em> east. ed. 24 Oct. 2003: A14. Print.</p><p>Hamer, John. Letter. <em>American Journalism Review</em> Dec. 2006/Jan. 2007: 7. Print.</p><p>Bagchi, Alaknanda. "Conflicting Nationalisms: The Voice of the Subaltern in Mahasweta Devi's Bashai Tudu." <em>Tulsa</em> <em>Studies in Women's Literature</em> 15.1 (1996): 41-50. Print.</p><p>Aristotle. <em>Poetics</em>. Trans. S. H. Butcher. <em>The Internet Classics Archive</em>. Web Atomic and Massachusetts Institute of Technology, 13 Sept. 2007. Web. 4 Nov. 2008. <a href="http://classics.mit.edu/Aristotle.html" class="uri">http://classics.mit.edu/Aristotle.html</a>.</p><h2 id="references-作业">references 作业</h2><p>1.If writers do not give credit for borrowed ideas or words, they make a serious error, i.e. ____.</p><p><font color=red>plagiarism</font></p><p>2.When you use a direct quotation in APA style, you must state the following elements except _____.</p><p><font color=red>the edition of the journal</font></p><h2 id="文献引用的三种格式">文献引用的三种格式</h2><p>文献引用一般有三种形式。（参考链接：<a href="https://blog.csdn.net/programmer_jiang/article/details/118557929#:~:text=Author%20Prominent%20Citation**：一般过去时**%20关注完成研究的人。%20一般以作者姓放在句首做主语，%20其后括号标注引注年份%20，汇报动词（reporting%20verb）做谓语动词。,higher%20in%20order%20to%20yield%20acceptable%20sliceability.%204">文献英语期末_hedging模糊语有哪些-CSDN博客</a>）</p><ul><li>Information Prominent Citation<strong>：一般现在时</strong></li></ul><p>关注前人研究的内容。一般陈述研究内容，引注将作者和年份放在句末括号里。 如：Scientific paper writing skill is usually adopted with learning by doing and formal training (Auvinen, 2015).</p><ul><li>Author Prominent Citation<strong>：一般过去时</strong></li></ul><p>关注完成研究的人。一般以作者姓放在句首做主语，<strong>其后括号标注引注年份</strong>，汇报动词（reporting verb）做谓语动词。 如：Theno et al. (1978) concluded that salt content had to be 2% or higher in order to yield acceptable sliceability. 4</p><ul><li>Weak Author Prominent Citation<strong>：现在完成时</strong></li></ul><p>关注一系列研究的相似结果。一般以 many researchers，many scholars 等一群人作为句子主语，谓语动词用完成时，引注将诸多作者和年份排列在句末括号里。 如： Researchers have noted that resolving this debate hinges on understanding the relationship between yield and biodiversity, the likelihood of land being spared, and external consequences of practices raising yield, such as agrochemical runoff (Grau et al., 2013; Green et al., 2005; Phalan et al., 2011).</p><h1 id="ch14-academic-poster-学术海报">Ch14 Academic Poster 学术海报</h1><h2 id="ex-14.1">ex 14.1</h2><p>1.Important information should be ____ from about 10 feet away.</p><p><font color=red>readable</font></p><p>2.Title is ____ and draws interest.</p><p><font color=red>short</font></p><p>3.Academic posters could be an ____way of communicating concisely, visually and attractively.</p><p><font color=red>effective</font></p><p>4.Use ____ language to present your work. Avoid ____ unless you're really positive that yours will be a specialist-only audience.</p><p><font color=red>plain、jargon and acronyms</font></p><p>5.An effective poster lets ____ tell the story, uses ____ sparingly, and keeps the sequence well-ordered and obvious.</p><p><font color=red>graphs and images、text</font></p><p>6.Academic posters summarize information or research concisely and attractively, to help publicize information or research and generate discussion.</p><p><font color=red>true</font></p><p>7.What roles does an academic poster play in academic communication？</p><ul><li><font color=red>Posters are widely used in the academic community.</font></li><li><font color=red>Most conferences include poster presentations in their program. </font></li><li><font color=red>Academic posters may be displayed at national or international conferences.</font></li><li><font color=red>They may also be published online as part of conference proceedings.</font></li><li><font color=red>Academic Posters can be used as assessment at university.</font></li></ul><h2 id="ex-14.2">ex 14.2</h2><p>1.An effective poster can make a ____impact, so it's worth developing your poster planning skills.</p><p><font color=red>strong</font></p><p>2.In some courses, ____ and ____ may be weighted at 60%, with ____ and ____weighted at 40%.</p><p><font color=red>content、structure、visual organization、presentation</font></p><p>3.If you are reporting on a piece of research with an academic poster , you can turn to the some language signals to ____, ____, ____, and ____.</p><p><font color=red>introduce the poster、locate a point on the poster、answer directly、handle complex questions</font></p><p>4.Academic posters need to show evidence of reading and research, so you must always include Literature cited.</p><p><font color=red>true</font></p><p>5.Acknowledgments section is to thank individuals for specific contributions to the project Also include in this section explicit disclosures for any conflicts of interest and conflicts of commitment.</p><p><font color=red>true</font></p><h2 id="poster">poster</h2><p>• An effective poster is a visual communications tool;</p><p>• An effective poster will get your main point(s) across to as many people as possible;</p><p>• An effective poster is focused on a single message; lets graphs and images tell the story; uses text sparingly; keeps the sequence well-ordered and obvious.</p><h1 id="ch15-linguistic-features-of-academic-english-学术英语的语言特点">Ch15 Linguistic Features of Academic English 学术英语的语言特点</h1><h2 id="ex-15.1">ex 15.1</h2><p>1.In writing academic paper, we should avoid expressing____ arising out of intuition, feeling, prejudice or your own experience.</p><p><font color=red>personal opinions</font></p><p>2.Another common feature of academic writing is nominalization, whereby verbs become _____.</p><p><font color=red>nouns</font></p><p>3.We can make tentative statements by applying ____ into our academic writing.</p><p><font color=red>models、verbs、adverbs、adjectives</font></p><h2 id="tips-for-academic-writing">Tips for Academic Writing</h2><p><strong>Why do we need a formal language style?</strong></p><p>To fulfil the expectations of academic readers.</p><p>It is important to follow the specific genre requirements in academic writing. This is a defining feature of academic writing.</p><p>Term papers and research reports are generally formal.</p><p><strong>How to achieve formality?</strong></p><p>Advanced and academic vocabulary</p><p>Long and complex sentences</p><p>The minimized use of 1st - and 2nd –person pronouns</p><p><strong>The use of advanced and academic vocabulary</strong></p><p>Avoid using simple and colloquial words</p><p><strong>Simple words</strong> <strong>Advanced words</strong></p><p>eat consume</p><p>try attempt/endeavor</p><p>keep remain/maintain</p><p>good (for) beneficial</p><p>bad(for) harmful/detrimental</p><p><strong>Avoid using phrasal verbs</strong></p><p>It would be better to use a single verb instead of a phrasal verb.</p><p><strong>Phrasal verbs</strong> <strong>One-word verbs</strong></p><p>look into investigate</p><p>find out discover</p><p>cut down reduce</p><p>go up increase</p><p>get rid of eliminate</p><p><strong>Avoid using shortened forms of words/contractions</strong></p><p><strong>Contractions Full forms</strong></p><p>won’t will not</p><p>didn’t did not</p><p>can’t cannot</p><p>it’s it is</p><p>you’re you are</p><p>​</p><p><strong>Academic Word List (AWL)</strong></p><p>Developed by Averil Coxhead, a scholar in New Zealand</p><p>570 word families</p><p>Formal, advanced and academic</p><p><strong>The use of long and complex sentences</strong></p><p>Subordinate clauses: add extra information to the main clause</p><p>Subordinate conjunctions 从属连词</p><p>Some youngsters admit getting restless if their phones is not nearby.</p><p>Relative pronouns 关系代词</p><p>It is advisable to write an outline, which will help you organize the essay in a more logical way.</p><p><strong>Subordinate conjunctions</strong> <strong>Relative pronouns</strong></p><p>Once after until that who whose</p><p>Provided that although when</p><p>Rather than as whenever which whoever whosever</p><p>Since because where</p><p>So that before whereas whichever whom whomever than even if wherever whether if</p><p>Though while unless why</p><p>l Do not always use too long and complex sentences in your essay, because readers will find them difficult to understand.</p><p>l The best way is to use a combination of simple and complex sentences.</p><p><strong>The minimized use of 1st - and 2nd –person pronouns</strong></p><p>First-person pronouns: I, we, us, my, our</p><p>second-person pronouns: you and your</p><p>E.g. Recently, we have discussed COVID-19 and China-US relations a lot.</p><p>→ Recently, COVID-19 and China-US relations have been much discussed.</p><p>Tip: Use third-person and passive voice.</p><h2 id="useful-expressions-in-results-and-tables-figures">Useful Expressions in Results and Tables &amp; Figures</h2><h3 id="useful-expressions-in-results"><strong>1.</strong> <strong>Useful Expressions in Results</strong></h3><p><strong>1.1 Active voice VS Passive voice</strong></p><p>Table 1 presents…and Table 2 presents…</p><p>Our hypothesis predicted….</p><p>Information…was obtained….</p><p>…this information was collected when</p><p><strong>1.2 Suitable reporting verbs</strong> show, indicate, reveal, report,</p><p>describe, explain, display, present…</p><p><strong>1.2.1 Locating the data</strong></p><p>As can be seen from Table 1…</p><p>… are shown/given/provided/ summarized in Table 1.</p><p>Table 1 demonstrates/</p><p>indicates/suggests….</p><p><strong>1.2.2 Highlighting the data</strong></p><p>…is exactly/approximately/almost the same as</p><p>… is completely/entirely/quite different from…</p><p>The main difference between…and… is that…</p><p><strong>1.2.3 Discussing the data</strong></p><p>The data clarify the relationship</p><p>between… and…</p><p>There is some evidence in the data to support our hypothesis, which proposed that…</p><p>This particular result may be</p><p>attributed to the influence of …</p><p><strong>1.3 Phrases of generality</strong></p><p>Overall</p><p>In general</p><p>On the whole</p><p>In the main</p><p>With …exception(s)</p><p>Overall, the results indicate that students performed above the 12th-grade level.</p><p>The overall results indicate…</p><p>The results indicate, overall, that…</p><p>In general, the experimental samples resisted…</p><p>With one exception, the experimental samples resisted…</p><h3 id="useful-expressions-in-tables-and-figures"><strong>2.</strong> <strong>Useful Expressions in</strong> <strong>Tables and Figures</strong></h3><p><strong>2.1 Very frequent and appropriate verbs</strong></p><p>reported, show, characterized, suggests, used, intended, contradict, suggest, prevail, focused, enables, speculate, maintain, compared, focused, claimed, shows, tend, represent</p><p><strong>2.2 Tense of verbs</strong></p><p>The tables show that those who reported driving unsupervised 1-12 times were 80% more likely to be involved in crash than those who reported never having done this.</p><h2 id="ex-15.2">ex 15.2</h2><p>1.Regardless of the placement, each figure must be _____ consecutively and complete with caption.</p><p><font color=red>numbered</font></p><p>2.What kind of tense do we usually use if we start a sentence with words like “Table 1 or Table n”?</p><p><font color=red>The present tense</font></p><p>3.We can place figures and tables within ____, or we can include them at____.</p><p><font color=red>the text of the results、the end of the report</font></p><h2 id="additional-material-for-describing-tables-and-figures-in-academic-language">Additional material for describing tables and figures in academic language</h2><h3 id="图形种类及概述法">1、 图形种类及概述法：</h3><p>泛指一份数据图表： a data graph(曲线图)/chart/diagram/illustration/table 饼图：pie chart 直方图或柱形图：bar chart/histogram 趋势曲线图：line chart/curve diagram 表格图：table 流程图或过程图：flow chart/sequence diagram 程序图：processing/procedures diagram</p><h3 id="常用的描述用法">2、常用的描述用法</h3><p>The table/chart diagram/graph shows （that） According to the table/chart diagram/graph As （is） shown in the table/chart diagram/graph As can be seen from the table/chart/diagram/graph/figures， figures/statistics shows （that）…… It can be seen from the figures/statistics We can see from the figures/statistics It is clear from the figures/statistics It is apparent from the figures/statistics table/chart/diagram/graph figures （that） …… table/chart/diagram/graph shows/describes/illustrates</p><h3 id="图表中的数据data具体表达法">3、图表中的数据（Data）具体表达法</h3><p>数据（Data）在某一个时间段固定不变：fixed in time 在一系列的时间段中转变：changes over time 持续变化的data在不同情况下： 增加：increase/raise/rise/go up …… 减少：decrease/grow down/drop/fall …… 波动：fluctuate/rebound/undulate/wave …… 稳定：remain stable/stabilize/level off ……</p><h3 id="二相关常用词组">二、相关常用词组</h3><h4 id="主章开头">1、主章开头</h4><p>图表类型：table（表格）、chart（图表）、diagram（图标）、graph（多指曲线图）、column chart（柱状图）、pie graph（饼图）、tree diagram（树形图） 描述：show、describe、illustrate、can be seen from、clear、apparent、reveal、represent 内容：figure、statistic、number、percentage、proportion</p><h4 id="表示数据变化的单词或者词组">2、表示数据变化的单词或者词组</h4><p>rapid/rapidly 迅速的，飞快的，险峻的 dramatic/dramatically 戏剧性的，生动的 significant/significantly 有意义的，重大的，重要的 sharp/sharply 锐利的，明显的，急剧的 steep/steeply 急剧升降的 steady/steadily 稳固的，坚定不移的 gradual/gradually 渐进的，逐渐的 slow/slowly 缓慢的，不活跃的 slight/slightly 稍微的、略微地 stable/stably 稳定的</p><h4 id="其它在描述中的常用到的词">3、其它在描述中的常用到的词</h4><p>significant changes 图中一些较大变化 noticeable trend 明显趋势 during the same period 在同一时期 grow/grew 增长 distribute 分布 unequally 不相等地 in the case of 在……的情况下 in terms of/in respect of/regarding 在……方面 in contrast 相反，大不相同 government policy 政府政策 market forces <a href="https://www.baidu.com/s?wd=市场力量&amp;tn=SE_PcZhidaonwhc_ngpagmjz&amp;rsv_dl=gh_pc_zhidao">市场力量</a> measure n.尺寸，方法，措施 v.估量，调节 forecast n.先见，预见 v.猜测</p><h3 id="三图表描述套句精选">三、图表描述套句精选</h3><p>1.The table shows the changes in the number of…over the period from…to…. 该表格描述了在……年之……年间……数量的变化。 2.The bar chart illustrates that…. 该柱状图展示了…… 3.The graph provides some interesting data regarding…. 该图为我们提供了有关……有趣数据。 4.The diagram shows （that）…. 该图向我们展示了…… 5.The pie graph depicts （that）…. 该圆形图揭示了…… 6.This is a cure graph which describes the trend of…. 这个曲线图描述了……的趋势。 7.The figures/statistics show （that）…. 数据（字）表明……</p><p>8.The tree diagram reveals how….</p><p>该树型图向我们揭示了如何……</p><p>9.The data/statistics show （that）….</p><p>该数据（字）可以这样理解……</p><p>10.The data/statistics/figures lead us to the conclusion that….</p><p>这些数据资料令我们得出结论…… 11.As is shown/demonstrated/exhibited in the diagram/graph/chart/table…. 如图所示…… 12.According to the chart/figures…. 根据这些表（数字）…… 13.As is shown in the table…. 如表格所示…… 14.As can be seen from the diagram，great changes have taken place in…. 从图中可以看出，……发生了巨大变化。 15.From the table/chart/diagram/figure，we can see clearly that……or it is clear/apparent from the chart that…. 从图表我们可以很清楚（明显）看到…… 16.This is a graph which illustrates…. 这个图表向我们展示了…… 17.This table shows the changing proportion of a &amp; b from……to…. 该表格描述了……年到……年间a与b的比例关系。 18.The graph，presented in a pie chart，shows the general trend in…. 该图以圆形图形式描述了……总的趋势。 19.This is a column chart showing…. 这是一个柱型图，描述了…… 20.As can be seen from the graph，the two curves show the fluctuation of…. 如图所示，两条曲线描述了……的波动情况。 21.Over the period from…to…, the…remained level. 在……至……期间，……基本不变。 22.In the year between…and…. 在……年到……期间…… 23.In the 3 years spanning from 1995 through 1998…. 1995年至1998三年里…… 24.From then on/from this time onwards…. 从那时起…… 25.The number of…remained steady/stable from （month/year） to （month/year）. ……月（年）至……月（年）……的数量基本不变。 26.The number sharply went up to…. 数字急剧上升至…… 27.The percentage of…stayed the same between…and…. ……至……期间……的比率维持不变。 28.The figures peaked at…in（month/year）. ……的数目在……月（年）达到顶点，为…… 29.The percentage remained steady at…. 比率维持在…… 30.The percentage of…is slightly larger/smaller than that of…. ……的比例比……的比例略高（低）。 31.There is not a great deal of difference between…and… ……与……的区别不大。 32.The graphs show a threefold increase in the number of…. 该图表表明……的数目增长了三倍。 33…decreased year by year while…increased steadily. ……逐年减少，而……逐步上升。 34.The situation reached a peak（a high point at）of …%. ……的情况（局势）到达顶（高）点，为……百分点。 35.The figures/situation bottomed out in…. 数字（情况）在……达到底部。</p><p>36.The figures reached the bottom/a low point/hit a trough.</p><p>数字（情况）达到底部（低谷）。</p><p>37.A is …times as much/many as b.</p><p>a是b的……倍。 38.A increased by…. a增长了…… 39.A increased to…. a增长到……</p><p>40.high/low/great/small/ percentage.</p><p>比率高（低） 41.There is an upward trend in the number of…. ……数字呈上升趋势。 42.Aconsiderable increase/decrease occurred from…to…. ……到……发生急剧上升。 43.From…to…the rate of decrease slow down. 从……到……，下降速率减慢。 44.from this year on，there was a gradual decline/ reduction in the…，reaching a figure of…. 从这年起，……逐渐下降至…… 45.be similar to… 与……相似 46.be the same as… 与……相同 47.There are a lot similarities/differences between…and…. ……与……之间有许多相似（不同）之处 48.A has something in common with b. a与b有共同之处。 49.The difference between a and b lies in…. a与b之间的差别在于…… 50.…（year）witnessed/saw a sharp rise//in….</p><p>……年……急剧上升。</p><h2 id="ex-15.3">ex 15.3</h2><p>1.We analyzed ____ variety of tissue samples.</p><p><font color=red>a</font></p><p>2.____colors affect our perception of reality.</p><p><font color=red>x</font></p><p>3.Those interested in donating the kidney should notify the hospital’s donation committee.</p><p><font color=red>false</font></p><p>4.The cheetah is the quickest of the land animals.</p><p><font color=red>true</font></p><h2 id="samples-for-articles">Samples for Articles</h2><ol type="1"><li><p>Becoming an expert takes a lot of ✗ experience.</p></li><li><p>The Cheetah is the quickest of the land animals.</p></li></ol><h2 id="culture-and-ethics-练习">culture and ethics 练习</h2><p>1.Ethics in academic writing exclude the following action(s):</p><p><font color=red>academic theft</font></p><p>2.Scientific writing is objective, impersonal and detached, so cultural difference cannot be detected in the academic field.</p><p><font color=red>false</font></p><h2 id="ex-15.4">ex 15.4</h2><p>1.In the paper discussed in this lecture, the word “Creator” should not be used because it ____.</p><p><font color=red>makes references to Creationism</font></p><p>2.Academic Integrity and ethics includes:</p><ul><li><font color=red>honesty</font></li><li><font color=red>no fabrication, falsification, or plagiarism</font></li><li><font color=red>honoring property rights</font></li></ul><h1 id="ch16-academic-correspondence-with-the-editors-与编辑的学术联系">Ch16 Academic Correspondence with the Editors 与编辑的学术联系</h1><h2 id="ex-16.1">ex 16.1</h2><p>1.You write a cover letter to______.</p><ul><li><font color=red>Introduce your paper to the editor</font></li><li><font color=red>Recommend reviewers to the editor</font></li><li><font color=red>Oppose reviewers to the editor</font></li></ul><p>2.What is a cover letter?</p><p><font color=red>A cover letter also called submission letter is a letter of transmittal to the editor of the journal for possible publication.</font></p><p>3.In order to choose your target journal, you need to find a journal without any peer review.</p><p><font color=red>false</font></p><h2 id="ex-16.2">ex 16.2</h2><p>1.If you want to know whether your paper is accepted or not, you need to write______.</p><p><font color=red>An inquiry letter</font></p><p>2.Peer review will provide you with comments and suggestions from ____and editors.</p><p><font color=red>reviewers</font></p><p>3.You can write a rebuttal letter to accuse the reviewers of bias or incompetence.</p><p><font color=red>false</font></p><h1 id="ch17-international-academic-conference-presentation-skills-国际学术会议宣讲技巧">Ch17 International Academic Conference Presentation Skills 国际学术会议宣讲技巧</h1><h2 id="ex-17">ex 17</h2><p>1.A presentation is the act of effective _____ communication with an audience.</p><p><font color=red>oral</font></p><p>2.Presentation skills consist of three major parts: ____,____ , and____ .</p><p><font color=red>audience analysis、delivery、managing stage fright</font></p><p>3.Which of the following should NOT be done in international academic conference presentation?</p><p><font color=red> speak towards the screen</font></p>]]></content>
      
      
      <categories>
          
          <category> 课堂笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> course </tag>
            
            <tag> 论文写作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习复习笔记</title>
      <link href="/2024/06/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2024/06/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>以下内容摘自《神经网络与深度学习》</p><h2 id="深度学习与神经网络定义">深度学习与神经网络定义</h2><p><strong>深度学习</strong>是<strong>机器学习</strong>的一个分支，主要解决贡献度分配问题 CAP。<strong>神经网络模型</strong>可以较好地解决 CAP 问题。然而，神经网络与深度学习不是等价的，只能说，神经网络是深度学习中主要采用的模型。</p><h2 id="机器学习">机器学习</h2><p><strong>机器学习</strong>是<strong>人工智能</strong>领域的一个重要研究方向，它的意思是“让机器来自动学习”，主要目的是设计分析一些<strong>学习算法</strong>，让计算机可以从数据中自动分析并获得规律，之后利用学习到的规律对未知数据进行预测，从而完成一些特定任务。从公式角度来说，相当于“构造学习一个特殊的映射函数，从输入映射到未知的输出”。</p><blockquote><p>关于人工智能的发展历史： <img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240619152307261.png" alt="image-20240619152307261" /></p></blockquote><p>以下是几个常见的机器学习问题：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240621145743526.png" alt="image-20240621145743526" /><figcaption aria-hidden="true">image-20240621145743526</figcaption></figure><h3 id="传统的机器学习">传统的机器学习</h3><p>传统的机器学习属于<strong>浅层学习</strong>，即不包含特征的学习，而是直接把特征输入到预测模型得到结果，因此模型只需要关注最后一步——构建预测函数即可。特征通过<u>人工经验或特征转换方法</u>从数据中得到（“特征工程”）。<font color=#ef042a>由于特征需要人为处理，因此开发一个机器学习系统的主要工作量都消耗在了预处理、特征提取和特征转换上。</font></p><p>传统的机器学习的流程如下：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240619154201499.png" alt="image-20240619154201499" /><figcaption aria-hidden="true">image-20240619154201499</figcaption></figure><ol type="1"><li><p>数据预处理：对数据的原始形式进行初步的数据清理（比如去掉一些 有缺失特征的样本，或去掉一些冗余的数据特征等）和加工（对数值特征进行缩 放和归一化等），并构建成可用于训练机器学习模型的数据集．</p></li><li><p>特征提取：从数据的原始特征中提取一些对特定机器学习任务有用的 高质量特征．比如在图像分类中提取边缘、尺度不变特征变换（Scale Invariant Feature Transform，SIFT）特征，在文本分类中去除停用词等．</p><ol type="1"><li>线性投影（子空间）：PCA、LDA</li><li>非线性嵌入：LLE、Isomap、t-SNE</li><li>自编码器</li></ol></li><li><p>特征转换：对特征进行进一步的加工，比如降维和升维． 很多特征转换方法也 都是机器学习方法． 降维包括特征抽取（Feature Extraction）和特征选择（Feature Selection）两种途径．常用的 特征转换方法有<strong>主成分分析</strong>（Principal Components Analysis，PCA）、 <strong>线性判别分析</strong>（Linear Discriminant Analysis，LDA）等．</p></li><li><p>预测：机器学习的核心部分，学习一个函数并进行预测．</p></li></ol><h3 id="表示学习">表示学习</h3><p>如果有一种算法能够自动学习出<strong>有用的特征</strong>（即“<strong>表示</strong>”），那么就能在传统机器学习的基础上大大提高性能。学习有用特征的过程被称为“表示学习”。</p><p>在机器学习中，通常有两种表示形式：</p><ul><li><strong>局部表示/离散表示/符号表示</strong>：如 one-hot 向量。优点在于简单、可解释性、稀疏性；缺点则是维度很高且无法扩展新的维度来表示新特征、相近特征之间无法区别。</li><li><strong>分布式表示</strong>：通常是低维的稠密向量，相比局部表示具有维度低、易表示新特征、可计算相近特征间的区别等优点。</li></ul><h3 id="深度学习">深度学习</h3><p>表示学习的关键是解决<u><strong>直观的底层特征</strong></u>和<strong><u>抽象的高层语义</u></strong>之间的<strong>语义鸿沟</strong>问题。</p><p><strong>人类的视觉原理</strong>：从原始信号摄入开始（瞳孔摄入像素），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。</p><p>机器的方法也类似：<u>构造多层的神经网络，较低层识别初级的图像特征（底层特征），若干层底层特征组成更上一层特征，最终通过多个层级的组合，在顶层作出分类得到高层语义。</u></p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240625205501581.png" alt="image-20240625205501581" style="zoom: 33%;" /></p><p><u>因此要得到好的高层语义表示（一般为分布式），通常需要从底层特征开始，经过多步非线性转换</u>，即该学习过程需要具备一定的“深度”。深层结构可以增加特征的重用性，从而增强表示能力。</p><p>这样我们就需要一种学习方法从数据中学习一个“深度模型”，该模型能从数据中自动学习到有效的特征表示，这就是<strong>深度学习</strong>。因此深度学习是机器学习的一个子问题。深度学习的数据处理流程如下：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240619162609584.png" alt="image-20240619162609584" /><figcaption aria-hidden="true">image-20240619162609584</figcaption></figure><p>事实上，对比传统的机器学习和深度学习流程图可发现，<strong>特征提取</strong>主要基于任务或先验去除无用特征，而<strong>表示学习</strong>则通过深度学习模型学习高层语义特征。</p><h2 id="机器学习的三个基本要素">机器学习的三个基本要素</h2><h3 id="模型">模型</h3><ul><li>线性模型：<span class="math inline">\(f(\bold{x};\theta)=\omega^T \bold{x}+b\)</span>. 其中 <span class="math inline">\(\theta\)</span> 包含了权重向量 <span class="math inline">\(\omega\)</span> 和偏置 <span class="math inline">\(b\)</span> ；</li><li>非线性模型：<span class="math inline">\(f(\bold{x};\theta)=\omega^T \phi(x)+b\)</span> .</li></ul><h3 id="学习准则">学习准则</h3><p>模型 <span class="math inline">\(f(\bold{x};\theta)\)</span> 的好坏可以通过<strong>期望风险/期望错误</strong> <span class="math inline">\(\mathcal{R}(\theta)\)</span> 来衡量，其定义为： <span class="math display">\[\mathcal{R}(\theta)=\mathbf{E}_{(x,y) \sim p_r(x,y))}[\mathcal{L}(y,f( \bold{x};\theta))],\]</span> 其中 <span class="math inline">\(p_r(\bold{x};\theta)\)</span> 为真实的数据分布，<span class="math inline">\(\mathcal{L}(y,f( \bold{x};\theta))\)</span>​ 为<strong>损失函数</strong>。</p><h4 id="损失函数">损失函数</h4><p><strong>损失函数</strong>是一个非负实数函数，用来衡量模型预测值与真实标签之间的差异。损失函数有很多种，</p><ol type="1"><li><strong>平方损失函数</strong>常用于标签 <span class="math inline">\(y\)</span> 值为连续实值时，定义为：<span class="math inline">\(\mathcal{L}(y,f(\bold{x};\theta))=\frac{1}{2}(y-f(\bold{x};\theta))^2\)</span>;</li><li><strong>交叉熵损失函数</strong>常用于分类问题。若共有 <span class="math inline">\(C\)</span> 个类别，则标签的真实分布 <span class="math inline">\(\bold{y}\)</span> 和模型预测分布 <span class="math inline">\(f(\bold{x};\theta)\)</span> 间的交叉熵为：<span class="math inline">\(\mathcal{L}(\bold{y},f(\bold{x};\theta))=-\sum_{c=1}^C y_c\log f_c(\bold{x};\theta)\)</span> ;</li></ol><h4 id="经验风险导致的过拟合-与-正则化">经验风险导致的过拟合 与 正则化</h4><p>一个好模型应该有较小的期望风险/期望错误，但是由于数据的真实分布一般是未知的，因此事实上无法计算其期望风险 <span class="math inline">\(\mathcal{R}(\theta)\)</span>​ , 而是用从训练集 <span class="math inline">\(D\)</span> 中计算得到的<strong>经验风险/经验错误</strong>来近似： <span class="math display">\[\mathcal{R}^{emp}_D(\theta)=\frac{1}{N} \sum^N_{n=1} \mathcal{L}(y^{(n)},f( \bold{x}^{(n)};\theta)),\]</span> 且我们把期望风险与经验风险间的差距称为“泛化错误”，定义为： <span class="math display">\[\mathcal{G}_D(f)=\mathcal{R}(f)-\mathcal{R}^{emp}_D(f),\]</span> 此时的学习准则就变为寻找一组参数 <span class="math inline">\(\theta^*\)</span> ，使得经验风险最小，这就是“<strong>经验风险最小化</strong>”准则 <strong>ERM</strong> 。然而该准则很容易导致“<strong>过拟合</strong>”，即模型在训练集上错误率很低，但是在未知数据上错误率很高。</p><p>过拟合问题往往是由<u>训练数据少</u>和<u>模型能力太强</u>（如用四次函数来拟合二次函数）等原因造成的。为了解决过拟合问题，一般会在经验风险最小化的基础上引入<strong>参数的正则化</strong>（Regularization）来限制模型能力，使其不要过度最小化经验风险。这种准则就是<strong>结构风险最小化 SRM</strong> 准则： <span class="math display">\[\begin{aligned}\theta^{*} &amp; =\underset{\theta}{\arg \min } \mathcal{R}_{\mathcal{D}}^{\text {struct }}(\theta) \\&amp; =\underset{\theta}{\arg \min } \mathcal{R}_{\mathcal{D}}^{e m p}(\theta)+\frac{1}{2} \lambda\|\theta\|^{2} \\&amp; =\underset{\theta}{\arg \min } \frac{1}{N} \sum^{N} \mathcal{L}\left(y^{(n)}, f\left(x^{(n)} ; \theta\right)\right)+\frac{1}{2} \lambda\|\theta\|^{2},\end{aligned}\]</span> 其中 <span class="math inline">\(||\theta||\)</span> 是 <span class="math inline">\(\mathcal{l}_2\)</span> 范数的正则化项， <span class="math inline">\(\lambda\)</span> 用来控制正则化的强度。当然正则化也可以使用其他函数，如 <span class="math inline">\(\mathcal{l}_1\)</span>​ 范数等。</p><p><strong>针对梯度下降的优化算法，除了加正则化项之外，还可以通过“提前停止”来防止过拟合。</strong>提取停止就是当模型在验证集上的错误率不再下降时，就停止迭代，即在梯度为0处停止迭代：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240621103900937.png" alt="image-20240621103900937" style="zoom:50%;" /></p><p>与“过拟合”相反的概念是“欠拟合”，即模型在训练集上的错误较高，一般是由模型能力不足造成的</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240621102022180.png" alt="image-20240621102022180" /><figcaption aria-hidden="true">image-20240621102022180</figcaption></figure><p>事实上，我们可以把所有损害优化的方法都统称为“正则化”，主要有以下两类：</p><ul><li>增加优化约束：L1/L2约束、数据增强；</li><li>干扰优化过程：权重衰减、随机梯度下降、提前停止。</li></ul><p>总之，机器学习中的学习准则并不仅仅是拟合训练集上的数据，同时也要使得泛化错误最低，即避免“过拟合”。我们可以将机器学习看作一个从有限、高维、有噪声的数据上得到更一般性规律的泛化问题．</p><h3 id="优化算法">优化算法</h3><p>（<strong>机器学习</strong>与<strong>最优化方法</strong>的关系）在确定好训练集、假设空间/模型函数族和学习准则后，寻找最优模型 <span class="math inline">\(f(\bold{x};\theta^*)\)</span> 就成为了一个最优化问题。机器学习的训练过程其实就是最优化问题的求解过程。</p><p>在机器学习中，优化可分为参数优化与超参数优化</p><ul><li>参数优化：即模型将自动优化可学习的参数 <span class="math inline">\(\theta\)</span> ;</li><li>超参数优化：超参数如聚类算法中的类别个数、梯度下降法中的步长、正则化 分布的参数． 项的系数、神经网络的层数、支持向量机中的核函数等，这些超参数一般是组合优化问题，很难自动学习，一般按<u>人的经验设定</u>，或<u>通过搜索法对一组超参数组合进行试错调整</u>。</li></ul><p>机器学习中的优化算法很多利用自成熟的<strong>最优化方法</strong>，如共轭梯度法、拟牛顿法和梯度下降法等。与最优化方法中不同的是，我们常将一维搜索步长 <span class="math inline">\(\alpha\)</span> 称为“<strong>学习率</strong>”。</p><p>梯度下降法——随机梯度下降法——小批量梯度下降法</p><p>随机梯度下降法：每次只取一个样本进行优化，因此实现简单，可在线更新参数；缺点是无法充分利用GPU的并行计算能力。</p><p>小批量梯度下降法：是前面二者的折中，每次取一小部分样本（实际为提高计算效率，通常取2的幂次方）进行计算，应用更为广泛。</p><details><summary>参考</summary><p><a href="https://www.bilibili.com/video/BV1C84y1R7qE/?spm_id_from=333.337.search-card.all.click&amp;vd_source=124ec79ebd3e16b0f454a3994a468f98">多视图几何MVS简介及MVSNet如何高效入门？_哔哩哔哩_bilibili</a></p></details>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/06/23/Diffusion%20Model%EF%BC%88%E4%B8%89%EF%BC%89/"/>
      <url>/2024/06/23/Diffusion%20Model%EF%BC%88%E4%B8%89%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考链接 :<a href="https://cloud.tencent.com/developer/article/2393003">【他山之石】Stable Diffusion 万字长文详解稳定扩散模型-腾讯云开发者社区-腾讯云 (tencent.com)</a></p><p><a href="https://blog.csdn.net/weixin_50973728/article/details/134920466">100：ReconFusion: 3D Reconstruction with Diffusion Priors-CSDN博客</a></p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>信息熵</title>
      <link href="/2024/06/21/%E4%BF%A1%E6%81%AF%E7%86%B5/"/>
      <url>/2024/06/21/%E4%BF%A1%E6%81%AF%E7%86%B5/</url>
      
        <content type="html"><![CDATA[<h2 id="自信息">自信息</h2><p>自信息（self information）定义为<strong>一个随机变量/事件 <span class="math inline">\(x_i\)</span> 的所包含的信息量</strong>。定义为： <span class="math display">\[I(x_i)=-\log{P(x_i)} \tag{1}\]</span> 关于该公式的理解：自信息可理解为"<strong>不确定性</strong>"。一件事情发生的概率越小，它发生时所包含的信息量越大。如果一个事情100%发生，那么就不包含信息量，即不含不确定性。</p><h2 id="熵">熵</h2><p>以下摘自李航《统计学习方法》</p><p>在信息论与概率统计中，<u>熵（entropy）是表示随机事件不确定性的度量。熵越大，随机变量的不确定性就越大</u>。当 <span class="math inline">\(X\)</span> 是一个取有限个值的离散随机变量，且其概率分布：<span class="math inline">\(P(X=x_i)=p_i,i=1,2,...,n\)</span> 时，该随机变量 <span class="math inline">\(X\)</span> 的熵定义为所有可能取值 <span class="math inline">\(X=x_i\)</span> 的自信息 对于 <span class="math inline">\(X\)</span> 的数学期望： <span class="math display">\[H(X)=-\sum^n_{i=1}p_i\log{p_i} \tag{2.1}\]</span> 若<span class="math inline">\(p_i=0\)</span> ，定义 <span class="math inline">\(0\log{0}=0\)</span> 。从定义可以验证，熵 <span class="math inline">\(H(p)\)</span> 的取值范围是 <span class="math inline">\([0,\log{n}]\)</span>，<strong>当分布越均匀，熵越大</strong>。熵最大时 <span class="math inline">\(P(X=x_i)= \frac{1}{n},i=1,2,..,n\)</span>。通常，上式中的对数以2为底（比特，bit）或以e为底（纳特，nat）。又由(1)式知，<span style="background:#f9eda6;">熵只依赖于变量 <span class="math inline">\(X\)</span> 的分布，而与 <span class="math inline">\(X\)</span> 取值无关。</span>所以也将 <span class="math inline">\(X\)</span> 的熵记作 <span class="math inline">\(H(p)\)</span>: <span class="math display">\[H(p)=-\sum^n_{i=1}p_i\log{p_i} \tag{2.2}\]</span></p><blockquote><p>熵编码（Entropy Encoding）：在对分布 <span class="math inline">\(p(y)\)</span> 的符号进行编码时，熵 <span class="math inline">\(H(p)\)</span> 也是理论上最优的平均编码长度，这种编码方式称为“熵编码”。</p></blockquote><h3 id="栗子变量为伯努利分布时的熵">栗子：变量为伯努利分布时的熵</h3><p>当随机变量服从伯努利分布，即 <span class="math inline">\(X\)</span> 的分布为：</p><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">X=1</th><th style="text-align: center;">X=0</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">P(X)</td><td style="text-align: center;">p</td><td style="text-align: center;">1-p</td></tr></tbody></table><p>熵 <span class="math inline">\(H(p)\)</span> 随概率 <span class="math inline">\(p\)</span> 变化的曲线图为：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240616095318685.png" alt="image-20240616095318685" style="zoom:50%;" /></p><ul><li>当 <span class="math inline">\(p=0,1\)</span> 时，随机变量没有不确定性，熵最小，取到0。</li><li>当 <span class="math inline">\(p=0.5\)</span> 时，随机变量不确定性最大，熵最大，取到1。</li></ul><h2 id="条件熵">条件熵</h2><p>条件熵 <span class="math inline">\(H(Y|X)\)</span> 表示在已知随机变量 <span class="math inline">\(X\)</span> 的条件下随机变量 <span class="math inline">\(Y\)</span> 的不确定性。对随机变量 <span class="math inline">\((X,Y)\)</span> ，其联合概率分布为：<span class="math inline">\(P(X=x_i,Y=y_j)=p_{ij},i=1,...,n;j=1,...,m\)</span>。其条件熵 <span class="math inline">\(H(Y|X)\)</span> 定义为给定 <span class="math inline">\(X=x_i\)</span> 条件下 <span class="math inline">\(Y\)</span> 的条件概率分布的熵对 <span class="math inline">\(X\)</span> 的数学期望： <span class="math display">\[H(Y|X)=\sum^n_{i=1}p_iH(Y|X=x_i) \tag{3}\]</span> 且这里， <span class="math inline">\(p_i=P(X=x_i)\)</span>，<span class="math inline">\(H(Y|X=x_i)=-\sum^m_{j=1}p_{ij}\log{p_{ij}}\)</span> 。</p><p>当熵和条件熵中的概率有数据估计（如极大似然估计）得到时，称其为<strong>经验熵 / 经验条件熵</strong>。<strong>互信息（mutual information）</strong>定义为熵 <span class="math inline">\(H(Y)\)</span> 与条件熵 <span class="math inline">\(H(Y|X)\)</span> 之差，表示得知 <span class="math inline">\(X\)</span> 的信息而使得 <span class="math inline">\(Y\)</span> 的不确定性减少的程度。</p><h2 id="最大熵模型与极大似然估计">最大熵模型与极大似然估计</h2><p>见 《统计学习方法》P110</p><h2 id="相对熵-kl-散度">相对熵 / KL 散度</h2><blockquote><p>来自百度百科：https://baike.baidu.com/item/%E7%9B%B8%E5%AF%B9%E7%86%B5/4233536</p></blockquote><p><strong>相对熵（relative entropy）</strong>，又被称为 KL散度（Kullback-Leibler divergence）或信息散度（information divergence），常定义为“<strong>用概率分布 <span class="math inline">\(q\)</span> 来近似另一个概率分布 <span class="math inline">\(p\)</span> 时所造成的信息损失量</strong>”，是对同一个随机变量的两个概率分布之间差异的非对称性度量。“非对称性”可以理解为参照物的不同，如 <span class="math inline">\(D_{KL}(P||Q)\)</span> 表示的是 <span class="math inline">\(P\)</span> 事件在 <span class="math inline">\(Q\)</span>​ 事件坐标系下的 KL 散度。</p><p>设 <span class="math inline">\(P(x),Q(x)\)</span> 是关于随机变量 <span class="math inline">\(X\)</span> 的两个概率分布，则在离散和连续随机变量的情形下，相对熵的表达式分别为: <span class="math display">\[KL\left( P||Q \right) =H(p,q)-H(p)=\sum{P\left( x \right) \log \frac{P\left( x \right)}{Q\left( x \right)}} \tag{4}\\KL\left( P||Q \right) =H(p,q)-H(p)=\int{P\left( x \right) \log \frac{P\left( x \right)}{Q\left( x \right)}}dx\]</span></p><blockquote><p>KL散度是按照概率分布q的最优编码对真实分布为p的信息进行编 码,其平均编码长度(即交叉熵)H(p,q)和p的最优平均编码长度 (即熵)H(p)之间的差异。</p></blockquote><p><strong>相对熵具有非负性</strong>，根据<a href="https://baike.baidu.com/item/%E5%90%89%E5%B8%83%E6%96%AF%E4%B8%8D%E7%AD%89%E5%BC%8F/22780937">吉布斯不等式</a>推导离散时的情况如下（事实上吉布斯不等式可以直接得到<span class="math inline">\(0 \geq \sum_{i=1}^{n} p_{i} \log q_{i}-\sum_{i=1}^{n} p_{i} \log p_{i}=\sum_{i=1}^{n} p_{i} \log \left(q_{i} / p_{i}\right)=-D_{\mathrm{KL}}(P \| Q)\)</span> ，百度百科这里大等于号后还用到了琴生不等式，这是由于通过琴生不等式可以证明吉布斯不等式）。由于 -log<span class="math inline">\(x\)</span> 是凸函数，因此根据相对熵的定义有： <span class="math display">\[\mathrm{KL}(P \| Q)=\sum_{x \in X} P(x) \log \frac{P(x)}{Q(x)}=-E\left[\log \frac{Q(x)}{P(x)}\right] \geq-\log \left[\sum_{x \in X} P(x) \frac{Q(x)}{P(x)}\right]=-\log \left[\sum_{x \in X} Q(x)\right]=0\]</span> 由上可知，<u>相对熵是恒大于等于0的。当且仅当两分布相同时，相对熵等于0。</u></p><h2 id="交叉熵">交叉熵</h2><p>将相对熵 / KL 散度进行化简： <span class="math display">\[\begin{aligned}D_{K L}(P \| Q) &amp; =\sum_{i=1}^{n} P\left(x_{i}\right) \log \left(\frac{P\left(x_{i}\right)}{Q\left(x_{i}\right)}\right) \\&amp; =\sum_{i=1}^{n} P\left(x_{i}\right) \log \left(P\left(x_{i}\right)\right)-\sum_{i=1}^{n} P\left(x_{i}\right) \log \left(Q\left(x_{i}\right)\right) \\&amp; =-H(P)+\left[-\sum_{i=1}^{n} P\left(x_{i}\right) \log \left(Q\left(x_{i}\right)\right)\right]\end{aligned}\]</span> 这里，前半部分 <span class="math inline">\(H(P)\)</span> 是确定的，表示概率分布 <span class="math inline">\(P(x)\)</span> 的熵。后半部分就是交叉熵（cross entropy）——概率分布 <span class="math inline">\(q\)</span> 的信息量 对概率分布 <span class="math inline">\(p\)</span> 的数学期望: <span class="math display">\[H(p,q)=\mathbb{E}_p[-\log q(x)]=-\sum_{i=1}^{n} p\left(x_{i}\right) \log q\left(x_{i}\right), \tag{5}\]</span> &gt; 交叉熵还被定义为按照概率分布 <span class="math inline">\(q\)</span> 的最优编码对真实分布为 <span class="math inline">\(p\)</span> 的信息进行编码的长度。</p><p>在机器学习中，如果 <span class="math inline">\(P(x)\)</span> 用来表示样本的真实分布，而 <span class="math inline">\(Q(x)\)</span> 用来表示模型所预测的分布。如果我们需要评估两个分布 之间的差距，可以使用 KL 散度，即 $D_{KL}( P||Q ) $ 。<strong>由于 KL 散度中的前一部分 <span class="math inline">\(-H(P)\)</span> 只与真值有关，是不变的，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用交叉熵作为损失函数来衡量真实分布与预测分布间的差异，通过最小化交叉熵损失来优化模型</strong>。</p><h3 id="交叉熵损失代码实现原理">交叉熵损失代码实现原理</h3><p>前面提到，用 <span class="math inline">\(P(x)\)</span> 用来表示样本的真实分布，而 <span class="math inline">\(Q(x)\)</span> 用来表示模型所预测的分布。则<font color=#df8400> <span class="math inline">\(p(x_i)\)</span> </font>可表示样本 <span class="math inline">\(x\)</span> 属于第 <span class="math inline">\(i\)</span> 个类别的真实概率，常用 one-hot 编码 <span class="math inline">\(target=(p(x_1),p(x_2),..,p(x_k),..,p(x_n))=(0,0,..,1,..,0)\)</span> 表示，第 k 位为 1 表示真实类别为第 k 类。<font color=#4eb434><span class="math inline">\(q(x_i)\)</span> </font>表示模型预测的样本 <span class="math inline">\(x\)</span> 属于第 <span class="math inline">\(i\)</span> 类的概率。</p><p>在 pytorch 中的交叉熵损失 <code>torch.nn.CrossEntropyLoss()</code> 包含两个主要部分：<strong>softmax</strong> 和 <strong>交叉熵计算</strong>。</p><p>模型的输出往往是各个分类 <span class="math inline">\(i\)</span> 的得分 <span class="math inline">\(z_i\)</span> ，因此我们需要通过最后加上 softmax 函数来将得分转换为和值为 1 的概率。softmax 的计算公式：$q(x_i)= $。计算完 softmax，我们就可以通过交叉熵公式计算损失如下： <span class="math display">\[\begin{align}H(P,Q)&amp;=-\sum_{i=1}^n p(x_i)\log q(x_i) \\&amp;=-p(x_1)\log q(x_1)-p(x_2)\log q(x_2)...-p(x_k)\log q(x_k)...-p(x_n)\log q(x_n) \\&amp;= 0-0..-p(x_k)\log q(x_k)..-0 \\&amp;=-p(x_k)\log q(x_k) = -\log q(x_k)\end{align}\]</span> 可见交叉熵损失函数最后只剩下正确类型的概率差距，为 <span class="math inline">\(-\log q(x_k)\)</span>。即<strong>交叉熵损失只关注了正确分类的差距</strong>。</p><h3 id="交叉熵损失代码实现">交叉熵损失代码实现</h3><p>由于代码内包含了 softmax 操作，因此实际应用时不需要额外在模型网络最后加上 softmax 层。且<strong>如果使用 nn.CrossEntropyLoss 损失函数，一般最后一层不需要激活函数</strong>。</p><p>下面是《神经网络与深度学习》中第四章里基于前馈神经网络 FNN 进行Iris鸢尾花分类任务的代码例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch_train</span><br><span class="line"><span class="keyword">import</span> torchmetrics</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FNNNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        nn.Module.__init__(self)</span><br><span class="line">        self.model = torch.nn.Sequential(</span><br><span class="line">            nn.Linear(input_size, hidden_size),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(hidden_size, output_size),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向计算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.model(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">input_size = <span class="number">4</span></span><br><span class="line">hidden_size = <span class="number">6</span></span><br><span class="line">output_size = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">model = FNNNet(input_size, hidden_size, output_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉熵损失函数</span></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动训练</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">train_dl = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), </span><br><span class="line">                                       batch_size=batch_size,</span><br><span class="line">                                       shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">val_dl = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val, y_val), </span><br><span class="line">                                       batch_size=batch_size,</span><br><span class="line">                                       shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">test_dl = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test, y_test), </span><br><span class="line">                                       batch_size=batch_size,</span><br><span class="line">                                       shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">metric = torchmetrics.Accuracy(task=<span class="string">&quot;multiclass&quot;</span>, num_classes=<span class="number">3</span>)</span><br><span class="line">model_metric = &#123;<span class="string">&#x27;acc&#x27;</span>: metric&#125;</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line">log_per_epoch = <span class="number">50</span></span><br><span class="line">his = torch_train.train(model, optimizer, loss, train_dl, val_dl, model_metric, </span><br><span class="line">                        epochs=epochs, log_per_epoch=log_per_epoch)</span><br><span class="line"></span><br><span class="line">torch_train.plot_his(his)</span><br><span class="line">torch_train.evaluate(model, test_dl, model_metric)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="参考">参考</h2><ul><li><a href="https://blog.csdn.net/tsyccnh/article/details/79163834">一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉-CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/115277553">损失函数：交叉熵详解 - 知乎 (zhihu.com)</a></li><li><a href="https://zhuanlan.zhihu.com/p/612236160">交叉熵(CrossEntropy)的理解与公式推导 - 知乎 (zhihu.com)</a></li><li><a href="https://blog.csdn.net/zishuijing_dd/article/details/132795167">【交叉熵损失torch.nn.CrossEntropyLoss详解-附代码实现】-CSDN博客</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信息论与编码 </tag>
            
            <tag> 统计学习方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2DGS</title>
      <link href="/2024/06/07/2DGS/"/>
      <url>/2024/06/07/2DGS/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>支持向量机 SVM</title>
      <link href="/2024/06/06/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
      <url>/2024/06/06/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<p>问题背景：<a href="https://blog.csdn.net/qq_40943760/article/details/113804784">第十九课.基于sklearn的SVM人脸识别_sklearn fit图片识别训练-CSDN博客</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>决策树</title>
      <link href="/2024/06/06/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
      <url>/2024/06/06/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<h2 id="问题背景">问题背景</h2><p><a href="https://scikit-learn.org.cn/view/274.html">使用多输出估算器完成人脸绘制-scikit-learn中文社区</a></p><p>目标是给定脸部的上半部分来预测下半部分。</p><h2 id="决策树的定义">决策树的定义</h2><p>决策树表示给定特征条件下类的条件概率分布P(Y|X)，形状是有向无环图。对于(X,y)型数据来说，特征X对于决策树中的<strong>内部节点——条件</strong>，标签y则对于<strong>叶节点——该条件下的结果类别</strong>。这些条件规则有着“互斥且完备”的特点，即该条件概率分布是定义在特征空间的一个“划分”上。</p><p>决策树的目标是根据给定的训练数据集构建一个决策树模型，是它能对实例进行正确的分类。这个寻找决策树的过程本质是从训练数据集中归纳出一组分类规则，最终找到的决策树可能没有，也可能不止一个。我们主要通过<strong>最小化损失函数</strong>来寻找最优决策树，损失函数通常取<u>正则化的极大似然函数</u>。决策树的学习通常包括3个步骤：<strong>特征选择</strong>、<strong>决策树的生成</strong>和<strong>决策树的修剪</strong>。</p><ul><li><strong>特征选择</strong>：如果特征数量很多，可以在训练开始时，对特征进行选择，只留下对训练数据有足够分类能力的特征。</li><li><strong>决策树的生成</strong>：开始，构建根节点，并将所有数据都放在根节点。选择一个最优特征，根据该特征将训练数据集划分为子集，如果这些子集已经能被基本正确分类，那么构建叶节点，将这些子集分到对应的叶节点中去；若还有子集不能被基本正确分类，就对这些子集选择新的最优特征，重复上述过程。如此递归进行，直至所有子集被正确分类，或没有合适的特征为止。这样就生成了一颗决策树。</li><li><strong>决策树的修剪</strong>：按训练数据生成的决策树可能对原数据有着很好的拟合能力，但是未必能正确预测新数据的类别，即可能发生“过拟合”。为此，我们对树进行自下而上的剪枝，去掉细分的叶节点，使其回退到更高的节点，然后将该更高的节点改为新的叶节点。这样将使树变得简单，具有更强的泛化能力，避免过拟合。</li></ul><p>下面将就这三个步骤展开详细说明。</p><h2 id="特征选择">特征选择</h2><p>特征选择的一般准则是“信息增益”或“信息增益比”。</p><h2 id="梯度提升决策树-gbdt">梯度提升决策树 GBDT</h2><p>梯度提升决策树（Gradient Boosting Decision Tree，简称GBDT），又叫 MART（Multiple Additive Regression Tree），起源于上世纪90年代，是一种迭代的集成学习方法。其核心思想是“将多个弱学习器（通常是决策树）的结果累加起来作为最终的预测输出”。</p><p><strong>GBDT过程可简要概括为：</strong></p><ul><li><strong>首先，进行初始化，</strong>以所分类别下的所有目标变量<span class="math inline">\(y\)</span>的均值或其他合适初值作为第一棵树的初始预测值；</li><li><strong>然后，通过多轮迭代改进模型，</strong>每一次迭代都会由全部特征生成一棵新决策树，<strong>新的树将拟合上一棵树的残差</strong>（残差是实际观测值<span class="math inline">\(y_i\)</span>与当前模型预测值<span class="math inline">\(\hat{y}_i\)</span>之间的差异，实际实现中通常取损失函数为<span class="math inline">\(L_{MSE}= \frac{1}{2} (y_i-\hat{y}_i)^2\)</span>，然后用MSE的负梯度<span class="math inline">\((y_i-\hat{y}_i)\)</span>来近似残差）；</li><li><strong>最后，将所有决策树的预测结果相加</strong>，得到最终的集成预测结果。</li></ul><p>该过程机制一方面保证了模型的预测能力将逐渐提高，另一方面其集成结果充分利用了每一棵树的贡献，能处理复杂的非线性关系和高维数据，比单一决策树或线性模型具有更高精度。GBDT 还有一个区别于SVM和MLP的地方在于，它<u>不需要对特征进行缩放或归一化</u>，即特征的尺度差异不会影响模型性能，这无疑减轻了数据预处理的负担。</p><blockquote><details><summary>GBDT与随机森林、SVM、MLP的比较</summary><h2 id="mcetoc_1hvoeaor1lo"><ol type="1"><li>与随机森林的比较</h2><p>随机森林（Random Forest）和GBDT都是集成学习方法，它们都通过组合多个决策树来提高预测性能。然而，它们在实现方式和特点上存在明显差异。</p><ul><li><strong>GBDT是一种序列化方法</strong>，每棵树都依赖于前一棵树的结果。这意味着GBDT的每一轮迭代都在尝试纠正前一轮的错误，因此每棵树都更加关注数据中的错误部分。相比之下，随机森林是一种并行方法，每棵树都是相互独立地构建的，它们之间没有先后顺序。这使得随机森林更容易实现并行化处理，适用于多核处理器和分布式计算环境。</li><li><strong>随机森林通过随机抽取特征子集来构建树</strong>，这种随机性有助于减少过拟合风险。相反，<strong>GBDT使用全部特征来构建树</strong>，但通过不断迭代来减少模型的残差，从而降低了过拟合的可能性。这意味着GBDT更容易受到训练数据中的噪声影响，但在训练集较小或噪声较少的情况下，GBDT通常能够获得较高的预测精度。</li><li>在某些问题上，<strong>GBDT的性能可能优于随机森林</strong>，特别是当问题具有复杂的非线性关系或需要高精度预测时。GBDT通常能够更好地拟合训练数据中的细节，因此在图像识别、自然语言处理等任务中表现出色。</li><li>然而，在某些情况下，<strong>随机森林可能更有效</strong>。例如，当处理高维数据集或需要处理大规模数据时，随机森林的并行化性质使其具有更好的性能和可扩展性。</li></ul><h2 id="mcetoc_1hvoeaor1lp"><ol start="2" type="1"><li>与支持向量机的比较</h2><p>支持向量机（Support Vector Machine，SVM）和GBDT是两种不同类型的机器学习算法，它们在目标任务、工作原理和应用领域上存在显著差异。</p><ul><li><strong>目标任务的不同</strong>：SVM主要用于分类问题，其目标是找到一个能够最大化类别间间隔的超平面。而GBDT不仅可以用于分类问题，还可以用于回归问题。GBDT通过迭代训练决策树来不断减少残差，从而实现对目标变量的回归或分类。</li><li><strong>工作原理的不同</strong>：SVM通过寻找支持向量来构建分类边界，这些支持向量是距离超平面最近的样本点。与之不同，GBDT通过组合多个决策树来实现分类或回归。每一棵树都关注数据中的误差部分，通过迭代逐渐改进模型的预测性能。</li><li><strong>适用领域的不同</strong>：SVM通常在小规模数据集上表现出色，特别是当数据线性可分时。它在文本分类、图像识别和生物信息学等领域得到广泛应用。相比之下，GBDT更适合处理大规模数据集和复杂非线性问题。它在金融风控、推荐系统、自然语言处理等领域具有广泛的应用前景。</li><li><strong>模型复杂度的不同</strong>：SVM通常生成稀疏模型，只有少数支持向量参与决策。GBDT的模型相对较复杂，它由多棵决策树组成，每棵树都可以有不同的深度。因此，SVM更容易解释和可视化，而GBDT通常需要更多的模型解释工作。</li></ul><h2 id="mcetoc_1hvoeaor1lq"><ol start="3" type="1"><li>与神经网络的比较</h2><p>神经网络（Neural Networks）和GBDT是两种不同的机器学习范式，它们在模型结构、训练方法和应用领域上有着显著的差异。</p><ul><li><strong>模型结构的不同</strong>：神经网络是一种模拟人类大脑神经元相互连接的模型，它由多个层次的神经元组成，包括输入层、隐藏层和输出层。每个神经元都与前一层的所有神经元相连接，形成复杂的网络结构。相比之下，GBDT是一种基于树结构的模型，每个树都是独立构建的，没有神经网络的复杂拓扑结构。</li><li><strong>训练方法的不同</strong>：神经网络通常使用反向传播算法来训练模型，通过梯度下降优化模型参数。与之不同，GBDT采用迭代的方式训练决策树，每一轮的目标是减少前一轮的残差。这两种训练方法在数学原理和计算复杂性上有显著差异。</li><li><strong>适用领域的不同</strong>：神经网络在计算机视觉、自然语言处理、语音识别等领域取得了巨大成功。它们特别擅长处理大规模数据和复杂模式识别任务。相比之下，GBDT更常用于结构化数据的分类和回归问题，如金融风控、广告点击率预测和客户流失预测。它们在解释性和模型理解上具有优势。</li><li><strong>计算资源的需求</strong>：神经网络通常需要大量的计算资源来训练和推断，特别是深度神经网络。相比之下，GBDT通常对计算资源的需求较低，能够在相对较小的硬件环境中运行。</li></ul></details></li></ol></li></ol></li></ol></blockquote><h3 id="boosting-串行-vs.-bagging-并行">Boosting 串行 vs. Bagging 并行</h3><h3 id="图解例子">图解例子</h3><h3 id="gbdt-优缺点">GBDT 优缺点</h3><p><strong>优点：</strong></p><ul><li>预测阶段，因为每棵树的结构都已确定，可并行化计算，计算速度快。</li><li>适用稠密数据，泛化能力和表达能力都不错，数据科学竞赛榜首常见模型。</li><li>可解释性不错，鲁棒性亦可，能够自动发现特征间的高阶关系。</li></ul><p><strong>缺点：</strong></p><ul><li>GBDT在高维稀疏的数据集上，效率较差，且效果表现不如SVM或神经网络。</li><li>适合数值型特征，在NLP或文本特征上表现弱。</li><li>训练过程无法并行，工程加速只能体现在单颗树构建过程中。</li></ul><h2 id="lightgbm">LightGBM</h2><h3 id="参考">参考</h3><ul><li><a href="https://blog.csdn.net/qq_41780234/article/details/135999504">梯度提升树系列1——梯度提升树（GBDT）入门：基本原理及优势_梯度提升树原理-CSDN博客</a></li><li><a href="https://blog.csdn.net/ShowMeAI/article/details/123402422">图解机器学习算法(9) | GBDT模型详解（机器学习通关指南·完结）-CSDN博客</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 统计学习方法 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KAN 网络</title>
      <link href="/2024/06/04/KAN%E7%BD%91%E7%BB%9C/"/>
      <url>/2024/06/04/KAN%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<p>KAN 的官方仓库：<a href="https://github.com/KindXiaoming/pykan">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a></p><p>KAN 和 NeRF 结合：https://github.com/lif314/X-KANeRF</p><p>1）相同网络层数下，MLP的参数更少，优化也更快，但质量没有KAN-based的好；（2）不同的基函数的实验结果也不同，目前来看，Gaussian RBF是最好的，训练时收敛最快，效果也是最好，总的训练时间也跟MLP的差不多；（3）现在的实验都只在lego这个数据集上测试，如果场景变大，需要的网络模型容量就要更大，这个时候MLP只能增加更多的层数，训练速度变慢，而KAN可以在较少的层数下拥有更多的参数，这个时候，KAN可能比MLP更适合NeRF。</p><p>在连续学习中：KAN &gt; MLP</p><p>KAN 的特点：1）可解释性， 2）记忆性</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>删除两个标记字符中间的内容代码存档</title>
      <link href="/2024/05/07/%E5%88%A0%E9%99%A4%E4%B8%AD%E9%97%B4%E7%9A%84%E5%86%85%E5%AE%B9%E4%BB%A3%E7%A0%81%E5%AD%98%E6%A1%A3/"/>
      <url>/2024/05/07/%E5%88%A0%E9%99%A4%E4%B8%AD%E9%97%B4%E7%9A%84%E5%86%85%E5%AE%B9%E4%BB%A3%E7%A0%81%E5%AD%98%E6%A1%A3/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考链接：</p><ul><li><a href="https://blog.csdn.net/u010713935/article/details/103876901">python 批量删除两个指定字符中间的字符串，字符串，删除指定字符_python把文件中两个字符串间的字符全部删除-CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/139596371">Python 正则表达re模块之findall()详解 - 知乎 (zhihu.com)</a></li></ul></blockquote><p>在搭建环境时，遇见了下面这种从 linux 系统导出的 yml 文件，导致在windows 系统上无法正确运行，因此需要对文件进行修改，即把每个程序包的第二个“=”后的乱码删去。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="string">_ipyw_jlab_nb_ext_conf=0.1.0=py39h06a4308_0</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">_libgcc_mutex=0.1=main</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">_openmp_mutex=4.5=1_gnu</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">absl-py=1.0.0=pyhd8ed1ab_0</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">addict=2.4.0=pyhd8ed1ab_2</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">alabaster=0.7.12=pyhd3eb1b0_0</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">anaconda-client=1.9.0=py39h06a4308_0</span></span><br></pre></td></tr></table></figure><p>编写代码如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;C:/Users/LENOVO/Desktop/diangong.txt&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:  <span class="comment">#打开文本</span></span><br><span class="line">    data = f.read()   <span class="comment">#读取文本</span></span><br><span class="line">    <span class="comment">#print(data)</span></span><br><span class="line">    </span><br><span class="line">data=<span class="built_in">list</span>(data)</span><br><span class="line">tt=<span class="number">0</span></span><br><span class="line">index=-<span class="number">1</span></span><br><span class="line">flag=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">    index+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> d==<span class="string">&quot;=&quot;</span>: </span><br><span class="line">        tt+=<span class="number">1</span></span><br><span class="line">        flag=<span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> tt%<span class="number">2</span>==<span class="number">0</span> <span class="keyword">and</span> flag==<span class="number">0</span>:    <span class="comment">#设置前后两个标记字符分别为@和#</span></span><br><span class="line">        data[index]=<span class="string">&quot;@&quot;</span></span><br><span class="line">        flag=<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> d==<span class="string">&quot;\n&quot;</span>: data[index]=<span class="string">&quot;#&quot;</span></span><br><span class="line">data=<span class="string">&quot;&quot;</span>.join(data)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment"># import string</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;&quot;删除两个标记字符中间的字符串内容&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">deletesACharacterBetweenTwoSpecifiedCharacters</span>(<span class="params">startStr,endStr,string,saveName</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    def deletesACharacterBetweenTwoSpecifiedCharacters(startStr,endStr,filename,saveName):</span></span><br><span class="line"><span class="string">    # 打开文件获取文件里面的内容</span></span><br><span class="line"><span class="string">    file = open(filename +&#x27;.txt&#x27;)</span></span><br><span class="line"><span class="string">    lines = file.readlines()</span></span><br><span class="line"><span class="string">    string = &#x27;&#x27;.join(lines)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 字符串截取，截取字符串中的两个指定的标识字符中间的内容</span></span><br><span class="line">    <span class="comment"># result = re.findall(&quot;aa(.*?)bb&quot;,string)</span></span><br><span class="line">    <span class="comment"># result = re.findall(startStr+&quot;(.*?)&quot;+endStr, string)</span></span><br><span class="line">    result = re.findall(<span class="string">r&#x27;@.*?#&#x27;</span>, string)</span><br><span class="line">    <span class="comment"># result  是筛选出来的中间字符列表格式</span></span><br><span class="line">    <span class="built_in">print</span>(result)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> ss <span class="keyword">in</span> result:</span><br><span class="line">        <span class="keyword">if</span> ss <span class="keyword">in</span> string: string=string.replace(ss, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    string=string.replace(<span class="string">&quot;  -&quot;</span>,<span class="string">&quot;\n  -&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(string)</span><br><span class="line"></span><br><span class="line">     <span class="comment"># 创建文件名 文件可读写性  编码格式</span></span><br><span class="line">    save = <span class="built_in">open</span>(saveName,mode=<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    save.write(string)</span><br><span class="line">    save.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">deletesACharacterBetweenTwoSpecifiedCharacters(<span class="string">&quot;@&quot;</span>,<span class="string">&quot;#&quot;</span>,data,<span class="string">&#x27;C:/Users/LENOVO/Desktop/res.txt&#x27;</span>)</span><br></pre></td></tr></table></figure><p>运行程序后结果格式如下：</p><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="string">_libgcc_mutex=0.1</span> </span><br><span class="line"><span class="bullet">-</span> <span class="string">_openmp_mutex=4.5</span> </span><br><span class="line"><span class="bullet">-</span> <span class="string">absl-py=1.0.0</span> </span><br><span class="line"><span class="bullet">-</span> <span class="string">addict=2.4.0</span> </span><br><span class="line"><span class="bullet">-</span> <span class="string">alabaster=0.7.12</span> </span><br><span class="line"><span class="bullet">-</span> <span class="string">anaconda-client=1.9.0</span> </span><br><span class="line"><span class="bullet">-</span> <span class="string">anaconda-project=0.10.1</span> </span><br><span class="line"><span class="bullet">-</span> <span class="string">anyio=2.2.0</span> </span><br><span class="line"><span class="bullet">-</span> <span class="string">appdirs=1.4.4</span> </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 代码箱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂七杂八小工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>三维深度补全</title>
      <link href="/2024/04/29/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E8%A1%A5%E5%85%A8/"/>
      <url>/2024/04/29/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E8%A1%A5%E5%85%A8/</url>
      
        <content type="html"><![CDATA[<blockquote><p>一些可能有用的链接：</p><p><a href="https://blog.csdn.net/qq_36686437/article/details/114160640">PCL点云处理算法汇总（C++长期更新版）_点云半径滤波器c++-CSDN博客</a></p></blockquote><h1 id="引言为啥重建的模型会有破洞">引言：为啥重建的模型会有破洞？</h1><blockquote><p>参考链接：<a href="https://zhuanlan.zhihu.com/p/490090910">教你如何解决实景三维模型破洞问题！ - 知乎 (zhihu.com)</a></p></blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240429210114920.png" alt="图来自【PCL专栏】三维点云空洞修复介绍（一）_点云建模孔洞修复-CSDN博客" /><figcaption aria-hidden="true">图来自<a href="https://blog.csdn.net/weixin_41512747/article/details/125460584">【PCL专栏】三维点云空洞修复介绍（一）_点云建模孔洞修复-CSDN博客</a></figcaption></figure><p>主要可以归纳为以下四个原因：</p><ul><li><p><strong>拍摄的照片重叠度不够。</strong>简言之就是需要从多个不同角度拍摄物体，以识别出物体的三维空间信息。同时在拍摄时，要保证前一阵与后一张的视觉变化不能太大，以保证上下有相同的参考信息。</p></li><li><p><strong>物体特征点不明显。</strong>在建模一些纯色、光滑的物体时经常出现破洞，正是因为建模时重复的地方太多，系统识别不出该位置的特征点，进而导致建模失败。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240429201053202.png" alt="image-20240429201053202" /><figcaption aria-hidden="true">image-20240429201053202</figcaption></figure></li><li><p><strong>物体有大量反光或透明。</strong>无论是反射光的颜色和亮度，还是半透明物体的颜色，都可能随着拍摄物体的角度改变而发生变化，因此破洞的概率很大。</p></li><li><p><strong>拍摄场景杂乱。</strong>在室内拍摄的话，可以在模型底部放置一个黑布或与物体色差较大的纸张作为模型对照参考，更方便计算机识别出拍摄对象。如果预算足够，还可以置办一个小型的柔光灯箱，再配上一个电动转盘，只需要站着不动，就能完成所有图像数据的采集！</p></li></ul><h1 id="d-点云补全">3D 点云补全</h1><blockquote><p>参考链接：<a href="https://blog.csdn.net/Yong_Qi2015/article/details/118123852">3D点云补全算法汇总及最新进展-CSDN博客</a> / <a href="https://blog.csdn.net/weixin_41512747/article/details/125460584">【PCL专栏】三维点云空洞修复介绍（一）_点云建模孔洞修复-CSDN博客</a>(赵江洪，孙铭悦，王殷瑞，等.<a href="https://so.csdn.net/so/search?q=三维点云&amp;spm=1001.2101.3001.7020">三维点云</a>孔洞修复方法综述[J]. 测绘科学，2021，46(1)：114-123.)</p></blockquote><p>先介绍三个概念：</p><ul><li><p>概念一：<font color=#df8400>partial observation</font>，一个观测或一个侧面观测全面的过程，很多时候比较具有误导性。点云补全也是从某一个侧面恢复到整体的一个过程。</p></li><li><p>概念二：<font color=#985fff>3D imagination</font>。对于整个3D世界的想象力，其深深植根于我们脑海中。</p></li><li><p>概念三：<font color=#4eb434>LiDAR sensor</font>，一种新的传感器。新一代的 iphone 除了普通的相机传感器之外，更多搭载雷达传感器。这种雷达传感器能够感知周围世界的深度信息，获取包含3D信息的点云数据，如这个椅子的距离信息，而不仅仅是 2D RGB 图像的一个像素值。</p></li></ul><blockquote><p><strong>LiDAR</strong>（Light Detection And Ranging; also LIDAR:Laser Imaging, Detection, And Ranging）: 即光探测和测距，简称为<strong>激光雷达</strong>。它是一种光学遥感技术，用于获得深度信息和点云数据并生成精确的数字化三维模型。In LiDAR, laser light is sent from a source (transmitter) and reflected from objects in the scene. The reflected light is detected by the system receiver and the time of flight (TOF) is used to develop a distance map of the objects in the scene.</p></blockquote><p>综合以上三个概念，首先是 <font color=#df8400>partial observation 片面的观察</font>。这个世界上到处都是片面的观察，如何才能获取真相？- 通过<font color=#985fff> 3D 的想象力</font>，从片面的观察中获得这个世界的真相。这个世界是如何表征 3D？- <font color=#4eb434>LiDAR sensor</font>，即通过手机或其他设备中的雷达传感器。</p><p>明确了这三个概念后，正式进入到 3D 点云补全专题。</p><p>点云补全涉及的相关问题：</p><ul><li>Low-Level: Fine-Grained 3D Shape Reconstruction，即高保真的 3D 形状重建。补全的终极目标是恢复的形状整体完整并且局部均匀。</li><li>Middle-Level: Relational Structures Prediction，即学习关联性的结构特征。关联性的结构推测可能可以隐式地计算形状自身结构的自注意力（如 attention），或者更进一步地显示加强结构关系（如 knowledge graph）。</li><li>High-Level: 3D Point Cloud Generation，即多模态的 3D 点云生成。可以尝试生成多个可能的候选方案，然后选出最佳的完整形状。</li></ul><p><strong>点云补全/点云修复（3D shape completion）主要有以下三个方式：基于几何、基于模型检索、基于深度学习。</strong></p><h2 id="基于几何">基于几何</h2><p>修复方法来自部分输入点云数据的几何形状提示来完成对点云缺失的修复，无需任何外部数据。</p><h3 id="三角网格化点云修复">三角网格化点云修复</h3><p>基于网格的空洞修复应用较为广泛，其主要思想是以多边形网络作为点云修复的边界进行数据的修复。算法流程： （1）寻找孔洞边界，提取边界点的坐标； （2）填充孔洞，对于孔洞边界临近点进行三角分析，或者拟合临近点曲面对缺失部分进行曲面拟合； （3）对曲面进行离散化采样，随机生成孔洞内部点。</p><h3 id="散乱点云模型修复">散乱点云模型修复</h3><p>一般过程：</p><p>识别孔洞边界，根据边界点特征进行离散化曲面构建；再通过曲面上的点，进行点云的修复。孔洞识别需要对空间进行划分，主要有三种方式：octree，kdtree和栅格划分。划分后通过对散乱点云建立空间拓扑关系，进行空间索引，识别孔洞边界，进而对点云进行修复。 【<em>A planar-reflective symmetry transform for 3D shapes</em>(2006)】提出利用平面反射对称变换，使用蒙特卡洛采样算法来计算曲面的变换，增加了迭代优化算法，精确地找到变化最大的局部值，增加了检索效率。</p><p>传统方法存在的问题：</p><ul><li>点云三维格网建立过程复杂</li><li>点云数据具有无序性和散乱性，三维点云投影到二维实践理论会造成大量拓扑关系丢失</li><li>目前提出的算法只针对个别模型或者场景</li></ul><h2 id="基于模型检索">基于模型检索</h2><p>基于模型检索的修复方法将点云转化为模型数据，通过部分输入模型数据与大型形状数据库中的模型进行匹配来完成对于扫描模型的修复。检索的关键是构建有效检索的特征。</p><h3 id="基于文本标注的模型检索">基于文本标注的模型检索</h3><p>人工对模型进行添加关键字、主观性强、效率低、不适合智能、高效的修复方式</p><h3 id="基于内容的模型检索">基于内容的模型检索</h3><p>是以三维模型中特征为依据，检索出与输入数据相似的数据库中模型的一种修复方法。</p><p><strong>直接检索</strong>：从数据库中直接检索适合的上下文模型，对检索到的模型进行扭曲变形，使输入数据相匹配。2005年Pauly从数据库中检索到的上相似模型，对数据库中的模型进行扭曲，使得到的数据与输入数据一致，最终获得合并的3D形状同时定义了与形状匹配的罚函数，以及可以计算上下文模型与输入数据的非刚性对其的优化函数。 <strong>零件组装</strong>：使用低质量的用户级扫描设备可以进行单次扫描重构几何模型。零件组装是3D形状数据库中存放构成模型的零件。将输入数据的局部坐标关联到每个零件，并且可以从数据库中探知零件所处的位置以及分布的方向。 <strong>变形对齐</strong>：检测和分割场景中的对象实例，在大型数据库中检索相似的模型，变形和组装检索的模型以完成模型的修复。随着基于数据驱动的神经网络的发展，该方法可以预测对象的姿态。</p><p>【<em>Aligning 3D models to RGB-D images of cluttered scenes</em>[C] (2015)】中将3D模型与混乱场景的RGB-D图像对齐，使用来自数据库中相应的3D模型来表示RGB-D场景中的对象，将场景中的实例对象分割出后，用包含合成对象渲染的图像中像素表面的法线 训练的卷积神经网络进行预测对象的姿态。</p><p>随着深度学习的发展如今已经出现根据文本、草图、实例等不同的数据类型实现三维模型的多模态模型检索的修复方法。 存在问题： （1）需要分割图像和识别对象类别的先验知识； （2）需要人工输入相应的点，确保非刚性对齐的准确性； （3）无法保障输出的物体是与扫描仪完全扫描的数据。</p><p>【<em>Shape completion from as Single RGBD image</em>[J] (2017)】提出了Morfit的修复方法，该修复方法利用曲线驱动的拓扑几何从不完整的点云进行交互式曲面修复。能够从具有尖锐特征的3D对象不完整和系数扫描中重建表面，该修复方法在相邻曲线轮廓之间进行最佳的插值，并将曲面捕捉到输入点。该修复方法是在用户编辑和修改之间进行交互，但需要大量人工操作，导致效率低下。</p><h3 id="基于深度学习">基于深度学习</h3><p>近年来，利用深度学习技术在三维深度补全上取得了很多进展。通常使用卷积神经网络（CNN）或者生成对抗网络（GAN）等方法，从局部观测到的三维信息中学习到全局的三维结构，并生成缺失的部分。这种方法可以利用大量的训练数据来学习三维场景的先验知识和结构。</p><h1 id="网格曲面补洞">网格曲面补洞</h1><blockquote><p>参考链接：<a href="https://www.cnblogs.com/bxyan/p/6679947.html">孔洞修补研究总结 - 懒小小喵 - 博客园 (cnblogs.com)</a></p><details><summary>关于其应用中的<font color=#ef042a>重建骨骼</font>，比较典型的方法有：</summary><p>1.用径向基函数从不完整的扫描数据生成连续网格；</p><p>2.基于CT等值面数据生成曲面；</p><p>3.用傅里叶级数拟合CT图像提取边缘轮廓曲线；</p><p>4.基于形状的利用数学形态学算子进行骨架提取的插值算法。</p></details></blockquote><p>所谓<strong>网格曲面孔洞修补问题</strong>指的是：寻找一个合理的算法在现有的不完整几何与拓扑信息的情况下，利用网格孔洞周边已有三角网格，创建一个尽可能忠实于原物体或场景的局部网格模型，由此达到对网格曲面孔洞的修补。</p><p><strong>常用的孔洞修补算法可以分为</strong><font color=#df8400>体素方法</font>、<font color=#985fff>基于三角剖分的方法</font>、<font color=#4eb434><strong>基于隐式曲面拟合的方法</strong></font>。</p><p>在<font color=#df8400>基于体素的方法</font>中，首先将一个网格模型变换成由离散体积（即体素）表示的模型，然后在体积空间中应用不同的方法修补空洞.采用的方法有<u>有向距离函数差分</u>或<u>体素滤波</u>，以及在此基础上使用<u>多种偏微分方程</u>进行优化的方法，消除重建三维结构中因采样问题产生的孔洞。</p><p>基于体素方法存在两方面的问题，首先，它针对的是点云数据，不能直接应用于Mesh模型；其次它是一种物理方法，通常只能够修复重建物体表面的小孔洞，并容易带来几何形变。</p><p>在<font color=#985fff>基于三角剖分的方法</font>中，首先对孔洞直接进行三角剖分得到初始网格，然后对初始网格进行优化。<strong>点集的三角剖分指的是</strong>：给定一组散乱数据点，如何将各数据点之间以三角形或四面体相互连接，形成三角网格，并使网格质量较优．该问题的解即是散乱点集的一个三角剖分，其实质是<u>以三角网格反映各个数据点与其邻近点之间的拓扑连接关系，从而揭示数据点之间的内在本质联系</u>．</p><p><font color=#4eb434><strong>基于隐式曲面拟合的方法</strong></font>是网格孔洞修补算法中最为常见的方法，该方法首先采样孔洞边界周围的点信息，采用隐式曲面拟合的方法拟合一曲面片光滑的覆盖孔洞，然后通过在曲面上重采样实现网格孔洞修补.<u>常见的曲面拟合</u>有：<u>二次曲面拟合、B-样条曲面拟合、三角Bézier曲面拟合及基于径向基函数的曲面拟合</u>等。</p><p><strong>对于模型的孔洞修补算法主要采用以下几种策略：</strong></p><p>1、<u>直接三角剖分</u>：早期的孔洞修补算法主要考虑如何对孔洞区域进行三角化。三角剖分本质上是一个非线性优化问题，为了获得该问题的最优解，可以使用动态规划算法、遗传算法、模拟退火算法等对孔洞区域进行剖分，以获得最优的三角化结果。已有方法：孔洞多边形的顶点构造新的三角片；各向异性孔洞修补算法。</p><p>2、<u>新增采样点</u>：已有方法：分段处理思想，根据孔洞的复杂性将孔洞分为若干个简单的子孔洞；三角片自然增长。</p><p>3、<u>采样点调整</u>：已有方法：根据曲率来调整新增点的位置；基于移动最小二乘法的补点方法；基于体素扩散方法来调整新增点的位置；基于径向基函数的调整方法；神经网络的方法。</p><p>4、<u>点云数据三角化</u>。</p><p>Delaunay三角剖分具有三角剖分最小内角为最大的性质，能够进行任意多连通域有限网格的自动生成可最大限度的保证网中三角形满足近似等边性，避免了过于狭长和尖锐的三角形的出现，是公认的最优三角网。其主流算法有三种：分割-归并法，逐点插入法，三角网生长法。</p><p><strong>一个理想的孔洞修补算法具有如下性质：</strong></p><p>（1）<u>自动性。</u>用户如果选择了孔洞，交互应该尽可能简单。</p><p>（2）<u>效率性。</u>合理的运行时间，良好的交互速度。</p><p>（3）<u>准确性。</u>用修补网格来填补孔洞，应该尽可能得和周围网格相融合，网格密度和形状应该和原孔洞边界相匹配。</p><p>（4）<u>鲁棒性。</u>能够处理任意网格的任意孔洞。这是很难被满足的。</p><p>基于RBF的三角网格补洞</p><blockquote><p>参考链接：<a href="https://www.cnblogs.com/shushen/p/5759679.html">三维网格补洞算法（Radial Basis Function） - 算法小丑 - 博客园 (cnblogs.com)</a></p></blockquote><p>基于Poisson方程的三角网格补洞</p><blockquote><p>参考链接：<a href="https://www.cnblogs.com/shushen/p/5864042.html">三维网格补洞算法（Poisson Method） - 算法小丑 - 博客园 (cnblogs.com)</a></p></blockquote><p>该算法首先需要根据孔洞边界生成一个初始化补洞网格，然后通过法向估算和Poisson方程来修正补洞网格中三角面片的几何形状，使其能够适应并与周围的原始网格融合。</p><h1 id="深度补全">深度补全</h1><blockquote><p>参考链接：<a href="https://developer.aliyun.com/article/1143287">2022最新综述！稀疏数据下的深度图补全（深度学习/非引导/RGB引导）（上）-阿里云开发者社区 (aliyun.com)</a> / <a href="http://cea.ceaj.org/CN/PDF/10.3778/j.issn.1002-8331.2209-0284?token=d9b2536c4a854a4ca6d475c31c16c0f0">2023-13 (ceaj.org)</a></p></blockquote><h2 id="非引导深度补全">非引导深度补全</h2><p>给定稀疏的深度图，非引导方法的目标是直接用深度神经网络模型补全它，即<strong>稀疏深度图的稠密化处理</strong>。以前的方法通常可以分为三组：使用1）稀疏感知CNN的方法，2）归一化CNN，和3）使用辅助图像进行训练的方法。</p><h2 id="rgb引导深度补全">RGB引导深度补全</h2><p>非引导方法通常表现不如RGB引导方法，并受到模糊效果和对象边界扭曲的影响，归因于关于自然场景的先验信息不足。利用RGB信息作为附加输入是简单而合理的。RGB图像提供关于场景结构的信息，包括纹理、线条和边缘，以补充稀疏深度图的缺失线索，并鼓励平滑区域内的深度连续性和边界处的不连续性。此外，它们包括一些单目线索，例如消失点[43]，用于促进深度估计，这些好处补充了稀疏深度图。</p><p>迄今为止，已经提出了不同类型的方法，它们主要可以分为五种类型：1）早期融合模型，2）晚期融合模型，3）显式3D表示模型，4）残余深度模型，以及5）基于空间传播网络（SPN）的模型。</p><p>来自chatgpt:</p><p>深度图补全是指从部分观测到的深度图像中推断出完整的深度图像的过程，通常用于计算机视觉和图像处理领域。以下是一些常见的深度图补全方法：</p><ol type="1"><li><strong>基于插值和填充的方法</strong>：<ul><li><p><strong>双线性插值（Bilinear Interpolation）</strong>：最简单的补全方法之一是使用双线性插值。该方法利用已知的深度值来预测缺失区域的深度，通过对周围已知深度值的加权平均来估计缺失区域的深度值。</p></li><li><p><strong>基于图像和深度信息融合（Image and Depth Fusion）</strong>：利用RGB图像和部分深度信息，通过图像分割或者区域填充的方法来推断缺失区域的深度值。</p></li></ul></li><li><strong>基于优化的方法</strong>：<ul><li><strong>能量最小化（Energy Minimization）</strong>：建立能量函数，包括深度一致性、图像一致性以及平滑性等约束，通过优化能量函数来求解最优的深度图补全结果。</li></ul></li><li><strong>基于图像和深度信息融合的方法</strong>：<ul><li><strong>基于多传感器融合（Multi-Sensor Fusion）</strong>：利用不同传感器（如RGB相机和深度传感器）提供的信息，结合图像特征和深度信息，进行深度图的补全。</li></ul></li><li><strong>基于学习的方法</strong>：<ul><li><strong>深度学习（Deep Learning）</strong>：近年来，利用深度学习技术在深度图补全上取得了显著的进展。使用卷积神经网络（CNN）或者生成对抗网络（GAN）等深度学习模型，从局部观测到的深度信息中学习全局的深度结构，生成缺失区域的深度图。</li></ul></li><li><strong>基于统计和概率的方法</strong>：<ul><li><strong>贝叶斯推断（Bayesian Inference）</strong>：利用已知的深度信息和先验知识，使用贝叶斯推断方法推断缺失区域的深度。</li></ul></li><li><strong>基于三维重建的方法</strong>：<ul><li><strong>基于三维重建的补全（3D Reconstruction-Based Completion）</strong>：利用局部的三维点云或曲面信息，结合几何和深度信息，推断缺失区域的深度。</li></ul></li><li><strong>基于传感器模型的方法</strong>：<ul><li><strong>传感器模型推断（Sensor Model Inference）</strong>：利用传感器的物理模型和观测到的数据，推断缺失区域的深度值。</li></ul></li></ol><p>这些方法在不同的应用场景和数据类型下具有各自的优势和适用性。选择合适的深度图补全方法通常取决于问题的特定要求，包括数据的类型、缺失区域的大小和形状，以及应用的实际需求。深度学习方法在深度图补全中表现出色，尤其是当数据量充足时，可以学习到更复杂的深度结构和场景信息。</p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PPT 转 PDF 小程序</title>
      <link href="/2024/04/22/%E5%88%B6%E4%BD%9C%20PPT%20%E8%BD%AC%20PDF%20%E5%B0%8F%E7%A8%8B%E5%BA%8F/"/>
      <url>/2024/04/22/%E5%88%B6%E4%BD%9C%20PPT%20%E8%BD%AC%20PDF%20%E5%B0%8F%E7%A8%8B%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[<blockquote><p>官方网址：<a href="https://pypi.org/project/Spire.Presentation/">Spire.Presentation · PyPI</a></p><p>参考说明：<a href="https://blog.csdn.net/Eiceblue/article/details/135415462">Python实现PowerPoint（PPT/PPTX）到PDF的批量转换_python ppt转pdf-CSDN博客</a></p><p>参考打包教程：<a href="https://blog.csdn.net/libaineu2004/article/details/112612421">Python脚本打包成exe，看这一篇就够了！_python 打包-CSDN博客</a></p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Windows 复现代码指南</title>
      <link href="/2024/04/20/Windows%20%E5%A4%8D%E7%8E%B0%E4%BB%A3%E7%A0%81%E6%8C%87%E5%8D%97/"/>
      <url>/2024/04/20/Windows%20%E5%A4%8D%E7%8E%B0%E4%BB%A3%E7%A0%81%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<h1 id="windows-复现代码指南">Windows 复现代码指南</h1><p><span style="background:#fbd4d0;">安装环境时需要注意<strong>Python版本</strong>、<strong>Pytorch版本</strong>和<strong>CUDA版本</strong>是否兼容！</span></p><h2 id="python修改pip源">Python修改pip源</h2><h3 id="一步到位版">一步到位版</h3><blockquote><p>参考网址：<a href="https://www.jianshu.com/p/b2412f7fc93f">pip换源一行命令直接搞定 - 简书 (jianshu.com)</a></p></blockquote><h4 id="使用官方源">使用官方源</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip config <span class="built_in">set</span> <span class="keyword">global</span>.index-url  https://pypi.org/simple  </span><br><span class="line">pip install xx -i https://pypi.org/simple</span><br></pre></td></tr></table></figure><h4 id="使用国内源">使用国内源：</h4><p>打开cmd,输入：<code>pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</code></p><p>或者如果临时使用的话，可以使用：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install +库名 -i +源</span><br><span class="line">eg:    pip install numpy -i http://mirrors.aliyun.com/pypi/simple/</span><br></pre></td></tr></table></figure><p><strong>几个国内源</strong></p><p>阿里云 <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fmirrors.aliyun.com%2Fpypi%2Fsimple%2F">http://mirrors.aliyun.com/pypi/simple/</a> 中国科技大学 <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fpypi.mirrors.ustc.edu.cn%2Fsimple%2F">https://pypi.mirrors.ustc.edu.cn/simple/</a> 豆瓣(douban) <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fpypi.douban.com%2Fsimple%2F">http://pypi.douban.com/simple/</a> 清华大学 <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fpypi.tuna.tsinghua.edu.cn%2Fsimple%2F">https://pypi.tuna.tsinghua.edu.cn/simple/</a> 中国科学技术大学 <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fpypi.mirrors.ustc.edu.cn%2Fsimple%2F">http://pypi.mirrors.ustc.edu.cn/simple/</a></p><h2 id="离线安装pytorch">离线安装Pytorch</h2><p>参考网址：<a href="https://blog.csdn.net/weixin_47142735/article/details/113684365">离线安装Pytorch 最简单 高效的方法_pytorch离线安装_正在学习的浅语的博客-CSDN博客</a></p><h3 id="在线安装指令离线下载地址">在线安装指令&amp;离线下载地址</h3><blockquote><p><strong>在线安装</strong>的一些指令例子：</p><ul><li>安装 torch 的官方指令网址：<a href="https://pytorch.org/get-started/previous-versions/#conda">Previous PyTorch Versions | PyTorch</a></li><li>安装单个 torch 包: pip --default-timeout=1000 install torch==1.10.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html</li><li>同时安装多个包：pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116</li></ul></blockquote><p>在线下载安装包，特别是比较大的安装包，很容易因为网络原因失败:</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231202203943.png" alt="image-20231202203938666" /><figcaption aria-hidden="true">image-20231202203938666</figcaption></figure><p>所以我们考虑先将需要的包下载后，再离线安装。下载地址：<span style="background:#fbd4d0;"><a href="http://download.pytorch.org/whl/torch_stable.html">download.pytorch.org/whl/torch_stable.html</a></span></p><blockquote><p><strong>附上pytorch其他相关包的下载地址：</strong></p><ul><li><p>下载 torch_scatter：<a href="https://pytorch-geometric.com/whl/">pytorch-geometric.com/whl/</a> （点击链接选择对应的 torch 与 cuda，具体可参考 <a href="https://blog.csdn.net/weixin_42421914/article/details/132875571">torch、torch-scatter版本依赖和下载安装教程_torch_scatter下载-CSDN博客</a>）</p></li><li><p><a href="https://mmcv.readthedocs.io/zh-cn/latest/get_started/installation.html">安装 MMCV — mmcv 2.1.0 文档</a></p></li></ul></blockquote><h3 id="下载离线安装包">下载离线安装包</h3><p>点进去后有pytorch安装包、torchaudio安装包和torchvision安装包等，可以通过<span style="background:#eef0f4;">Ctrl + F</span>寻找需要下载的包。主要有两种pytorch安装包（<span style="background:#f9eda6;">一般只使用gpu版本的安装包</span>）：</p><ol type="1"><li><p>cpu版本pytorch,<font color=red>开头为cpu</font>;</p></li><li><p>gpu版本pytorch,<font color=red>开头为cu</font>，如cu111表示gpu版本pytorch，且该pytorch的cuda版本为11.1;</p></li></ol><p>cp表示python版本，linux/window 表示系统版本。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231202204318.png" alt="cpu" /><figcaption aria-hidden="true">cpu</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231202204355.png" alt="gpu" /><figcaption aria-hidden="true">gpu</figcaption></figure><p><strong>注意，我们要根据自己cuda的版本和系统版本来下载安装包，且一定要使Python版本、Pytorch版本和CUDA版本三者兼容！！！</strong>下面附一张版本对应关系表（也可以去问chatgpt）： <img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231202212206.png" alt="cuda_pytorch" /></p><blockquote><ol type="1"><li>torch_scatter 与 pytorch版本对应关系：见<a href="https://pytorch-geometric.com/whl/">pytorch-geometric.com/whl/</a></li><li>torchvision 与 pytorch 版本 linux 系统对应关系（如果直接安装torchvision，可能会自动安装最新的版本，同时也把torch升级到最新，因此安装时可以参考联合安装的方法）</li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240118150625277.png" alt="image-20240118150625277" /><figcaption aria-hidden="true">image-20240118150625277</figcaption></figure></blockquote><h3 id="离线安装以pycharm安装为例">离线安装（以Pycharm安装为例）</h3><p>打开Pycharm中的终端，切换到需要安装包的环境，先使用<span style="background:#dad5e9;">cd指令</span>跳转到下载文件夹；然后<span style="background:#d4e9d5;">pip install 安装包名.后缀</span>进行安装；最后可使用<span style="background:#f9eda6;">conda list 或 python - import torch - torch.cuda.is_available()（需要完成cuda+cudnn+torch配套安装）</span>进行查看。<span style="background:#fbd4d0;">大功告成！</span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231202212635162.png" alt="image-20231203191239667" /><figcaption aria-hidden="true">image-20231203191239667</figcaption></figure><p><strong>值得注意的是，一定要使用纯英文的路径，并且安装包的名字不饿能有任何变动，这些问题都会导致安装的失败。</strong>如果在终端中直接安装，别忘了先用<span style="background:#eef0f4;">conda activate</span>先激活环境。</p><h2 id="安装cuda">安装cuda</h2><h3 id="cuda版本选择">cuda版本选择</h3><p>法1：查看 NVIDIA 控制面板，点击系统信息，查看组件</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20241112151133485.png" alt="image-20241112151133485" /><figcaption aria-hidden="true">image-20241112151133485</figcaption></figure><p>可以看到其中第三行为CUDA 12.3.107 driver，因此可以选择CUDA版本最高为12.3。</p><p>法2：打开cmd，输入nvidia-smi 查看 driver api 的 cuda版本（显示为支持的最高版本 cuda）。</p><h3 id="安装过程">安装过程</h3><p>cuda官网下载地址：<span style="background:#fbd4d0;"><a href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA Toolkit Archive | NVIDIA Developer</a></span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231203184824.png" alt="image-20231203184822148" /><figcaption aria-hidden="true">image-20231203184822148</figcaption></figure><p>按照图中白色位置选择下载包，然后双击选择自定义安装。</p><p>在文件资源管理器中创建一个名为cuxx的文件夹，用来存放临时的cuda安装包；然后设置安装路径（安装包在安装后可直接删除）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203190515923.png" alt="image-20231203190515923" /><figcaption aria-hidden="true">image-20231203190515923</figcaption></figure><p>检查完系统兼容性后，点击<span style="background:#d4e9d5;">同意并继续</span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203190837475.png" alt="image-20231202212635162" /><figcaption aria-hidden="true">image-20231202212635162</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203190858663.png" alt="image-20231203190858663" /><figcaption aria-hidden="true">image-20231203190858663</figcaption></figure><p>勾选<span style="background:#d4e9d5;">自定义</span>，点击<span style="background:#d4e9d5;">下一步</span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203191209483.png" alt="image-20231203191209483" /><figcaption aria-hidden="true">image-20231203191209483</figcaption></figure><p>如果是<strong>第一次安装</strong>需要把组件都勾选上，但是<span style="background:#f9eda6;">浅黄色背景一般情况下只勾选第一个 cuda </span>！如需更新显卡驱动的可以勾选另外两个选项，但是建议不怎么做。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203191239667.png" alt="image-20231203190837475" /><figcaption aria-hidden="true">image-20231203190837475</figcaption></figure><p><span style="background:#fbd4d0;"><strong>选择安装位置，并记住安装路径：</strong></span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203191504491.png" alt="image-20231203192352716" /><figcaption aria-hidden="true">image-20231203192352716</figcaption></figure><p>等待安装完成</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203191624776.png" alt="image-20231203193341199" /><figcaption aria-hidden="true">image-20231203193341199</figcaption></figure><h3 id="验证是否安装成功">验证是否安装成功</h3><p>打开<span style="background:#dad5e9;">设置 - 系统 - 系统信息 - 高级系统设置 - 高级 - 环境变量</span>，出现橙色框中的两个环境变量（也可在cmd中输入<code>set cuda</code>查看cuda设置的环境变量）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203192352716.png" alt="image-20231203191624776" /><figcaption aria-hidden="true">image-20231203191624776</figcaption></figure><p><span style="background:#f9eda6;">重启电脑！！（否则nvcc -V会找不到指令）</span>进入cmd界面，输入 <code>nvcc -V / nvcc --version</code> 查看版本号，出现如下界面，说明 cuda 安装成功啦！</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203193341199.png" alt="image-20231203191504491" /><figcaption aria-hidden="true">image-20231203191504491</figcaption></figure><h2 id="安装cudnn">安装cudnn</h2><p>注：cudnn是用于配置深度学习使用，相当于cuda的一个专为深度学习运算进行优化的补丁</p><h3 id="安装过程-1">安装过程</h3><p>cudnn 官方下载地址：<span style="background:#d4e9d5;"><a href="https://developer.nvidia.com/rdp/cudnn-archive">cuDNN Archive | NVIDIA Developer</a></span></p><p>进入cudnn下载时需要注册或登录账号，然后选择需要对应版本下载安装包。这里下载的是压缩后的安装包。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203193624549.png" alt="image-20231203193813587" /><figcaption aria-hidden="true">image-20231203193813587</figcaption></figure><p>解压后出现三个文件夹和License</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203193813587.png" alt="image-20231203193624549" /><figcaption aria-hidden="true">image-20231203193624549</figcaption></figure><p><span style="background:#fbd4d0;">找到cuda的安装路径，如：<strong>C:FilesGPU Computing Toolkit1.1</strong></span>。将cudnn三个文件夹里的内容分别替换到对应的文件夹里。</p><p>打开<span style="background:#dad5e9;">设置 - 系统 - 系统信息 - 高级系统设置 - 高级 - 环境变量</span>，再找到<span style="background:#d4e9d5;">系统变量 - path</span>，将以下三个变量添加进去，完成安装(<font color=red>注意修改对应变量名喔</font>)</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\bin</span><br><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\include</span><br><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\lib</span><br><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\libnvvp</span><br></pre></td></tr></table></figure><h3 id="验证是否安装成功-1">验证是否安装成功</h3><p>在cmd中使用<span style="background:#d4e9d5;">cd命令</span>进入cuda的安装路径找到测试工具，如：<span style="background:#f9eda6;">C:FilesGPU Computing Toolkit1.1_suite</span>，执行如下两个.exe文件</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203195334390.png" alt="image-20231203195334390" /><figcaption aria-hidden="true">image-20231203195334390</figcaption></figure><p>执行：<code>deviceQuery.exe</code>查询本机的GPU设备和<code>bandwidthTest.exe</code>测试带宽，如果结果都为PASS，说明运行正常</p><h2 id="安装多个版本的cuda">安装多个版本的cuda</h2><blockquote><p>详细步骤参考：<a href="https://blog.csdn.net/m0_46579864/article/details/122887343">详细讲解如何在win10系统上安装多个版本的CUDA_如何同时安装cuda11.8 和 cuda12.0-CSDN博客</a></p><p>下面假设新安装的 cuda 版本为 11.7。</p></blockquote><h3 id="安装步骤">安装步骤</h3><p>1.首先老规矩，在命令行 <code>cmd</code> 输入 <code>nvidia-smi</code> 查看电脑驱动支持的最高版本 cuda (作者这里查看了，是 cuda 12.3)。</p><p>2.新建文件夹<code>v11.7</code>存放cuda ,这一步选择默认位置 <span style="background:#fbd4d0;">C:GPU Computing Toolkit</span> 新建即可，方便之后切换环境时更改地址里的版本号。</p><p>3.在官网上下载好 <code>CUDA Toolkit</code> 包进行离线安装（网速快的话在线安装也可）。双击安装包后出现的第一个目录是临时目录，下载后会自动删除。<span style="background:#fbd4d0;">这里需要注意的是，安装选项选择<strong>自定义</strong>，且自定义安装选项里只勾选<strong>CUDA 组件</strong>即可</span>。</p><p>4.为新安装的cuda配置cudnn :将下载的<strong>cuDNN</strong>压缩包解压，并把<strong>cuda</strong>下的“bin”、“include”、“lib”这三个文件拷贝到新CUDA的安装位置<span style="background:#fbd4d0;">C:FilesGPU Computing Toolkit1.7</span>上。</p><p>5.添加新cuda到环境变量：在地址栏搜索“环境变量”直接进入，在环境变量中直接更改之前cuda环境变量的版本号就OK！</p><p>6.重启电脑，通过在终端输入指令<code>nvcc -V</code> 与 <code>set cuda</code> 查看cuda是否安装成功。</p><h3 id="切换不同版本的cuda">切换不同版本的cuda</h3><p>搜索“环境变量”进入，在<code>PATH</code>中将想要切换的cuda对应环境变量移动到最上面。</p><h2 id="拆卸某版本的cuda">拆卸某版本的cuda</h2><blockquote><p>详情参考：<a href="https://segmentfault.com/a/1190000044598668">Windows 下 CUDA, cudnn, pytoch 卸载、更新、安装 - 个人文章 - SegmentFault 思否</a></p><p>这里以拆卸 cuda 11.2 为例</p></blockquote><p>1.打开<code>控制面板</code>或<code>设置-应用</code>，拆卸含"cuda 11.2"字样的程序。</p><p>2.搜索<code>环境变量</code>进入，删除cuda11.2的环境变量。</p><p>3.进入cuda目录，一般为<code>C:\ProgramData\NVIDIA GPU Computing Toolkit\CUDA</code>和<code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA</code>，删除残留文件。</p><h1 id="问题汇总">问题汇总：</h1><h2 id="cuda-driver-version-is-insufficient-for-cuda-runtime-version"><strong>CUDA driver version is insufficient for CUDA runtime version</strong></h2><p>cuda驱动程序版本和cuda运行时版本不匹配：问题可大可小，可能是程序装错了，也可能你的电脑根本配不了cuda环境（如amd显卡）</p><h2 id="failed-to-initialize-nvml-unknown-error"><strong>Failed to initialize NVML: Unknown Error</strong></h2><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216143546376.png" alt="image-20231216143546376" /><figcaption aria-hidden="true">image-20231216143546376</figcaption></figure><p>在复现代码时，配置完cuda后，想验证<code>torch.cuda.is_available()</code>，结果过了很久也没有输出；然后又输入<code>nvidia-smi</code>查看显卡配置，报错！</p><p>在网上查找后发现了几个方法：</p><h3 id="win11下运行nvidia-smi报错failed-to-initialize-nvml-unknown-error-csdn博客"><strong><a href="https://blog.csdn.net/qq_39499680/article/details/134855395">WIN11下运行nvidia-smi报错Failed to initialize NVML: Unknown Error-CSDN博客</a></strong></h3><p>使用Everything(或电脑自带的文件搜索)去查找nvidia-smi的位置，然后进入文件目录（如：<span style="background:#d4e9d5;">C:_dispig.inf_amd64_49aadc39d4f73881</span>），点击运行 <span style="background:#f9eda6;color:red">setup.exe</span> 重新安装即可。</p><p>尝试方法一，在点击 setup.exe 后报错：<span style="background:#fb6172d0;color:white">所需文件丢失</span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216144502672.png" alt="image-20231216144502672" /><figcaption aria-hidden="true">image-20231216144502672</figcaption></figure><h3 id="考虑重新安装nvidia驱动"><strong>考虑重新安装nvidia驱动</strong></h3><h4 id="卸载驱动">2.1 卸载驱动</h4><p>打开<font color=purple>设备管理器</font>，点击<span style="background:#dad5e9;">显卡适配器-驱动程序-卸载设备</span>，然后重启电脑</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216151900323.png" alt="image-20231216151900323" /><figcaption aria-hidden="true">image-20231216151900323</figcaption></figure><p>发现这时显卡已经变为<font color=purple>Microsoft基本显示适配器</font>：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216152946150.png" alt="image-20231216152946150" /><figcaption aria-hidden="true">image-20231216152946150</figcaption></figure><h4 id="重新安装驱动">2.2 重新安装驱动</h4><p>打开nvidia驱动下载地址：<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn#">官方驱动 | NVIDIA</a>，按自己的电脑型号选择合适的驱动（笔记本产品系列是<font color=red>notebooks</font>），开始下载</p><blockquote><ul><li><strong>查看驱动支持的最高版本cuda</strong>: <a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#title-resolved-issues">CUDA 12.4 Update 1 Release Notes (nvidia.com)</a></li></ul></blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216153411364.png" alt="image-20231216153411364" /><figcaption aria-hidden="true">image-20231216153411364</figcaption></figure><p>完成下载后双击，在解压提示框中点击OK，等待进度完成（此处默认安装路径是：<font color=orange>C:\546.33_Win10-DCH_64</font>）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154244543.png" alt="image-20231216154244543" /><figcaption aria-hidden="true">image-20231216154244543</figcaption></figure><p>等待检查系统兼容性完成，后续工作见图</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154626870.png" alt="image-20231216154626870" /><figcaption aria-hidden="true">image-20231216154626870</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154713103.png" alt="image-20231216154713103" /><figcaption aria-hidden="true">image-20231216154713103</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154731465.png" alt="image-20231216154731465" /><figcaption aria-hidden="true">image-20231216154731465</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154826092.png" alt="image-20231216154826092" /><figcaption aria-hidden="true">image-20231216154826092</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154918798.png" alt="image-20231216154918798" /><figcaption aria-hidden="true">image-20231216154918798</figcaption></figure><blockquote><p>安装快完成时会提示<strong>是否立即重启计算机完成安装</strong>，点击确定前记得保存未保存的文件</p></blockquote><p>耐心等待安装完成后，再次打开<strong>设备管理器</strong>，即可发现安装成功！</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216155635451.png" alt="image-20231216155635451" /><figcaption aria-hidden="true">image-20231216155635451</figcaption></figure><p>我们打开终端，输入<code>nvidia-smi</code>,成功输出显卡驱动信息</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216155757435.png" alt="image-20231216155757435" /><figcaption aria-hidden="true">image-20231216155757435</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 环境搭建 </tag>
            
            <tag> cuda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/04/19/%E7%82%B9%E4%BA%91%E4%B8%8E%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"/>
      <url>/2024/04/19/%E7%82%B9%E4%BA%91%E4%B8%8E%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<p><a href="https://aitechtogether.com/article/46747.html">浅谈点云与三维重建 | AI技术聚合 (aitechtogether.com)</a></p><p><a href="https://blog.csdn.net/Yong_Qi2015/article/details/118123852">3D点云补全算法汇总及最新进展-CSDN博客</a></p><p><a href="https://zhuanlan.zhihu.com/p/370997659">CVPR 2021 | RfD-Net: 从点云中重建三维物体实例 - 知乎 (zhihu.com)</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>最优化问题概述</title>
      <link href="/2024/04/11/%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E6%A6%82%E8%BF%B0/"/>
      <url>/2024/04/11/%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="预备知识">预备知识</h1><h1 id="基本模型">基本模型</h1><p>最优化问题的一般数学模型为： <span class="math display">\[\begin{align}\min&amp;\text{\ }f\left( x \right) \tag{1.1}\\s.t.\ h_i\left( x \right) =&amp;0,\ i=1,...,m;\tag{1.2}\\g_j\left( x \right) \ge &amp;0,\ j=1,...,p\text{。}\tag{1.3}\end{align}\]</span></p><blockquote><p><span class="math inline">\(s.t.\)</span> 为英文“subject to"缩写，表示受限制于。</p></blockquote><p>其中，<span class="math inline">\(x=(x_1,x_2,..,x_n)^T\)</span> 即 <strong><span class="math inline">\(x\)</span> 是n维向量</strong>。在实际问题中，<span class="math inline">\(x_1,x_2,...,x_n\)</span> 常被称为<strong>决策变量</strong>。求极小值的函数<span class="math inline">\(f(x)\)</span> 称为<strong>目标函数</strong>。<span class="math inline">\(h_i(x)=0\)</span> 称为<strong>等式约束</strong>。<span class="math inline">\(g_j(x)\ge 0\)</span> 称为<strong>不等式约束</strong>。对于求目标函数极大值的问题，可以通过 <span class="math inline">\(max\text{\ }f(x)=min \text{\ }[-f(x)]\)</span> 来化为求解极小值问题。</p><p>满足等式约束和不等式约束的解 <span class="math inline">\(x\)</span> 称为<u>可行解/可行点/容许解</u>。全体可行解构成的集合称为<u>可行域/容许集</u>，记为 <span class="math inline">\(D\)</span> ： <span class="math display">\[D=\left\{ x|h_i\left( x \right) =0,i=1,···,m;g_j\left( x \right) \ge 0,j=1,···,p,x∈R^n \right\}\]</span> 若 <span class="math inline">\(h_i(x)\)</span> 和 <span class="math inline">\(g_j(x)\)</span> 是连续函数，则 <span class="math inline">\(D\)</span> 是闭集。</p><h2 id="整体最优解与局部最优解">整体最优解与局部最优解</h2><p>若 <span class="math inline">\(x^*\in D\)</span> ，对于一切 <span class="math inline">\(x \in D\)</span> 恒有 <span class="math inline">\(f(x^*)\le f(x)\)</span> ，则称 <span class="math inline">\(x^*\)</span> 为最优化问题的<u>整体最优解</u>。 若 <span class="math inline">\(x^*\in D\)</span> ，对于一切 <span class="math inline">\(x \in D\)</span> 且 <span class="math inline">\(x\ne x^*\)</span> 恒有 <span class="math inline">\(f(x^*)&lt; f(x)\)</span> ，则称 <span class="math inline">\(x^*\)</span> 为最优化问题的<u>严格整体最优解</u>。</p><p>若 <span class="math inline">\(x^* \in D\)</span> ，存在 <span class="math inline">\(x^*\)</span> 的某邻域 <span class="math inline">\(N_{\epsilon}(x^*)=\{x| \text{\ }\lVert x-x^* \rVert &lt;\epsilon,\epsilon&gt;0 \}\)</span> ，使得对一切 <span class="math inline">\(x\in D\bigcap N_{\epsilon}(x^*)\)</span> ，有 <span class="math inline">\(f(x^*)\le f(x)\)</span> ，则称 <span class="math inline">\(x^*\)</span> 为最优化问题的<u>局部最优解</u>。 若 <span class="math inline">\(x^* \in D\)</span> ，存在 <span class="math inline">\(x^*\)</span> 的某邻域 <span class="math inline">\(N_{\epsilon}(x^*)=\{x| \text{\ }\lVert x-x^* \rVert &lt;\epsilon,\epsilon&gt;0 \}\)</span> ，使得对一切 <span class="math inline">\(x\in D\bigcap N_{\epsilon}(x^*)\)</span> 且 <span class="math inline">\(x\ne x^*\)</span> ，有 <span class="math inline">\(f(x^*) &lt; f(x)\)</span> ，则称 <span class="math inline">\(x^*\)</span> 为最优化问题的<u>严格局部最优解</u>。</p><p>显然，整体最优解一定是局部最优解，反之不然。一般情况下，很难求出整体最优解，往往只能求出局部最优解。<span style="background:#daf5e9;">最优解 <span class="math inline">\(x^*\)</span> 对应的目标函数值 <span class="math inline">\(f(x^*)\)</span> 称为最优值，常用 $ f^*$ 表示。</span></p><h2 id="模型分类">模型分类</h2><p><strong>线性规划</strong>： 约束函数都是线性函数。</p><p><strong>非线性规划</strong>：约束函数至少有一个是非线性函数。</p><p><strong>二次规划</strong>：目标函数是二次函数，约束函数都是线性函数。</p><p><strong>多目标规划</strong>：目标函数不是数量函数，而是向量函数。</p><p><strong>动态规划</strong>：用于解决多阶段决策问题。</p><h1 id="一般算法">一般算法</h1><p>最优化问题的算法一般有一下迭代格式：</p><blockquote><p>[!IMPORTANT]</p><p>给定初始点 <span class="math inline">\(x_0\)</span> ，令 <span class="math inline">\(k=0\)</span> 。</p><p>1.确定 <span class="math inline">\(x_k\)</span> 处的<strong>可行下降方向</strong> <span class="math inline">\(\boldsymbol{p}_{k}\)</span> ，使得 <span class="math inline">\(f(x_k)\)</span> 沿着该方向移动时函数值有所下降且始终位于可行域内;</p><p>2.以 <span class="math inline">\(x_k\)</span> 为出发点，确定步长 <span class="math inline">\(\alpha_k&gt;0\)</span> ，使得以 <span class="math inline">\(\boldsymbol{p}_{k}\)</span> 为方向作射线 <span class="math inline">\(x_k+\alpha_k\boldsymbol{p}_{k}\)</span> 时有 <span class="math inline">\(f(x_k+\alpha_k\boldsymbol{p}_{k})&lt;f(x_k)\)</span> ; <span style="background:#fbd4d0;">（该步骤也称为“一维搜索”）</span></p><p>3.令 <span class="math inline">\(x_{k+1}=x_k+\alpha_k\boldsymbol{p}_{k}\)</span>;</p><p>4.若 <span class="math inline">\(x_{k+1}\)</span> 满足某种终止准则，或达到最大迭代次数，则停止迭代且以 <span class="math inline">\(x_{k+1}\)</span> 为近似最优解。否则令 <span class="math inline">\(k=k+1\)</span> ，转第1步。</p></blockquote><p><strong>根据不同的原则选择不同的搜索方向 <span class="math inline">\(p_k\)</span> ，就可以得到不同的算法。</strong>（该过程类似于神经网络中的求解损失函数极小值的梯度下降过程。换个思路说，求解损失函数极小值的过程，也是一种最优化问题。）</p><p>求解步长的过程，即步骤2，也称为<u>一维搜索</u>或<u>线性搜索</u>。当搜索方向确定后，一维搜索的优劣便成为求解最优化问题的关键。</p><h2 id="算法的收敛性">算法的收敛性</h2><p>我们称一个最优化算法是收敛的当且仅当<strong>该算法构造的点列 <span class="math inline">\(\{x_k\}\)</span> 能在有限步内得到最优化问题的最优解 <span class="math inline">\(x^*\)</span></strong> 。换言之，点列 <span class="math inline">\(\{x_k\}\)</span> 有极限点，且极限点是最优解 <span class="math inline">\(x^*\)</span> ，则算法收敛。</p><h3 id="局部收敛与全局收敛">局部收敛与全局收敛</h3><p>一个算法是否收敛，往往与初始点 <span class="math inline">\(x_0\)</span>​ 的选取有关：</p><ul><li>如果只有当 <span class="math inline">\(x_0\)</span> 充分接近最优解 <span class="math inline">\(x^*\)</span> 时，由算法产生的点列才收敛于 <span class="math inline">\(x^*\)</span> ，则该算法为具有<span style="background:#f9eda6;"><strong>局部收敛性</strong></span>的算法。</li><li>如果对于任意的初始点 <span class="math inline">\(x_0 \in D\)</span> ，由算法产生的点列都收敛于最优解 <span class="math inline">\(x^*\)</span>​ ，则这个算法是具有<span style="background:#f9eda6;"><strong>全局收敛性</strong></span>的算法。</li></ul><p>由于最优解一般未知，因此具有全局收敛性的算法更有实用意义。</p><p>此外，算法的收敛快慢对算法的优劣评价也至关重要。下面给出有关收敛速度的概念。</p><h3 id="线性收敛超线性收敛与-p-阶收敛">线性收敛、超线性收敛与 <span class="math inline">\(p\)</span>​​ 阶收敛</h3><blockquote><p>[!NOTE]</p><p>牛顿法是平方收敛的，弦截法是1.618阶收敛的，梯度下降法是线性收敛的，随机梯度下降法是次线性收敛的。</p></blockquote><p>设序列 <span class="math inline">\(\{x_k\}\)</span> 收敛于 <span class="math inline">\(x^*\)</span> ,且 <span class="math display">\[\underset{k\rightarrow \infty}{\lim}\frac{\epsilon _{k+1}}{\epsilon _k}=\underset{k\rightarrow \infty}{\lim}\frac{\lVert x_{k+1}-x^* \rVert}{\lVert x_k-x^* \rVert}=\beta，\]</span> 若 <span class="math inline">\(\beta=1\)</span> ，则称序列 <span class="math inline">\(\{x_k\}\)</span> 是<strong>次线性收敛</strong>(sublinear convergence)的； 若 <span class="math inline">\(0&lt; \beta &lt;1\)</span> ，则称序列 <span class="math inline">\(\{x_k\}\)</span> 是<strong>线性收敛</strong>（linear convergence），称 <span class="math inline">\(\beta\)</span> 为收敛比； 若 <span class="math inline">\(\beta=0\)</span> ，则称序列 <span class="math inline">\(\{x_k\}\)</span> 是<strong>超线性收敛</strong>(superlinear convergence)的。</p><p>再次，设序列 <span class="math inline">\(\{x_k\}\)</span> 收敛于 <span class="math inline">\(x^*\)</span> ，且对于某个实数 <span class="math inline">\(p \ge 1\)</span> 有 <span class="math display">\[\underset{k\rightarrow \infty}{\lim}\frac{\epsilon _{k+1}}{\epsilon _k}=\underset{k\rightarrow \infty}{\lim}\frac{\lVert x_{k+1}-x^* \rVert}{\lVert x_k-x^* \rVert ^p}=\beta ,\ 0&lt;\beta &lt;+\infty ,\]</span> 则称序列 <span class="math inline">\(\{x_k\}\)</span> 是 <strong><span class="math inline">\(p\)</span>​ 阶收敛</strong>的。当 <span class="math inline">\(p=2\)</span>​ 时，二阶收敛也称平方收敛。</p><p><span style="background:#daf5e9;">如果我们说一个算法是线性收敛等的，是指算法产生的序列是线性收敛等的。</span></p><blockquote><p>[!CAUTION]</p><p>此处若差值写为误差 <span class="math inline">\(e_k\)</span>​ 形式，可类比于现代数值计算中的迭代绝对误差等内容：<a href="https://baike.baidu.com/item/二阶收敛/2838248">二阶收敛_百度百科 (baidu.com)</a></p><p>另一种收敛定义：<a href="https://baike.baidu.com/item/二阶收敛/2838248">二阶收敛_百度百科 (baidu.com)</a></p></blockquote><p>我们也可以将收敛速度量化为以下形式：</p><ul><li><strong>次线性收敛</strong>： $O(  ) , O(  ) , O(  ) $ 等</li><li><strong>线性收敛</strong>： <span class="math inline">\(O(log\frac{1}{\epsilon})\)</span></li><li><strong>超线性收敛</strong>： <span class="math inline">\(O(log(log \frac{1}{\epsilon}))\)</span></li></ul><p>为了更好地理解上述收敛速度的概念，这里举一个栗子（<a href="https://zhuanlan.zhihu.com/p/278151142">收敛速度的三种形式 - 知乎 (zhihu.com)，附画图代码</a>）：</p><p>令：</p><ul><li><span class="math inline">\(a_k=1/k\)</span> ，这里 <span class="math inline">\(a_k\)</span> 是次线性收敛的</li><li><span class="math inline">\(b_k=1/k^2\)</span> ，这里 <span class="math inline">\(b_k\)</span> 也是次线性收敛的</li><li><span class="math inline">\(c_k=1/2^k\)</span> ，这里 <span class="math inline">\(c_k\)</span>​ 是线性收敛的</li><li><span class="math inline">\(d_k=1/2^{2^k}\)</span> ，这里 <span class="math inline">\(d_k\)</span> 是平方收敛的</li></ul><p>这四个序列都以0为极限，可以感受到， 哪怕是<strong>普通的线性收敛都已经达到指数级别的收敛速度了</strong>（这也是我们为什么把线性收敛也叫指数收敛的原因！）超线性收敛的效果更是难以想象！</p><p>画出四个数列的对数图：</p><figure><img src="C:\Users\21218\AppData\Roaming\Typora\typora-user-images\image-20240227123343023.png" alt="image-20240227123343023" /><figcaption aria-hidden="true">image-20240227123343023</figcaption></figure><p>线性收敛在对数坐标图上已经是一条直线了，平方收敛就更是一条急速向下的曲线，这也就是为什么牛顿法能够在短短四五步迭代就达到我们期望的误差限的原因。至于次线性收敛，则一般<strong>是以多项式的速度逼近</strong>，但即便都是次线性收敛，其实快慢也是可以有区别的（具体区别方法参考栗子链接）。</p><h2 id="迭代终止准则">迭代终止准则</h2><p>对于一种算法，我们还要给出某种终止准则。当某次迭代满足终止准则时，就停止迭代，而以这次迭代得到的 <span class="math inline">\(x_k\)</span> 或 <span class="math inline">\(x_{k+1}\)</span> 作为最优解 <span class="math inline">\(x^*\)</span> 的近似解。常用的终止准则有以下四种：</p><ul><li>$x_{k+1}-x_k &lt;$ 或 $&lt;$ ；</li><li>$| f( x_{k+1} ) -f( x_k ) |&lt;$ 或 $&lt;$ ；</li><li>$f( x_k ) =_k &lt;$ ； #梯度足够小</li><li>上述三种终止准则的组合；</li></ul><p>其中 <span class="math inline">\(\varepsilon &gt; 0\)</span> 是预先给定的足够小的实数。</p><blockquote><p>当 <span class="math inline">\(f(x)\)</span> 具有连续的一阶偏导数时，记 <span class="math inline">\(f(x)\)</span> 在 <span class="math inline">\(x_k\)</span> 处的梯度为 $f( x_k )=_k $​ 。由泰勒公式， <span class="math display">\[f\left( x_k+\alpha p_k \right) =f\left( x_k \right) +\alpha \boldsymbol{g}_{k}^{T}\boldsymbol{p}_{k}+o\left( \alpha \right)\]</span> 其中， <span class="math inline">\(\boldsymbol{p}_{k}\)</span> 为函数在点 <span class="math inline">\(x_k\)</span> 处的下降方向（ <span class="math inline">\(\boldsymbol{g}_{k}^{T}\boldsymbol{p}_{k}&lt;0\)</span> ），<span class="math inline">\(\alpha\)</span> 为步长。</p></blockquote><h1 id="二维最优化问题的几何解释">二维最优化问题的几何解释</h1><p>这里举一个栗子：</p><figure><img src="C:\Users\21218\AppData\Roaming\Typora\typora-user-images\image-20240227141939293.png" alt="image-20240227141939293" /><figcaption aria-hidden="true">image-20240227141939293</figcaption></figure><figure><img src="C:\Users\21218\AppData\Roaming\Typora\typora-user-images\image-20240227142006937.png" alt="image-20240227142006937" /><figcaption aria-hidden="true">image-20240227142006937</figcaption></figure><h1 id="一维搜索">一维搜索</h1><p>即使目标函数 <span class="math inline">\(f(x)\)</span> 是一元函数， 求最小值点也经常需要使用数值迭代方法。 另外，在多元目标函数优化中， 一般每次迭代从上一步的 <span class="math inline">\(\boldsymbol x^{(t)}\)</span> 先确定一个下降方向 <span class="math inline">\(\boldsymbol p^{(t)}\)</span> , 然后对派生出的一元函数 <span class="math inline">\(h(\alpha) = f(\boldsymbol x^{(t)} + \alpha \boldsymbol p^{(t)}),\alpha \ge 0\)</span>, 求最小值点 $min f(x^{(t)} + p^{(t)})=min  h() $ 得到下降的步长 <span class="math inline">\(\alpha_t\)</span> ， 并令 <span class="math inline">\(\boldsymbol x^{(t+1)} = \boldsymbol x^{(t)} + \alpha_t \boldsymbol p^{(t)}\)</span> ， 求步长的过程称为<strong>一维搜索/一维最优化问题</strong>。 所以一维搜索就是求解一元函数 <span class="math inline">\(h(\alpha)\)</span> 的最优化问题。</p><p>搜索可以是求一元函数 <span class="math inline">\(h(\alpha)\)</span> 的精确最小值点， 也可以求一个使得目标函数下降足够多的 <span class="math inline">\(\alpha\)</span> 作为步长。</p><p>有几种求解一维最优化问题的方法：</p><ul><li><strong>Fibonacci 法/分数法</strong><ul><li><span style="background:#daf5e9;">线性收敛</span>，收敛比为 <span class="math inline">\(\frac{\sqrt{5}-1}{2}\)</span>​</li><li>要求函数在初始区间上是下单峰函数，且函数值可求。</li></ul></li><li><strong>黄金分割法/0.618法</strong><ul><li><span style="background:#daf5e9;">线性收敛</span>，收敛比为 <span class="math inline">\(\frac{\sqrt{5}-1}{2}\)</span>​​</li><li>要求函数在初始区间上是下单峰函数，且函数值可求</li><li>事实上，黄金分割法是 Fibonacci 法的极限形式。</li></ul></li><li><strong>平分法</strong><ul><li><span style="background:#daf5e9;">线性收敛</span>，收敛比为 <span class="math inline">\(\frac{1}{2}\)</span></li><li>要求函数在初始区间上是下单峰函数，且具有连续的一阶导数</li></ul></li><li><strong>抛物线法/二次插值法</strong><ul><li><span style="background:#daf5e9;">超线性收敛</span></li><li>用二次多项式逼近函数。利用若干点处的函数值构造一个多项式，用这个多项式的极小点来作为原函数极小点的近似值。</li></ul></li><li><strong>非精确一维搜索 Wolfe 算法</strong><ul><li>在求解非线性规划问题时，前几种精确一维搜索很难达到真正的精确值，因此我们想只计算少量的几个函数值，就可以得到一个满足 <span class="math inline">\(f(x_{k+1})&lt;f(x_k)\)</span> 的近似点 <span class="math inline">\(x_{k+1}=x_k+\alpha_k\boldsymbol{p}_{k}\)</span></li><li>要求产生的点列 <span class="math inline">\(\{x_k\}\)</span> 具有某种收敛性质。所以除了对下降方向 <span class="math inline">\(\boldsymbol{p}_{k}\)</span> 有要求外，对步长 <span class="math inline">\(\alpha_k\)</span> 也有要求，即目标函数 <span class="math inline">\(f(x)\)</span> 要"充分地下降"。</li></ul></li></ul><blockquote><p>[!NOTE]</p><p>1.在 Fibonacci 法和黄金分割法中，都要求 <span class="math inline">\(f(x)\)</span>​ 在初始区间上是下单峰函数。因此我们可以预先用“进退法”求符合该要求的初始区间。</p><p>2.<u>非精确一维搜索</u>，就是找到一个步长 <span class="math inline">\(\alpha\)</span> 满足一定条件即可，好比Wolfe-Powell,Armijo条件； <u>精确一维搜索</u>，就是找到一个参数 <span class="math inline">\(\alpha\)</span> ，使得 <span class="math inline">\(min \text{ \ } f(x+\alpha\boldsymbol{p}_{k})\)</span>​ 。</p></blockquote><h2 id="fibonacci-法分数法">Fibonacci 法/分数法</h2><h3 id="原理讨论">原理讨论</h3><p>设 <span class="math inline">\(f(x)\)</span> 在区间 <span class="math inline">\([a,b]\)</span> 上是下单峰函数，即 <span class="math inline">\(f(x)\)</span> 在区间内有唯一极小值点 <span class="math inline">\(x^*\)</span> 。我们想求最小值点，即求该极小值点 <span class="math inline">\(x^*\)</span> 。 <span class="math inline">\(f(x)\)</span> 在 <span class="math inline">\(x^*\)</span> 左边单调递减，右边单调递增。那么我们该如何求该极小值呢？</p><p>思路很简单，只需指定步长，从区间 <span class="math inline">\([a,b]\)</span> 左端点搜索至右端点，首先在区间内取两个点 <span class="math inline">\(x_1\)</span> , <span class="math inline">\(x_2\)</span> 且 <span class="math inline">\(x_1 &lt; x_2\)</span> 。计算这两点处的函数值 <span class="math inline">\(f(x_1)\)</span> , <span class="math inline">\(f(x_2)\)</span>:</p><ul><li>若 <span class="math inline">\(f(x_1)&lt;f(x_2)\)</span> ,说明 <span class="math inline">\(x_2\)</span> 位于 <span class="math inline">\(x^*\)</span> 右侧，则 <span class="math inline">\(x^* \in [a,x_2]\)</span> ；</li><li>若 <span class="math inline">\(f(x_1) \ge f(x_2)\)</span> ,说明 <span class="math inline">\(x_1\)</span> 位于 <span class="math inline">\(x^*\)</span> 左侧，则 <span class="math inline">\(x^* \in [x_1,b]\)</span>​ 。</li></ul><p>由此，将原区间 <span class="math inline">\([a,b]\)</span> 缩短为 <span class="math inline">\([a,x_2]\)</span> 或 <span class="math inline">\([x_1,b]\)</span>​ 。因为新区间内包含一个已经计算过函数值的点，因此只需根据步长再新取一试点，就可将该新区间又缩短一次。不断重复该过程，直至最终的区间长度满足给定精确度为止。</p><blockquote><p>这里的“新区间内包含一个已经计算过函数值的点”是指：由于 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span> 处的函数值已计算，若缩短后区间为 <span class="math inline">\([a,x_2]\)</span> ，则该区间包含 <span class="math inline">\(x_1\)</span> ，只需再取一点 <span class="math inline">\(x_3\)</span> ,比较 <span class="math inline">\(f(x_1)\)</span> 和 <span class="math inline">\(f(x_3)\)</span> 的值即可。整个过程需要新计算的函数值只有 <span class="math inline">\(f(x_3)\)</span> 。缩短后区间为 <span class="math inline">\([x_1, b]\)</span> 的情况同理。</p></blockquote><figure><img src="C:\Users\21218\AppData\Roaming\Typora\typora-user-images\image-20240228184442917.png" alt="image-20240228184442917" /><figcaption aria-hidden="true">image-20240228184442917</figcaption></figure><p>这种遍历法虽然简单，但是需要迭代的次数多，要不断计算函数值 <span class="math inline">\(f(x)\)</span> ，浪费计算资源严重。<strong>我们想要在初始区间 <span class="math inline">\([a, b]\)</span> 长度一定，且迭代次数固定为 <span class="math inline">\(n\)</span> 的条件下，根据步长新取点来缩短区间后，得到最终的区间最短。</strong><span style="background:#daf5e9;">换言之，问按什么方式取点，求 <span class="math inline">\(n\)</span> 次函数值后可最多将多长的原始区间缩短为最终区间长度为 1 ？</span></p><p>设 <span class="math inline">\(L_n\)</span> 表示试点个数为 <span class="math inline">\(n\)</span> 、最终区间长度为 1 时的原始区间 <span class="math inline">\([a,b]\)</span> 的最大可能长度，即 <span class="math inline">\(L_n = b-a\)</span> 。现在需找出 <span class="math inline">\(L_n\)</span> 的一个上界。设最初的两个试点为 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span> ，且 <span class="math inline">\(x_1&lt;x_2\)</span> 。</p><ul><li>如果极小值点位于 <span class="math inline">\([a,x_1]\)</span> 内，除去 <span class="math inline">\(x_2\)</span> ，则我们至多还有 <span class="math inline">\(n-2\)</span> 个试点，因此 <span class="math inline">\(x_1-a \le L_{n-2}\)</span> ；</li><li>如果极小值点位于 <span class="math inline">\([x_1,b]\)</span> 内，包括 <span class="math inline">\(x_2\)</span> ，则我们至多还有 <span class="math inline">\(n-1\)</span> 个试点，因此 $ b-x_1 L_{n-1}$ 。</li></ul><p>因为 <span class="math inline">\(L_n = b-a = (b-x_1)+(x_1-a) \le L_{n-1} + L_{n-2}\)</span> ，故 <span class="math inline">\(L_n \le L_{n-1}+L_{n-2}\)</span>。</p><p>又显然不计算函数值或只计算一个函数值无法缩短区间，故 <span class="math inline">\(L_0=L_1=1\)</span>。</p>将两个关系式合并，可知若原始区间长度满足递推关系： <span class="math display">\[\begin{align}L_n &amp;=L_{n-1}+L_{n-2},\ n\ge 2, \\L_0 &amp;=L_1=1,\end{align}\]</span> 则 <span class="math inline">\(L_n\)</span> 为最大原始区间的长度。观察发现，该递推关系恰与 Fibonacci 数列 <span class="math inline">\(\{ F_n \}\)</span> 的递推关系 ${<span class="math display">\[\begin{array}{l}    F_n=F_{n-1}+F_{n-2},\,\,n\ge 2,\\    F_0=F_1=1,\\\end{array}\]</span><p>. $ 相同，其中 <span class="math inline">\(F_n\)</span> 表示第 <span class="math inline">\(n+1\)</span> 个 Fibonacci 数。不妨以 <span class="math inline">\(F_n\)</span> 代 <span class="math inline">\(L_n\)</span> 表示表示试点个数为 <span class="math inline">\(n\)</span> 、最终区间长度为 1 时的原始区间 <span class="math inline">\([a,b]\)</span>​ 的最大可能长度。</p><blockquote><p>由以上讨论可知，若原区间长度为1，计算n次函数值所能获得的最大缩短率(缩短后的区间长度与原区间长度之比)为1/<span class="math inline">\(F_n\)</span>。例如<span class="math inline">\(F_{20}\)</span>=10946，所以计算20个函数值即可把原长度为L的区间缩短为 <span class="math inline">\(L/10946\)</span> 长度的区间。</p></blockquote><p><span style="background:#daf5e9;">故答：由于求 <span class="math inline">\(n\)</span> 次函数值后最终区间长度为 1 ，则原始区间长度最长为 <span class="math inline">\(F_n\)</span> 。</span></p><p>若原始区间为 <span class="math inline">\([a,b]\)</span> ,要求最终的区间长度小等于<u>绝对精度</u> <span class="math inline">\(\varepsilon(\varepsilon &gt; 0)\)</span>​ ，则有 <span class="math display">\[\varepsilon \ge \frac{b-a}{F_n}\Rightarrow F_n\ge \frac{b-a}{\varepsilon}。 \tag{1}\]</span> <strong>由（1）式可确定试点个数 <span class="math inline">\(n\)</span>​ 。</strong></p><blockquote><p>若换一种说法，已知原区间为 <span class="math inline">\([a_0,b_0]\)</span> ，要将其长度缩短为原来的 <span class="math inline">\(\delta\)</span> 倍，则如何确定试点个数 <span class="math inline">\(n\)</span> ?</p><p>设缩短后区间为 <span class="math inline">\([a_{n-1},b_{n-1}]\)</span> ，则缩短后区间长度满足: <span class="math inline">\(b_{n-1}-a_{n-1} \le (b_0-a_0)\delta\)</span> ，即: <span class="math display">\[F_n\ge \frac{b_0-a_0}{b_{n-1}-a_{n-1} }\ge \frac{1}{\delta}\]</span> 式中 <span class="math inline">\(\delta\)</span> 为一个正小数，称为区间缩短的<u>相对精度</u>。通过式子 <span class="math inline">\(F_n \ge \frac{1}{\delta}\)</span> 即可确定满足条件的最小的 <span class="math inline">\(n\)</span> 。有时给出区间缩短的绝对精度 <span class="math inline">\(\varepsilon\)</span> ，即要求 <span class="math display">\[b_{n-1}-a_{n-1} \le \varepsilon\]</span> 显然，相对精度与绝对精度有关系式： <span class="math inline">\(\varepsilon = (b_0-a_0)\delta\)</span> 。</p></blockquote><p>试点个数 <span class="math inline">\(n\)</span> 确定后，由于 Fibonacci 数列 <span class="math inline">\(\{F_n\}=\{1,1,2,3,...,F_{n-1},F_n \}\)</span> 中首项 1 表示最终区间长度，尾项 <span class="math inline">\(F_n\)</span> 表示原始区间长度，因此我们应该倒过来从右往左看。即试点个数确定后，区间缩短率依次为 <span class="math display">\[\frac{F_{n-1}}{F_n},\ \frac{F_{n-2}}{F_{n-1}},\ ...\ ,\frac{3}{5},\ \frac{2}{3},\ \frac{1}{2}\]</span> 因此<strong>最初的两个试点 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span> 应该满足以下关系式</strong>： <span class="math display">\[\begin{align}\frac{x_1-a}{b-a} &amp;=1-\frac{F_{n-1}}{F_n}=\frac{F_{n-2}}{F_n},\\\frac{x_2-a}{b-a} &amp;=\frac{F_{n-1}}{F_n},\\end{align}\]</span> 即取试点时， <span class="math display">\[\begin{align}x_1&amp;=a+\frac{F_{n-2}}{F_n}\left( b-a \right)=b-\frac{F_{n-1}}{F_n}\left( b-a \right)\\x_2&amp;=a+\frac{F_{n-1}}{F_n}\left( b-a \right)\end{align}\]</span> 显然 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span> 关于区间 <span class="math inline">\([a,b]\)</span> 对称，即有 <span class="math inline">\(x_1-a=b-x_2\)</span>​ 。</p><p>具体过程可参考下图：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240305195155122.png" alt="image-20240305195155122" /><figcaption aria-hidden="true">image-20240305195155122</figcaption></figure><p>通过计算<span class="math inline">\(f(x_1)\)</span> , <span class="math inline">\(f(x_2)\)</span> 并比较其大小就得到一个新的区间。新区间不妨仍然记为区间 <span class="math inline">\([a,b]\)</span> 。这就完成了一次迭代。现在假设已经迭代了 <span class="math inline">\(i-1\)</span> 次,<strong>在第 <span class="math inline">\(i\)</span> 次迭代开始时</strong>,我们还有 <span class="math inline">\(n-i+1\)</span> 个试点,其中包括已经计算过函数值的一个试点。这时令 <span class="math display">\[\begin{array}{l}x_{1}=a+\frac{F_{n-i-1}}{F_{n-i+1}}(b-a), \\x_{2}=a+\frac{F_{n-i}}{F_{n-i+1}}(b-a),\end{array}\]</span> <span class="math inline">\(x_1\)</span> , <span class="math inline">\(x_2\)</span> 中已有一个已经计算过函数值,只需再计算另一点的函数值并进行比较,便可完成第 <span class="math inline">\(i\)</span> 次迭代。<strong>当 <span class="math inline">\(i=n-1\)</span> 时,即进行最后一次迭代时</strong>,由于 <span class="math inline">\(F_0=F_1=1\)</span> , <span class="math inline">\(x_1\)</span> 与 <span class="math inline">\(x_2\)</span> 重合,且已计算过函数值,因此第 <span class="math inline">\(n\)</span> 个试点应选在离该点距离为一个充分小的正数 <span class="math inline">\(\varepsilon\)</span> 处。</p><p>归纳以上讨论,就得到一个求解问题 <span class="math display">\[\underset{a\le x\le b}{\min}\ f\left( x \right)\]</span> 的方法,这个方法叫 Fibonacci 法。</p><h3 id="迭代步骤">迭代步骤</h3><p>根据上述讨论，可将 Fibonacci 法的迭代步骤简述如下：</p><p>需要指出的是,在使用 Fibonacci 法之前必须事先计算出计算函数值的次数 <span class="math inline">\(n\)</span> 。<span style="background:#daf5e9;">除了第一次迭代需要计算两个函数值之外,其余每次迭代只需计算一个函数值</span>。可以证明,在借助于计算 <span class="math inline">\(n\)</span> 个函数值的所有非随机搜索方法中, Fibonacci 法可使原始区间与最终区间长度之比达到最大值。这是它的优点。而 Fibonacci 法的主要缺点是区间缩短率不固定，选取试点的公式不是固定的，这样就增加了计算量。</p><h3 id="代码实现">代码实现</h3><h2 id="黄金分割法">黄金分割法</h2><p>在最优化方法这个数学分支中，可以证明斐波拉契法是压缩比（在同样迭代次数下开始区间和最终区间的比值）最高的方法，黄金分割比相比它压缩比小一些，但是算法会比较简单，占用计算资源也会略少。</p><h1 id="参考链接">参考链接</h1><ul><li><a href="https://www.math.pku.edu.cn/teachers/lidf/docs/statcomp/html/_statcompbook/opt-1d.html#opt-1d-bins">36 一维搜索与求根 | 统计计算 (pku.edu.cn)</a></li><li><a href="https://blog.csdn.net/hei653779919/article/details/106387340">最优化问题——一维搜索(一)_精确一维搜索步长公式-CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/78507430">【优化】一维搜索方法 - 知乎 (zhihu.com)</a></li><li><a href="https://blog.csdn.net/weixin_44044411/article/details/88091024">【python】最优化方法之一维搜索（黄金分割法+斐波那契法）_斐波那契法一维搜索-CSDN博客</a></li><li>《最优化方法》解可新、韩立兴等，天津大学出版社</li></ul>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 最优化方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TV 正则化</title>
      <link href="/2024/04/10/TV%20%E6%AD%A3%E5%88%99%E5%8C%96/"/>
      <url>/2024/04/10/TV%20%E6%AD%A3%E5%88%99%E5%8C%96/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>TensoRF</title>
      <link href="/2024/04/10/TensoRF%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>/2024/04/10/TensoRF%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<hr /><blockquote><p><strong>论文名称： TensoRF: Tensorial Radiance Fields （TensoRF: 张量辐射场）</strong></p><p>代码地址： [GitHub - apchenstu/TensoRF: <a href="https://github.com/apchenstu/TensoRF">ECCV 2022] Tensorial Radiance Fields, a novel approach to model and reconstruct radiance fields</a></p><p>论文主页： https://apchenstu.github.io/TensoRF/</p></blockquote><h1 id="论文基本信息">论文基本信息</h1><p>TensoRF: Tensorial Radiance Fields<sup>[1]</sup>是由来自上海科技大学的 Chen Anpei 等人发表在 <strong>2022 ECCV</strong> 会议上的SCI论文。</p><p>主要亮点在使用了张量分解技术（tensor decomposition techniques），即<strong>以分解后的低秩张量分量来建模辐射场</strong>。文中提出了一种新的分解技术——VM分解，来克服传统CP分解的局限性。</p><ul><li>高效：在少于 30 min 的时间内重建每个场景的辐射场（NeRF[3] 需要20+ hours）</li><li>质量高：比 NeRF好</li><li>内存占用小：仅需75MB</li></ul><p><strong>sparkling</strong>：如果不用CP分解和VM分解，用Tucker分解与BTD分解来压缩模型参数或节约计算成本加速如何呢</p><h1 id="预备知识">预备知识</h1><h2 id="模型压缩之参数矩阵近似">模型压缩之参数矩阵近似</h2><blockquote><p>之前在<a href="https://hahahaha5606.github.io/2024/03/17/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B9%8B%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/">模型压缩之知识蒸馏</a>中提过，模型压缩技术大致可分为五种：</p><ul><li><strong>模型剪枝</strong>：即移除对结果作用较小的组件，如减少 head 的数量和去除作用较少的层，共享参数等，ALBERT属于这种；</li><li><strong>量化</strong>：比如将 float32 降到 float8；</li><li><strong>知识蒸馏</strong>：将 teacher 的能力蒸馏到 student上，一般 student 会比 teacher 小。我们可以把一个大而深的网络蒸馏到一个小的网络，也可以把集成的网络蒸馏到一个小的网络上；</li><li><strong>参数共享</strong>：通过共享参数，达到减少网络参数的目的，如 ALBERT 共享了 Transformer 层；</li><li><strong>参数矩阵近似</strong>：通过矩阵的低秩分解或其他方法达到降低矩阵参数的目的。</li></ul></blockquote><p>在 TensoRF 中使用到的恰好是这五种模型压缩技术中的<strong>参数矩阵近似</strong>下的<strong>矩阵的低秩分解</strong>。这里的矩阵分解技术主要分为两种： CP 分解和 VM 分解。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240406143709598.png" alt="image-20240406143709598" /><figcaption aria-hidden="true">image-20240406143709598</figcaption></figure><h3 id="cp-分解">CP 分解</h3><p>CP 分解是一种较为经典的矩阵分解技术。我们知道，向量是一维，矩阵是二维，张量就是三维及以上。<strong>从这个角度来看，张量就是高维的矩阵、高高维的向量，因此如果能用向量或矩阵来表示张量。就能达到降维的目的。</strong>以一个三维的张量 <span class="math inline">\(\tau \in \mathbb{R}^{I×J×K}\)</span> 为例，CP分解将其因式分解<code>factorize</code>为一系列<code>秩1张量元</code>(rank-one tensor component)的和： <span class="math display">\[\tau =\sum_{r=1}^R{\text{v}_{r}^{1}\circ}\text{v}_{r}^{2}\circ \text{v}_{r}^{3},\tag{1}\]</span> 这里的 <span class="math inline">\(1、2、3\)</span> 表示区分三个不同维度/mode的 <span class="math inline">\(\mathbb{R}^I、\mathbb{R}^J、\mathbb{R}^K\)</span> 的向量 <span class="math inline">\(\mathbb{v}_r\)</span> ，这三个向量的外积 <span class="math inline">\({\text{v}_{r}^{1}\circ}\text{v}_{r}^{2}\circ \text{v}_{r}^{3}\)</span> 是一个秩1张量元 ( rank-one tensor component )。CP 分解将 <span class="math inline">\(\tau\)</span> 分解为 <span class="math inline">\(R\)</span> 个秩1张量元之和，<span class="math inline">\(R\)</span> 是CP秩。根据外积定义，每个张量元 <span class="math inline">\(\tau_{ijk}\)</span> 可以写成如下的标量积的和（sum of scalar products）的形式： <span class="math display">\[\tau_{ijk}= \sum^R_{r=1} \mathbf{v}^1_{r,i} \mathbf{v}^2_{r,j} \mathbf{v}^3_{r,k}\tag{2}\]</span> 这里，<span class="math inline">\(i、j、k\)</span>​​ 表示三个 mode 的索引（<span class="math inline">\(i=1,...,I; j=1,...,J; k=1,...,K\)</span>）。</p><p><strong>CP 分解的优点</strong>在于它可以直接被用于我们的张量辐射场中，并重建出高质量的结果如表1。</p><p><strong>但是它的局限性在于</strong>：由于高度压缩（multiple compact rank-one components）(<span style="background:#daf5e9;">这些秩1张量因子又被分解为多个向量因子的外积，相当于 CP 分解用了大量的 pure vector factors</span>)，CP 分解需要很多 components 来建模复杂场景，进而导致高计算成本，拖慢了速度。</p><h3 id="vm-分解">VM 分解</h3><p><strong>VM 分解（vector-matrix decomposition，VM）</strong>是一种新的向量矩阵分解技术。它是作者受到分项分解（block term decomposition, BTD）的启发提出的，可以更 efficient 地重建辐射场。</p><p>VM 分解中不仅用了一维向量因子 vector factor 来降低张量维度，还用到了二维矩阵因子 matrix factor ： <span class="math display">\[\tau =\sum_{r=1}^{R_1}{\mathbf{v}_{r}^{1}}\circ \mathbf{M}_{r}^{2,3}+\sum_{r=1}^{R_2}{\mathbf{v}_{r}^{2}}\circ \mathbf{M}_{r}^{1,3}+\sum_{r=1}^{R_3}{\mathbf{v}_{r}^{3}}\circ \mathbf{M}_{r}^{1,2}\tag{3}\]</span> 这里的矩阵因子 <span class="math inline">\(\mathbf{M}_{r}^{2,3}、\mathbf{M}_{r}^{1,3}、\mathbf{M}_{r}^{1,2}\)</span> 分别表示3-mode中的三种不同切片，即 <span class="math inline">\(\mathbb{R}^{J×K}、\mathbb{R}^{I×K}、\mathbb{R}^{I×J}\)</span>。对于每个 component，我们放宽它的两个mode的秩可以为任意大 arbitrarily large，用矩阵表示；只限制一个mode是秩1的，用向量表示。这样，<span style="background:#daf5e9;">相比于CP中使用三个分离的向量因子表示张量元，我们用一个矩阵因子和一个向量因子表示张量元，放宽了对秩的限制，且 <span class="math inline">\(R_1、R_2、R_3\)</span>​ 可以根据每个mode的复杂程度设置为不同的值</span>，可以进一步节约计算成本，使速度加快（见表2、表3），从而弥补了 CP 分解的不足。 作者还提到，VM 分解可被视为 BTD 分解的特例。</p><blockquote><p>ppdop：<span style="background:#eef0f4;">这里对秩的理解是这样，向量可以视为一阶张量，且作为张量可以用向量做外积表示，因此根据秩1张量的定义，“向量”张量的秩是1。而矩阵是二阶张量，且未必能用多个向量做外积表示，因此"矩阵"张量的秩是不固定的，可以是 arbitrarily large 的。</span></p></blockquote><p>注意 <strong>VM 分解的每个张量元都比 CP 分解中的参数数量更多</strong>（一个矩阵的参数 vs. 两个向量的参数），因此一方面导致了更低的压缩 lower compactness 的缺点（见表1），另一方面也可视为表示更加复杂的高维数据特征的优点，即在建模相同复杂度的功能时需要的 component 的数量更少。 <span style="background:#eef0f4;">嗯，提出缺点后立即找补，好办法~</span></p><p>虽然 VM 分解在 compactness 上无法达到 CP 分解那么高，但是相比于稠密网格表示<code>dense grid representation</code> 来说仍具有很大优势，将内存复杂度从 <span class="math inline">\(O(N^3)\)</span> 降低至 <span class="math inline">\(O(N^2)\)</span> 。</p><h2 id="用张量来表示重建场景">用张量来表示重建场景</h2><p><strong>作者直接用三维空间的 <span class="math inline">\(\mathbf{XYZ}\)</span> 坐标对应到三维张量的三个维度，且 <span class="math inline">\(\mathbf{XYZ}\)</span> 轴每个方向的特征网格分辨率（数量）为 <span class="math inline">\(IJK\)</span></strong> 。同时虽然前面在讲 VM 分解时提到三个维度的 分解秩可以是不同的，但是作者在文章中认为场景的三个坐标轴方向的复杂度可以是一样的，因此把它们设定为一样的，即 <span class="math inline">\(R_1=R_2=R_3=R\)</span> 。那么 VM 分解的公式就可以重新写成下式（4.1），并进一步简化为（4.2）： <span class="math display">\[\begin{align}\tau &amp;=\sum_{r=1}^{R}{\mathbf{v}_{r}^{X}}\circ \mathbf{M}_{r}^{Y,Z}+{\mathbf{v}_{r}^{Y}}\circ \mathbf{M}_{r}^{X,Z}+{\mathbf{v}_{r}^{Z}}\circ \mathbf{M}_{r}^{X,Y}\tag{4.1}\\&amp;=\sum_{r=1}^R{\mathcal{A}_{r}^{X}+}\mathcal{A}_{r}^{Y}+\mathcal{A}_{r}^{Z}\tag{4.2}\end{align}\]</span> 这样，每个张量元素就可写成： <span class="math display">\[\begin{align}\tau_{ijk}&amp;= \sum_{r=1}^R\text{v}^X_{r,i}\text{M}^{Y,Z}_{r,jk} + \text{v}^Y_{r,j}\text{M}^{X,Z}_{r,ik} + \text{v}^Z_{r,k}\text{M}^{X,Y}_{r,ij} \tag{5.1} \\&amp;=\sum^R_{r=1} \sum_m \mathcal{A}^m_{r,ijk} \ , \  m\in XYZ  \tag{5.2}\end{align}\]</span> 类似，可以简化 CP 分解的表达式为： <span class="math display">\[\tau=\sum_{r=1}^R{\text{v}_{r}^{X}\circ}\text{v}_{r}^{Y}\circ \text{v}_{r}^{Z}=\sum_{r=1}^R \mathcal{A}^{\gamma}\]</span> 此时 CP 分解后每个张量元素表达为： <span class="math display">\[\tau_{ijk}= \sum^R_{r=1} \mathbf{v}^X_{r,i} \mathbf{v}^Y_{r,j} \mathbf{v}^Z_{r,k}=\sum^R_{r=1} \mathcal{A}_{r,ijk}^m  \ ,  \ m=\gamma\]</span></p><h1 id="算法框架">算法框架</h1><h2 id="总览">总览</h2><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240409144231888.png" alt="image-20240409144231888" /><figcaption aria-hidden="true">image-20240409144231888</figcaption></figure><blockquote><p>上图是 TensoRF（VM）的重建和渲染框架图。有两条管线，上管线计算 view-dependent 颜色，下管线计算体密度。</p><p>TensoRF (VM) reconstruction and rendering. We model radiance fields as tensors using a set of vectors (v) and matrices (M), which describe the scene along their corresponding (XYZ) axes and are used for computing volume density σ and view-dependent color c in differentiable ray marching. For each shading location x = (x, y, z), we use linearly/bilinearly sampled values from the vector/matrix factors to efficiently compute the corresponding trilinearly interpolated values (A(x)) of the tensor components. The density component values ( Aσ(x)) are summed to get the volume density (σ) directly. The appearance values (Ac(x)) are concatenated into a vector (⊕[Amc (x)]m) that is then multiplied by an appearance matrix B and sent to the decoding function S for RGB color (c) regression.</p></blockquote><p>论文的总框架公式与 NeRF 中一样，都是由<u>三维位置和视角方向</u>映射到<u>体密度和 view-dependent 的颜色</u>的函数： <span class="math display">\[f:(\mathbf{x},d) \rightarrow (\sigma,c)\]</span> 且该函数支持体渲染中可微的 ray marching 。和 DVGO (2022, Sun et al.)[2] 类似地，本文使用常规的三维体素特征网格 <span class="math inline">\(\mathcal{G}\)</span> 来建模函数 <span class="math inline">\(f\)</span> ，我们在 feature channel 把网格分为<font color=#4eb434>几何网格 <span class="math inline">\(\mathcal{G}_{\sigma}\)</span></font> 和<font color=#df8400>外观网格 <span class="math inline">\(\mathcal{G}_c\)</span>​ </font>，分别用来得到体密度和 view-dependent 颜色。其中，只有外观网格 <span class="math inline">\(\mathcal{G}_c\)</span> 会用到视角 <span class="math inline">\(d\)</span>​ 。</p><p><strong>总的来说，基于网格的连续辐射场可以被表示为：</strong> <span class="math display">\[\sigma,c = \mathcal{G}_{\sigma}(\mathbf{x}),S(\mathcal{G}_c(\mathbf{x}),d),\tag{6}\]</span> 这里，<span class="math inline">\(\mathcal{G}_{\sigma}(\mathbf{x}),\mathcal{G}_c(\mathbf{x})\)</span> 分别表示<u>对位置 <span class="math inline">\(\mathbf{x}\)</span>​ 的两个网格进行特征的三线性插值</u>。</p><blockquote><p>外观网格和几何网格进行 VM 分解后，共有 <span class="math inline">\(3R_{\sigma}+3R_c\)</span> 个矩阵，<span class="math inline">\(3R_{\sigma}+6R_c\)</span> 个向量。且 <span class="math inline">\(R_{\sigma}\)</span> 和 <span class="math inline">\(R_c\)</span> 都远远小于 <span class="math inline">\(XYZ\)</span> 轴方向上的网格数 <span class="math inline">\(IJK\)</span>，因此 VM 分解能达到很高的 compact representation。</p></blockquote><h2 id="外观网格">外观网格</h2><p><font color=#df8400>在外观网格 <span class="math inline">\(\mathcal{G}_c\)</span> 中</font>，我们使用预先选定的函数 <span class="math inline">\(S\)</span> 来实现从<u>外观特征向量 <span class="math inline">\(\mathcal{G}_c(x)\)</span> 和视角方向 <span class="math inline">\(d\)</span></u> 到<u>所求颜色 <span class="math inline">\(c\)</span></u> 的映射。例如，这个函数 <span class="math inline">\(S\)</span> 可以是一个小的 MLP 或 SH 函数，那么 <span class="math inline">\(\mathcal{G}_c\)</span> 中包含的分别就是 neural features 或 SH 系数（这两种函数都在我们的模型中工作良好，见表1）。因此 <span class="math inline">\(\mathcal{G}_c \in \mathbb{R}^{I×J×K×P}\)</span> 储存四维张量，其中 <span class="math inline">\(IJK\)</span> 是<span class="math inline">\(\mathbf{XYZ}\)</span> 轴每个方向的特征网格分辨率（数量），而 <span class="math inline">\(P\)</span> 则是外观特征 mode 的输出通道数。</p><p><strong>注意到外观特征 mode 一般比另外三个 mode 更低维，也就是 lower rank，因此我们仅仅用向量因子 <span class="math inline">\(\mathbf{b}_r\)</span> 表示它</strong>（共有 <span class="math inline">\(3R_c\)</span> 个 <span class="math inline">\(\mathbf{b}_r\)</span>）；而另外三个 mode 则作为三维张量的形式被 CP/VM 分解。特别地，张量 <span class="math inline">\(\mathcal{G}_c\)</span> 的 VM 因式分解为： <span class="math display">\[\begin{align}\mathcal{G}_c &amp;=\sum_{r=1}^{R_c}{\mathbf{v}_{c,r}^{X}}\circ \mathbf{M}_{c,r}^{Y,Z}\circ \mathbf{b}_{3r-2} +{\mathbf{v}_{c,r}^{Y}}\circ \mathbf{M}_{c,r}^{X,Z}\circ \mathbf{b}_{3r-1}+{\mathbf{v}_{c,r}^{Z}}\circ \mathbf{M}_{c,r}^{X,Y}\circ \mathbf{b}_{3r}\tag{8.1}\\&amp;=\sum_{r=1}^{R_c}{\mathcal{A}_{c,r}^{X}\circ \mathbf{b}_{3r-2}+}\mathcal{A}_{c,r}^{Y}\circ \mathbf{b}_{3r-1}+\mathcal{A}_{c,r}^{Z}\circ \mathbf{b}_{3r} \tag{8.2}\end{align}\]</span> 将所有表示外观特征mode的向量因子 <span class="math inline">\(\mathbf{b}_r\)</span> 按列 stack together，我们就得到了一个大小为 <span class="math inline">\(P×3R_c\)</span> 的二维矩阵 <span class="math inline">\(\mathbf{B}\)</span>​​ ，该矩阵包含了全局外观特征。</p><p>注意！<strong>在直接计算索引 <span class="math inline">\(ijk\)</span> 处的单个体素的外观特征向量时</strong>，外观 mode 仍然用的是完整的 <span class="math inline">\(P\)</span>-channel 特征向量 <span class="math inline">\(\mathbf{b}_r\)</span> ，表示为： <span class="math display">\[\mathcal{G}_{c,ijk} = \sum_{r=1}^{R_c}{\mathcal{A}_{c,r,ijk}^{X}\mathbf{b}_{3r-2}+}\mathcal{A}_{c,r,ijk}^{Y}\mathbf{b}_{3r-1}+\mathcal{A}_{c,r,ijk}^{Z}\mathbf{b}_{3r} \tag{10}\]</span> 为了进一步简化计算，我们将所有的 <span class="math inline">\(\mathcal{A}_{c,r,ijk}^{m}\)</span> （<span class="math inline">\(m\in \{XYZ\},r=1,...,R_c\)</span>）值堆叠成一个 <span class="math inline">\(3R_c\)</span> 的向量，记作 <span class="math inline">\(\oplus[\mathcal{A}^m_{c,ijk}]_{m,r}\)</span> 。符号“<span class="math inline">\(\oplus\)</span>”表示串联 concatenation。那么（10）式就可以被简化为： <span class="math display">\[\mathcal{G}_{c,ijk} = \mathbf{B} (\oplus[\mathcal{A}^m_{c,ijk}]_{m,r}) \tag{11}\]</span> 这样，当同时计算大量体素时，我们只需先将所有 <span class="math inline">\(\mathcal{A}_{c,r,ijk}^{m}\)</span> 串联成列向量，然后一次性与 shared 矩阵 <span class="math inline">\(\mathbf{B}\)</span> 相乘即可。而不用像(10)式那样反复相乘再相加。</p><h2 id="几何网格">几何网格</h2><p><font color=#4eb434>而在几何网格 <span class="math inline">\(\mathcal{G}_{\sigma}\)</span> 中</font>，我们只考虑输出单通道——网格值只表示体密度 <span class="math inline">\(\sigma\)</span> 的情况，而不需要额外的转换函数。则 <span class="math inline">\(\mathcal{G}_{\sigma} \in \mathbb{R}^{I×J×K}\)</span> 储存表示位置的三维张量。</p><p>为了 compact components，我们将张量进行 VM 分解或 CP 分解。特别地，在 VM 分解中，三维几何张量 <span class="math inline">\(\mathcal{G}_{\sigma}\)</span> 被因式分解为： <span class="math display">\[\begin{align}\mathcal{G}_{\sigma} &amp;=\sum_{r=1}^{R_{\sigma}}{\mathbf{v}_{\sigma,r}^{X}}\circ \mathbf{M}_{\sigma,r}^{Y,Z}+{\mathbf{v}_{\sigma,r}^{Y}}\circ \mathbf{M}_{\sigma,r}^{X,Z}+{\mathbf{v}_{\sigma,r}^{Z}}\circ \mathbf{M}_{\sigma,r}^{X,Y}\tag{7.1}\\&amp;=\sum_{r=1}^{R_{\sigma}} \sum_{m \in XYZ}{\mathcal{A}_{\sigma,r}^{m}}\tag{7.2}\end{align}\]</span> 同时，<strong>在索引 <span class="math inline">\(ijk\)</span> 处的单个体素的密度值可以直接表示为</strong>： <span class="math display">\[\mathcal{G}_{\sigma,ijk}=\sum_{r=1}^{R_{\sigma}} \sum_{m \in XYZ}{\mathcal{A}_{\sigma,r,ijk}^{m}}\tag{9}\]</span> 这样，计算时只需要计算索引处的相应向量因子和矩阵因子的乘积即可。</p><h2 id="改进的三线性插值">改进的三线性插值</h2><blockquote><p>作者认为，“Na¨ıvely achieving trilinear interpolation is costly, as it requires evaluation of 8 tensor values and interpolating them, increasing computation by a factor of 8 compared to computing a single tensor element” 简单地实现三线性插值的成本很高，因为它需要评估 8 个张量值并对它们进行插值，与计算单个张量元素相比，计算量增加了 8 倍！</p><p>对此作者的解决方案是,"We avoid recovering 8 individual tensor el- ements for trilinear interpolation and instead directly recover the interpolated value, leading to low computation and memory costs at run time."我们避免为三线性插值恢复 8 个单独的张量元素，而是直接恢复插值，从而降低运行时的计算和内存成本。</p></blockquote><p>改进后，三线性插值两个网格的公式为： <span class="math display">\[\begin{align}\mathcal{G}_{\sigma}(\mathbf{x}) &amp;= \sum_r\sum_m \mathcal{A}^m_{\sigma,r}(\mathbf{x})   \tag{13}\\\mathcal{G}_c(\mathbf{x}) &amp;=\mathbf{B}(\oplus [\mathcal{A}^m_{c,r} (\mathbf{x})]_{m,r})  \tag{14}\end{align}\]</span> <strong>将上式代入(6)式，那么因式分解后辐射场就可表示为：</strong> <span class="math display">\[\sigma,c = \sum_r\sum_m \mathcal{A}^m_{\sigma,r}(\mathbf{x}),S(\mathbf{B}(\oplus [\mathcal{A}^m_{c,r} (\mathbf{x})]_{m,r}) ,d),\tag{15}\]</span> 这样，只要给定任意三维位置与视觉方向，我们就行计算出连续的体密度与 view-dependent 颜色。</p><h2 id="体渲染">体渲染</h2><p>渲染图像时，我们仍然使用 NeRF 中的可微体渲染。 <img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240409202309869.png" alt="image-20240409202309869" style="zoom:80%;" /></p><h2 id="重建张量辐射场与损失函数">重建张量辐射场与损失函数</h2><blockquote><p>损失函数的内容在论文的附录部分</p></blockquote><p>给定一组具有已知相机姿势的多视图输入图像，我们的张量辐射场通过梯度下降针对每个场景进行优化，最大限度地减少 <font color=#985fff>L2 渲染损失，</font>仅使用地面真实像素颜色作为监督。</p><p>我们的辐射场通过张量分解来解释，并通过一组全局向量和矩阵进行建模，作为在优化中关联和正则整个场的基本因子。<strong>然而，这有时会导致梯度下降中的过度拟合和局部最小值问题，从而导致观测值较少的区域出现异常值或噪声。我们利用压缩感知<code>compressive sensing</code>中常用的标准<font color=#df8400> regularization terms</font>，包括向量和矩阵因子上的 <font color=orange>L1 norm 损失</font>和<font color=orange> TV（total variation）损失</font>，从而有效地解决这些问题。</strong>我们发现仅应用 L1 稀疏性损失对于大多数数据集就足够了。然而，<u>对于输入图像很少（如 LLFF[36]）或不完美的捕获条件（如具有不同曝光度和不一致掩模的 Tanks and Temples [26,31]）的真实数据集，<strong>TV 损失比 L1 norm 损失更有效。</strong></u> 总的损失函数表达如下： <span class="math display">\[\mathcal{L}=||C- \tilde{C}||_2^2+\omega·\mathcal{L}_{reg},\]</span> 其中，<span class="math inline">\(\tilde{L}\)</span> 是真实颜色，而 <span class="math inline">\(\omega\)</span> 是 regularization term 的权重。</p><ul><li><p>在 Synthetic NeRF 和 Synthetic NSVF 数据集上我们使用 L1 稀疏损失+<span class="math inline">\(\omega=0.0004\)</span>。我们直接对密度参数应用<font color=#df8400> L1 稀疏损失</font>： <span class="math display">\[\mathcal{L}_{L1}= \frac{1}{N} \sum^{R_{\sigma}}_{r=1}(||\mathbf{M}_{\sigma,r}||+||\mathbf{v}_{\sigma.r}||),\]</span> 这里"|| · ||"表示绝对值，而 <span class="math inline">\(N=R_{\sigma}·(I\cdot J+I\cdot K+J\cdot K+I+J+K)\)</span> 为总的参数数量。</p></li><li><p>在输入图像很少的真实数据集 LLFF ，或捕获条件不完美的 T&amp;T 数据集上，我们则使用更为有效的 <font color=#df8400>TV 损失</font>： <span class="math display">\[\mathcal{L}_{TV}= \frac{1}{N}\sum(\sqrt{\bigtriangleup ^2 \mathcal{A}^m_{\sigma,r}}+0.1 \cdot\sqrt{\bigtriangleup ^2 \mathcal{A}^m_{C,r}}),\]</span> 这里 <span class="math inline">\(\bigtriangleup^2\)</span> 是相邻矩阵因子或向量因子之间的差值的平方。在使用 TV 损失时，设置 <span class="math inline">\(\omega=1\)</span> 。</p></li></ul><p>为了进一步提高质量并避免局部最小值，我们应用从粗到细的重建。与之前需要对其稀疏选择的体素集进行独特细分的从粗到细的技术不同，我们的从粗到细的重建是通过对 XYZ-mode 向量和矩阵因子进行线性和双线性 upsampling 上采样来简单地实现的。</p><h2 id="实现细节">实现细节</h2><p>见：</p><p><a href="https://zhuanlan.zhihu.com/p/596250930">【论文阅读】TensoRF: Tensorial Radiance Fields - 知乎 (zhihu.com)</a></p><p><a href="https://apchenstu.github.io/TensoRF/">TensoRF: Tensorial Radiance Fields (apchenstu.github.io)</a></p><h1 id="实验">实验</h1><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240408192850614.png" alt="image-20240408192850614" style="zoom:80%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240409204044354.png" alt="image-20240409204044354" style="zoom:80%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240409112153321.png" alt="image-20240409112153321" style="zoom:80%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240409204110423.png" alt="image-20240409204110423" style="zoom:80%;" /></p><h1 id="bib-citation">Bib Citation</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@INPROCEEDINGS&#123;Chen2022ECCV,</span><br><span class="line">  author = &#123;Anpei Chen and Zexiang Xu and Andreas Geiger and Jingyi Yu and Hao Su&#125;,</span><br><span class="line">  title = &#123;TensoRF: Tensorial Radiance Fields&#125;,</span><br><span class="line">  booktitle = &#123;European Conference on Computer Vision (ECCV)&#125;,</span><br><span class="line">  year = &#123;2022&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="参考文献">参考文献</h1><p>[1] Chen A, Xu Z, Geiger A, et al. Tensorf: Tensorial radiance fields[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022: 333-350.</p><p>[2] Sun C, Sun M, Chen H T. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction[C], Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 5459-5469.</p><p>[3] Mildenhall B, Srinivasan P P, Tancik M, et al. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis[C], European Conference on Computer Vision. Cham: Springer International Publishing, 2020: 405-421.</p><ul><li><a href="https://zhuanlan.zhihu.com/p/596250930">【论文阅读】TensoRF: Tensorial Radiance Fields - 知乎 (zhihu.com)</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 辐射场 </tag>
            
            <tag> 加速算法 </tag>
            
            <tag> ECCV2022 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一种模型压缩技术：参数矩阵近似</title>
      <link href="/2024/04/10/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B9%8B%E5%8F%82%E6%95%B0%E7%9F%A9%E9%98%B5%E8%BF%91%E4%BC%BC/"/>
      <url>/2024/04/10/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B9%8B%E5%8F%82%E6%95%B0%E7%9F%A9%E9%98%B5%E8%BF%91%E4%BC%BC/</url>
      
        <content type="html"><![CDATA[<blockquote><p>之前在<a href="https://hahahaha5606.github.io/2024/03/17/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B9%8B%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/">模型压缩之知识蒸馏</a>中提过，模型压缩技术大致可分为五种：</p><ul><li><strong>模型剪枝</strong>：即移除对结果作用较小的组件，如减少 head 的数量和去除作用较少的层，共享参数等，ALBERT属于这种；</li><li><strong>量化</strong>：比如将 float32 降到 float8；</li><li><strong>知识蒸馏</strong>：将 teacher 的能力蒸馏到 student上，一般 student 会比 teacher 小。我们可以把一个大而深的网络蒸馏到一个小的网络，也可以把集成的网络蒸馏到一个小的网络上；</li><li><strong>参数共享</strong>：通过共享参数，达到减少网络参数的目的，如 ALBERT 共享了 Transformer 层；</li><li><strong>参数矩阵近似</strong>：通过矩阵的低秩分解或其他方法达到降低矩阵参数的目的。</li></ul><p>这篇文章应理解 TensoRF 的需要诞生，罗列了有关<strong>参数矩阵近似中的矩阵低秩分解</strong>的相关知识。</p></blockquote><h1 id="张量的基本概念">张量的基本概念</h1><p>首先，什么是张量？<span style="background:#FFCC99;"> 其实从维度的角度看很简单：向量是一维，矩阵是二维，张量就是三维及以上。</span> 或者从计算机里阶数的角度看：向量是一阶张量[]，矩阵是二阶张量[[],[]]，“n张平面”构成一个"立方体"是三阶张量[[[],[],...,[]],[[],[],...,[]]]]，...，将 k 个 n-1 阶张量组合成一个数组，就是 n 阶张量。</p><p><strong>常见的张量实例有：</strong></p><ul><li>向量数据：2D 张量，形状为 (samples, features) 。</li><li>时间序列数据或序列数据：3D 张量，形状为 (samples, timesteps, features) 。</li><li>图像：4D张量，形状为 (samples, height, width, channels) 或 (samples, channels, height, width) 。</li><li>视频：5D张量，形状为 (samples, frames, height, width, channels) 或 (samples,frames, channels, height, width)。</li></ul><h1 id="纤维-fiber切片-slice">纤维 fiber，切片 slice</h1><blockquote><p>图解请看<a href="https://blog.csdn.net/weixin_49883619/article/details/109902127">这个链接</a>。</p></blockquote><p><strong>纤维( fiber )</strong>是指<strong>从张量中抽取一维向量的操作</strong>。在矩阵中固定其中一个维度，可以得到行或者列。类似于矩阵操作，保留一个维度变化，固定其它维度，可以得到有纤维的概念。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240409090724264.png" alt="image-20240409090724264" style="zoom: 80%;" /></p><blockquote><p>图来自 Kolda 的论文[1]（2009，Kolda et al.）：<a href="https://www.cs.cornell.edu/courses/cs6241/2020sp/readings/Kolda-Bader-2009-survey.pdf">Tensor Decompositions and Applications | SIAM Review | Vol. 51, No. 3 | Society for Industrial and Applied Mathematics (cornell.edu)</a></p></blockquote><p>以三维的张量为例，向量是一条线，矩阵是一个面，三维张量就是矩阵（面）的高阶形式，是一个长方体。我们把长方体的三个方向/mode（上下列、左右行、前后管）记作 <span class="math inline">\(i,j,k\)</span> ，那么如果保留管<span class="math inline">\(k\)</span>变化，固定住行<span class="math inline">\(j\)</span>、列<span class="math inline">\(i\)</span>，就得到管纤维 <span class="math inline">\(X_{ij}\)</span>​。</p><p><strong>切片（slice）</strong>是指<strong>在张量中抽取二维矩阵的操作。</strong>在张量中如果保留两个维度变化，固定其它维度，可以得到一个矩阵，这个矩阵即为张量的切片。</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240409090751040.png" alt="image-20240409090751040" style="zoom:80%;" /></p><p>同样以三维的张量为例，如果我们保留行<span class="math inline">\(j\)</span>、列<span class="math inline">\(i\)</span> 变化，固定住管<span class="math inline">\(k\)</span> ，那么我们就能得到一个正面方向的切片 <span class="math inline">\(X_{::k}\)</span>。</p><h1 id="回顾奇异值分解svd">回顾奇异值分解（SVD）</h1><blockquote><p><a href="https://zhuanlan.zhihu.com/p/29846048">奇异值分解（SVD） - 知乎 (zhihu.com)</a></p><p><a href="https://www.zhihu.com/question/506931984">(99+ 封私信 / 81 条消息) 知道特征值和特征向量 怎么求原矩阵? - 知乎 (zhihu.com)</a></p></blockquote><p><strong>奇异值分解(Singular Value Decomposition，SVD)</strong>是在机器学习领域广泛应用的算法，它不光可以用于降维算法(如PCA 降维)中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。</p><p>我们在高等代数中学过关于特征值与特征向量的定义： <span class="math display">\[A\alpha =\lambda \alpha\]</span> 其中，<span class="math inline">\(A\)</span> 是 <span class="math inline">\(n\)</span> 维的原方阵，<span class="math inline">\(\lambda\)</span> 是矩阵 <span class="math inline">\(A\)</span> 的一个<code>特征值</code>， <span class="math inline">\(\alpha\)</span> 是对应于<code>特征值</code> <span class="math inline">\(\lambda\)</span> 的 <span class="math inline">\(n\)</span> 维的<code>特征向量</code>。</p><p><strong>若矩阵 <span class="math inline">\(A\)</span> 满秩</strong>，则它有 <span class="math inline">\(n\)</span> 个特征值 <span class="math inline">\(\lambda_1,\lambda_2,...,\lambda_n\)</span> ，且 <span class="math inline">\(\alpha_1,\alpha_2,...,\alpha_n\)</span> 是分别对应于这些特征值的其中一个向量。以 <span class="math inline">\(n=3\)</span>​ 时的情况为例，即 <span class="math display">\[A\alpha _1=\lambda_1 \alpha _1 \\A\alpha _2=\lambda_2 \alpha _2 \\A\alpha _3=\lambda_3 \alpha _3\]</span> 那么我们有以下推导： <span class="math display">\[A\left( \alpha _1\ \alpha _2\ \alpha _3 \right) =\left( \lambda _1\alpha _1\ \lambda _2\alpha _2\ \lambda _3\alpha _3 \right) =\left( \alpha _1\ \alpha _2\ \alpha _3 \right) \left( \begin{matrix}    \lambda _1&amp;     &amp;       \\    &amp;       \lambda _2&amp;     \\    &amp;       &amp;       \lambda _3\\\end{matrix} \right)\]</span> 令 <span class="math inline">\(( \alpha _1\ \alpha _2\ \alpha _3)=P\)</span> ，那么 <span class="math display">\[\begin{align}AP&amp;=P\left( \begin{matrix}    \lambda _1&amp;     &amp;       \\    &amp;       \lambda _2&amp;     \\    &amp;       &amp;       \lambda _3\\\end{matrix} \right)\\A&amp;=P\left( \begin{matrix}    \lambda _1&amp;     &amp;       \\    &amp;       \lambda _2&amp;     \\    &amp;       &amp;       \lambda _3\\\end{matrix} \right) P^{-1}\end{align}\]</span> 也就是说，如果矩阵 <span class="math inline">\(A\)</span> 的特征值和特征向量都已知，那么就能以特征值为主元构造对角阵 <span class="math inline">\(P\)</span> ，并根据 <span class="math inline">\(A=PBP^{-1}\)</span> 求出原矩阵 <span class="math inline">\(A\)</span> 。</p><p>一般在计算时，我们还会将已知的这些特征向量 <span class="math inline">\(\alpha_1,\alpha_2,...,\alpha_n\)</span> 标准化，即满足 <span class="math inline">\(||\alpha_i||_2=1\)</span> 或 <span class="math inline">\(\alpha_i^T \alpha_i=1\)</span> 。标准化后得到的特征向量是一组标准正交基，满足 <span class="math inline">\(P^TP=I\)</span> ，即 <span class="math inline">\(P^{-1}=P^T\)</span>，此时的 <span class="math inline">\(P\)</span> 也称为酉矩阵/正交矩阵。这样我们就得到 <span class="math inline">\(A\)</span> 的新特征分解表达式： <span class="math display">\[A=P\left( \begin{matrix}    \lambda _1&amp;     &amp;       \\    &amp;       \lambda _2&amp;     \\    &amp;       &amp;       \lambda _3\\\end{matrix} \right) P^T=\sum_{i=1}^3{\lambda _i\alpha _i\alpha _{i}^{T}}\]</span> <strong>特征分解要求 <span class="math inline">\(A\)</span> 必须为方阵，那么如果 <span class="math inline">\(A\)</span> 不是方阵，怎么进行对其进行分解呢？这时，我们的 SVD 登场了！</strong></p><h2 id="svd-定义">SVD 定义</h2><p>SVD 能对非方阵的矩阵进行分解，比特征分解更有普适性。</p><p>假设原矩阵 <span class="math inline">\(A\)</span> 是一个 <span class="math inline">\(m×n\)</span> 的矩阵，与特征分解类似地，<strong>我们定义它的奇异值分解为：</strong> <span class="math display">\[A=U\varLambda V^T\]</span> 其中，<span class="math inline">\(U\)</span> 是 <span class="math inline">\(m×m\)</span> 的酉矩阵，<span class="math inline">\(\varLambda\)</span> 是由奇异值组成的 <span class="math inline">\(m×n\)</span> 的对角阵，<span class="math inline">\(V\)</span> 是 <span class="math inline">\(n×n\)</span> 的酉矩阵。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240406184708508.png" alt="image-20240406184708508" /><figcaption aria-hidden="true">image-20240406184708508</figcaption></figure><p>接下来，我们只需设法求出这三个矩阵即可。</p><h3 id="右奇异矩阵">右奇异矩阵</h3><p>想法就是，原矩阵 <span class="math inline">\(A\)</span> 是 <span class="math inline">\(m×n\)</span> 的非方阵，我们只需做一个小小的变换，将其变为 <span class="math inline">\(n×n\)</span> 方阵 <span class="math inline">\(A^TA\)</span> 就可代入特征分解法中进行计算。</p><p>若 <span class="math inline">\(A\)</span> 列满秩，则 <span class="math inline">\(A^TA\)</span> 有特征值 <span class="math inline">\(\lambda_1,\lambda_2,...,\lambda_n\)</span> 和其对应的特征向量 <span class="math inline">\(\alpha_1,\alpha_2,...,\alpha_n\)</span> ，满足： <span class="math display">\[\left( A^TA \right) \alpha _i=\lambda _i\alpha _i\]</span> 将这些特征向量标准化，得到一组由标准正交基 <span class="math inline">\(v_1,v_2,...,v_n\)</span> 组成的正交矩阵 <span class="math inline">\(V\in R^{n×n}\)</span> ，满足（这里同样以 <span class="math inline">\(n=3\)</span> 为例）： <span class="math display">\[\begin{align}\left( A^TA \right) V&amp;=V\left( \begin{matrix}    \lambda _1&amp;     &amp;       \\    &amp;       \lambda _2&amp;     \\    &amp;       &amp;       \lambda _3\\\end{matrix} \right)\\A^TA&amp;=V\left( \begin{matrix}    \lambda _1&amp;     &amp;       \\    &amp;       \lambda _2&amp;     \\    &amp;       &amp;       \lambda _3\\\end{matrix} \right) V^T\end{align}\]</span> 这里的正交矩阵 <span class="math inline">\(V\)</span> 就是我们在SVD定义中的矩阵 <span class="math inline">\(V\)</span> 。一般地，我们将 <span class="math inline">\(V\)</span> 中的每个特征向量叫做原矩阵 <span class="math inline">\(A\)</span> 的<strong>右奇异向量</strong>。</p><h3 id="左奇异矩阵">左奇异矩阵</h3><p>同理地，我们将代入矩阵分解中的式子替换为 <span class="math inline">\(m×m\)</span> 的方阵 <span class="math inline">\(AA^T\)</span> ，就可以得到左奇异的形式。</p><p>若 <span class="math inline">\(A\)</span> 行满秩，则 <span class="math inline">\(AA^T\)</span> 有特征值 <span class="math inline">\(\lambda_1,\lambda_2,...,\lambda_m\)</span> 和其对应的特征向量 <span class="math inline">\(\beta_1,\beta_2,...,\beta_m\)</span> ，满足： <span class="math display">\[\left( AA^T \right) \beta _i=\lambda _i\beta _i\]</span> 将这些特征向量标准化，得到一组由标准正交基 <span class="math inline">\(u_1,u_2,...,u_m\)</span> 组成的正交矩阵 <span class="math inline">\(U\in R^{m×m}\)</span> 。该正交矩阵 <span class="math inline">\(U\)</span> 就是我们在SVD定义中的矩阵 <span class="math inline">\(U\)</span> 。一般地，我们将 <span class="math inline">\(U\)</span> 中的每个特征向量叫做原矩阵 <span class="math inline">\(A\)</span> 的<strong>左奇异向量</strong>。</p><h3 id="奇异值矩阵">奇异值矩阵</h3><p>由于 <span class="math inline">\(\varLambda\)</span> 除了对角线上是奇异值，<u>其他位置都是0</u>，那我们只需要求出每个奇异值<span class="math inline">\(\sigma_i\)</span>就可以了，有： <span class="math display">\[A=U\varLambda V^T\Rightarrow AV=U\varLambda \Rightarrow Av_i=\sigma _iu_i\Rightarrow \sigma _i=\frac{Av_i}{u_i}\]</span> 这样我们可以求出每个奇异值，进而求出奇异值矩阵 <span class="math inline">\(\varLambda\)</span> 。</p><blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240406193252469.png" alt="image-20240406193252469" /><figcaption aria-hidden="true">image-20240406193252469</figcaption></figure></blockquote><h2 id="svd-计算举例">SVD 计算举例</h2><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/ff2ba132bm9bbb93f1988a97f9088fef.jpg" alt="ff2ba132bm9bbb93f1988a97f9088fef" style="zoom: 67%;" /></p><blockquote><p><font color=#ef042a>这里有一个小错误</font>：在对第②步的 <span class="math inline">\(\lambda_2=1\)</span> 求特征向量时，由于使用的是基础解系法，应该令自由变量 <span class="math inline">\(x_2=1\)</span> ，进而特征向量 <span class="math inline">\(\alpha_2=(-1,1)^T\)</span>，再而 <span class="math inline">\(v_2=1/ \sqrt{2}(-1,1)^T\)</span> 。</p></blockquote><h2 id="svd-的特性">SVD 的特性</h2><p>对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。</p><p>也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵： <span class="math display">\[A_{m×n}=U_{m×m}\varLambda _{m×n}V_{n×n}^{T}\approx U_{m×k}\varLambda _{k×k}V_{k×n}^{T}\]</span> 其中，<span class="math inline">\(k\)</span> 比 <span class="math inline">\(n\)</span> 要小很多。如下图所示，现在我们的矩阵 <span class="math inline">\(A\)</span> 只需要绿色的部分的三个小矩阵就可以近似描述了。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240406201519266.png" alt="image-20240406201519266" /><figcaption aria-hidden="true">image-20240406201519266</figcaption></figure><p>由于这个重要的性质，SVD可以用于数据降维（PCA降维），来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引（LSI）。</p><h1 id="从-svd-到-秩1张量">从 SVD 到 秩1张量</h1><p>首先，来看看 SVD： <span class="math display">\[A=U\varLambda V^T=\sum_{i=1}^r{\sigma _iu_iv_{i}^{T}}\]</span> 第一个等号：在前面的回顾中我们可从 SVD 的定义知，一个秩为 <span class="math inline">\(r\)</span> 的矩阵可以分解为左右奇异向量矩阵以及一个奇异值矩阵的乘积形式。</p><p>第二个等号：我们现在不妨以另外一种角度来看待矩阵的SVD分解。<strong>我们如果拿出第一个左奇异向量 <span class="math inline">\(u_1\)</span> 以及第一个右奇异向量 <span class="math inline">\(v_1\)</span> ，这两个向量做外积 <span class="math inline">\(u_1v_1^T\)</span> ，我们就可以得到一个矩阵，同时这两个奇异向量对应同一个奇异值 <span class="math inline">\(\sigma_1\)</span> ，我们尝试将奇异值理解为这两个向量外积得到的这个矩阵的在原始矩阵中所占的权重，以此类推我们就可以得到所有奇异值对应的左右奇异向量外积的结果矩阵，然后将这些矩阵加起来就得到了原始矩阵 <span class="math inline">\(A\)</span> 。</strong>（这一想法是可验证的）</p><p>现在，我们知道了二维矩阵的SVD分解是<u>将二维矩阵分解为多组向量的外积-权重和</u>。然后，联系向量是一维，矩阵是二维，张量是三维及以上的概念，给出秩1张量的定义：<u>若张量 <span class="math inline">\(\chi \in R^{n_1×...×n_d}\)</span> 可以被写为 <span class="math inline">\(d\)</span> 个向量的外积时，则 <span class="math inline">\(rank(\chi)=1\)</span> ，即张量 <span class="math inline">\(\chi\)</span> 是秩1张量</u>。这里我们可以将秩1张量 <span class="math inline">\(\chi\)</span> 分解为以下形式： <span class="math display">\[\chi =U^{(1)}\circ U^{(2)}\circ ... \circ U^{(d)}\]</span> 其中， <span class="math inline">\(U^{(i)}\)</span> 是一维向量，符号 “<span class="math inline">\(\circ\)</span> ” 表示外积。关于张量的秩的更多内容请看<a href="https://zhuanlan.zhihu.com/p/382219345">这个链接</a>。</p><h1 id="cp-分解">CP 分解</h1><p><strong>CP分解是用低秩张量（秩1张量）的和来近似原始张量，然后将秩1张量表达为一系列向量的外积的形式，从而将高维的张量用低维的向量表示，以达到降低维度和提取张量中潜在结构的目的。</strong></p><p>之前我们在二维矩阵的 SVD 分解中讲到：矩阵的 SVD 分解= <span class="math inline">\(\sum\)</span> [对应奇异值权重·（左奇异向量 <span class="math inline">\(\circ\)</span> 右奇异向量）]。我们不妨将"[·]"中的带有权重的向量外积矩阵称为“因子矩阵”，那么<span style="background:#daf5e9;">矩阵的 SVD 分解就是将矩阵因式分解为许多因子矩阵之和</span>。</p><p>将这种思想由二维矩阵的 SVD 分解推广到三维以上的张量的 CP 分解中，即<span style="background:#daf5e9;"> CP 分解是将张量近似地因式分解为许多张量因子之和</span>。<strong>以使用最为广泛的三维张量为例</strong>，定义一个三维张量 <span class="math inline">\(\chi \in R^{I×J×K}\)</span> 的 CP 分解为： <span class="math display">\[\chi \approx \sum^R_{r=1} a_r \circ b_r \circ c_r ，\tag{1}\]</span> 这里，向量 <span class="math inline">\(a_r,b_r,c_r\)</span> 的维度分别是 <span class="math inline">\(R^I,R^J,R^K\)</span> ，它们作外积后得到一个秩1张量 <span class="math inline">\(a_r \circ b_r \circ c_r\)</span> 。 <span class="math inline">\(R\)</span> 个秩1张量之和就是原张量，<span class="math inline">\(R\)</span> 就是我们所说的 CP 秩。更直观的理解请看下图：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240408160633017.png" alt="image-20240408160633017" /><figcaption aria-hidden="true">image-20240408160633017</figcaption></figure><blockquote><p>且由外积的定义，对原张量 <span class="math inline">\(\chi\)</span> 中的每个元素都有： <span class="math display">\[x_{ijk} \approx \sum^R_{r=1} a_{ir}b_{jr}c_{kr} \ \ for\ i=1,...,I,j=1,...,J,k=1,...,K, \tag{2}\]</span></p></blockquote><p>我们称(1)式中那些通过外积组成秩1张量元的向量集合为“因子矩阵（factor matrices）”，如 <span class="math inline">\(A=[a_1,a_2,...,a_R]\)</span> ,类似地构造 <span class="math inline">\(B、C\)</span> 。这样，式(1)中的<strong>每个mode可写为如下矩阵形式</strong>： <span class="math display">\[\mathrm{X}_{(1)} \approx \mathrm{A}(\mathrm{C}\odot \mathrm{B})^\mathsf{T},\\\mathrm{X}_{(2)} \approx \mathrm{B}(\mathrm{C}\odot \mathrm{A})^\mathsf{T},\\\mathrm{X}_{(3)} \approx \mathrm{C}(\mathrm{B}\odot \mathrm{A})^\mathsf{T}.\]</span> 这里的符号“<span class="math inline">\(\odot\)</span>”表示 Khatri–Rao product。有时候我们也可以把这三个矩阵形式的方程写成 <span class="math inline">\(\chi\)</span> 的正面切片 frontal slices 形式（即用 <span class="math inline">\(c_{k::}\)</span>表示）： <span class="math display">\[\mathcal{X} \approx \mathrm{A}\mathrm{D}^{(k)}\mathrm{B}^\mathsf{T},\, \text{ where }\, \mathrm{D}^{(k)} \equiv \text{diag}(c_{k::}) \, \text{ for $\, k=1,...,K.$}\]</span> 当然啦，正面切片也可以换成侧面或水平的切片。不过这种切片形式的表达很难推广到超过三维的张量中（我们不知道四维以上的形状是怎样的）。因此我们想到了以下矩阵形式的表达。</p><p>利用 Kolda (2006, Kolda et al.)[2]的命名方式，我们可以进一步简化 CP 模型： <span class="math display">\[\chi \approx [\![A,B,C]\!]\equiv \sum_{r=1}^R a_r \circ b_r \circ c_r ，\]</span> 为了便利，我们通常假设 <span class="math inline">\(A,B,C\)</span> 的列向量是归一化的，且它们的权重被储存在向量 <span class="math inline">\(\lambda \in \mathbb{R}^R\)</span> 中，即： <span class="math display">\[\chi \approx [\![\lambda;A,B,C]\!] = \sum_{r=1}^R \lambda_r a_r \circ b_r \circ c_r,\]</span> <strong>如果将其推广到 <span class="math inline">\(N\)</span> 维张量（<span class="math inline">\(N\)</span>th-order tensor）的情形中</strong>， <span class="math inline">\(\chi \in \mathbb{R}^{I_1×I_2×…×I_N}\)</span> 的 CP 分解为： <span class="math display">\[\mathcal{X} \approx [\![\lambda \:; \mathrm{A}^{(1)}, \mathrm{A}^{(2)},\dots,\mathrm{A}^{(N)}]\!] \equiv \sum_{r=1}^R \lambda_r \: \mathrm{a}_r^{(1)} \circ \mathrm{a}_r^{(1)}\circ \dots \circ \mathrm{a}_r^{(N)},\]</span> 其中，<span class="math inline">\(\lambda \in \mathbb{R}^R\)</span> ， <span class="math inline">\(A^{(n)} \in \mathbb{R}^{I_n × R}\)</span> ， <span class="math inline">\(a_r^{(n)}\in \mathbb{R}^R\)</span> ，<span class="math inline">\(n=1,...,N\)</span> 。这种情况下，mode-<span class="math inline">\(n\)</span>​ 矩阵化的版本为： <span class="math display">\[\mathrm{X}_{(n)}\approx \mathrm{A}^{(n)}\Lambda(\mathrm{A}^{(N)}\circ \dots \circ \mathrm{A}^{(n+1)} \circ \mathrm{A}^{(n-1)} \cdot \dots \cdot \mathrm{A^{(1)})^\mathsf{T}},\]</span> 这里 <span class="math inline">\(\Lambda = diga(\lambda)\)</span> 。</p><h1 id="tuker-分解">Tuker 分解</h1><h1 id="btd-分解">BTD 分解</h1><h1 id="参考链接">参考链接</h1><p><a href="https://zhuanlan.zhihu.com/p/302453223">深入理解 | CP、Tucker分解 - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/FDS99999/article/details/131234000">张量分解（Cp、Tuker、Block-Term）_tucker分解-CSDN博客</a></p><p>[张量分解与应用-学习笔记<a href="https://www.cnblogs.com/lywangjapan/p/12068703.html">02] - LyWangJapan - 博客园 (cnblogs.com)</a> (主要是关于文献[1]的翻译理解)</p><p>[1] Kolda T G, Bader B W. Tensor decompositions and applications[J]. SIAM review, 2009, 51(3): 455-500.</p><p>[2] Kolda T G. Multilinear operators for higher-order decompositions[R]. Sandia National Laboratories (SNL), Albuquerque, NM, and Livermore, CA (United States), 2006.</p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 矩阵分解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>矩阵的各种&quot;积&quot;</title>
      <link href="/2024/04/10/%E7%9F%A9%E9%98%B5%E7%9A%84%E5%90%84%E7%A7%8D%E7%A7%AF/"/>
      <url>/2024/04/10/%E7%9F%A9%E9%98%B5%E7%9A%84%E5%90%84%E7%A7%8D%E7%A7%AF/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://blog.csdn.net/xuehuitanwan123/article/details/104291475">矩阵的Kronecker积、Khatri-Rao积、Hadamard积-CSDN博客</a></p><p><a href="https://blog.csdn.net/xq151750111/article/details/121049396">矩阵篇（三）-- 矩阵的普通乘积、Hadamard 积、Kronecker 积及其性质_哈达玛积-CSDN博客</a></p><p>张贤达《矩阵分析与应用》第二版</p></blockquote><h1 id="kronecker-积">Kronecker 积</h1><p><strong>克罗内克积</strong>（<strong>Kronecker product</strong> ,以德国数学家利奥波德·克罗内克命名）是两个任意大小矩阵之间的运算，也称“直积（direct product）”或“张量积(tensor product)”，符号为“<span class="math inline">\(\otimes\)</span>”。矩阵 <span class="math inline">\(A_{m×n}\)</span> 和矩阵 <span class="math inline">\(B_{p×q}\)</span> 的克罗内克积是一个 <span class="math inline">\(mp×nq\)</span> 的分块矩阵，其定义如下： <span class="math display">\[(A_{m×n}) \otimes (B_{p×q})= (A\otimes B)_{mp×nq}= [a_{ij}B]_{i=1,j=1}^{m,n} =\left[ \begin{matrix}    a_{11}B&amp;        \cdots&amp;     a_{1n}B\\    \vdots&amp;     \ddots&amp;     \vdots\\    a_{m1}B&amp;        \cdots&amp;     a_{mn}B\\\end{matrix} \right]\]</span> 具体写为： <span class="math display">\[\boldsymbol{A} \otimes \boldsymbol{B}=\left[\begin{array}{cccccccccc}a_{11} b_{11} &amp; a_{11} b_{12} &amp; \cdots &amp; a_{11} b_{1 q} &amp; \cdots &amp; \cdots &amp; a_{1 n} b_{11} &amp; a_{1 n} b_{12} &amp; \cdots &amp; a_{1 n} b_{1 q} \\a_{11} b_{21} &amp; a_{11} b_{22} &amp; \cdots &amp; a_{11} b_{2 q} &amp; \cdots &amp; \cdots &amp; a_{1 n} b_{21} &amp; a_{1 n} b_{22} &amp; \cdots &amp; a_{1 n} b_{2 q} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; &amp; &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\a_{11} b_{p 1} &amp; a_{11} b_{p 2} &amp; \cdots &amp; a_{11} b_{p q} &amp; \cdots &amp; \cdots &amp; a_{1 n} b_{p 1} &amp; a_{1 n} b_{p 2} &amp; \cdots &amp; a_{1 n} b_{p q} \\\vdots &amp; \vdots &amp; &amp; \vdots &amp; \ddots &amp; &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\\vdots &amp; \vdots &amp; &amp; \vdots &amp; &amp; \ddots &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\a_{m 1} b_{11} &amp; a_{m 1} b_{12} &amp; \cdots &amp; a_{m 1} b_{1 q} &amp; \cdots &amp; \cdots &amp; a_{m n} b_{11} &amp; a_{m n} b_{12} &amp; \cdots &amp; a_{m n} b_{1 q} \\a_{m 1} b_{21} &amp; a_{m 1} b_{22} &amp; \cdots &amp; a_{m 1} b_{2 q} &amp; \cdots &amp; \cdots &amp; a_{m n} b_{21} &amp; a_{m n} b_{22} &amp; \cdots &amp; a_{m n} b_{2 q} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; &amp; &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\a_{m 1} b_{p 1} &amp; a_{m 1} b_{p 2} &amp; \cdots &amp; a_{m 1} b_{p q} &amp; \cdots &amp; \cdots &amp; a_{m n} b_{p 1} &amp; a_{m n} b_{p 2} &amp; \cdots &amp; a_{m n} b_{p q}\end{array}\right]\]</span></p><h2 id="外积与-kronecker-积">外积与 Kronecker 积</h2><p>一个列向量与一个行向量做乘积称作向量的“外积”，外积可以视为一种特殊的克罗内克积： <span class="math display">\[\boldsymbol{x} \circ \boldsymbol{y}=\boldsymbol{x} \otimes \boldsymbol{y}^T=\boldsymbol{x}\boldsymbol{y}^T\]</span> 其结果是一个<u>矩阵</u>。</p><p>注意区别两个列向量直接做克罗内克积： <span class="math display">\[\boldsymbol{x} \otimes \boldsymbol{y}= vec(\boldsymbol{x} \circ \boldsymbol{y})=vec(\boldsymbol{x} \boldsymbol{y}^T)\]</span> 其结果是两个向量的外积按左右上下的顺序向量化 vectorization 的结果，仍然是一个<u>向量</u>。</p><h2 id="性质">性质</h2><h1 id="khatri-rao-积">Khatri-Rao 积</h1><p>Khatri-Rao 积是一种特殊的克罗内克积，符号为“<span class="math inline">\(\odot\)</span>”，定义为两个<u>具有相同列数</u>的矩阵 <span class="math inline">\(A_{I×K}\)</span> 与 <span class="math inline">\(B_{J×K}\)</span> 的对应列向量的克罗内克积的排列矩阵 <span class="math inline">\((A\odot B)_{IJ×K}\)</span> : <span class="math display">\[A\odot B=[a_1 \otimes b_1,a_2\otimes b_2,...,a_k\otimes b_k]\]</span> 为了更直观地理解，我们举个例子。在克罗内克积中我们讲过，两个列向量的克罗内克积就是他们做外积再向量化的结果，即若 <span class="math inline">\(A=[\boldsymbol{a}_1,\boldsymbol{a}_2]\)</span> , <span class="math inline">\(B=[\boldsymbol{b}_1,\boldsymbol{b}_2]\)</span> ，且： <span class="math display">\[\boldsymbol{a}_1\circ \boldsymbol{b}_1=\left[ \begin{array}{c}    1\\    3\\    5\\\end{array} \right] \times \left[ \begin{matrix}    2&amp;      3\\\end{matrix} \right] =\left[ \begin{matrix}    2&amp;      3\\    6&amp;      9\\    10&amp;     15\\\end{matrix} \right]\\\boldsymbol{a}_2\circ \boldsymbol{b}_2=\left[ \begin{array}{c}    2\\    4\\    6\\\end{array} \right] \times \left[ \begin{matrix}    6&amp;      4\\\end{matrix} \right] =\left[ \begin{matrix}    12&amp;     8\\    24&amp;     16\\    36&amp;     24\\\end{matrix} \right] \]</span> 则它们的 Khatri-Rao 积为： <span class="math display">\[A\odot B=\left[ \begin{matrix}    1&amp;      2\\    3&amp;      4\\    5&amp;      6\\\end{matrix} \right] \odot \left[ \begin{matrix}    2&amp;      6\\    3&amp;      4\\\end{matrix} \right] =\left[ \begin{matrix}    2&amp;      12\\    3&amp;      8\\    6&amp;      24\\    9&amp;      16\\    10&amp;     36\\    15&amp;     24\\\end{matrix} \right]\]</span></p><h2 id="性质-1">性质</h2><p><span class="math display">\[A \bigodot B \bigodot C=(A \bigodot B) \bigodot C=A \bigodot(B \bigodot C) ,\\\quad(A \bigodot B)^{T}(A \bigodot B)=A^{T} A * B^{T} B.\]</span></p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 矩阵分解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一种模型压缩技术：知识蒸馏</title>
      <link href="/2024/04/10/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B9%8B%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"/>
      <url>/2024/04/10/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B9%8B%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="模型压缩">模型压缩</h1><p><span style="background:#fbd4d0;"><strong>模型压缩技术大致可分为5种</strong>：</span></p><ul><li><strong>模型剪枝</strong>：即移除对结果作用较小的组件，如减少 head 的数量和去除作用较少的层，共享参数等，ALBERT属于这种；</li><li><strong>量化</strong>：比如将 float32 降到 float8；</li><li><strong>知识蒸馏</strong>：将 teacher 的能力蒸馏到 student上，一般 student 会比 teacher 小。我们可以把一个大而深的网络蒸馏到一个小的网络，也可以把集成的网络蒸馏到一个小的网络上；</li><li><strong>参数共享</strong>：通过共享参数，达到减少网络参数的目的，如 ALBERT 共享了 Transformer 层；</li><li><strong>参数矩阵近似</strong>：通过矩阵的低秩分解或其他方法达到降低矩阵参数的目的。</li></ul><blockquote><p>来自：<a href="https://blog.csdn.net/weixin_43694096/article/details/127505946">一文搞懂【知识蒸馏】【Knowledge Distillation】算法原理_知识蒸馏算法-CSDN博客</a></p><p><strong>轻量化网络的方式</strong></p><ol type="1"><li>压缩已训练好的模型：知识蒸馏、权值量化、权重剪枝、通道剪枝、注意力迁移</li><li>直接训练轻量化网络：SqueezeNet、MobileNetv1v2v3、MnasNet、SHhffleNet、Xception、EfficientNet、EfficieentDet</li><li>加速卷积运算：im2col+GEMM、Wiongrad、低秩分解</li><li>硬件部署：TensorRT、Jetson、TensorFlow-Slim、openvino、FPGA集成电路</li></ol></blockquote><h1 id="知识蒸馏">知识蒸馏</h1><p>在化学中，蒸馏（Distillation）是一种热力学分离工艺。它利用混合液体或液-固体系中各组分的沸点不同的原理，使不同沸点组分依次先蒸发冷凝，以达到分离组分的目的。例如，下图为实验室制取蒸馏水的示意图。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/imagezhengliu.png" alt="imagezhengliu" /><figcaption aria-hidden="true">imagezhengliu</figcaption></figure><p>类比地，知识蒸馏的混合物则是指原模型中含有大量的知识，我们要设法将原来的大模型中的知识”蒸馏“到小模型中（知识迁移）。大模型就像是一个全能的"集大成者"，而小模型则是各个领域的"专家"。换言之，知识蒸馏就是用相同的数据集训练不同的模型，然后 average 这些模型的 predictions 。</p><h2 id="动机与应用">动机与应用</h2><p>一般而言，大模型性能良好且泛化能力强，但是网络较为复杂。而相反地，小模型网络简单，参数量少，但是表达能力有限。因此如果我们能用<u>大模型学习到的知识去指导小模型，就能使得小模型在具备和大模型相当的性能的同时大大减少参数量</u>，进而实现模型压缩与加速！这就是知识蒸馏与迁移学习在模型优化中的应用。</p><p>在知识蒸馏中，我们将复杂的大模型称为<strong>教师模型（Teacher）</strong>，小模型称为<strong>学生模型（Student）</strong>。整体训练过程大致如下图：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240307182432025.png" alt="image-20240307182432025" /><figcaption aria-hidden="true">image-20240307182432025</figcaption></figure><blockquote><p>复杂笨重但是效果好的Teacher模型不上线，就单纯是个导师角色，真正部署上线进行预测任务的是灵活轻巧的Student小模型。</p></blockquote><p>通过知识蒸馏以实现压缩模型的好处如下：</p><p>1.<strong>提升模型精度</strong>：对模型A的精度不满意。训练一个高精度 Teacher 模型B，然后用模型B对需要提高精度的 Student 模型A进行知识蒸馏，最后得到一个更高精度的模型A。</p><p>2.<strong>降低模型时延：</strong>对模型A的时延不满意。先找一个时延低、参数量小但是精度可能低的模型B，然后训练一个高精度但是高时延的模型C来对模型B进行知识蒸馏，使得模型C在达到低时延的同时接近需要降低时延的模型A的精度。</p><p>3.<strong>标签之间的域集成</strong>：想要训练一个能识别猫、狗、香蕉和苹果的模型A。可以先用猫狗数据集训练一个能识别猫和狗的 Teacher 模型B，再用香蕉苹果数据集训练一个能识别香蕉和苹果的 Teacher 的模型C，最后用模型B和模型C蒸馏出能识别四种事物的模型A。这样便能将两个不同域的数据集进行集成。</p><p>3.<strong>标签之间的域迁移</strong>：可以先用猫狗数据集训练一个能识别猫和狗的 Teacher 模型B，再用香蕉苹果数据集训练一个能识别香蕉和苹果的 Teacher 的模型C，最后用模型C蒸馏出能识别四种事物的模型B。这样便能将两个不同域的数据集进行迁移。</p><blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240307181641653.png" alt="image-20240307181641653" /><figcaption aria-hidden="true">image-20240307181641653</figcaption></figure></blockquote><h2 id="发展">发展</h2><p>Hinton 等人最早在2015年的<a href="https://arxiv.org/pdf/1503.02531.pdf">《Distilling the knowledge in a neural network》</a>[1]中提出“知识蒸馏（Knowledge Distillation, KD）”的概念。知识蒸馏不仅是模型压缩的一种手段，也可以视为是将模型的能力进行迁移，根据迁移的方法不同可以分为<span style="background:#FFCC99;"><strong>基于目标蒸馏</strong>（ Soft-target 蒸馏 / Logits 方法蒸馏）</span>和<span style="background:#daf5e9;"><strong>基于特征蒸馏</strong></span>两个大方向。</p><h1 id="类1目标蒸馏">类1：目标蒸馏</h1><h2 id="算法全貌">算法全貌</h2><p>目标蒸馏方法中最经典的论文就是来自于2015年Hinton发表的一篇神作《Distilling the Knowledge in a Neural Network》[1]。在这篇论文中，Hinton以经典的数字数据集 MNIST 的<strong>分类问题</strong>为例展开。分类问题的共同点是模型最后会有一个 Softmax 层，其输出值对应了相应类别的概率值。这里我们将 Softmax 函数层的输出概率值称为 <strong>Soft-target</strong>。</p><blockquote><p><strong>关于 Softmax 函数详见：</strong></p><ul><li><a href="https://zhuanlan.zhihu.com/p/105722023">一文详解Softmax函数 - 知乎 (zhihu.com)</a></li><li><a href="https://blog.csdn.net/bitcarmanlee/article/details/82320853">入门级都能看懂的softmax详解-CSDN博客</a></li></ul><p>简言之，相对于二分类的逻辑回归， Softmax 函数能实现多分类。之所以认为它是"soft"的，是因为它的输出不是非黑即白的，而是<strong>事物属于各个类别的概率值（置信度）</strong>。Softmax 的输出和值为 1 。</p><p>Softmax 实现多分类的过程大致是<strong>将各类别的最终得分 Logits 归一化为和值为 1 的概率</strong>。它会放大 Logits 之间的差异，使得得分高的类别概率值偏大，得分低的类别概率值偏小，实现两极分化。</p><p>在进行 k 分类问题的解决时，如果这些类别之间是互斥的（事物只能属于一种类别），则使用类别数 n=k 的 Softmax 回归；如果这些类别不是互斥的（事物可以属于多个类别），则使用 k 个二分类的逻辑回归分类器。</p></blockquote><p>在分类问题的传统解法中，模型损失函数的设置往往是为了使该模型的预测值尽快能接近真实值（Hard-target），这种过程是对真实值求极大似然。</p><p>而在知识蒸馏中，我们预先训练了一个泛化能力强的 Teacher 模型，其最后的 Softmax 层输出的概率值作为 Soft-target 指导 Student 模型。事实上，目标蒸馏迁移泛化能力的方法就是<strong>用 Teacher 模型 Softmax 层的输出概率 Soft-target 替换真实值 Hard-target 来设置 Student 模型的另一个新损失</strong>。新损失(Soft-target)和传统损失(Hard-target)<u>共同用于训练</u> Student 模型。</p><p>那么我们这么做有什么根据吗？</p><figure><img src="https://pic3.zhimg.com/v2-f5dcb7d95a9c78ec49923221091079ca_r.jpg" alt="img1" /><figcaption aria-hidden="true">img1</figcaption></figure><blockquote><p>分类一张“2”的图片：</p><ul><li>Hard-target：原始数据集标注的 one-hot 标签，除了真实标签为 1，其他标签都是 0。</li><li>Soft-target： Teacher 模型 Softmax 层输出的类别概率，每个类别都分配了概率，预测的真实标签概率最高。同时所有概率和为 1 。</li></ul><p>我们往往也称<u>真实标签</u>为<u>正标签</u>，<strong>错误标签</strong>为<strong>负标签</strong>。</p></blockquote><p>Softmax 层的输出，除了正标签之外，<strong>负标签也带有Teacher模型归纳推理的大量信息</strong>，<strong>比如某些负标签对应的概率远远大于其他负标签，则代表 Teacher模型在推理时认为该样本与该负标签有一定的相似性</strong>。而在传统的训练过程(Hard-target)中，所有负标签都被统一对待。也就是说，知识蒸馏的训练方式（Soft-target）使得每个样本给 Student 模型带来的信息量大于传统的训练方式!</p><p>此外， Soft-target 分布的熵相对高时，其 Soft-target 蕴含的知识就更丰富。同时，使用 Soft-target 训练时，梯度的方差会更小，训练时可以使用更大的学习率，所需要的样本也更少。这也解释了为什么通过蒸馏的方法训练出 Student 模型相比使用完全相同的模型结构和训练数据只使用Hard-target的训练方法得到的模型，拥有更好的泛化能力。</p><h2 id="具体过程">具体过程</h2><blockquote><p>Softmax 函数的介绍见上文</p></blockquote><p>对于一般的分类问题，在 DNN 到达最后的 Softmax 层之前，会得到各类别的最终得分 Logits （第 <span class="math inline">\(i\)</span> 个类别的得分为 <span class="math inline">\(z_i\)</span> ）。然后神经网络再使用 Softmax 层来实现 Logits 向 probabilities 的转换。令 <span class="math inline">\(q_i\)</span> 表示事物属于第 <span class="math inline">\(i\)</span> 种类别的概率，则原始的 Softmax 函数如下： <span class="math display">\[q_i=softmax(z_i)=\frac{\exp \left( z_i \right)}{\sum_j{\exp \left( z_j \right)}},\]</span> 但是直接使用 Softmax 层的输出值作为 Soft-target ，这又会带来一个问题: 当 Softmax 函数输出的概率分布熵相对较小时，负标签的值都很接近0，对损失函数的贡献非常小，小到可以忽略不计。因此"<strong>温度 </strong><span class="math inline">\(T\)</span> "这个变量就派上了用场。下面的公式是加了温度 <span class="math inline">\(T\)</span> 这个变量之后的 Softmax 函数: <span class="math display">\[q_i=softmax(z_i/T)=\frac{\exp \left( z_i/T \right)}{\sum_j{\exp \left( z_j/T \right)}},\]</span> 其中，当温度 <span class="math inline">\(T=1\)</span> 时，这就是标准的 Softmax 函数公式。<strong><span class="math inline">\(T\)</span>​ 越高，Softmax 函数的 output probability distribution 越趋于平滑，其分布的熵越大，负标签携带的信息会被相对地放大，模型训练将更加关注负标签</strong>。</p><blockquote><p><strong>！！！温度 <span class="math inline">\(T\)</span>​ 的调节：</strong></p><p>最开始提到过，化学中的蒸馏是利用混合物中各组分沸点不同的原理，通过逐步升温冷凝，将各组分分离开来。而在知识蒸馏中，通过逐步升温，我们可以分离出负标签携带的不同程度信息：Student 模型在训练时对负标签的关注程度（或者说负标签携带的信息量）则会随着温度 <span class="math inline">\(T\)</span> 的升高而增大，如下图：</p><figure><img src="https://pic2.zhimg.com/80/v2-41f8f48ad3c8a2b3097a73fec0ff0c9d_720w.webp" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>图中，中间 Softmax 函数输出在原始 <span class="math inline">\(T=1\)</span> 时的特例，左边为 <span class="math inline">\(T\rightarrow 0\)</span> 的情况，右边为 <span class="math inline">\(T \rightarrow \infty\)</span> 的情况。</p><p>可见，当 <span class="math inline">\(T\)</span> 逐渐变小时，概率分布会变得越来越“陡峭”，接近于 Hard-target （负标签携带的信息量为 0 ），Student 模型对负标签（尤其是显著低于平均值的负标签）的关注变小；当 <span class="math inline">\(T\)</span> 逐渐增大至无穷时，概率分布则愈发“平缓”，Softmax 输出概率值趋向平均分布，信息熵逐渐增大，模型相对更多地关注到负标签。</p><p>然而无论如何，Softmax 输出即 Soft-target 都有忽略较小的概率值 <span class="math inline">\(p_i\)</span>​ 携带的信息倾向，更多地关注概率值高于平均值的负标签（它携带的信息确实更多）。但由于 Teacher 模型的训练过程决定了负标签部分概率值都比较小，并且负标签的值越低，其信息就越不可靠。因此温度的选取需要进行实际实验的比较，本质上就是在下面两种情况之中取舍:</p><ul><li>当想从负标签中学到一些信息量的时候，温度 <span class="math inline">\(T\)</span> 应调高一些；</li><li>当想减少负标签的干扰，少学一些信息的时候，温度 <span class="math inline">\(T\)</span> 应调低一些；</li></ul><p>总的来说，T的选择和 Student 模型的大小有关， Student 模型参数量比较小的时候，相对比较低的温度就可以了。因为参数量小的模型不能学到所有 Teacher 模型的知识，所以可以适当忽略掉一些负标签的信息。</p><p>最后，在整个知识蒸馏过程中，我们先让温度 <span class="math inline">\(T\)</span> 升高，提取负标签中的信息；然后在 Hard-target 阶段恢复“低温“（ <span class="math inline">\(T=1\)</span> ），从而将原模型中的知识提取出来，因此将其称为是蒸馏，实在是妙啊。</p></blockquote><figure><img src="https://pic2.zhimg.com/80/v2-90c65f1d4dcec36f1ff7dd38a96c3cdd_720w.webp" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><blockquote><p>目标知识蒸馏的步骤图</p></blockquote><p>目标蒸馏的整体过程主要包括以下步骤：</p><ol type="1"><li>预训练 Teacher 模型；</li><li>在高温环境 <span class="math inline">\(T_{high}=t\)</span> 下 Teacher 模型产生 Soft-target；</li><li><span style="background:#daf5e9;">使用 {Soft-target, <span class="math inline">\(T_{high}=t\)</span> } 和 {Hard-target, <span class="math inline">\(T=1\)</span> } 同时训练 Student 模型</span>；</li><li>设置温度 <span class="math inline">\(T=1\)</span> 的环境下， Student 模型？？？</li></ol><p>其中第2、3步统称为：高温蒸馏的过程。高温蒸馏过程的损失函数 <span class="math inline">\(L\)</span> 由 Soft-target 对应的蒸馏损失 <code>distillation loss</code> <span class="math inline">\(L_{soft}\)</span> 和 Hard-target 对应的学生原损失 <code>student loss</code> <span class="math inline">\(L_{hard}\)</span> 加权求和得到： <span class="math display">\[L=\alpha L_{soft}+\beta L_{hard},\]</span></p><h3 id="蒸馏损失">蒸馏损失</h3><p>Teacher 模型和 Student 模型同时输入 transfer set (这里可以直接复用训练 Teacher 模型用到的 training set )，用 Teacher 模型在高温 <span class="math inline">\(t\)</span> 下产生的各类别概率 Softmax distribution 来作为 Soft-target <span class="math inline">\(q_i^t\)</span> 。 Student 模型在相同温度 <span class="math inline">\(t\)</span> 条件下的 Softmax 输出预测 <span class="math inline">\(p_i^t\)</span> 和 Teacher 模型产生的 Soft-target <span class="math inline">\(q_i^t\)</span> 的交叉熵 cross entropy 就是<strong>Loss函数的第一部分</strong> <span class="math inline">\(L_{soft}\)</span>​ ，具体形式如下所示： <span class="math display">\[\begin{align}L_{soft}&amp;=-\sum_i^N{p_{i}^{t}\log \left( q_{i}^{t} \right)}\\p_{i}^{t}&amp;=\frac{\exp \left( y_i/T \right)}{\sum_k^N{\exp \left( y_k/T \right)}}\\q_{i}^{t}&amp;=\frac{\exp \left( z_i/T \right)}{\sum_k^N{\exp \left( z_k/T \right)}}\end{align}\]</span> 其中， <span class="math inline">\(y_i\)</span> 表示 Teacher 模型的 Logits , <span class="math inline">\(z_i\)</span> 表示 Student 模型的 Logits , <span class="math inline">\(N\)</span> 表示总类别数。</p><h3 id="学生模型原损失">学生模型原损失</h3><p>在温度 <span class="math inline">\(T=1\)</span> 的条件下（不加温）， Student 模型产生的预测输出 <span class="math inline">\(q_i^1\)</span> 与 ground truth 真实值 <span class="math inline">\(c_i\)</span> 之间的 cross entropy 就是<strong>Loss函数的第二部分 <span class="math inline">\(L_{hard}\)</span> </strong>: <span class="math display">\[\begin{align}L_{hard}&amp;=-\sum_i^N{c_{i}\log \left( q_{i}^{1} \right)}\\q_{i}^{1}&amp;=\frac{\exp \left( z_i \right)}{\sum_k^N{\exp \left( z_k \right)}}\end{align}\]</span> 其中， <span class="math inline">\(c_i\)</span> 表示第 <span class="math inline">\(i\)</span> 个类别的真实值，且 <span class="math inline">\(c_i \in {0,1}\)</span> ，正标签取 1 ，负标签取 0 。</p><p>第二部分 <span class="math inline">\(L_{hard}\)</span> 的必要性其实很好理解：Teacher 模型也有一定的错误率，使用 ground truth 可以有效降低错误被传播给 Student 模型的可能性。打个比喻，老师虽然学识远远超过学生，但是他仍然有出错的可能，而这时候如果学生在老师的教授之外，可以同时参考到标准答案，就可以有效地降低被老师偶尔的错误“带偏”的可能性。</p><h3 id="损失权重">损失权重</h3><blockquote><p>理论推导见：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/102038521">【经典简读】知识蒸馏(Knowledge Distillation) 经典之作 - 知乎 (zhihu.com)</a></li><li><a href="https://baihaoran.xyz/2020/05/04/Knowledge-Distillation.html">baihaoran.xyz</a></li></ul></blockquote><p>实验发现，当 <span class="math inline">\(L_{hard}\)</span> 权重 <span class="math inline">\(\beta\)</span> 较小时，能产生最好的效果，这是一个经验性的结论。这里给出经过推导的结论：由于 <span class="math inline">\(L_{soft}\)</span> 贡献的梯度大约为 <span class="math inline">\(L_{hard}\)</span> 的 <span class="math inline">\(1/t^2\)</span>，因此在同时使用 Soft-target 和 Hard-target 的时候，需要在 <span class="math inline">\(L_{soft}\)</span> 的权重上乘以 <span class="math inline">\(t^2\)</span>​ 的系数，这样才能保证Soft-target和Hard-target贡献的梯度量基本一致。</p><h2 id="当trightarrow-infty时的特殊形式">当<span class="math inline">\(T\rightarrow \infty\)</span>时的特殊形式</h2><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240307211645510.png" alt="image-20240307211645510" /><figcaption aria-hidden="true">image-20240307211645510</figcaption></figure><h1 id="类2特征蒸馏">类2：特征蒸馏</h1><p>与目标蒸馏通过将 Teacher 模型的 Softmax 层输出概率值作为 Soft-target 来训练 Student 模型的损失函数 <span class="math inline">\(L_{soft}\)</span> 不同,特征蒸馏同时将使 Student 模型网络的中间层学习 Teacher 模型网络的中间层特征。最早提出这个想法的是论文《Fitnets: Hints for thin deep nets》[2]。接下来以这篇文章为主，介绍特征蒸馏方法的原理。</p><figure><img src="https://pic2.zhimg.com/80/v2-dba8b40b66d3c9170d984a6c4eeec3f5_720w.webp" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="何为-thin-deep-net-及主要-idea">何为 thin deep net 及主要 idea</h2><p>这篇论文首先提出一个案例，既宽又深的模型通常需要大量的乘法运算，从而导致对内存和计算的高需求。因此，即使网络在准确性方面是性能最高的模型，其在现实世界中的应用也受到限制。</p><p>为了解决这类问题，我们需要通过模型压缩（也称为知识蒸馏）将知识从复杂的模型转移到参数较少的简单模型。</p><p>到目前为止，知识蒸馏技术已经考虑了Student网络与Teacher网络有相同或更小的参数。<font color=#985fff>这里有一个洞察点是，深度是特征学习的基本层面，到目前为止尚未考虑到Student网络的深度。一个具有比Teacher网络更多的层但每层具有较少神经元数量的Student网络称为“<strong>thin deep network</strong>”。</font></p><p>因此，该篇论文主要针对Hinton提出的知识蒸馏法进行扩展，允许 Student 网络可以比 Teacher 网络更深更窄，并使用 Teacher 网络的输出和中间层的特征作为提示，改进训练过程和 Student 网络的性能。</p><h2 id="二阶段的模型结构">二阶段的模型结构</h2><blockquote><p>Student网络不仅仅通过目标蒸馏拟合Teacher网络的 Soft-target ，而且通过特征蒸馏拟合隐藏层的输出（Teacher网络抽取的特征）:</p><ul><li>第一阶段让Student网络去学习Teacher网络的隐藏层输出（特征蒸馏）；</li><li>第二阶段使用Soft-target来训练Student网络（目标蒸馏）。</li></ul></blockquote><p>论文将 Student 网络模型 FitNet 设置得比 Teacher 网络模型更深更窄。文中将“hints”定义为 Teacher 网络某一隐藏层 hint layer 的输出，且该输出将被用于指导 FitNet 网络中选定隐藏层 the guided layer 的训练过程，即使得两个 layer 的输出逐渐趋于一致。</p><p>那么如何选取 guided/hint layer 对呢？</p><p>注意，学习 hints是正则化的形式。因此，必须有选择地确定 hint/guided layer 对，以便学生网络不会过度正则化。我们设置的 guided 层越深，我们给网络的灵活性就越低，因此，FitNets 更有可能遭受过度正则化的影响。在我们的例子中，我们选择 hint 作为教师网络的中间层。同样，我们选择 guided 层作为学生网络的中间层。（Note that having hints is a form of regularization and thus, the pair hint/guided layer has to be chosen such that the student network is not over-regularized. The deeper we set the guided layer, the less flexibility we give to the network and, therefore, FitNets are more likely to suffer from over-regularization. In our case, we choose the hint to be the middle layer of the teacher network. Similarly, we choose the guided layer to be the middle layer of the student network.）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/liucheng.png" alt="liucheng" /><figcaption aria-hidden="true">liucheng</figcaption></figure><blockquote><p><strong>图解：</strong></p><p><font color=#4eb434>图（a）绿框</font>表示 <span class="math inline">\(W_{Hint}\)</span> ，即 Teacher 网络从第一层到中间层——前 <span class="math inline">\(h\)</span> 层（first layer-&gt;hint layer）的参数；<font color=#ef042a>图（a）红框</font>表示 <span class="math inline">\(W_{Guided}\)</span>​ ，即 Student 网络的第一层到中间层——前 <span class="math inline">\(g\)</span> 层（first layer-&gt;guided layer）的参数。</p><p><font color=#0091ff>图（b）蓝框</font>表示 <span class="math inline">\(W_{r}\)</span> ，即使更窄的 <span class="math inline">\(W_{Guided}\)</span> 的输出尺寸(guided layer)转换为更宽的 <span class="math inline">\(W_{hint}\)</span> 的尺寸（hint layer）的工具层 convolutional regressor 的参数。选择卷积型的 regressor 主要是为了缓解由全连接 regressor 带来的参数增多和内存消耗增大的影响，具体分析论文已给出。</p></blockquote><p><strong>把“宽”且“深”的网络蒸馏成“瘦”且“更深”的网络，需要进行两阶段的训练：</strong></p><p><strong>第一阶段 Hints Trianing/特征蒸馏：</strong></p><p>根据 Teacher 模型的中间层输出来指导预训练 Student 模型。在训练之初 Student 网络进行随机初始化，并需要学习一个映射函数 <span class="math inline">\(W_r\)</span> 使得 <span class="math inline">\(W_{Guided}\)</span> 的维度匹配 <span class="math inline">\(W_{Hint}\)</span>​ ，得到 Student 模型在下一阶段的参数初始化值，并最小化两者网络输出的 MSE 差异作为损失，如下： <span class="math display">\[\mathcal{L}_{H T}\left(\mathbf{W}_{\text {Guided }}, \mathbf{W}_{\mathbf{r}}\right)=\frac{1}{2}\left\|u_{h}\left(\mathbf{x} ; \mathbf{W}_{\text {Hint }}\right)-r\left(v_{g}\left(\mathbf{x} ; \mathbf{W}_{\text {Guided }}\right) ; \mathbf{W}_{\mathbf{r}}\right)\right\|^{2},\]</span> 这里，<span class="math inline">\(u_h\)</span> and <span class="math inline">\(v_g\)</span> are the teacher/student deep nested functions up to their respective hint/guided layers with parameters <span class="math inline">\(W_{Hint}\)</span> and <span class="math inline">\(W_{Guided}\)</span> 。</p><p><strong>第二阶段 Knowledge Distillation/目标蒸馏</strong>：</p><p>在训练好 guided layer之后，将当前的参数作为网络的初始参数，利用知识蒸馏的方式训练 Student 网络的所有层参数，使 Student 学习 Teacher 的输出。由于 Teacher 对于简单任务的预测非常准确，在分类任务中近乎 one-hot 输出，因此为了弱化预测输出，使所含信息更加丰富，作者使用 Hinton 等人论文《Distilling knowledge in a neural network》中提出的 Softmax 改造方法，即在 Softmax 前引入 <span class="math inline">\(\tau\)</span> 缩放因子，将 Teacher 和 Student 的 pre-softmax 输出 Logits 均除以 <span class="math inline">\(\tau\)</span>​ 。也就是上面我们讲的加了温度的 Softmax 。此时的损失函数为： <span class="math display">\[\mathcal{L}_{K D}\left(\mathbf{W}_{\mathbf{S}}\right)=\mathcal{H}\left(\mathbf{y}_{\text {true }}, \mathrm{P}_{\mathrm{S}}\right)+\lambda \mathcal{H}\left(\mathrm{P}_{\mathrm{T}}^{\tau}, \mathrm{P}_{\mathrm{S}}^{\tau}\right),\]</span> 其中， <span class="math inline">\(\mathcal{H}\)</span> 指交叉熵损失函数； <span class="math inline">\(\lambda\)</span> 是一个可调整参数，以平衡两个交叉熵；第一部分为 Student 的预测输出与 Ground-truth 的交叉熵损失；第二部分为在 <span class="math inline">\(\tau\)</span> 温度下 Student 与 Teacher 的 Softmax 输出的交叉熵损失。</p><h1 id="参考文献">参考文献</h1><p>[1] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. "Distilling the knowledge in a neural network." arixv preprint arixv:1503.02531 (2015).</p><p>[2] Adriana, Romero, Ballas Nicolas, K. Samira Ebrahimi, Chassang Antoine, Gatta Carlo, and Bengio Yoshua. "Fitnets: Hints for thin deep nets." <em>Proc. ICLR</em> 2, no. 3 (2015): 1.</p><h2 id="参考链接">参考链接</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/319880839">深度学习中的知识蒸馏技术 - 知乎 (zhihu.com)</a></li><li><a href="https://arxiv.org/pdf/1503.02531.pdf">1503.02531.pdf (arxiv.org)</a></li><li><a href="https://arxiv.org/pdf/1412.6550.pdf">1412.6550.pdf (arxiv.org)</a></li><li><a href="https://blog.csdn.net/weixin_43694096/article/details/127505946">一文搞懂【知识蒸馏】【Knowledge Distillation】算法原理_知识蒸馏算法-CSDN博客</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 蒸馏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/04/03/%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%953%20%E6%97%A0%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/"/>
      <url>/2024/04/03/%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%953%20%E6%97%A0%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.cnblogs.com/molinchn/p/13641437.html">【复习笔记】最优化方法 - 3. 无约束优化方法 - Molinchn - 博客园 (cnblogs.com)</a></p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DVGO</title>
      <link href="/2024/03/24/DVGO%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>/2024/03/24/DVGO%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文名称：<strong>Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction</strong> <strong>（DVGO： 超快速收敛的辐射场重建）</strong></p><p>代码地址： <a href="https://github.com/sunset1995/DirectVoxGO">GitHub - sunset1995/DirectVoxGO: Direct voxel grid optimization for fast radiance field reconstruction.</a></p><p>论文主页： <a href="https://sunset1995.github.io/dvgo/">DVGO (sunset1995.github.io)</a></p><p>参考网址：</p><ul><li>[<a href="https://zhuanlan.zhihu.com/p/584734270">NeRF-训练加速] DVGO - 知乎 (zhihu.com)</a></li><li><a href="https://blog.csdn.net/hy592070616/article/details/120623303">机器学习中的数学——激活函数（十）：Softplus函数-CSDN博客</a></li></ul></blockquote><h1 id="论文基本信息">论文基本信息</h1><p>Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction<sup>[1]</sup>是由来自清华大学的 Cheng Sun 等人发表在 <strong>2022 CVPR(Oral)</strong> 会议上的SCI论文。</p><p>DVGO 的主要目的是<u>加速 NeRF<sup>[2]</sup> 的训练</u>（从数个小时到十几分钟的逐个场景重建，且不需要预训练），采用的方式是将场景使用显式体素表达。DVGO 仍属于<strong>稠密重建</strong>。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240305143817326.png" alt="image-20240305143817326" /><figcaption aria-hidden="true">image-20240305143817326</figcaption></figure><h2 id="优点">优点</h2><ol type="1"><li><p><strong>在体素密度直接优化中，采用了两个先验算法来避免几何陷入局部最优解：</strong>直接优化密度体素网格会导致超快收敛，但容易出现次优解，所提出方法在自由空间分配"云"，并试图将光度损失与云拟合，而不是搜索具有更好多视图一致性的几何。对这个问题的解决方案简单而有效。<u>首先</u>，初始化密度体素网格，以产生非常接近于零的不透明度，以避免几何解决方案偏向于相机的附近平面。<u>其次</u>，给较少视图可见的体素一个较低的学习率，可以避免仅为解释少量视图的观察而分配的冗余体素。所提出的解决方案可以成功地避免次优几何，并在五个数据集上表现良好。</p></li><li><p><strong>提出了先插值后激活的体素网格插值，它可以在较低的网格分辨率下实现清晰的边界建模：</strong>之前的工作要么对激活的不透明度进行体素网格插值，要么使用最近邻插值，从而在每个网格单元中产生光滑的表面。从数学和经验上证明，所提出的后激活可以在单个网格单元内建模(超越)尖锐的线性表面。因此，可以使用更少的体素来实现更好的质量——具有160^3个密集体素的方法在大多数情况下已经优于NeRF。</p></li></ol><blockquote><p>[!IMPORTANT]</p><h3 id="features">Features</h3><ul><li><p><span style="background:#daf5e9;">Speedup NeRF by replacing the MLP with the voxel grid.</span></p></li><li><p><span style="background:#daf5e9;">Simple scene representation:</span></p></li><li><p><em>Volume densities</em>: dense voxel grid (3D).</p></li><li><p><em>View-dependent colors</em>: dense feature grid (4D) + shallow MLP.</p></li><li><p><span style="background:#daf5e9;">Pytorch cuda extention built just-in-time for another 2--3x speedup.</span></p></li><li><p><span style="background:#daf5e9;">O(N) realization for the distortion loss proposed by mip-nerf 360</span></p></li><li><p>The loss improves our training time and quality.</p></li><li><p>We have released a self-contained pytorch package: <a href="https://github.com/sunset1995/torch_efficient_distloss">torch_efficient_distloss</a>.</p></li><li><p>Consider a batch of 8192 rays X 256 points.</p><ul><li>GPU memory consumption: 6192MB =&gt; 96MB.</li><li>Run times for 100 iters: 20 sec =&gt; 0.2sec.</li></ul></li><li><p><span style="background:#daf5e9;"><strong>Supported datasets:</strong></span></p></li><li><p><strong><em>Bounded inward-facing</em>: <a href="https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1">NeRF</a>, <a href="https://dl.fbaipublicfiles.com/nsvf/dataset/Synthetic_NSVF.zip">NSVF</a>, <a href="https://dl.fbaipublicfiles.com/nsvf/dataset/BlendedMVS.zip">BlendedMVS</a>, <a href="https://dl.fbaipublicfiles.com/nsvf/dataset/TanksAndTemple.zip">T&amp;T (masked)</a>, <a href="https://drive.google.com/open?id=1ScsRlnzy9Bd_n-xw83SP-0t548v63mPH">DeepVoxels</a>.</strong></p></li><li><p><strong><em>Unbounded inward-facing</em>: <a href="https://drive.google.com/file/d/11KRfN91W1AxAW6lOFs4EeYDbeoQZCi87/view?usp=sharing">T&amp;T</a>, <a href="https://drive.google.com/file/d/1gsjDjkbTh4GAR9fFqlIDZ__qR9NYTURQ/view?usp=sharing">LF</a>, <a href="https://jonbarron.info/mipnerf360/">mip-NeRF360</a>.</strong></p></li><li><p><strong><em>Foward-facing</em>: <a href="https://drive.google.com/drive/folders/14boI-o5hGO9srnWaaogTU5_ji7wkX2S7">LLFF</a>.</strong></p></li></ul><details><summary>Directory structure for the datasets</summary><p><span style="font-size: 18pt;"><strong>data</strong></span><br /><span style="background-color: #bfedd2;">├── nerf_synthetic     # Link: https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1</span><br /><span style="background-color: #bfedd2;">│   └── [chair|drums|ficus|hotdog|lego|materials|mic|ship]</span><br /><span style="background-color: #bfedd2;">│       ├── [train|val|test]</span><br /><span style="background-color: #bfedd2;">│       │   └── r_*.png</span><br /><span style="background-color: #bfedd2;">│       └── transforms_[train|val|test].json</span><br />│<br /><span style="background-color: #fbeeb8;">├── Synthetic_NSVF     # Link: https://dl.fbaipublicfiles.com/nsvf/dataset/Synthetic_NSVF.zip</span><br /><span style="background-color: #fbeeb8;">│   └── [Bike|Lifestyle|Palace|Robot|Spaceship|Steamtrain|Toad|Wineholder]</span><br /><span style="background-color: #fbeeb8;">│       ├── intrinsics.txt</span><br /><span style="background-color: #fbeeb8;">│       ├── rgb</span><br /><span style="background-color: #fbeeb8;">│       │   └── [0_train|1_val|2_test]_*.png</span><br /><span style="background-color: #fbeeb8;">│       └── pose</span><br /><span style="background-color: #fbeeb8;">│           └── [0_train|1_val|2_test]_*.txt</span><br />│<br /><span style="background-color: #f8cac6;">├── BlendedMVS         # Link: https://dl.fbaipublicfiles.com/nsvf/dataset/BlendedMVS.zip</span><br /><span style="background-color: #f8cac6;">│   └── [Character|Fountain|Jade|Statues]</span><br /><span style="background-color: #f8cac6;">│       ├── intrinsics.txt</span><br /><span style="background-color: #f8cac6;">│       ├── rgb</span><br /><span style="background-color: #f8cac6;">│       │   └── [0|1|2]_*.png</span><br /><span style="background-color: #f8cac6;">│       └── pose</span><br /><span style="background-color: #f8cac6;">│           └── [0|1|2]_*.txt</span><br />│<br /><span style="background-color: #eccafa;">├── TanksAndTemple     # Link: https://dl.fbaipublicfiles.com/nsvf/dataset/TanksAndTemple.zip</span><br /><span style="background-color: #eccafa;">│   └── [Barn|Caterpillar|Family|Ignatius|Truck]</span><br /><span style="background-color: #eccafa;">│       ├── intrinsics.txt</span><br /><span style="background-color: #eccafa;">│       ├── rgb</span><br /><span style="background-color: #eccafa;">│       │   └── [0|1|2]_*.png</span><br /><span style="background-color: #eccafa;">│       └── pose</span><br /><span style="background-color: #eccafa;">│           └── [0|1|2]_<em>.txt</span><br />│<br /><span style="background-color: #c2e0f4;">├── deepvoxels         # Link: https://drive.google.com/drive/folders/1ScsRlnzy9Bd_n-xw83SP-0t548v63mPH</span><br /><span style="background-color: #c2e0f4;">│   └── [train|validation|test]</span><br /><span style="background-color: #c2e0f4;">│       └── [armchair|cube|greek|vase]</span><br /><span style="background-color: #c2e0f4;">│           ├── intrinsics.txt</span><br /><span style="background-color: #c2e0f4;">│           ├── rgb/</em>.png</span><br /><span style="background-color: #c2e0f4;">│           └── pose/<em>.txt</span><br />│<br /><span style="background-color: #bfedd2;">├── nerf_llff_data     # Link: https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1</span><br /><span style="background-color: #bfedd2;">│   └── [fern|flower|fortress|horns|leaves|orchids|room|trex]</span><br />│<br /><span style="background-color: #fbeeb8;">├── tanks_and_temples  # Link: https://drive.google.com/file/d/11KRfN91W1AxAW6lOFs4EeYDbeoQZCi87/view?usp=sharing</span><br /><span style="background-color: #fbeeb8;">│   └── [tat_intermediate_M60|tat_intermediate_Playground|tat_intermediate_Train|tat_training_Truck]</span><br /><span style="background-color: #fbeeb8;">│       └── [train|test]</span><br /><span style="background-color: #fbeeb8;">│           ├── intrinsics/</em>txt</span><br /><span style="background-color: #fbeeb8;">│           ├── pose/<em>txt</span><br /><span style="background-color: #fbeeb8;">│           └── rgb/</em>jpg</span><br />│<br /><span style="background-color: #f8cac6;">├── lf_data            # Link: https://drive.google.com/file/d/1gsjDjkbTh4GAR9fFqlIDZ__qR9NYTURQ/view?usp=sharing</span><br /><span style="background-color: #f8cac6;">│   └── [africa|basket|ship|statue|torch]</span><br /><span style="background-color: #f8cac6;">│       └── [train|test]</span><br /><span style="background-color: #f8cac6;">│           ├── intrinsics/*txt</span><br /><span style="background-color: #f8cac6;">│           ├── pose/<em>txt</span><br /><span style="background-color: #f8cac6;">│           └── rgb/</em>jpg</span><br />│<br /><span style="background-color: #eccafa;">├── 360_v2             # Link: https://jonbarron.info/mipnerf360/</span><br /><span style="background-color: #eccafa;">│   └── [bicycle|bonsai|counter|garden|kitchen|room|stump]</span><br /><span style="background-color: #eccafa;">│       ├── poses_bounds.npy</span><br /><span style="background-color: #eccafa;">│       └── [images_2|images_4]</span><br />│<br /><span style="background-color: #c2e0f4;">├── nerf_llff_data     # Link: https://drive.google.com/drive/folders/14boI-o5hGO9srnWaaogTU5_ji7wkX2S7</span><br /><span style="background-color: #c2e0f4;">│   └── [fern|flower|fortress|horns|leaves|orchids|room|trex]</span><br /><span style="background-color: #c2e0f4;">│       ├── poses_bounds.npy</span><br /><span style="background-color: #c2e0f4;">│       └── [images_2|images_4]</span><br />│<br /><span style="background-color: #bfedd2;">└── co3d               # Link: https://github.com/facebookresearch/co3d</span><br /><span style="background-color: #bfedd2;">    └── [donut|teddybear|umbrella|...]</span><br /><span style="background-color: #bfedd2;">        ├── frame_annotations.jgz</span><br /><span style="background-color: #bfedd2;">        ├── set_lists.json</span><br /><span style="background-color: #bfedd2;">        └── [129_14950_29917|189_20376_35616|...]</span><br /><span style="background-color: #bfedd2;">            ├── images</span><br /><span style="background-color: #bfedd2;">            │   └── frame<em>.jpg</span><br /><span style="background-color: #bfedd2;">            └── masks</span><br /><span style="background-color: #bfedd2;">                └── frame</em>.png</span></p><p> </p></blockquote><blockquote><p><strong>DVGO v2</strong></p><p>论文地址：<a href="https://arxiv.org/pdf/2206.05085.pdf">2206.05085.pdf (arxiv.org)</a></p><p>论文代码：同DVGO</p><p>摘要：In this technical report, we improve the DVGO framework (called DVGOv2), which is based on Pytorch and uses the simplest dense grid representation. First, we reimplement part of the Pytorch operations with cuda, achieving 2–3× speedup. The cuda extension is automatically compiled just in time. <span style="background:#FFCC99;">Second, we extend DVGO to support Forward-facing and Unbounded Inward-facing capturing.(该改进之处正是DVGO 1.0 版本在论文末尾提出的不足之处)</span> Third, we improve the space time complexity of the distortion loss proposed by mip-NeRF 360 from O(<span class="math inline">\(N^2\)</span>) to O(<span class="math inline">\(N\)</span>). The distortion loss improves our quality and training speed. Our efficient implementation could allow more future works to benefit from the loss.</p></blockquote><h2 id="缺陷">缺陷</h2><ul><li>稠密重建：需要输入百张图片，需要的样本量大</li><li>几何不太好（见 voxurf 的对比）</li></ul><h1 id="算法框架">算法框架</h1><h2 id="dvgo-全貌">DVGO 全貌</h2><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240227190531174.png" alt="image-20240227190531174" /><figcaption aria-hidden="true">image-20240227190531174</figcaption></figure><p>从上左图可以看到，DVGO 将整个场景表示为两个 voxel grid——密度体素网格和特征体素网格，并且渲染图像使用的依然是 NeRF 中的体渲染，对于从像素上发射的射线上的每一个点，首先在这两个 voxel grid 上 进行三线性插值，分别得到这个点的密度特征和颜色特征，再通过各自的解码过程最终得到该点的颜色和密度。<u>这样就把整个场景表示在 voxel grid 和对应的解码器里。</u></p><p>在训练的时候，DVGO 采用了 coarse to fine 的训练方式。在 coarse 阶段，使用先验信息和多视角图像训练两个粗粒度的voxel，然后使用其中的密度场确定场景中的空白区域（free space）。在 fine 阶段，利用 coarse 阶段确定的密度场可以得到更紧密的 bbox，可以将 grid 定义在这个 bbox 内，减少无关变量的训练。并且在体渲染的时候还可以通过粗的密度场提前得知射线上哪些点应当被跳过（空白点和被遮住的那些点）。</p><h2 id="类似-nerf-的体渲染">类似 NeRF 的体渲染</h2><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf9.png" /></p><p>NeRF 将场景用多层感知机 MLP 表示，输入相机位置坐标 <span class="math inline">\(\boldsymbol{x}\)</span> 并输出密度 <span class="math inline">\(\sigma\)</span> 和中间特征 <span class="math inline">\(e\)</span> 。 然后再次用这个中间特征 <span class="math inline">\(e\)</span> 和视角 <span class="math inline">\(\boldsymbol{d}\)</span> 作为输入，推测出这个点的颜色 <span class="math inline">\(\boldsymbol{c}\)</span>​ ，下面将这两个过程分开写，其实就是 NeRF 中的网络： <span class="math display">\[\begin{align}\left( \sigma ,e \right) &amp;=MLP^{\left( pos \right)}\left( x \right) ,\tag{1.a}\\c&amp;=MLP^{\left( rgb \right)}\left( e,d \right) , \tag{1.b}\end{align}\]</span> 然后通过体渲染得到像素点的值： <span class="math display">\[\begin{align}\hat{C}(\boldsymbol{r}) &amp; =\left(\sum_{i=1}^{K} T_{i} \alpha_{i} \boldsymbol{c}_{i}\right)+T_{K+1} \boldsymbol{c}_{\mathrm{bg}}, \tag{2.a}\\\alpha_{i} &amp; =\operatorname{alpha}\left(\sigma_{i}, \delta_{i}\right)=1-\exp \left(-\sigma_{i} \delta_{i}\right),  \tag{2.b}\\T_{i} &amp; =\prod_{j=1}^{i-1}\left(1-\alpha_{j}\right),\tag{2.c}\end{align}\]</span> 其中的符号和 NeRF 中的一致，就不详细介绍。不同的是这里加上了背景的考虑（加上背景的原因可参考<a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODA3MDkyMA==&amp;mid=2247485148&amp;idx=1&amp;sn=4e6acc1b358e624ac2301729d35cc89d&amp;chksm=cf81f0bbf8f679ad3f990f91d5c84fcc67cc6948c3cbc0c34c59874f7795e9263f2382979414&amp;token=400584181&amp;lang=zh_CN#rd">NeRF入门之体渲染 (Volume Rendering) (qq.com)</a>）：这里 <span class="math inline">\(\boldsymbol{c}_{bg}\)</span> 是一个实现定义好的背景像素值，透射比 <span class="math inline">\(T_{K+1}\)</span> 表示的是背景点被击中的概率。</p><blockquote><p>[!NOTE]</p><p>其实一条射线上只会采样 K 个点，这里假设最后还有一个虚拟的背景点，挂在每条射线的最后。当这条射线上所有采样点被击中的概率都很低时，就会呈现出背景的颜色。</p></blockquote><h2 id="求-alpha-后激活密度体素网格">求 <span class="math inline">\(\alpha\)</span> : 后激活密度体素网格</h2><blockquote><p>[!TIP]</p><p><strong>体素网格如何表示颜色/密度/特征？</strong></p><p>文中用以下式子概括： <span class="math display">\[interp\left( x,\boldsymbol{V} \right) :\left( \mathbb{R}^3,\mathbb{R}^{C\times N_x\times N_y\times N_z} \right) \rightarrow \mathbb{R}^C,\]</span> 其中 <span class="math inline">\(x\)</span> 为查询的三维点， <span class="math inline">\(\boldsymbol{V}\)</span> 表示 <span class="math inline">\(x, y, z\)</span> 方向上体素数量分别为 <span class="math inline">\(N_x, N_y, N_z\)</span> 的体素网格， <span class="math inline">\(C\)</span> 表示颜色/密度/特征的维度数。在本文中，运用的插值方法是三线性插值（trilinear interpolation）。</p></blockquote><p>前面介绍 DVGO 将场景表示为两种 voxel grid 和解码器的组合，这里想要探讨的问题是<u>如何从 density voxel grid 中得到一个点的密度信息</u>。</p><p>首先，density 体素网格上面定义了场景的密度信息，但不需要直接是真实的密度值。具体来说每个 voxel 上的值是一个待训练的参数实数（该实数的正负性不能保证），可理解为密度特征。<span style="background:#daf5e9;">由于密度是非负的，我们需要先通过激活函数 relu 或 softplus 将这个实数映射为一个大于零的数才能将输出与真实密度值建立关系。然后用每个点的密度值计算每个点的 <span class="math inline">\(\alpha\)</span> （见公式2.b）</span>。</p><p>因此我们想要进行后续体渲染的话，需要对一个点做下面三件事：<strong>插值、激活和求 <span class="math inline">\(\alpha\)</span>​</strong> 。</p><p>关于<strong>插值</strong>比较好理解，因为网格是稀疏的，而我们做体渲染的时候需要的是空间中任意点的信息，所以最简单直接的方式就是用空间点附近的 voxel 进行插值。</p><p>关于<strong>激活</strong>，这里采用的是 Mip-NeRF<sup>[3]</sup> 中的 shifted softplus，其中 <span class="math inline">\(\ddot{\sigma}\)</span> 就是 density grid 上的原始值，取值范围是 R，经过激活后得到非负的密度值 <span class="math inline">\(\sigma\)</span>。 <span class="math inline">\(b\)</span> 是一个超参数（hyperparameter） : <span class="math display">\[\sigma=\operatorname{softplus}(\ddot{\sigma})=\log (1+\exp (\ddot{\sigma}+b)),\]</span></p><blockquote><p>这里不用 relu 激活函数的原因是：Using softplus instead of ReLU is crucial to optimize voxel density directly, as it is irreparable when a voxel is falsely set to a negative value with ReLU as the density activation. Conversely, softplus allows us to explore density very close to 0.</p><p>意思就是，如果一个密度为正的地方的 <span class="math inline">\(\ddot{ \sigma}\)</span>​ 如果是负值会导致其经过 relu 之后是0，没有梯度能更新这个值让它能移动到正的部分。</p><p><strong>softplus 函数可以视为 relu 函数的平滑，随着 <span class="math inline">\(\ddot{ \sigma}\)</span> 的变小，函数值会无限接近于0但不会真正变为0</strong>。其函数表达式与图像如下： <span class="math display">\[softplus\left( x \right) =\log \left( 1+e^x \right)\]</span> <img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240101125119035.png" alt="image-20240101125119035" /></p></blockquote><p>关于<strong>求 <span class="math inline">\(\alpha\)</span></strong> ，则<span style="background:#f9eda6;">是 NeRF 中计算一个点的瞬时击中率的操作，即式（2.b）</span>。</p><p>除了激活 softplus 操作必须在求 <span class="math inline">\(\alpha\)</span> 前外，显然这三件事是可以交换的。在给定查询三维点 <span class="math inline">\(x\)</span> 和密度体素网格中的密度信息 <span class="math inline">\(\boldsymbol{V}^{(density)}\)</span> 后，我们考虑了三种不同的顺序来得到 <span class="math inline">\(\alpha \in [0,1]\)</span> 值：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301192035676.png" alt="image-20240301192035676" /><figcaption aria-hidden="true">image-20240301192035676</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301201349921.png" alt="image-20240301193624141" /><figcaption aria-hidden="true">image-20240301193624141</figcaption></figure><p>一般情况下，我们想到的是第三种情况：先插值、再激活，最后求 <span class="math inline">\(\alpha\)</span> 。 论文中根据插入三线性插值 interp 的顺序将其扩展为三种情况并展开对比发现，不同的顺序得到的结果还有所差异。<span style="background:#daf5e9;"><strong>所谓的“后激活”（Post-activated）指的是激活为 <span class="math inline">\(\alpha\)</span> 在插值之后(i.e. to activate density into alpha after interpolation)</strong></span>。作者证明，后激活能够用更少的网格单元生成尖锐的表面(决策边界)。作者在二维图像上证明了后激活的优越性：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301193624141.png" alt="image-20240301201349921" /><figcaption aria-hidden="true">image-20240301201349921</figcaption></figure><p>可以看到 ，后激活能够表示出更 sharp 的边缘，对应在 NeRF 中就是更 sharp 的几何。在二值图像上的实验也证明了同样的观点：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301211423996.png" alt="image-20240301193832857" /><figcaption aria-hidden="true">image-20240301193832857</figcaption></figure><blockquote><p>论文的补充材料中给出了后激活策略可以在单个网格单元中产生尖锐的线性表面（决策边界）的证明。论文先证明了一维网格和二维网格中的情况，然后利用这两种情况推导出三维网格中的情况。</p></blockquote><h2 id="由粗到细的训练过程">由粗到细的训练过程</h2><p>和 NeRF 的重要性采样类似，DVGO 也采用了两阶段的采样过程。</p><h3 id="coarse-to-fine-的好处">coarse to fine 的好处</h3><p>作者之所以选择 coarse to fine 的训练方式是因为：通常情况下，<u>场景由空闲空间(即未占用空间)主导</u>，即场景中的空白区占大部分，有物体的区域就一小部分。基于此，我们的目标是高效地找到感兴趣的粗三维区域，然后重构需要更多计算资源的精细细节和视图依赖效果。这样可以大大减少后期精细阶段每条射线上查询的点的数量。</p><p>在 NeRF 中也有类似的操作，主要目的还是增强采样点的针对性，尽量将采样点放在更有用的位置，不在无用位置进行采样。本文进行 coarse to fine 后，就可以快速有效地筛选出有物体存在的网格，进而在 fine 粒度的训练中跳过大量无关“空白点”，只关注有物体占据的网格，进一步加快了训练速度。</p><h3 id="体素分配方法">体素分配方法</h3><p>不论是 coarse 还是 fine 阶段，我们都需要知道应该在场景的什么位置创建 grid，才能包含所有要采样的点。作者采用的方式是找一个 BBox (bounding box) 将所有的摄像机截锥紧紧地包起来，像下面这样：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301201828110.png" alt="image-20240301201828110" /><figcaption aria-hidden="true">image-20240301201828110</figcaption></figure><p>寻找 BBox 的代码如下(.ipynb_checkpoints/run-checkpoint.py)：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_compute_bbox_by_cam_frustrm_bounded</span>(<span class="params">cfg, HW, Ks, poses, i_train, near, far</span>):</span><br><span class="line">    xyz_min = torch.Tensor([np.inf, np.inf, np.inf])</span><br><span class="line">    xyz_max = -xyz_min</span><br><span class="line">    <span class="keyword">for</span> (H, W), K, c2w <span class="keyword">in</span> <span class="built_in">zip</span>(HW[i_train], Ks[i_train], poses[i_train]):</span><br><span class="line">        rays_o, rays_d, viewdirs = dvgo.get_rays_of_a_view(</span><br><span class="line">                H=H, W=W, K=K, c2w=c2w,</span><br><span class="line">                ndc=cfg.data.ndc, inverse_y=cfg.data.inverse_y,</span><br><span class="line">                flip_x=cfg.data.flip_x, flip_y=cfg.data.flip_y)</span><br><span class="line">        <span class="keyword">if</span> cfg.data.ndc:</span><br><span class="line">            pts_nf = torch.stack([rays_o+rays_d*near, rays_o+rays_d*far])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pts_nf = torch.stack([rays_o+viewdirs*near, rays_o+viewdirs*far])</span><br><span class="line">        xyz_min = torch.minimum(xyz_min, pts_nf.amin((<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>)))</span><br><span class="line">        xyz_max = torch.maximum(xyz_max, pts_nf.amax((<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>)))</span><br><span class="line">    <span class="keyword">return</span> xyz_min, xyz_max</span><br></pre></td></tr></table></figure><blockquote><p>[!TIP]</p><p>这里的代码逻辑是，首先拿到训练集中所有的相机位置和视角，然后在生成射线，根据 near 和 far 确定射线上的采样点，把所有这些采样点放一块，求所有这些点的 x,y,z 的最值。通过这样可以保证 grid 能够包含所有要采样的点，并且不会包含过多的无关位置。</p></blockquote><p>令BBox的长宽高分别为 <span class="math inline">\(L_{x}^{\left( c \right)},L_{y}^{\left( c \right)},L_{z}^{\left( c \right)}\)</span> 。设粗粒度训练阶段的期望体素总数为超参数 <span class="math inline">\(M^{(c)}\)</span> ，则可计算每个体素的大小 size 为 <span class="math inline">\(s^{\left( c \right)}=\sqrt[3]{L_{x}^{\left( c \right)}\cdot L_{y}^{\left( c \right)}\cdot L_{z}^{\left( c \right)}/M^{\left( c \right)}}\)</span> ，进而可算得 BBox 长宽高方向上分别有 $N_{x}^{( c )},N_{y}^{( c )},N_{z}^{( c )}=L_{x}^{( c )}/s^{( c )} ,L_{y}^{( c )}/s^{( c )} ,L_{z}^{( c )}/s^{( c )} $ 个体素。</p><h3 id="粗粒度训练">粗粒度训练</h3><h4 id="几何与颜色表示">几何与颜色表示</h4><ul><li><strong>场景几何表示</strong>：通过粗粒度的密度体素网格 <span class="math inline">\(\boldsymbol{ V}^{(density)(c)}\)</span> 和后激活策略建模。这里 <span class="math inline">\(\boldsymbol{V}^{\left( density \right) \left( c \right)}\in \mathbb{R}^{1\times N_{x}^{\left( c \right)}\times N_{y}^{\left( c \right)}\times N_{z}^{\left( c \right)}}\)</span> 。</li><li><strong>颜色表示</strong>：通过在粗粒度阶段的颜色体素网格 <span class="math inline">\(\boldsymbol{ V}^{(rgb)(c)}\)</span> ，来建模视觉无关(view-invariant)的颜色 emissions 。这里 <span class="math inline">\(\boldsymbol{V}^{\left( rgb\right) \left( c \right)}\in \mathbb{R}^{3\times N_{x}^{\left( c \right)}\times N_{y}^{\left( c \right)}\times N_{z}^{\left( c \right)}}\)</span> 。</li></ul><p>任意三维点的密度和颜色信息可通过插值公式查询： <span class="math display">\[\begin{array}{l}\ddot{\sigma}^{\left( c \right)}=interp\left( x,\boldsymbol{V}^{\left( density \right) \left( c \right)} \right) ,\\c^{\left( c \right)}=interp\left( x,\boldsymbol{V}^{\left( rgb \right) \left( c \right)} \right) ,\end{array}\]</span> 这里 <span class="math inline">\(\ddot{\sigma}^{\left( c \right)}\)</span> 表示原始的可正可负的体密度信息。由于颜色和视角无关，所以不需要输入视角，颜色和密度一样直接插值就能得到。而且需要注意的是：<span style="background:#fbd4d0;"><strong>在 coarse 阶段没有使用任何 MLP</strong></span>。</p><p>粗粒度训练阶段的<strong>点采样公式</strong>如下： <span class="math display">\[\begin{array}{l}\boldsymbol{x}_{0}=\boldsymbol{o}+t^{(\text {near) }} \boldsymbol{d} ,\\\boldsymbol{x}_{i}=\boldsymbol{x}_{0}+i \cdot \delta^{(\mathrm{c})} \cdot \frac{\boldsymbol{d}}{\|\boldsymbol{d}\|^{2}},\end{array}\]</span> 显然， <span class="math inline">\(\boldsymbol{o}\)</span> 是相机中心， <span class="math inline">\(\boldsymbol{d}\)</span> 是射线投射方向。 $ t ^{ (near) } $ 是相机近平面 camera near bound ，<span class="math inline">\(\delta^{(\mathrm{c})}\)</span> 则是可根据体素大小 <span class="math inline">\(s^{(c)}\)</span> 自适应选择的超参数步长。查询索引 <span class="math inline">\(i\)</span> 取值从 1 到 $t^{( far )} <sup>2/</sup>{( c )} $ 。</p><h4 id="低密度初始化先验">低密度初始化先验</h4><blockquote><p>[!IMPORTANT]</p><p>作者发现，在训练开始时，远离相机的点的重要性由于累积的透过率项而降低。因此，粗密度体素网格可能会意外地被困在一个次优的“cloudy”几何，它在相机附近的平面密度更高。</p><p>导致这个问题的原因如果初始化做的不好 ，那么后面的采样点都被前面的遮住了，后面的采样点的被击中的概率 <span class="math inline">\(T_i\)</span> 就很低，那么这些位置就不会被训练到，网络就一直在纠结是不是前面那些点的颜色或者密度调的不够好，殊不知是重心在射线的后面。</p><p>作者解决这个问题的方法简单而有效，初始化密度网格的时候小心点，尽量让一开始的时候，射线上每个点都是可见的，也就是他们的透射比 <span class="math inline">\(T_i\)</span> 都比较高。</p></blockquote><p>由式(2.c)，</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301214544618.png" alt="image-20240301211423996" /><figcaption aria-hidden="true">image-20240301211423996</figcaption></figure><blockquote><p>中间的推导详见论文的补充材料。</p></blockquote><p>所以我们只需要让 <span class="math inline">\(b=log((1-\alpha^{(init)})^{-1/s}-1)\)</span> 即可。我们可以在代码中给超参数 <span class="math inline">\(\alpha^{(init)}\)</span> 一个很小的初值就可以了，代码中给的值一般是 <span class="math inline">\(10^{-6}\)</span> （coarse）和 <span class="math inline">\(10^{-6}\)</span> （fine）。这一段对应的代码块为：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.alpha_init = alpha_init</span><br><span class="line">self.register_buffer(<span class="string">&#x27;act_shift&#x27;</span>, torch.FloatTensor([np.log(<span class="number">1</span>/(<span class="number">1</span>-alpha_init) - <span class="number">1</span>)]))</span><br></pre></td></tr></table></figure><h4 id="基于视角的学习率先验">基于视角的学习率先验</h4><blockquote><p>[!IMPORTANT]</p><p>意思是 grid 中的不同 voxel 在训练集中的可见性是不同的，有一些 voxel 在大部分训练集视角下都可见，而有一些只在少部分视角下可见。比较有代表性的就是包含表面的那些 voxel 和表面内部的那些 voxel。</p><p>作者认为，可见度不同的 voxel 上的特征应该具有不同的学习率，可见性好的学习率更大，否则设置更小的学习率。</p><p>具体来说，用 <span class="math inline">\(n_j\)</span>表示第 <span class="math inline">\(j\)</span> 个 voxel 可以被看见的视角个数，用 <span class="math inline">\(n_{max}\)</span> 表示所有 <span class="math inline">\(n_j\)</span> 的最大值， <span class="math inline">\(n_j/n_{max}\)</span> 为每个 voxel 的学习率的缩放因子。</p></blockquote><p>对应的代码为：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">voxel_count_views</span>(<span class="params">self, rays_o_tr, rays_d_tr, imsz, near, far, stepsize, downrate=<span class="number">1</span>, irregular_shape=<span class="literal">False</span></span>):</span><br><span class="line">       <span class="built_in">print</span>(<span class="string">&#x27;dvgo: voxel_count_views start&#x27;</span>)</span><br><span class="line">       far = <span class="number">1e9</span>  <span class="comment"># the given far can be too small while rays stop when hitting scene bbox</span></span><br><span class="line">       eps_time = time.time()</span><br><span class="line">       N_samples = <span class="built_in">int</span>(np.linalg.norm(np.array(self.world_size.cpu())+<span class="number">1</span>) / stepsize) + <span class="number">1</span></span><br><span class="line">       rng = torch.arange(N_samples)[<span class="literal">None</span>].<span class="built_in">float</span>()</span><br><span class="line">       count = torch.zeros_like(self.density.get_dense_grid())</span><br><span class="line">       device = rng.device</span><br><span class="line">       <span class="keyword">for</span> rays_o_, rays_d_ <span class="keyword">in</span> <span class="built_in">zip</span>(rays_o_tr.split(imsz), rays_d_tr.split(imsz)):</span><br><span class="line">           ones = grid.DenseGrid(<span class="number">1</span>, self.world_size, self.xyz_min, self.xyz_max)</span><br><span class="line">           <span class="keyword">if</span> irregular_shape:</span><br><span class="line">               rays_o_ = rays_o_.split(<span class="number">10000</span>)</span><br><span class="line">               rays_d_ = rays_d_.split(<span class="number">10000</span>)</span><br><span class="line">           <span class="keyword">else</span>:</span><br><span class="line">               rays_o_ = rays_o_[::downrate, ::downrate].to(device).flatten(<span class="number">0</span>,-<span class="number">2</span>).split(<span class="number">10000</span>)</span><br><span class="line">               rays_d_ = rays_d_[::downrate, ::downrate].to(device).flatten(<span class="number">0</span>,-<span class="number">2</span>).split(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">           <span class="keyword">for</span> rays_o, rays_d <span class="keyword">in</span> <span class="built_in">zip</span>(rays_o_, rays_d_):</span><br><span class="line">               vec = torch.where(rays_d==<span class="number">0</span>, torch.full_like(rays_d, <span class="number">1e-6</span>), rays_d)</span><br><span class="line">               rate_a = (self.xyz_max - rays_o) / vec</span><br><span class="line">               rate_b = (self.xyz_min - rays_o) / vec</span><br><span class="line">               t_min = torch.minimum(rate_a, rate_b).amax(-<span class="number">1</span>).clamp(<span class="built_in">min</span>=near, <span class="built_in">max</span>=far)</span><br><span class="line">               t_max = torch.maximum(rate_a, rate_b).amin(-<span class="number">1</span>).clamp(<span class="built_in">min</span>=near, <span class="built_in">max</span>=far)</span><br><span class="line">               step = stepsize * self.voxel_size * rng</span><br><span class="line">               interpx = (t_min[...,<span class="literal">None</span>] + step/rays_d.norm(dim=-<span class="number">1</span>,keepdim=<span class="literal">True</span>))</span><br><span class="line">               rays_pts = rays_o[...,<span class="literal">None</span>,:] + rays_d[...,<span class="literal">None</span>,:] * interpx[...,<span class="literal">None</span>]</span><br><span class="line">               ones(rays_pts).<span class="built_in">sum</span>().backward()</span><br><span class="line">           <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">               count += (ones.grid.grad &gt; <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>这段代码也十分巧妙。作者首先构造了一个 <em>grid</em> 对象 <em>ones</em>，它是作者创建的一个类，这个类的作用就是构造可以训练的 voxel grid，并且可以完成插值等操作。然后在训练集视角上获取射线并在射线上采样点 <em>rays_pts</em>。对 <em>rays_pts</em> 中的每个点，找到它在 <em>ones</em> 中的插值结果，对应的语句是 <em>ones(rays_pts)。</em>然后将这些点的插值结果加起来：<em>ones(rays_pts).sum()。</em>可想而知，到目前为止，只有 <em>rays_pts</em> 中的那些点所在的 voxel 参与了加法，这个时候我们对加法结果进行反向传播：<em>ones(rays_pts).sum().backward()</em>，参与插值的那些 voxel 的梯度会是正的，<strong>我们找到这些梯度为正的 voxel：<em>ones.grid.grad &gt; 1</em>，就可以确定哪些 voxel 是可见的</strong>。count 是和 grid 同纬度的 tensor，<em>ones.grid.grad &gt; 1</em> 得到的是一个 bool tensor，其中为 True 的那些就是可见 voxel，两个加起来就能实现统计的目的。</p><h3 id="细粒度训练">细粒度训练</h3><h4 id="几何与颜色表示-1">几何与颜色表示</h4><p>与 coarse 阶段不同的是，fine 阶段采用了更密集的网格，并且使用显隐混合的表达方式（explicit-implicit hybrid representation），其实就是用一个 MLP 当解码器接在颜色网格后面，增强网络对颜色的表达能力。</p><p>fine 阶段查询一个空间点的密度和颜色的方式为： <span class="math display">\[\begin{array}{l}    \ddot{\sigma}^{\left( f \right)}=interp\left( x,\boldsymbol{V}^{\left( density \right) \left( f \right)} \right) ,\\    c^{\left( f \right)}=MLP_{\varTheta}^{\left( rgb \right)}\left( interp\left( x,\boldsymbol{V}^{\left( feat \right) \left( f \right)} \right) ,\boldsymbol{x,d} \right) ,\\\end{array}\]</span> 这对应于框架图的左半部分。</p><p>在 fine 阶段，作者采用了一系列的加速手段：</p><ol type="1"><li><strong>Known free space and unknown space</strong> 这里说明了怎么定义 free space 和 unknown space，方式就是手动设置一个阈值，之下的就是 free space。</li><li><strong>Fine voxels allocation</strong> coarse 阶段确定 grid 范围的方式是找到一个包含所有采样点的 BBox。但是有些地方是空的，所以我们可以更具 coarse 阶段得到的 density grid 确定 fine 阶段 grid 所在的区域，将有限的 voxel 尽可能地安排在有密度的地方。</li><li><strong>Progressive scaling</strong> 受到 NSVF 的启发，作者也是逐渐减小 voxel 的大小。随着训练不断的倍增 grid 的密度，直到最后达到指定数量的 voxel 停止倍增。</li><li><strong>Fine-stage points sampling</strong> 在 NeRF 中，一个场景的 near-bound 和 far-bound 是定好的一个大致的范围，但是由于我们这里明确知道场景就在空间的一个 grid 内，所以更好的方式是将 near-bound 和 far-bound 设置为每条射线进入 grid 和离开 grid 的点。（在代码中没找到对应部分 ）</li><li><strong>Free space skipping</strong> 意思是通过 coarse 阶段已经知道场景中哪些位置是空的了，那就不需要在这些位置采样点浪费时间了。</li><li><strong>Training objective for fine representation</strong> 使用与粗阶段相同的训练损失，但使用更小的权值作为正则化损失，因为作者发现，根据经验，它会带来略好的质量。</li></ol><h4 id="损失函数">损失函数</h4><p>Loss Function包含三个部分：来自NeRF的 MSE loss 以及两个从 DVGO 中借鉴的 loss。 <span class="math display">\[\begin{align}    \mathcal{L}_{photo}&amp;=\frac{1}{|\mathcal{R}|}\sum_{r\in \mathcal{R}}{|}\hat{C}\left( \boldsymbol{r} \right) -C\left( \boldsymbol{r} \right) |_{2}^{2}\\    \mathcal{L}_{all\_pts}&amp;=\sum_{i=1}^N{T}_i\left( 1-\exp \left( -\sigma _i\delta _i \right) \right) |\mathbf{c}_i-C\left( \mathbf{r} \right) |_{2}^{2}\mid\\    \mathcal{L}_{bg}&amp;=-T_{N+1}\log \left( T_{N+1} \right) -\left( 1-T_{N+1} \right) \log \left( 1-T_{N+1} \right)\\\\    L&amp;=L_{photo}+\alpha L_{all\_pts}+\beta L_{bg}\\\end{align}\]</span> MSE loss 的作用是使得建模出来的辐射场在训练集视角上渲染要和原图尽可能一致。权重为 1。</p><p><span class="math inline">\(L_{all\_pts}\)</span> 的意思是从一个像素中发射的射线上的所有像素点都应该和该像素尽可能接近，这个约束能够帮助网络一开始不要陷入 local minima，使得优化过程更加稳定。但是这个约束并不是符合实际的，因为一条射线上的点的颜色常常是不同的，所以作者给了一个很小的系数 <span class="math inline">\(\alpha=0.1\)</span>（coarse）和 <span class="math inline">\(\alpha=0.01\)</span>（coarse）。</p><p><span class="math inline">\(L_{bg}\)</span> 中 <span class="math inline">\(T_{N+1}\)</span> 表示射线击中背景的概率，也可以理解为射线一路上什么都没有击中，一路通到背景上。 <span class="math inline">\(L_{bg}\)</span> 的函数图像如下，它的作用是让一个像素点要么为背景颜色，要么没有背景颜色，尽量避免是二者的混合。作者设置其系数为 $  <span class="math inline">\(（coarse）和\)</span>$（fine）。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301193832857.png" alt="image-20240301214544618" /><figcaption aria-hidden="true">image-20240301214544618</figcaption></figure><h1 id="实验">实验</h1><h2 id="实现细节">实现细节</h2><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">coarse</th><th style="text-align: center;">fine</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">voxel 数量</td><td style="text-align: center;"><span class="math inline">\(100^3\)</span></td><td style="text-align: center;"><span class="math inline">\(160^3\)</span></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\alpha\)</span> 初始值</td><td style="text-align: center;"><span class="math inline">\(10^{-6}\)</span></td><td style="text-align: center;"><span class="math inline">\(10^{-2}\)</span></td></tr><tr class="odd"><td style="text-align: center;">采样间距/步长</td><td style="text-align: center;">coarse_voxel_size(<span class="math inline">\(\delta^{(c)}\)</span>)/2</td><td style="text-align: center;">fine_voxel_size(<span class="math inline">\(\delta^{(f)}\)</span>)/2</td></tr><tr class="even"><td style="text-align: center;">MLP</td><td style="text-align: center;">—</td><td style="text-align: center;">C -&gt; 128 -&gt; 128 -&gt; 3</td></tr><tr class="odd"><td style="text-align: center;">optimizer</td><td style="text-align: center;">Adam</td><td style="text-align: center;">Adam</td></tr><tr class="even"><td style="text-align: center;">lr for grid</td><td style="text-align: center;">0.1</td><td style="text-align: center;">0.1</td></tr><tr class="odd"><td style="text-align: center;">lr for MLP</td><td style="text-align: center;">—</td><td style="text-align: center;">0.001</td></tr></tbody></table><h2 id="实验结果">实验结果</h2><h2 id="消融实验">消融实验</h2><h1 id="代码">代码</h1><details><summary>问题箱</summary>1.DVGO是否可以与diffusion结合？ 2.DVGO是否可以与注意力机制结合？ 比如从多视角图像中提取特征时</details><h2 id="代码结构">代码结构</h2><details><summary>.ipynb_checkpoints /原始文件看不见这个，从云平台下载后显示的隐藏文件</summary>run-checkpoint.py /内容与主函数运行文件 run.py 相同</details><details><summary>configs /各种数据集的配置文件</summary><pre><code>blendedmvs 文件夹 &lt;/br&gt;</code></pre>co3d 文件夹</br> ... ...</br> default.py</details><details><summary>data /存放使用的数据</summary><p><span style="font-family: arial, helvetica, sans-serif; font-size: 14pt;">.ipynb_checkpoints 文件夹    /空的，也是从云平台下载后显示的隐藏文件夹</span></p><p><span style="font-family: arial, helvetica, sans-serif; font-size: 14pt;">nerf_synthetic</span></p><p style="padding-left: 30px;"><span style="font-family: arial, helvetica, sans-serif; font-size: 12pt;">lego</span></p><p style="padding-left: 60px;"><span style="font-family: arial, helvetica, sans-serif; color: #e67e23;">pre_train    /预训练用 200 张图片</span></p><p style="padding-left: 60px;"><span style="font-family: arial, helvetica, sans-serif; color: #e67e23;">test    /测试用 200 张图片，每张图片有原图、depth 深度图和 normal 法线图</span></p><p style="padding-left: 60px;"><span style="font-family: arial, helvetica, sans-serif; color: #e67e23;">train    /训练用 100 张图片</span></p><p style="padding-left: 60px;"><span style="font-family: arial, helvetica, sans-serif; color: #e67e23;">val    /验证用 100 张图片，和训练用的 100 张不一样哟</span></p><p style="padding-left: 60px;"><span style="font-family: arial, helvetica, sans-serif; color: #169179;">transforms_pre_train.json</span></p><p style="padding-left: 60px;"><span style="font-family: arial, helvetica, sans-serif; color: #169179;">transforms_test.json</span></p><p style="padding-left: 60px;"><span style="font-family: arial, helvetica, sans-serif; color: #169179;">transforms_train.json</span></p><p style="padding-left: 60px;"><span style="font-family: arial, helvetica, sans-serif; color: #169179;">transforms_val.json</span></p></details><details><summary>figs /存放两张示例图片</summary><pre><code>debug_cam_and_bbox.png &lt;/br&gt;</code></pre>debug_coarse_volume.png</details><details><summary>lib /C文件、dvgo模型的py文件和加载数据集的文件等</summary><p><span style="font-size: 12pt;">cuda    /存放用来加速的 cpp 文件、cu 文件</span></p><p style="padding-left: 40px;"><span style="color: #169179;">adam_upd.cpp</span></p><p style="padding-left: 40px;"><span style="color: #169179;">adam_upd_kernel.cu</span></p><p style="padding-left: 40px;"><span style="color: #e67e23;">render_utils.cpp</span></p><p style="padding-left: 40px;"><span style="color: #e67e23;">render_utils_kernel.cu</span></p><p style="padding-left: 40px;"><span style="color: #169179;">total_variation.cpp</span></p><p style="padding-left: 40px;"><span style="color: #169179;">total_variation_kernel.cu</span></p><p style="padding-left: 40px;"><span style="color: #e67e23;">ub360_utils.cpp</span></p><p style="padding-left: 40px;"><span style="color: #e67e23;">ub360_utils_kernels.cu</span></p><p><span style="color: #e03e2d; font-size: 12pt;">dbvgo.py    /DirectBiVoxGO</span></p><p><span style="color: #e03e2d; font-size: 12pt;">dcvgo.py    /DirectContractedVoxGO：elif cfg.data.unbounded_inward</span></p><p><span style="color: #e03e2d; font-size: 12pt;">dmpigo.py    /DirectMPIGO：if cfg.data.ndc</span></p><p><span style="color: #e03e2d; font-size: 12pt;">dvgo.py    /DirectVoxGO：else</span></p><p><span style="color: #e03e2d; font-size: 12pt;">grid.py    /DenseGrid 和 TensoRFGrid 两种模式</span></p><p><span style="color: #169179; font-size: 12pt;">load_blendedmvs.py</span></p><p><span style="color: #169179; font-size: 12pt;">load_blender.py</span></p><p><span style="color: #169179; font-size: 12pt;">load_co3d.py</span></p><p><span style="color: #169179; font-size: 12pt;">... ...</span></p><p><span style="color: #169179; font-size: 12pt;">load_tankstemple.py</span></p><p><span style="color: #e67e23; font-size: 12pt;">masked_adam.py    /扩展 Adam 优化器：1.使其支持 per-voxel 学习率；2.masked update (ignore zero grad) which speeduping training</span></p><p><span style="color: #3598db; font-size: 12pt;">utils.py    /Misc    |checkpoint utils    |Evaluation metrics (ssim,lpips)    </span></p><p style="padding-left: 40px;"> </p></details><details><summary>logs /存放输出的结果数据</summary><p><strong>nerf_synthetic</strong></p><p style="padding-left: 40px;"><strong>dvgo_lego</strong></p><p style="padding-left: 80px;"><span style="background-color: #fbeeb8;">render_test_fine_last</span></p><p style="padding-left: 120px;">video.depth.mp4</p><p style="padding-left: 120px;">video.rgb.mp4</p><p style="padding-left: 80px;"><span style="background-color: #fbeeb8;">args.txt</span></p><p style="padding-left: 80px;"><span style="background-color: #fbeeb8;">coarse_last.tar</span></p><p style="padding-left: 80px;"><span style="background-color: #fbeeb8;">fine_last.tar</span></p><p style="padding-left: 80px;"><span style="background-color: #fbeeb8;">config.py</span></p></details><details><summary>tools /神奇工具箱</summary><p><span style="color: #e03e2d; font-size: 14pt;">colmap_utils</span></p><p style="padding-left: 40px;"><span style="color: #3598db; font-size: 12pt;">colmap_read_model.py</span></p><p style="padding-left: 40px;"><span style="color: #3598db; font-size: 12pt;">colmap_wrapper.py</span></p><p style="padding-left: 40px;"><span style="color: #3598db; font-size: 12pt;">pose_utils.py</span></p><p><span style="color: #e03e2d; font-size: 14pt;">imgs2poses.py</span></p><p><span style="color: #e03e2d; font-size: 14pt;">vis_train.py</span></p><p><span style="color: #e03e2d; font-size: 14pt;">vis_volume.py  </span></p><p style="padding-left: 40px;"> </p></details><p>.gitignore /告诉Git忽略哪些文件<br> IMPROVING_LOG.md /数据集在不同显卡环境下的评价指标<br> LICNESE /许可类型<br> README.md /说明文件<br> requirements.txt /需要的python依赖包列表<br> run.py /主函数运行文件<br></p><h2 id="主程序-run.py">主程序 run.py</h2><ul><li><p><span style="background:#daf5e9;">def config_parser</span> /定义命令行参数</p></li><li><p><span style="background:#f9eda6;">def render_viewpoints(no grad)</span> /从给定视图渲染图像，如果真实图给了还会计算测试的评价指标</p></li><li><p><span style="background:#daf5e9;">def seed_everything</span> /固定随机种子以确保实验的可复现性（一些 pytorch 操作比如网格采样的反向传播是不确定的）</p></li><li><p><span style="background:#f9eda6;">def load_everything</span> /以字典形式加载图片、位姿、相机设置等信息，将其设置为张量</p></li><li><p><span style="background:#daf5e9;">def _compute_bbox_by_cam_frustrm_bounded</span> /根据有边界的相机视锥体计算 bbox</p></li><li><p><span style="background:#f9eda6;">def _compute_bbox_by_cam_frustrm_unbounded</span> /根据无边界的相机视锥体计算 bbox：找一个立方体紧紧地包住所有相机中心</p></li><li><p><span style="background:#daf5e9;">def compute_bbox_by_cam_frustrm</span> /根据cfg.data.unbounded_inward是否为真选择上面两种 bbox 计算方式</p></li><li><p><span style="background:#f9eda6;">def compute_bbox_by_coarse_geo(no grad)</span> /根据粗阶段几何（alpha值是否达到门槛确定）确定细阶段的计算区域</p></li><li><p><span style="background:#daf5e9;">def create_new_model</span> /创建并返回dvgo模型和优化器：model=dvgo.DirectVoxGO</p></li><li><p><span style="background:#f9eda6;">def load_existed_model</span> /和 def create_new_model 一样用于初始化模型和优化器（两者是if…else… 关系）</p></li><li><p><span style="background:#daf5e9;">def scene_rep_reconstruction</span> /细粒度阶段的训练重建函数</p><details><summary><p>点击查看该函数详情</p></summary><p><ol type="1"><li>init 初始化</p><p><ol start="2" type="1"><li>寻找是否存在 checkpoint 路径</p><p><ol start="3" type="1"><li>init 模型和优化器</p><p><ol start="4" type="1"><li>init 渲染参数</p><p><ol start="5" type="1"><li>init 批射线采样器（batch rays sampler）：<span style="background:#FFCC99;">def gather_training_rays</span></p><p><ol start="6" type="1"><li>基于视角的学习率</p><p><ol start="7" type="1"><li>运行 GO GO！</p><p>   ​    renew occupancy grid</p><p>   ​    progress scaling checkpoint</p><p>   ​    random sample rays</p><p>   ​    volume rendering</p><p>   ​    gradient descent step</p><p>   ​    update lr</p><p>   ​    check log &amp; save</p></details></li></ol></li></ol></li></ol></li></ol></li></ol></li></ol></li></ol></li><li><p><span style="background:#f9eda6;">def train</span> /训练函数</p><details><summary><p>点击查看 train 函数详情</p></summary><p><p><span style="color: #e03e2d;"><strong>init </strong>： 初始化存放输出结果的文件夹</span></p></p><p><p><span style="color: #3598db;"><strong>coarse geometry searching （only works for inward bounded scenes）</strong>：通过粗粒度训练中的 compute_bbox_by_cam_frustrm 寻找物体所在位置</span></p></p><p><p><span style="color: #e03e2d;"><strong>fine detail reconstruction </strong>：先通过 compute_bbox_by_coarse_geo 确定细粒度阶段的训练区域，再由 scene_rep_reconstruction 进行细粒度重建</span></p></p></details></li><li><p><strong><span style="background:#daf5e9;">if __name__=='__main__' </span></strong> /主函数</p><details><summary><p>点击查看主函数详情</p></summary><p><p><span style="color: #e67e23;"><strong>load setup </strong>：加载 parser、args 和 cfg （mmcv.Config.fromfile）等</span></p></p><p><p><span style="color: #169179;"><strong>init environment </strong>：设置环境为 cuda ，并通过 seed_everything 确保实验的可复现性</span></p></p><p><p><span style="color: #e67e23;"><strong>load images / poses / camera settings / data split </strong>：借助 load_everything 函数完成</span></p></p><p><p><span style="color: #169179;"><strong>export scene bbox and camera poses in 3d for debugging and visualization</strong> ：导出可视化等结果为 tar 压缩文件（coarse_last.tar）</span></p></p><p><p><span style="color: #e67e23;"><strong>train </strong>：如果 args.render_only 函数为假，通过 train 函数进行训练</span></p></p><p><p><span style="color: #169179;"><strong>load model for rendering</strong> ：根据 cfg.data 类型选择导入模型类型来进行相应渲染，设置渲染结果放入 fine_last.tar</span></p></p><p><p><span style="color: #e67e23;"><strong>render trainset and eval </strong>：如果相应渲染为 args.render_train，对训练渲染并将结果放入 render_train_{ckpt_name}</span></p></p><p><p><span style="color: #169179;"><strong>render testset and eval </strong>：如果相应渲染为 args.render_test，对测试渲染并将结果放入 render_test_{ckpt_name}</span></p></p><p><p><span style="color: #e67e23;"><strong>render video</strong> ：如果相应渲染为 args.video，对视频渲染并将结果放入 render_video_{ckpt_name}</span></p></p><p><p><span style="color: #169179;"><strong> 输出结束信息</strong></span></p></p></details></li></ul><h1 id="bib-citation">Bib Citation</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;SunSC22,</span><br><span class="line">  author    = &#123;Cheng Sun and Min Sun and Hwann&#123;-&#125;Tzong Chen&#125;,</span><br><span class="line">  title     = &#123;Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction&#125;,</span><br><span class="line">  booktitle = &#123;CVPR&#125;,</span><br><span class="line">  year      = &#123;2022&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="参考文献">参考文献</h1><p>[1] Sun C, Sun M, Chen H T. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction[C], Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 5459-5469.</p><p>[2] Mildenhall B, Srinivasan P P, Tancik M, et al. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis[C], European Conference on Computer Vision. Cham: Springer International Publishing, 2020: 405-421.</p><p>[3] Barron J T, Mildenhall B, Tancik M, et al. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields[C], Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021: 5855-5864.</p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2022 </tag>
            
            <tag> voxel grid </tag>
            
            <tag> 训练加速 </tag>
            
            <tag> 新视图合成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Diffusion Model（一）</title>
      <link href="/2024/03/16/Diffusion%20Model%20%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2024/03/16/Diffusion%20Model%20%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.bilibili.com/video/BV14o4y1e7a6/?vd_source=124ec79ebd3e16b0f454a3994a468f98">【Diffusion扩散模型+对比学习】2023最容易出论文的两个方向！由浅入深了解扩散模型＆对比学习，源码复现+模型解读（超详细保姆级入门教程）_哔哩哔哩_bilibili</a></p><p><a href="https://www.zhihu.com/question/545764550">(99+ 封私信 / 81 条消息) 怎么理解今年 CV 比较火的扩散模型（DDPM）？ - 知乎 (zhihu.com)</a></p><p><a href="https://arxiv.org/pdf/2209.00796.pdf">Diffusion Models: A Comprehensive Survey of Methods and Applications (arxiv.org)</a> (Diffusion 综述)</p><p><a href="https://zhuanlan.zhihu.com/p/562389931">扩散模型(Diffusion Model)首篇综述-Diffusion Models: A Comprehensive Survey of Methods and Applications - 知乎 (zhihu.com)</a></p></blockquote><blockquote><p><strong>Diffusion Model</strong> 是一种和 <strong>VAE、GAN、flow Model</strong> 等模型一样的生成模型(generative model)。</p></blockquote><h1 id="扩散模型的应用">扩散模型的应用</h1><p>根据综述文章（<a href="https://arxiv.org/pdf/2209.00796.pdf">Diffusion Models: A Comprehensive Survey of Methods and Applications (arxiv.org)</a>），我们可以将Diffusion的应用分为六类：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240225153110951.png" alt="image-20240225153110951" /><figcaption aria-hidden="true">image-20240225153110951</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240225132730831.png" alt="Diffusion_applications" /><figcaption aria-hidden="true">Diffusion_applications</figcaption></figure><ul><li><span style="background:#daf5e9;">Computer Vision 计算机视觉</span>——点云完整和生成</li><li>Natural Language Generation 自然语言生成</li><li>Temporal Data Modeling 时态数据建模</li><li><span style="background:#daf5e9;">Multi-Modal Learning 多模态生成</span>——由文本生成3D图像</li><li>Robust Learning 鲁棒地学习</li><li>Interdisciplinary Applications 跨学科应用</li></ul><h2 id="unconditional-and-conditional-diffusion-models">Unconditional and Conditional Diffusion Models</h2><p><strong>无条件扩散模型</strong>（unconditional diffusion models）和 <strong>条件扩散模型</strong>（conditional diffusion models）是扩散模型的两种基本应用范式。首先被发展的是无条件扩散模型，条件扩散模型则紧随其后。</p><p>无条件扩散模型通常被用于探索生成模型性能的上限。条件扩散模型则更多是用于应用级内容，因为它能够根据我们的倾向控制生成结果。</p><p><strong>条件扩散模型的四类在不同条件下的应用</strong>：</p><ul><li><p>扩散模型中的条件反射机制 Conditioning Mechanisms in Diffusion Models</p><p>条件反射机制主要有四种，包括<span style="background:#d4e9d5;">串联concatenation</span>、<span style="background:#fbd4d0;">基于梯度gradient-based</span>、<span style="background:#f9eda6;">交叉注意力gradient-based</span>和<span style="background:#dad5e9;">自适应层归一化adaptive layer normalization(adaLN)</span>。</p></li><li><p>标签和分类器上的条件扩散 Condition Diffusion on Labels and Classifiers</p></li><li><p>文本、图像和语义图上的条件扩散 Condition Diffusion on Texts, Images, and Semantic Maps</p></li><li><p>图形上的条件扩散 Condition Diffusion on Graphs</p></li></ul><h2 id="计算机视觉-computer-vision">计算机视觉 Computer Vision</h2><p><strong>1.图像超分辨率、修复、翻译和编辑 ( Image Super Resolution, Inpainting, Restoration, Translation, and Editing)</strong></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240225142347409.png" alt="image-20240225140505154" /><figcaption aria-hidden="true">image-20240225140505154</figcaption></figure><blockquote><p>图：由<strong>潜在扩散模型(Latent Diifusion Model, LDM)<sup>[1]</sup></strong>生成的图像超分辨率结果。潜在扩散模型（LDM）在不牺牲质量的情况下简化了扩散模型去噪的训练和采样过程。</p></blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240225140505154.png" alt="image-20240225141526648" /><figcaption aria-hidden="true">image-20240225141526648</figcaption></figure><blockquote><p>图：<strong>RePaint<sup>[2]</sup></strong> 生成的图像修复结果</p></blockquote><p><strong>2.语义分割 (Semantic Segmentation)</strong></p><p><strong>3.视频生成 (Video Generation)</strong></p><p><strong>4.点云完整和生成 (Point Cloud Completion and Generation)</strong></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240225141526648.png" alt="image-20240225142347409" /><figcaption aria-hidden="true">image-20240225142347409</figcaption></figure><blockquote><p>图：点云扩散过程的有向图形模型。<strong>Diffusion probabilistic models for 3d point cloud generation[3]</strong>采用了将点云视为热力学系统中的粒子的方法，使用热浴 (hit bath) 来促进从原始分布到噪声分布的扩散。</p></blockquote><p><strong>5.异常检测 (Anomaly Detection)</strong></p><h2 id="多模态生成-multi-modal-generation">多模态生成 Multi-Modal Generation</h2><p><strong>1.由文本生成图像 (Text-to-Image Generation)</strong></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240225151212293.png" alt="image-20240225143715162" /><figcaption aria-hidden="true">image-20240225143715162</figcaption></figure><blockquote><p>图：使用<strong>LDM<sup>[1]</sup>、Imagen<sup>[4]</sup> 和 ConPreDiff<sup>[5]</sup></strong> 进行文本到图像的合成示例</p></blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240225150546193.png" alt="image-20240225150546193" /><figcaption aria-hidden="true">image-20240225150546193</figcaption></figure><blockquote><p>图：与以前的SOTA模型相比，<strong>RPG<sup>[10]</sup></strong> 表现出在生成的图像中表达复杂和合成文本提示的卓越能力（彩色文本表示关键部分）。RPG 提出了一种全新的免训练文本到图像生成/编辑框架，利用多模态 LLM [339] 强大的思维链推理能力来增强文本到图像扩散模型的组合性。这个新的 RPG 框架以闭环方式统一了文本引导图像生成和图像编辑任务。值得注意的是，如图所示，RPG 优于所有 SOTA 方法，例如 SDXL 、DALL-E 3 和 InstructPix2Pix ，证明了其优越性。此外，RPG 框架是用户友好的，可以推广到不同的 MLLM 架构和扩散主干网（例如 ControlNet）。</p></blockquote><p><strong>2.由场景图生成图像 (Scene Graph-to-Image Generation)</strong></p><p><strong>3.<span style="background:#daf5e9;">由文本生成3D图像 (Text-to-3D Generation)</span></strong></p><p>3D内容生成在游戏、娱乐和机器人模拟等广泛应用中有着很高的需求。使用自然语言增强 3D 内容生成对新手和有经验的艺术家都有很大帮助。</p><p><strong>DreamFusion<sup>[6]</sup></strong>采用预先训练好的2D文本到图像扩散模型进行文本到3D合成。它优化了具有概率密度蒸馏损失的随机初始化的 3D 模型（神经辐射场或 NeRF），该模型利用 2D 扩散模型作为参数化图像生成器的先验优化。</p><p>为了获得NeRF的快速和高分辨率优化，<strong>Magic3D<sup>[7]</sup></strong>提出了一种基于级联低分辨率图像扩散先验和高分辨率潜在扩散先验的两阶段扩散框架。</p><p>为了实现高保真3D创作，<strong>Make-It-3D <sup>[8]</sup></strong>优化了神经辐射场，在正面视图处结合了参考图像的约束条件，在新视图处加入了扩散先验和扩散先验，从而优化了神经辐射场，将粗糙模型增强为有纹理的点云，并通过扩散先验和参考图像的高质量纹理来增加真实感。</p><p><strong>ProlificDreamer <sup>[9]</sup></strong> 提出了变分分数蒸馏 （VSD），基于文本提示作为随机变量优化 3D 场景的分布，以使用 KL 散度作为度量，将来自各个角度的渲染图像分布与预训练的 2D 扩散模型紧密对齐。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240225143715162.png" alt="image-20240225151212293" /><figcaption aria-hidden="true">image-20240225151212293</figcaption></figure><blockquote><p>图：ProlificDreamer 可以生成高保真和逼真的 3D 纹理网格。</p></blockquote><p><strong>4.由文本生成动作 (Text-to-Motion Generation)</strong></p><p><strong>5.由文本生成视频 (Text-to-Video Generation)</strong></p><p><strong>6.由文本生成音频 (Text-to-Audio Generation)</strong></p><h1 id="未来研究方向">未来研究方向</h1><p>A.重审假设。需要重新审视和分析扩散模型中的许多典型假设。例如，假设扩散模型的正向过程完全消除了数据中的所有信息并且使其等效于先前分布可能并不总是成立。实际上，完全删除信息是在有限时间内无法实现，了解何时停止前向噪声处理以在采样效率和采样质量之间取得平衡是非常有意义的。</p><p>B. 理论理解。diffusion model已经成为一个强大的框架，可以在大多数应用中与生成对抗性网络（GAN）竞争，而无需诉诸对抗性训练。对于特定的任务，我们需要了解为什么以及何时扩散模型会比其他网络更加有效，理解扩散模型和其他生成模型的区别将有助于阐明为什么扩散模型能够产生优秀的样本同时拥有高似然值。另外，系统地确定扩散模型的各种超参数也是很重要的。</p><p>C. 潜在表示。diffusion model如何在隐空间中提供良好的latent representation，以及如何将其用于data manipulation的任务也是值得研究的。</p><p>D. AIGC 和 Diffusion Foundation Models。将diffusion model和generative foundation model结合，探索更多类似于从AI 文生图扩散模型 Stable Diffusion 到 ChatGPT，GPT-4等的有趣的人工智能生成内容（AIGC）应用。</p><h1 id="参考文献">参考文献</h1><details><summary>点我查看</summary><p>[1] Rombach R, Blattmann A, Lorenz D, et al. High-resolution image synthesis with latent diffusion models[C], Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 10684-10695.</p><p>[2] Lugmayr A, Danelljan M, Romero A, et al. Repaint: Inpainting using denoising diffusion probabilistic models[C], Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 11461-11471.</p><p>[3] Luo S, Hu W. Diffusion probabilistic models for 3d point cloud generation[C], Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 2837-2845.</p><p>[4] Saharia C, Chan W, Saxena S, et al. Photorealistic text-to-image diffusion models with deep language understanding[J]. Advances in Neural Information Processing Systems, 2022, 35: 36479-36494.</p><p>[5] Yang L, Liu J, Hong S, et al. Improving diffusion-based image synthesis with context prediction[J]. Advances in Neural Information Processing Systems, 2024, 36.</p><p>[6] Poole B, Jain A, Barron J T, et al. Dreamfusion: Text-to-3d using 2d diffusion[J]. arxiv preprint arxiv:2209.14988, 2022.</p><p>[7] Lin C H, Gao J, Tang L, et al. Magic3d: High-resolution text-to-3d content creation[C], Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 300-309.</p><p>[8] Tang J, Wang T, Zhang B, et al. Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior[J]. arxiv preprint arxiv:2303.14184, 2023.</p><p>[9] Wang Z, Lu C, Wang Y, et al. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation[J]. Advances in Neural Information Processing Systems, 2024, 36.</p><p>[10] Yang L, Yu Z, Meng C, et al. Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs[J]. arxiv preprint arxiv:2401.11708, 2024.</p></details>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>马尔科夫过程</title>
      <link href="/2024/03/16/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E8%BF%87%E7%A8%8B/"/>
      <url>/2024/03/16/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信息论与编码 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DreamFusion 论文笔记</title>
      <link href="/2024/03/11/DreamFusion/"/>
      <url>/2024/03/11/DreamFusion/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>论文名称： DreamFusion: Text-to-3D using 2D Diffusion (DreamFusion: 使用 2D 扩散模型实现从文本到3D模型)</strong></p><p>论文代码：</p><ul><li><p><a href="https://github.com/vikasTmz/Dreamfusion?tab=readme-ov-file">GitHub - vikasTmz/Dreamfusion: A pytorch implementation of text-to-3D dreamfusion, powered by stable diffusion.</a></p></li><li><p><a href="https://github.com/ashawkey/stable-dreamfusion">GitHub - ashawkey/stable-dreamfusion: Text-to-3D &amp; Image-to-3D &amp; Mesh Exportation with NeRF + Diffusion.</a></p><p>论文主页： <a href="https://dreamfusion3d.github.io/">DreamFusion: Text-to-3D using 2D Diffusion (dreamfusion3d.github.io)</a></p></li></ul><p>参考网址：</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> diffusion </tag>
            
            <tag> CVPR </tag>
            
            <tag> 2023 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparseFusion 论文笔记</title>
      <link href="/2024/03/11/SparseFusion/"/>
      <url>/2024/03/11/SparseFusion/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文名称： SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction (SparseFusion: 蒸馏提取有条件视图的扩散，用于三维重建)</p><p>论文代码： <a href="https://github.com/zhizdev/sparsefusion">GitHub - zhizdev/sparsefusion: Official PyTorch implementation of SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction</a></p><p>论文主页： <a href="https://sparsefusion.github.io/">SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction</a></p><p>参考网址：</p></blockquote><h1 id="论文基本信息">论文基本信息</h1>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> diffusion </tag>
            
            <tag> CVPR </tag>
            
            <tag> 2023 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DreamAvatar 论文笔记</title>
      <link href="/2024/03/11/DreamAvatar/"/>
      <url>/2024/03/11/DreamAvatar/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.bilibili.com/video/BV1SN411i7d8/?spm_id_from=333.337.search-card.all.click&amp;vd_source=124ec79ebd3e16b0f454a3994a468f98">2023082【跨模态学习驱动的三维理解与生成】韩锴：Text-and-Shape Guided 3D Human Avatar Generation……_哔哩哔哩_bilibili</a></p></blockquote><blockquote><p><strong>论文名称： DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via</strong> <strong>Diffusion Models （DreamAvatar: 文本和形状引导的基于扩散模型的3D人体角色生成）</strong></p><p>代码地址： 未开源</p><p>论文主页： <a href="https://yukangcao.github.io/DreamAvatar/">DreamAvatar (yukangcao.github.io)</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>群策群力战NeuS，乃知NeRF难自遮</title>
      <link href="/2024/03/11/NeuS/"/>
      <url>/2024/03/11/NeuS/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文名称: <strong>NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction</strong>（NeuS: 通过体积渲染学习神经隐式曲面，用于多视图重建）</p><p>代码地址：<a href="https://github.com/Totoro97/NeuS">Totoro97/NeuS: Code release for NeuS (github.com)</a></p><p>论文主页: <a href="https://lingjie0206.github.io/papers/NeuS/">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction (lingjie0206.github.io)</a></p><p>论文翻译：<a href="https://zhuanlan.zhihu.com/p/577893396">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction - 知乎 (zhihu.com)</a> （来自别的友友的翻译~）</p></blockquote><h1 id="论文基本信息">论文基本信息</h1><p>NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction<sup>[1]</sup>是<strong>2021</strong>年由来自香港大学、Max Planck研究所和德州农工大学的Peng Wang 等人发表在 <strong>NeurIPS (Spotlight)</strong> 会议上的SCI论文。</p><h2 id="论文摘要">论文摘要</h2><p>我们提出了一种新颖的神经表面重建方法，称为NeuS，用于从2D图像输入以高保真度重建对象和场景。</p><p>现有的神经表面重建方法，例如DVR [Niemeyer等人，2020] 和IDR [Yariv等人，2020]，需要前景掩模<span style="background:#eef0f4;">(foreground mask)</span>作为监督，容易被困在局部最小值中，并且因此与具有严重自遮挡<span style="background:#eef0f4;">(self-occlusion)</span>或薄结构的对象的重建作斗争。同时，用于新视图合成的最近的神经方法，例如NeRF [Mildenhall等人，2020] 及其变体，使用体积渲染来产生具有优化鲁棒性的神经场景表示，即使对于高度复杂的对象也是如此。但是，很难从这种学习的隐式表示中提取高质量的曲面，因为表示中没有足够的曲面约束<span style="background:#eef0f4;">(surface constraints)</span>。</p><p>在NeuS中，我们建议将曲面表示为有符号距离函数 (SDF) 的零级集<span style="background:#eef0f4;">(zero-level set)</span>，并开发一种新的体渲染方法来训练神经SDF表示。我们观察到传统的体绘制方法会导致表面重建的固有几何误差 (即偏差 <span style="background:#eef0f4;">bias</span>)，因此提出了一种新的公式，该公式在一阶近似中没有偏差，因此<strong>即使没有掩模监督</strong>，也可以实现更准确的表面重建。在DTU数据集和BlendedMVS数据集上的实验表明，NeuS在高质量的表面重建方面的性能优于现有技术，特别是对于具有复杂结构和自遮挡的对象和场景。</p><h2 id="创新点idea">创新点/IDEA</h2><p><strong>NeRF旨在进行新视图合成而不是表面重建</strong>，因此NeRF仅学习体积密度场，因此很难从中提取高质量的表面。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240211204429014.png" alt="image-20240211204429014" /><figcaption aria-hidden="true">image-20240211204429014</figcaption></figure><blockquote><p>图1 : ( a) 表面渲染和体积渲染的图示。(b) 竹花机的玩具实例，其中花机顶部有咬合物。与最先进的方法相比，我们的方法可以处理闭塞并获得更好的重建质量。</p></blockquote><p>针对该问题，该文章提出通过引入由SDF引起的密度分布，应用体渲染方法来学习隐式SDF表示。又因简单地将标准体渲染方法应用于与SDF相关的密度将导致重构表面中的可识别偏差，所以文章提出一种新颖的体绘制方案，以确保在SDF的一阶近似中进行无偏表面重建。</p><p><strong><span style="background:#daf5e9;">给定一组3D对象的有位姿图像{<span class="math inline">\(I_k\)</span>}，我们的目标是重建其表面<span class="math inline">\(S\)</span></span>。表面由神经隐式SDF的零级集表示。为了学习神经网络的权重，我们开发了一种新颖的体渲染方法，用于从隐式SDF渲染图像，并最小化渲染图像与输入图像之间的差异。这种体渲染方法可确保NeuS中的稳健优化，以重建复杂结构的对象。</strong></p><h1 id="相关工作">相关工作</h1><h2 id="classical-multi-view-surface-and-volumetric-reconstruction">Classical Multi-view Surface and Volumetric Reconstruction</h2><ul><li>基于点和表面的重建<ul><li>基于点和表面的重建方法通过利用图像间光度一致性来估计每个像素的深度图，然后将深度图融合为全局密集点云；</li><li>表面重建通常是通过诸如筛选的泊松表面重建之类的方法进行后处理 ；</li><li>总结：重建质量很大程度上依赖于对应匹配的质量，对于没有丰富纹理的对象，匹配对应的困难往往会导致重建结果出现严重的伪影和缺失部分；</li></ul></li><li>体积重建<ul><li>体积重建方法通过从多视图图像中估计体素网格中的占用率和颜色并评估每个体素的颜色一致性来规避显式对应匹配的困难；</li><li>总结：由于可实现的体素分辨率有限，这些方法无法实现高精度；</li></ul></li></ul><h2 id="neural-implicit-representation">Neural Implicit Representation</h2><ul><li>一些方法通过引入归纳偏差来在深度学习框架中强制实现3D理解。这些归纳偏差可以是显式表示，例如体素网格 ，点云 ，网格和隐式表示。</li><li>由于神经网络编码的隐式表示是连续的并且可以实现高空间分辨率，因此最近引起了很多关注。此表示已成功应用于形状表示，新颖的视图合成 和多视图3D重建。</li></ul><h1 id="算法框架">算法框架</h1><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240422182047922.png" alt="image-20240422182047922" /><figcaption aria-hidden="true">image-20240422182047922</figcaption></figure><h2 id="场景表示-scene-representation">场景表示 Scene representation</h2><p><u>第一步是构造从3D模型到图像的渲染方法</u>（在传统图形学大概可称为<u>光栅化</u>）。与NeRF类似（除了将密度 <span class="math inline">\(\sigma\)</span> 替换为 <span class="math inline">\(sdf\)</span> ），定义重建对象的<u>场景</u>由两个函数表示: <span class="math display">\[f:(x,y,z)-&gt;sdf \\c:(x,y,z,view dir)-&gt;color\]</span></p><ul><li><span class="math inline">\(f:\mathbb{R} ^3\rightarrow \mathbb{R}\)</span> 将空间位置<span class="math inline">\(x\in \mathbb{R} ^3\)</span>映射为从<span class="math inline">\(x\)</span>到对象的有符号距离</li><li><span class="math inline">\(c:\mathbb{R} ^3\times \mathbb{S}^2\rightarrow \mathbb{R} ^3\)</span> 对与点<span class="math inline">\(x\in \mathbb{R} ^3\)</span>和视角方向<span class="math inline">\(v\in \mathbb{S} ^2\)</span>相关的颜色进行编码</li></ul><p>这两个函数都由多层感知机MLP编码。重建对象的表面<span class="math inline">\(S\)</span>由SDF的零级集表示（<span style="background:#daf5e9;">SDF函数，即<span class="math inline">\(f(x)\)</span>的零值面表达形状）</span>，即 <span class="math display">\[\tag{1}\label{eq1}S=\left\{ x\in \mathbb{R} ^3|f\left( x \right) =0 \right\} .\]</span> <u>第二步是构造体渲染训练SDF网络。</u>为了应用体渲染方法来训练SDF网络，和传统体渲染不同，引入概率密度函数$_s( f( x ) ) $作为不透明度<code>opacity</code> <span class="math inline">\(\alpha\)</span>/ <code>“密度”</code><span class="math inline">\(\sigma\)</span> ，称为 <strong>S-density</strong> 。其中<span class="math inline">\(f(x)\)</span>是采样点<span class="math inline">\(x\)</span>的<code>符号距离函数SDF</code>，<span class="math inline">\(s\)</span>是一个<strong>trainable parameter</strong>，外层是逻辑密度分布函数(logistic density function): <span class="math display">\[\phi_s(x)=\Phi_s&#39;(x)=\frac{se^{-sx}}{(1+e^{-sx})^2}\tag{2}.\]</span> 逻辑密度分布函数是sigmoid函数 <span class="math inline">\(\Phi_s(x)=\frac{1}{1+e^{-sx}}\)</span> 的一阶导数。<span style="background:#fbd4d0;">原则上，概率密度函数<span class="math inline">\(\phi _S\left( x \right)\)</span>可以是以0为中心的任何单峰 unimodal（即铃形 bell-shaped）密度函数，这里我们选逻辑密度分布是为了计算方便。</span>逻辑密度分布函数<span class="math inline">\(\phi _s\left( x \right)\)</span>的标准差是<span class="math inline">\(1/s\)</span>，也是一个<strong>trainable parameter</strong>，即随着网络收敛<span class="math inline">\(1/s\)</span>趋向于0，同时也意味着此时概率密度函数pdf在射线上的某个空间点达到了峰值。</p><figure><img src="https://pic4.zhimg.com/80/v2-f85d8dcaa50d39f38f7b76a85ce0da4b_1440w.webp" alt="输入图片描述" /><figcaption aria-hidden="true">输入图片描述</figcaption></figure><p>从上图可知，<span class="math inline">\(\phi(x)\)</span>为一个在0点对称的概率密度函数，在0点达到最大值，数据从0点开始衰减。</p><h2 id="体渲染-rendering">体渲染 Rendering</h2><p>给定一个像素，相机中心为<span class="math inline">\(\mathbf{o}\)</span>，光线方向（即射线ray的单位向量）为<span class="math inline">\(\mathbf{v}\)</span>，从相机中心到该像素的射线上一点为<span class="math inline">\(\mathbf{p}(t)=\mathbf{o}+t\mathbf{v}，t\geqslant0\)</span>。该像素颜色<span class="math inline">\(C\left( \mathbf{o},\mathbf{v} \right)\)</span>的一般渲染通式为： <span class="math display">\[C\left( \mathbf{o},\mathbf{v} \right) =\int_0^{+\infty}{w\left( t \right) c\left( \mathbf{p}\left( t \right) ,\mathbf{v} \right) dt}\tag{3}.\]</span> &gt; 该通式其实就是 NeRF 中的未经离散化的体渲染公式： &gt; <span class="math display">\[&gt;   C=\int T(t)c(t)\sigma(t)dt\\&gt;   T(t)=e^{-\int_0^t \sigma(s)dt}\\&gt;   w(t)=T(t)\sigma(t)&gt;   \]</span></p><p><span class="math inline">\(c\left( \mathbf{p}\left( t \right) ,\mathbf{v} \right)\)</span>是在射线上一点<span class="math inline">\(\mathbf{p}\)</span>沿着视角方向<span class="math inline">\(\mathbf{v}\)</span>的颜色。<span class="math inline">\(w(t)\)</span>是关于颜色的权重函数，因此要求<span class="math inline">\(w(t)&gt;0\)</span>且<span class="math inline">\(\int_0^{+\infty}w(t)dt=1\)</span>。</p><p>作者认为从2D图像学习精确的SDF表达的关键是<strong>建立一个合适的基于SDF的权重函数 <span class="math inline">\(w(t)\)</span></strong>，有两点要求：</p><ul><li><strong>Unbiased</strong>：对于相机射线<span class="math inline">\(\mathbf{p}(t)\)</span>和表面surface的交点<span class="math inline">\(\mathbf{p}(t^*)\)</span>，<span class="math inline">\(w(t)\)</span>应在<span class="math inline">\(t^*\)</span>处取得局部最大值（即<u>相机射线</u>和<u>SDF零级集zero-level set（表面surface）</u>的交点处的像素贡献最大）。也就是说，<span class="math inline">\(\frac{dw}{dt}=0\ when \ t=t^*\)</span>。</li><li><strong>Occulusion-aware</strong>：同一条射线上两个深度<span class="math inline">\(t_0\)</span>和<span class="math inline">\(t_1\)</span>，若<span class="math inline">\(f\left( t_0 \right) =f\left( t_1 \right)\)</span>但<span class="math inline">\(t_0&lt;t_1\)</span>，那么应该有<span class="math inline">\(w(t_0)&gt;w(t_1)\)</span>（即考虑自体遮挡，如果一条射线多次交叉表面（穿越多个表面），应该更多地使用最靠近相机的交点的颜色（射线所遇到的第一个表面交点所带来的颜色））。</li></ul><p>作者讨论了两种<span class="math inline">\(w(t)\)</span>表达，最终选择了<span class="math inline">\((b)\)</span>形式。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240212171629587.png" alt="image-20240212171629587" /><figcaption aria-hidden="true">image-20240212171629587</figcaption></figure><blockquote><p>图2</p></blockquote><h3 id="naive的wt颜色权重函数设计">naive的<span class="math inline">\(w(t)\)</span>颜色权重函数设计</h3><p>直接利用先前NeRF中提出的 volume rendering pipeline： <span class="math display">\[w\left( t \right) =T\left( t \right) \sigma \left( t \right)\tag{4}.\]</span> - <span class="math inline">\(\sigma \left( t \right)\)</span>是传统体渲染中的体密度volume density，被设置为与S-density相同，其中 <span class="math inline">\(\mathbf{p}(t)\)</span> 表示采样点的<span class="math inline">\((x,y,x)\)</span>坐标：</p><p><span class="math display">\[\sigma(t)=S-density=\phi_s(f(\mathbf{p}(t))).\]</span></p><ul><li><span class="math inline">\(T(t)=\exp(-\int_0^t\sigma(u){\rm d}u)\)</span>是沿射线的累积透射率accumulated transmittance。</li></ul><p>该方法只能做到occulusion-aware，但是由于它在重建表面引入了固有误差，因此有bias，在射线打到表面之前<span class="math inline">\(w(t)\)</span>已经取得了局部最优，如图2(a)所示（具体推导可参考原文补充材料）。</p><blockquote><p><font color=#4eb434>naive 的权重设计不满足 <strong>unbiased</strong> 的原因推导</font></p><p>下面来算一下： <span class="math display">\[  \frac{dw}{dt}=\frac{dT(t)}{dt}\sigma(t)+T(t)\frac{d \sigma(t)}{dt}\\  =-e^{-\int_0^t \sigma(s)dt}\sigma(t)\sigma(t)+T(t)\frac{d \sigma(t)}{dt}\\  =T(t)(\frac{d \sigma(t)}{dt}-\sigma^2(t))  \]</span> 把符号距离函数的公式代入：</p><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240422161748615.png" alt="image-20240422161748615" /> <span class="math display">\[  \frac{dw(t)}{dt}=T(t^*)(\frac{d\phi_s(f(p(t))}{dt}-\phi_s ^2(f(p(t)))\\  =T(t^*)(\phi_s&#39; (f(p(t))(\bigtriangledown f(p(t).\bold v)-\phi_s ^2(f(p(t)))\\  =T(t^*)(\phi_s&#39; (f(p(t))cos\theta-\phi_s ^2(f(p(t)))  \]</span> 假设<span class="math inline">\(t^*\)</span>为这一射线与曲面第一次相交的点。 <span class="math display">\[  f(p(t^*))=0\\  \phi_s&#39;(f(p(t^*))=0  \]</span> 所以： <span class="math display">\[  \frac{dw(t^*)}{dt^*}=-T(t^*)\phi_s^2(0)&lt;0  \]</span> 从上面的推导可知，在曲面上的一个点，梯度不为 0。因此原naive idea是有问题的。</p></blockquote><h3 id="neus的wt颜色权重函数设计">NeuS的<span class="math inline">\(w(t)\)</span>颜色权重函数设计</h3><p>首先讨论一种straight-forward 方式： <span class="math display">\[w(t) = \frac{ \phi_s(f(\mathbf{p}(t)))}{ \int_0^{+\infty} \phi_s( f( \mathbf{p}(u) ) ) {\rm d}u }. \tag{5}\label{eq5}\]</span> 该方式直接使用归一化的S-density作为权重函数，显然无偏，但是没有occlusion-aware：两个SDF交叉点将在<span class="math inline">\(w(t)\)</span>中产生两个等值的峰，即公式(5)值适用于每条射线与曲线只有一个交点的情况下。</p><p>接下来，我们需要再对其进行改造，使其能够满足occlusion-aware的性质。自然地，NeRF本身基本的公式就是occlusion-aware，文章还是采用了基本公式的逻辑，但是在其基础上做了微小的改动，即<span style="background:#daf5e9;">自行定义了一个不透明的密度函数<span class="math inline">\(\rho(t)\)</span>（opaque density），对应着传统公式中的 <span class="math inline">\(\sigma(t)\)</span></span>，因此，权重函数变为了： <span class="math display">\[w(t)=T(t)\rho(t),\\\,\,T(t)=\exp(-\int_0^t \rho(u){\rm d}u). \tag{6}\label{eq6}\]</span></p><p>注意，这里 <span class="math inline">\(\rho(t) \neq \textcolor{orange}{\phi_s(f(p(t))=S-density=\sigma(t)}\)</span> 。</p><h4 id="rhot的推导思路"><span class="math inline">\(\rho(t)\)</span>的推导思路</h4><p>首先，我们考虑一种简单情况：即只有一个表面交点且表面是平面，此时从<span class="math inline">\(\eqref{eq5}\)</span> <span class="math inline">\(\eqref{eq6}\)</span>式出发导出<span class="math inline">\(\rho(t)\)</span>以<span class="math inline">\(f(\mathbf{p}(t))\)</span>为输入的表达式，然后再推广到多次表明相交的情况。</p><p>在这种简单的特殊情况下，我们很容易地知道符号距离函数sdf，即<span class="math inline">\(f(\mathbf{p}(t))\)</span>有以下的关系： <span class="math display">\[f(\mathbf{p}(t))=-|cos(\theta)|\cdot(t-t^*),\,\,where\,\, f(\mathbf{p}(t^*))=0 \tag{7}.\]</span> 其中，<span class="math inline">\(\theta\)</span>是视角方向<span class="math inline">\(\mathbf{v}\)</span>与外表面法向量<span class="math inline">\(\mathbf{n}\)</span>之间的夹角。由于表面surface被假定为一个平面plane，因此<span class="math inline">\(|cos(\theta)|\)</span>是一个常量。此时根据<span class="math inline">\(\eqref{eq5}\)</span>式有： <span class="math display">\[\begin{aligned}w(t) &amp; =\lim _{t^{*} \rightarrow+\infty} \frac{\phi_{s}(f(\mathbf{p}(t)))}{\int_{0}^{+\infty} \phi_{s}(f(\mathbf{p}(u))) \mathrm{d} u} \\&amp; =\lim _{t^{*} \rightarrow+\infty} \frac{\phi_{s}(f(\mathbf{p}(t)))}{\int_{0}^{+\infty} \phi_{s}\left(-|\cos (\theta)|\left(u-t^{*}\right)\right) \mathrm{d} u} \\&amp; =\lim _{t^{*} \rightarrow+\infty} \frac{\phi_{s}(f(\mathbf{p}(t)))}{\int_{-t^{*}}^{+\infty} \phi_{s}\left(-|\cos (\theta)| u^{*}\right) \mathrm{d} u^{*}} \\&amp; =\lim _{t^{*} \rightarrow+\infty} \frac{\phi_{s}(f(\mathbf{p}(t)))}{|\cos (\theta)|^{-1} \int_{-|\cos (\theta)| t^{*}}^{+\infty} \phi_{s}(\hat{u}) \mathrm{d} \hat{u}} \\&amp; =|\cos (\theta)| \phi_{s}(f(\mathbf{p}(t))) .\end{aligned} \tag{8}\]</span> 又根据<span class="math inline">\(\eqref{eq6}\)</span>式， <span class="math display">\[T(t)\rho(t)=|\cos (\theta)| \phi_{s}(f(\mathbf{p}(t))).\tag{9}\]</span> 已知累积透射率accumulated transmittance <span class="math inline">\(T(t)=\exp(-\int_0^t \rho(u){\rm d}u)\)</span>，则： <span class="math display">\[T\left( t \right) \rho \left( t \right) =-\frac{dT}{dt}\left( t \right).\tag{10.a}\]</span></p><p><span class="math display">\[|\cos (\theta)| \phi_{s}(f(\mathbf{p}(t)))=-\frac{d\varPhi _s}{dt}\left( f\left( \mathbf{p}\left( t \right) \right) \right) .\tag{10.b}\]</span></p><p>从而， <span class="math display">\[\frac{dT}{dt}\left( t \right)=\frac{d\varPhi _s}{dt}\left( f\left( \mathbf{p}\left( t \right) \right) \right)\]</span> 对两边积分得， <span class="math display">\[T\left( t \right) =\varPhi _s\left( f\left( \mathbf{p}\left( t \right) \right) \right)\tag{11}\]</span> 最后取对数，再对两边积分， <span class="math display">\[\int_{0}^{t} \rho(u) \mathrm{d} u=-\ln \left(\Phi_{s}(f(\mathbf{p}(t)))\right) \\\Rightarrow \rho(t)=\frac{-\frac{\mathrm{d} \Phi_{s}}{\mathrm{~d} t}(f(\mathbf{p}(t)))}{\Phi_{s}(f(\mathbf{p}(t)))}. \tag{12}\]</span> 上式是单个surface的情况，当光线在两个surface之间时,<span class="math inline">\(-\frac{d\Phi_s(f(p(t)))}{dt}\)</span> 会变成负。因此为了防止它为负，拓展到多surface的情况时需要加一个为 0 的约束，即： <span class="math display">\[\rho(t)=\max \left(\frac{-\frac{\mathrm{d} \Phi_{s}}{\mathrm{~d} t}(f(\mathbf{p}(t)))}{\Phi_{s}(f(\mathbf{p}(t)))}, 0\right) .\tag{13}\label{eq13}\]</span> 且： <span class="math display">\[C\left( \mathbf{o},\mathbf{v} \right) =\int_0^{+\infty}{w\left( t \right) c\left( \mathbf{p}\left( t \right) ,\mathbf{v} \right) dt} \tag{3}\]</span> 在补充材料中我们给出了在单面交和多面交情况下，由<span class="math inline">\(\eqref{eq13}\eqref{eq6}\)</span>式定义的权重函数在SDF的一阶逼近中是无偏的定理证明。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240213173052919.png" alt="image-20240213173052919" /><figcaption aria-hidden="true">image-20240213173052919</figcaption></figure><blockquote><p>图3 多个表面相交情况下，权重分布图示。由图可见，相交的第一个面的贡献度是最大的，后面进行了衰减，符合 occlusion-aware的要求；且我们设计的<span class="math inline">\(w(t)\)</span>还保证了 SDF的估计是无偏 unbiased 的。</p></blockquote><h3 id="离散化处理-discretization">离散化处理 Discretization</h3><p>跟原始nerf一样，我们推出来的这个连续积分的形式（式(3)），计算机是无法处理的，所以我们需要对这个公式进行离散化处理，方便我们编程。为了获得不透明度opacity相关的 <span class="math inline">\(\rho(t)\)</span>和权重函数<span class="math inline">\(w(t)\)</span>的离散对应部分，我们采用与NeRF中使用的近似方案，该方案沿着射线对<span class="math inline">\(n\)</span>个点进行采样<span class="math inline">\(\{\mathbf{p}_i=\mathbf{o}+t_i\mathbf{v}|i=1,...,n,t_i&lt;t_{i+1}\}\)</span>，以计算射线的近似像素颜色<span class="math inline">\(\hat{C}\)</span>如下： <span class="math display">\[\hat{C}=\sum_{i=1}^n{T_i\alpha _ic_i},\]</span> <span class="math display">\[T_i=\prod_{j=1}^{i-1}{\left( 1-\alpha _j \right)},   \tag{14}\label{eq14}\]</span></p><p><span class="math display">\[\alpha _i=1-\exp \left( -\int_{t_i}^{t_{i+1}}{\rho \left( t \right) dt} \right) =\max \left( \frac{\Phi _s(f(\mathbf{p}(t_i)))-\Phi _s(f(\mathbf{p}(t_{i+1})))}{\Phi _s(f(\mathbf{p}(t_i)))},0 \right).\]</span></p><p>其中<span class="math inline">\(T_i\)</span>是离散的累积透射率accumulated transmittance，<span class="math inline">\(\alpha_i\)</span>是离散的不透明度opacity 值，关于<span class="math inline">\(\alpha_i\)</span>的详细推导在补充材料中给出。</p><blockquote><p><font color=#0091ff>关于离散化公式 (14) 的推导</font></p><p>原始 NeRF 的体渲染公式： <span class="math display">\[  C=\int T(t)c(t)\sigma(t)dt  \]</span> 在离散情况下，我们先把射线分为一段一段，假定每一段内颜色<span class="math inline">\(c(t)\)</span>和密度<span class="math inline">\(\sigma(t)\)</span>是不变的，先算出其每段的不透明度： <span class="math display">\[  \alpha_i=1-e^{\int_{t_i}^{t_{i+1}}\sigma(t)dt}=1-e^{\delta_i \sigma_i} \\  T_{i}=e^ {-\sum_{j=1}^{i-1} \sigma_{j} \delta_{j}}= \prod^{i-1}_{j=1}{(1-\alpha_i)}  \]</span> 最后的颜色预测为： <span class="math display">\[  C=c_0\alpha_0+c_1\alpha_1(1-\alpha_0)+...+c_N\alpha_N(1-\alpha_{N-1})...(1-\alpha_0)  \]</span> NeuS 的方法也是一样，只是把 <span class="math inline">\(\sigma(t)\)</span> 用 <span class="math inline">\(\rho(t)\)</span> 代替： <span class="math display">\[  \begin{align}  \alpha_i&amp;=1-e^{\int_{t_i}^{t_{i+1}}\rho(t)dt}\\  &amp;=1-e^{\int_{t_i}^{t_{i+1}}-\frac{\frac{d\Phi_s(f(p(t)))}{dt}}{\Phi_s(f(p(t)))}dt}\\  &amp;=1-e^{ln\Phi_s(f(p(t))+C|_{t_i}^{t_{i+1}}}\\  &amp;=1-e^{ln\Phi_s(f(p(t_{i+1}))-ln\Phi_s(f(p(t_{i}))}\\  &amp;=1-\frac{\Phi_s(f(p(t_{i+1}))}{\Phi_s(f(p(t_{i}))}\\  &amp;=\frac{\Phi_s(f(p(t_{i}))-\Phi_s(f(p(t_{i+1}))}{\Phi_s(f(p(t_{i}))}  \end{align}  \]</span></p></blockquote><h4 id="离散化的采样方式">离散化的采样方式</h4><p>在真正的实现过程中，实际上有两种采样方式：</p><ol type="1"><li>直接采样射线上的点：<span class="math inline">\(\mathbf{q}_i=\mathbf{o}+t_i\mathbf{v}\)</span></li><li>采样射线上某一小段的中点： <span class="math inline">\(\mathbf{p}_i = \mathbf{o} + \frac{ t_i + t_{i+1} }{2} \mathbf{v}\)</span></li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240213205224343.png" alt="image-20240213205224343" /><figcaption aria-hidden="true">image-20240213205224343</figcaption></figure><blockquote><p>图4 两种采样方式。</p><p>具体而言，当我们<strong>计算<span class="math inline">\(\alpha\)</span>值</strong>时，我们采用section points的方式，计算公式与<span class="math inline">\(\eqref{eq14}\)</span>保持一致，即<span class="math inline">\(\max \left( \frac{\Phi _s(f(\mathbf{q}(t_i)))-\Phi _s(f(\mathbf{q}(t_{i+1})))}{\Phi _s(f(\mathbf{q}(t_i)))},0 \right)\)</span>。当<strong>计算颜色值</strong>时，我们采用mid-point方式。</p></blockquote><h2 id="训练-training">训练 Training</h2><h3 id="损失函数">损失函数</h3><p>随机采样一批(a batch)像素，以及其对应的在世界坐标系下的射线，即： <span class="math display">\[P=\{C_k,M_k,\mathbf{o}_k,\mathbf{v}_k\}\]</span> 其中，<span class="math inline">\(C_k\)</span>是像素颜色，<span class="math inline">\(M_k\in\{0,1\}\)</span>是可选的mask值。采样像素数量（batch size）设为m，一条射线上的采样点数（point sampling size）设为n。损失函数定义为： <span class="math display">\[L=L_{color}+\lambda L_{reg}+\beta L_{mask}.\tag{15}\]</span> 其中，</p><ul><li><span class="math inline">\(L_{color}=\frac{1}{m}\sum_k{R\left( \hat{C}_k,C_k \right)}\)</span> ——渲染图和真实图差异。类似IDR，<span class="math inline">\(R\)</span>采用L1损失，这使得我们的观察对outliers鲁棒且训练稳定</li><li><span class="math inline">\(L_{reg}=\frac{1}{nm}\sum_{k,i}{\left( |\nabla f\left( \mathbf{\hat{p}}_{k,i} \right) |_2-1 \right) ^2}\)</span>——在采样点sample points上加的Eikonal项，用于正则化<span class="math inline">\(f_{\theta}\)</span>的SDF值，即SDF法向量的损失</li><li><span class="math inline">\(L_{mask}=BCE(M_k,\hat{O}_k)\)</span>——可选mask项，BCE（binary cross entropy）即二值交叉熵。其中<span class="math inline">\(\hat{O}_k=\sum_{i=1}^n{T_{k,i}\alpha _k}\)</span>是在相机射线上采样点的权重求和，这里的预测可微分mask就是通过射线上权重<span class="math inline">\(w(t)\)</span>的和求出的。</li></ul><h3 id="分层采样-hierarchical-sampling">分层采样 Hierarchical Sampling</h3><blockquote><p>原来的NeRF采样过程：</p><p>首先均匀采样一组点，然后 coarse importance sampling + fine importance sampling。<strong>coarse sampling</strong> 64个点，其概率值由 <strong>固定的很大的</strong>标准差 <span class="math inline">\(1/s\)</span>定义的<span class="math inline">\(\phi_s(f(x))\)</span>给出；<strong>fine sampling</strong> 64个点，其概率值由 <strong>learned</strong> <span class="math inline">\(1/s\)</span> 定义的 <span class="math inline">\(\phi_s(f(x))\)</span>给出。</p></blockquote><p>基本没怎么改动 NeRF 原来的 stratified sampling 过程（首先，均匀地在射线上进行采样，然后迭代地在粗概率估计峰值处执行重要性采样。）:</p><p>但不同于NeRF同时优化 coarse network和 fine network，这里只训练了一个network。首先均匀采样64个点作为coarse sampling 点，其概率由 <strong>固定的很大的</strong>标准差 <span class="math inline">\(1/s\)</span>定义的S-density <span class="math inline">\(\phi_s(f(x))\)</span>给出；然后利用这组coarse点、其对应的由<strong>固定的很大的</strong>标准差<span class="math inline">\(1/s\)</span>定义的<span class="math inline">\(\phi_s(f(x))\)</span>值和学得的标准差构造pdf，采样64个fine sampling点。</p><h1 id="代码">代码</h1><blockquote><p>代码注释小技巧：在注释代码时，如果想区别原来就有的代码和我们后来加上的代码，可以通过 Ctrl+R 将所有原来就有的注释符号 '#' 改为' '###'。在后面添加的代码注释则都以 '#' 开头。</p><p>代码解析参考链接：<a href="https://blog.csdn.net/yangyu0515/article/details/131608144">【三维重建】【深度学习】NeuS代码Pytorch实现--训练阶段代码解析(上)-CSDN博客</a></p></blockquote><h2 id="运行指令">运行指令</h2><ul><li><strong>Training without mask</strong></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python exp_runner.py --mode train --conf ./confs/womask.conf --<span class="keyword">case</span> &lt;case_name&gt;</span><br></pre></td></tr></table></figure><ul><li><strong>Training with mask</strong></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python exp_runner.py --mode train --conf ./confs/wmask.conf --<span class="keyword">case</span> &lt;case_name&gt;</span><br></pre></td></tr></table></figure><ul><li><strong>Extract surface from trained model</strong>——The corresponding mesh can be found in <code>exp/&lt;case_name&gt;/&lt;exp_name&gt;/meshes/&lt;iter_steps&gt;.ply</code>.</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python exp_runner.py --mode validate_mesh --conf &lt;config_file&gt; --<span class="keyword">case</span> &lt;case_name&gt; --is_continue <span class="comment"># use latest checkpoint</span></span><br></pre></td></tr></table></figure><ul><li><strong>View interpolation</strong>——The corresponding image set of view interpolation can be found in <code>exp/&lt;case_name&gt;/&lt;exp_name&gt;/render/</code>.</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python exp_runner.py --mode interpolate_&lt;img_idx_0&gt;_&lt;img_idx_1&gt; --conf &lt;config_file&gt; --<span class="keyword">case</span> &lt;case_name&gt; --is_continue <span class="comment"># use latest checkpoint</span></span><br></pre></td></tr></table></figure><h2 id="代码结构">代码结构</h2><details><summary>confs /配置文件 configurations</summary><p><span style="color: #800000;">thin_structure.conf    /薄结构的情况</span><br /><span style="color: #800000;">wmask.conf    /有mask</span><br /><span style="color: #800000;">womask.conf    /没有mask</span><br /><span style="color: #800000;">womask_dtu_large_roi.conf    /不知道是什么</span></p></details><details><summary>exp /输出的结果 export</summary><div><strong><span style="font-size: 14pt;">data_BlendedMVS</span></strong></div><div style="padding-left: 40px;"><strong><span style="font-size: 12pt;">bmvs_bear</span></strong></div><div style="padding-left: 80px;"><span style="color: #800000;">wmask/有mask</span></div><div style="padding-left: 120px;"><span style="color: #f1c40f;">logs/日志</span></div><div style="padding-left: 120px;"><span style="color: #f1c40f;">recording/训练记录</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">models</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">dataset.py</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">embedder.py</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">fields.py</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">renderer.py</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">config.conf</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">exp_runner.py</span></div><div style="padding-left: 80px;"><span style="color: #800000;">womask_sphere/无mask</span></div><div style="padding-left: 120px;"><span style="color: #f1c40f;">logs</span></div><div style="padding-left: 120px;"><span style="color: #f1c40f;">recording</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">models</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">dataset.py/加载数据</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">embedder.py</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">fields.py</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">renderer.py</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">config.conf</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">exp_runner.py</span></div></details><details><summary>models /模型</summary><p><span style="color: #800000;">dataset.py    /生成数据集，加载数据集</span><br /><span style="color: #800000;">embedder.py</span><br /><span style="color: #800000;">fields.py /存放了四个用于训练的网络类</span><br /><span style="color: #800000;">renderer.py</span></p></details><details><summary>preprocess_custom_data</summary><p><span style="font-size: 14pt; color: #800000;">aruco_preprocess</span><br /><span style="color: #3598db;">    calibration.cpp</span><br /><span style="color: #3598db;">    CMakeLists.txt</span><br /><span style="color: #3598db;">    gen_cameras.py</span><br /><span style="color: #3598db;">    run.sh</span><br /><span style="font-size: 14pt; color: #800000;">colmap_preprocess</span><br /><span style="color: #3598db;">    colmap_read_model.py</span><br /><span style="color: #3598db;">    colmap_wrapper.py</span><br /><span style="color: #3598db;">    gen_cameras.py</span><br /><span style="color: #3598db;">    imgs2poses.py</span><br /><span style="color: #3598db;">    pose_utils.py</span><br /><span style="font-size: 14pt; color: #800000;">static</span><br /><span style="color: #3598db;">    aruco_board<span style="color: #3598db;">.png</span><br /><span style="color: #3598db;">    interest_sparse_points.png</span><br /><span style="color: #3598db;">    raw_sparse_points.png</span><br /><span style="font-size: 14pt; color: #800000;">readme.md</span></span></p></details><details><summary>static</summary><p><span style="color: #800000;">intro_1_compressed.gif</span><br /><span style="color: #800000;">intro_2_compressed.gif</span></p></details><p>exp_runner.py /主程序：包含训练、验证、测试和预测<br> README.md /项目说明<br> requirements.txt /需要的python依赖包列表<br> public_data<br> .gitignore /告诉Git忽略哪些文件<br> LICENSE /许可类型<br></p><h2 id="主程序-exp_runner.py">主程序 exp_runner.py</h2><p>主要分为两部分：</p><ul><li><p><strong>class Runner</strong></p><p>|<span style="background:#daf5e9;">def __init__</span> /初始化函数</p><div class="line-block">  |self.Networks /同时将其加入 params_to _train(来自 /models/fields.py)</div><div class="line-block">  | |nerf_outside—NeRF</div><div class="line-block">  | |sdf_network—SDFNetwork</div><div class="line-block">  | |deviation_network—SingleVarianceNetwork</div><div class="line-block">  | |color_network—RenderingNetwork</div><div class="line-block">  |self.renderer /neus渲染器,继承自/models/renderer.py</div><div class="line-block">  | |self.nerf_outside</div><div class="line-block">  | |self.sdf_network</div><div class="line-block">  | |self.deviation_network</div><div class="line-block">  | |self.color_network</div><div class="line-block">  | |**self.conf['model.neus_renderer']</div><p>|<span style="background:#f9eda6;">def train(self)</span> /当mode为'train'时执行训练函数</p><p>|<span style="background:#daf5e9;">def get_image_perm(self)</span> /将图片打乱顺序</p><p>|<span style="background:#f9eda6;">def get_cos_anneal_ratio(self)</span></p><p>|<span style="background:#daf5e9;">def update_learning_rate(self)</span> /更新学习率</p><p>|<span style="background:#f9eda6;">def file_backup(self)</span></p><p>|<span style="background:#daf5e9;">def load_checkpoint(self, checkpoint_name)</span></p><p>|<span style="background:#f9eda6;">def save_checkpoint(self)</span> /存档中间结果</p><p>|<span style="background:#daf5e9;">def validate_image(self, idx=-1, resolution_level=-1)</span></p><p>|<span style="background:#f9eda6;">def render_novel_image</span> /渲染新图片</p><p>|<span style="background:#daf5e9;">def validate_mesh</span> /当mode为'validate_mesh'时执行</p><p>|<span style="background:#f9eda6;">def interpolate_view(self, img_idx_0, img_idx_1)</span> /当mode以'interpolate'开头时执行</p></li><li><p><strong>if __name__ == '__main__'</strong></p><p>|print开始提示信息</p><p>|log输出格式</p><p>|创建arg = parser.parse_args</p><div class="line-block">  |conf</div><div class="line-block">  |mode</div><div class="line-block">  |mcube_threshold</div><div class="line-block">  |is_continue</div><div class="line-block">  |gpu</div><div class="line-block">  |case: 样本集名称，默认值为空，需要由运行指令传入</div><p>|创建runner = Runner(args.xxx,...)，传入arg参数</p><p>|用if循环判断args.mode值以执行不同runner类的函数</p></li></ul><h2 id="配置文件夹confs">配置文件夹confs</h2><p>配置文件：主要是 general、dataset、train 和 model 四部分内容</p><ul><li><p>|<span style="background:#daf5e9;">general</span></p><p>|<span style="background:#daf5e9;">dataset</span></p><p>|<span style="background:#daf5e9;">train</span></p><p>|<span style="background:#daf5e9;">model</span></p><div class="line-block">  |nerf</div><div class="line-block">  |sdf_network</div><div class="line-block">  |variance_network</div><div class="line-block">  |rendering_network</div><div class="line-block">  |neus_renderer</div></li></ul><h2 id="models-模型文件夹">models 模型文件夹</h2><h3 id="fields.py">fields.py</h3><p>fields.py 中存放了从IDR（https://github.com/lioryariv/idr）中借鉴的两个网络 <span style="background:#daf5e9;">SDFNetwork</span> 和 <span style="background:#daf5e9;">RenderingNetwork</span> ，从nerf-pytorch（https://github.com/yenchenlin/nerf-pytorch）中借鉴的网络 <span style="background:#f9eda6;">NeRF</span> 和最后一个网络 <span style="background:#dad5e9;">SingleVarianceNetwork</span> 。</p><h3 id="renderer.py">renderer.py</h3><ul><li><p>|def extract_fields</p><p>|def extract_geometry</p><p>|def sample_pdf</p><p>|class NeuSRenderer</p></li></ul><h3 id="dataset.py">dataset.py</h3><ul><li><p>|def load_K_Rt_from_P</p><p>|class Dataset /用于加载处理数据data</p><div class="line-block">  |def __init__</div><div class="line-block">  |def gen_rays_at /Generate rays at world space from one camera</div><div class="line-block">  |def gen_random_rays_at /Generate random rays at world space from one camera</div><div class="line-block">  |def gen_rays_between /Interpolate pose between two cameras.</div><div class="line-block">  |def near_far_from_sphere</div><div class="line-block">  |def image_at</div></li></ul><h1 id="bib-citation">Bib Citation</h1><figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">@article&#123;wang2021neus,</span><br><span class="line">      title=&#123;NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction&#125;, </span><br><span class="line">      author=&#123;Peng Wang and Lingjie Liu and Yuan Liu and Christian Theobalt and Taku Komura and Wenping Wang&#125;,</span><br><span class="line">  journal=&#123;NeurIPS&#125;,</span><br><span class="line">      year=&#123;2021&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="参考文献">参考文献</h1><p><strong>[1] Wang P, Liu L, Liu Y, et al. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction[J]. arixv preprint arixv:2106.10689, 2021.</strong></p><h2 id="参考网址">参考网址：</h2><ul><li><p>https://www.jianguoyun.com/p/DWJ0GRYQx_2EDBjAi6kFIAA</p></li><li><p><a href="https://blog.csdn.net/pylittlebrat/article/details/127503069">Neus学习笔记-CSDN博客</a></p></li><li><p><a href="https://longtimenohack.com/posts/paper_reading/2021_wang_neus/">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction - Jianfei Guo (longtimenohack.com)</a></p></li><li><p><a href="https://blog.csdn.net/flow_specter/article/details/126222914">论文笔记：NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction_neus 论文解读-CSDN博客</a></p></li><li><p>[<a href="https://zhuanlan.zhihu.com/p/496752239">论文笔记]NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction - 知乎 (zhihu.com)</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeurIPS 2021 </tag>
            
            <tag> volume rendering </tag>
            
            <tag> neural surface reconstruction </tag>
            
            <tag> SDF </tag>
            
            <tag> 三维重建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>岂知辗转复相见，阖应辨得故友来</title>
      <link href="/2024/02/15/%E5%B8%B8%E8%A7%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Pytorch%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E8%A7%84%E5%BE%8B/"/>
      <url>/2024/02/15/%E5%B8%B8%E8%A7%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Pytorch%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E8%A7%84%E5%BE%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>常见Pytorch代码模型结构规律总结</p><p>参考链接：<a href="https://www.zhihu.com/question/406133826">一个完整的Pytorch深度学习项目代码，项目结构是怎样的？ - 知乎 (zhihu.com)</a></p></blockquote><h1 id="常见模型结构">常见模型结构</h1><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">|--project<span class="emphasis">_name/</span></span><br><span class="line"><span class="emphasis">|    |--data/ # 数据</span></span><br><span class="line"><span class="emphasis">|    |--datasets/ # 生成数据集，加载处理数据集</span></span><br><span class="line"><span class="emphasis">|    |    |--data_</span>loader.py</span><br><span class="line">|    |--models/ # 模型的综合训练、测试、验证、预测等</span><br><span class="line">|    |    |--model.py</span><br><span class="line">|    |--configs/ # 配置文件，例如模型参数、优化器参数、训练和测试参数等</span><br><span class="line">|    |    |--config.py</span><br><span class="line">|    |--model<span class="emphasis">_hub/ # 预训练模型权重</span></span><br><span class="line"><span class="emphasis">|    |--utils/ # 大杂烩辅助模块，可以是日志、评价指标等常用接口文件</span></span><br><span class="line"><span class="emphasis">|    |    |--utils.py</span></span><br><span class="line"><span class="emphasis">|    |    |--metrics.py</span></span><br><span class="line"><span class="emphasis">|    |    |--visualization.py</span></span><br><span class="line"><span class="emphasis">|    |--tools/ # 训练，评估，预测的脚本</span></span><br><span class="line"><span class="emphasis">|    |    |--train.py</span></span><br><span class="line"><span class="emphasis">|    |    |--test.py</span></span><br><span class="line"><span class="emphasis">|    |    |--eval.py</span></span><br><span class="line"><span class="emphasis">|    |    |--predict.py</span></span><br><span class="line"><span class="emphasis">|    |--outputs/ # 输出的结果</span></span><br><span class="line"><span class="emphasis">|    |    |--checkpoints/ # 训练好的模型文件</span></span><br><span class="line"><span class="emphasis">|    |    |--logs/ # 日志</span></span><br><span class="line"><span class="emphasis">|    |    |--images/ # 可视化的结果图片</span></span><br><span class="line"><span class="emphasis">|    |--requirements.py # 依赖包列表</span></span><br><span class="line"><span class="emphasis">|    |--README.md # 项目说明</span></span><br><span class="line"><span class="emphasis">|    |--.gitignore #告诉Git忽略哪些文件</span></span><br></pre></td></tr></table></figure><p>关于配置文件configs，可以使用YAML或JSON格式，在<code>train.py</code>或<code>test.py</code>中，可以使用<code>PyYAML</code>库或<code>json</code>库来加载这些配置文件。例如，YAML配置文件可以包括以下内容：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">model:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">model1</span></span><br><span class="line">  <span class="attr">input_size:</span> <span class="number">224</span></span><br><span class="line">  <span class="attr">output_size:</span> <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">optimizer:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">Adam</span></span><br><span class="line">  <span class="attr">lr:</span> <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line"><span class="attr">train:</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">32</span></span><br><span class="line">  <span class="attr">epochs:</span> <span class="number">100</span></span><br><span class="line">  <span class="attr">validation_split:</span> <span class="number">0.2</span></span><br></pre></td></tr></table></figure><h1 id="模型的参数可选">模型的参数(可选)</h1><p>一个深度学习网络有很多的参数可以配置，一般分成以下三类：</p><ul><li>数据集参数（文件路径、batch_size等）</li><li>训练参数（学习率、训练epoch等）</li><li>模型参数（输入的大小，输出的大小）</li></ul><p>这些参数可以写一个类保存，也可以写一个字典，然后使用json保存，这些都是需要自己去实现的，但是这些都是一些细枝末节东西，写了几次，找到一个自己最喜欢的方式就可以，不是深度学习项目中必要的部分。</p><h1 id="network-模型的定义">Network 模型的定义</h1><p>创建一个Network类，继承<code>torch.nn.Module</code>，在构造函数中用初始化成员变量为具体的网络层，在forward函数中使用成员变量搭建网络架构，模型的使用过程中pytorch会自动调用forword进行参数的前向传播，构建计算图。<strong>以下拿一个简单的CNN图像分类模型举例</strong>：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Network, self).__init__()</span><br><span class="line">        <span class="comment"># 灰度图像的channels=1即in_channels=1 输出为10个类别即out_features=10</span></span><br><span class="line">        <span class="comment"># parameter(形参)=argument(实参) 卷积核即卷积滤波器 out_channels=6即6个卷积核 输出6个feature-maps(特征映射)</span></span><br><span class="line">        <span class="comment"># 权重shape 6*1*5*5</span></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">6</span>)  <span class="comment"># 二维批归一化 输入size=6</span></span><br><span class="line">        <span class="comment"># 权重shape 12*1*5*5</span></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">12</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 全连接层：fc or dense or linear out_features即特征(一阶张量)</span></span><br><span class="line">        <span class="comment"># 权重shape 120*192</span></span><br><span class="line">        self.fc1 = nn.Linear(in_features=<span class="number">12</span>*<span class="number">4</span>*<span class="number">4</span>, out_features=<span class="number">120</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm1d(<span class="number">120</span>)  <span class="comment"># 一维批归一化 输入size=120</span></span><br><span class="line">        <span class="comment"># 权重shape 60*120</span></span><br><span class="line">        self.fc2 = nn.Linear(in_features=<span class="number">120</span>, out_features=<span class="number">60</span>)</span><br><span class="line">        <span class="comment"># 权重shape 10*60</span></span><br><span class="line">        self.out = nn.Linear(in_features=<span class="number">60</span>, out_features=<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, t</span>):</span><br><span class="line">        <span class="comment"># (1) input layer</span></span><br><span class="line">        t = t</span><br><span class="line">        <span class="comment"># (2) hidden conv layer</span></span><br><span class="line">        t = F.relu(self.conv1(t))  <span class="comment"># (28-5+0)/1+1=24 输入为b(batch_size)*1*28*28 输出为b*6*24*24 relu后shape不变</span></span><br><span class="line">        t = F.max_pool2d(t, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)  <span class="comment"># (24-2+0)/2+1=12 输出为b*6*12*12</span></span><br><span class="line">        t = self.bn1(t)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (3) hidden conv layer</span></span><br><span class="line">        t = F.relu(self.conv2(t))  <span class="comment"># (12-5+0)/1+1=8 输出为b*12*8*8 relu后shape不变</span></span><br><span class="line">        t = F.max_pool2d(t, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)  <span class="comment"># (8-2+0)/2+1=4 输出为b*12*4*4</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (4) hidden linear layer</span></span><br><span class="line">        t = F.relu(self.fc1(t.reshape(-<span class="number">1</span>, <span class="number">12</span>*<span class="number">4</span>*<span class="number">4</span>)))  <span class="comment"># t.reshape后为b*192 全连接层后输出为b*120 relu后shape不变</span></span><br><span class="line">        t = self.bn2(t)</span><br><span class="line">        <span class="comment"># (5) hidden linear layer</span></span><br><span class="line">        t = F.relu(self.fc2(t))  <span class="comment"># 全连接层后输出为b*60 relu后shape不变</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (6) output layer</span></span><br><span class="line">        t = self.out(t)  <span class="comment"># 全连接层后输出为b*10 relu后shape不变</span></span><br><span class="line">        <span class="keyword">return</span> t</span><br></pre></td></tr></table></figure><h1 id="transforms">Transforms</h1><p>数据处理可以直接使用<code>torchvision.transforms</code>下的处理函数，包括均值，随机旋转，随机裁剪等等，也可以自己实现一些pytorch中没有实现的处理函数，<strong>下面拿一个分割网络的处理函数举例</strong>，可支持同时对传入的Image和GroundTruth进行处理，使用时直接按照顺序构造ProcessImgAndGt即可。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ProcessImgAndGt</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, transforms</span>):</span><br><span class="line">            self.transforms = transforms</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img, label</span>):</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> self.transforms:</span><br><span class="line">            img, label = t(img, label)</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Resize</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, height, width</span>):</span><br><span class="line">        self.height = height</span><br><span class="line">        self.width = width</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img, label</span>):</span><br><span class="line">        img = img.resize((self.width, self.height), Image.BILINEAR)</span><br><span class="line">        label = label.resize((self.width, self.height), Image.NEAREST)</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Normalize</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, mean, std</span>):</span><br><span class="line">        self.mean, self.std = mean, std</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img, label</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            img[:, :, i] -= <span class="built_in">float</span>(self.mean[i])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            img[:, :, i] /= <span class="built_in">float</span>(self.std[i])</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ToTensor</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.to_tensor = torchvision.transforms.ToTensor()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img, label</span>):</span><br><span class="line">        img, label = self.to_tensor(img), self.to_tensor(label).long()</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">transforms = ProcessImgAndGt([</span><br><span class="line">    Resize(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">    Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>]),</span><br><span class="line">    ToTensor()</span><br><span class="line">])</span><br></pre></td></tr></table></figure><h1 id="dataset-数据处理和加载">Dataset 数据处理和加载</h1><p>一般需要我们自己定义：创建一个数据集类，继承torch.utils.data.Dataset，只需重写__init__构造函数，__getitem__迭代器遍历函数以及__len__函数。</p><ul><li>在__init__函数中读取传入的数据集路径下的指定数据文件，<strong>还是拿一个分割网络的dataset流程举例</strong>，其他分类分类模型可以直接将GroundTruth替换为对应label即可，将拼接处理好的图片文件路径和GroundTruth文件路径作为元组存入一个为列表的成员变量file_list中；</li><li>在__getitem__中根据传入的索引从file_list取对应的元素，并且通过Transforms进行处理；</li><li>在__len__中返回len(self.file_list)即可。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset_path, transforms</span>):</span><br><span class="line">        <span class="built_in">super</span>(TrainDataset, self).__init()</span><br><span class="line">        self.dataset_path = dataset_path</span><br><span class="line">        self.transforms = transforms</span><br><span class="line">        <span class="comment"># 根据具体的业务逻辑读取全部数据路径作为加载数据的索引</span></span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">dir</span> <span class="keyword">in</span> os.listdir(dataset_path):</span><br><span class="line">            image_dir = os.path.join(dataset_path, <span class="built_in">dir</span>)</span><br><span class="line">            gt_path = image_dir + <span class="string">&#x27;/GT/&#x27;</span></span><br><span class="line">            img_path = image_dir + <span class="string">&#x27;/Frame/&#x27;</span></span><br><span class="line">            img_list = []</span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> os.listdir(img_path):</span><br><span class="line">                <span class="keyword">if</span> name.endswith(<span class="string">&#x27;.png&#x27;</span>):</span><br><span class="line">                    img_list.append(name)</span><br><span class="line">            self.file_list.extend([(img_path + name, gt_path + name) <span class="keyword">for</span> name <span class="keyword">in</span> img_list])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):   </span><br><span class="line">        img_path, label_path = self.file_list[idx]</span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_path).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">        label = Image.<span class="built_in">open</span>(label_path).convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">        img, label = self.transforms(img, label)</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.file_list)</span><br></pre></td></tr></table></figure><h1 id="train-and-validate-模型的训练和运行">Train and Validate 模型的训练和运行</h1><p><span style="background:#daf5e9;">训练模型一般分成以下几个步骤:</span></p><p>定义网络 定义数据 定义损失函数和优化器 开始训练 <strong>训练网络</strong> <u>将梯度置为0</u> <u>求loss</u> <u>反向传播</u> <u>更新参数</u> <u>更新优化器的学习率（可选）</u> <strong>测试网络</strong> <strong>可视化处理各种指标</strong> <strong>计算在验证集上的指标 （可选）</strong></p><h2 id="optimizer">Optimizer</h2><p>选择优化器进行模型参数更新，要创建优化器必须给它一个可进行迭代优化的包含了全部参数的列表 然后可以指定针对这些参数的学习率（learning_rate），权重衰减（weight_decay），momentum等，</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters(), lr = <span class="number">0.0001</span>)</span><br></pre></td></tr></table></figure><p>或者是可以指定针对哪些参数执行不一样的优化策略，根据不同层的name对不同层使用不同的优化策略。列表中的每一项都可以是一个dict，dict中params对应当前项的参数列表，可以对当前项指定学习率或者是衰减策略。对base_params使用的1e-4的学习率，对finetune_params使用1e-3的学习率，对两者一起使用1e-4的权重衰减</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">base_params = [params <span class="keyword">for</span> name, params <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> (<span class="string">&quot;xxx&quot;</span> <span class="keyword">in</span> name)]</span><br><span class="line">finetune_params = [params <span class="keyword">for</span> name, params <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> (<span class="string">&quot;yyy&quot;</span> <span class="keyword">in</span> name)]</span><br><span class="line">optimizer = optim.Adam([</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: base_params&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: finetune_params, <span class="string">&quot;lr&quot;</span>: <span class="number">1e-3</span>&#125;</span><br><span class="line">], lr=<span class="number">1e-4</span>, weight_decay=<span class="number">1e-4</span>);</span><br></pre></td></tr></table></figure><h2 id="run">Run</h2><p>基础组件都写好了，剩下的就是组成一个完整的模型结构。</p><ol type="1"><li>实例化模型对象，并将其加载到GPU中</li><li>根据需要构建数据预处理对象，传入数据集对象中进行读取数据时的数据处理</li><li>构建训练和测试的数据集对象，并将其传入torch.utils.data.DataLoader，指定batch_size（训练或测试是每次读取多少条数据）、shuffle（读取数据时是否打乱）、num_workers（开启多少线程进行数据加载，为0时(不推荐)用主线程在训练模型时进行数据加载）等参数</li><li>使用torch.optim.Adam构建优化器对象，这里根据不同层的name对不同层使用不同的优化策略</li><li>训练20个epoch，并且每5个epoch在测试集上跑一遍，这里只计算了损失，对于其他评价指标直接计算即可</li><li>根据条件对指定epoch的模型进行保存</li></ol><ul><li><strong>optimizer.zero_grad() # pytorch会积累梯度，在优化每个batch的权重的梯度之前将之前计算出的每个权重的梯度置0</strong><br /></li><li><strong>loss.backward() # 在最后一个张量上调用反向传播方法，在计算图中计算权重的梯度</strong><br /></li><li><strong>optimizer.step() # 使用预先设置的学习率等参数根据当前梯度对权重进行更</strong></li><li><strong>model.train()</strong> # <strong>保证BN层能够继续计算数据的均值和方差并进行更新，保证dropout层会按照设定的参数设置保留激活单元的概率（保留概率=p）</strong></li><li><strong>model.eval()</strong> # <strong>BN层会停止计算均值和方差，直接使用训练时的参数，dropout层利用了训练好的全部网络连接，不随机舍弃激活单元</strong></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Network().cuda()</span><br><span class="line"><span class="comment"># 构建数据预处理</span></span><br><span class="line">transforms = ProcessImgAndGt([</span><br><span class="line">    Resize(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">    Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>]),</span><br><span class="line">    ToTensor()</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 构建Dataset</span></span><br><span class="line">train_dataset = MyDataset(train_dataset_path, transforms)</span><br><span class="line"><span class="comment"># DataLoader</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset,</span><br><span class="line">                                                   batch_size=<span class="number">12</span>,</span><br><span class="line">                                                   shuffle=<span class="literal">True</span>,</span><br><span class="line">                                                   num_workers=<span class="number">4</span>,</span><br><span class="line">                                                   pin_memory=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># TestDataset</span></span><br><span class="line">test_dataset = MyDataset(test_dataset_path, transforms)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset,</span><br><span class="line">                                                  batch_size=<span class="number">4</span>,</span><br><span class="line">                                                  shuffle=<span class="literal">True</span>,</span><br><span class="line">                                                  num_workers=<span class="number">2</span>,</span><br><span class="line">                                                  pin_memory=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># optimizer需要传入全部需要更新的参数名称，这里是对不用的参数执行不同的更新策略 </span></span><br><span class="line">base_params = [params <span class="keyword">for</span> name, params <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> (<span class="string">&quot;xxx&quot;</span> <span class="keyword">in</span> name)]</span><br><span class="line">finetune_params = [params <span class="keyword">for</span> name, params <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> (<span class="string">&quot;yyy&quot;</span> <span class="keyword">in</span> name)]</span><br><span class="line">optimizer = torch.optim.Adam([</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: base_params, <span class="string">&quot;lr&quot;</span>: <span class="number">1e-3</span>, ...&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: finetune_params, <span class="string">&quot;lr&quot;</span>: <span class="number">1e-4</span>, ...&#125;</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> trian_loader:</span><br><span class="line">        images. gts = batch[<span class="number">0</span>].cuda(), batch[<span class="number">1</span>].cuda()</span><br><span class="line">        preds = model(iamges)</span><br><span class="line">        loss = F.cross_entropy(preds, gts)</span><br><span class="line">        optimizer.zero_grad()    <span class="comment"># pytorch会积累梯度，在优化每个batch的权重的梯度之前将之前计算出的每个权重的梯度置0</span></span><br><span class="line">        loss.backward()          <span class="comment"># 在最后一个张量上调用反向传播方法，在计算图中计算权重的梯度 </span></span><br><span class="line">        optimizer.step()         <span class="comment"># 使用预先设置的学习率等参数根据当前梯度对权重进行更新</span></span><br><span class="line">        epoch_loss += loss * trian_loader.batch_size</span><br><span class="line">        <span class="comment"># 计算其他标准</span></span><br><span class="line">    loss = epoch_loss / <span class="built_in">len</span>(train_loader.dataset)</span><br><span class="line">    <span class="comment"># .......</span></span><br><span class="line">    <span class="comment"># 每隔几个epoch在测试集上跑一下</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        test_epoch_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> test_batch <span class="keyword">in</span> test_loader:</span><br><span class="line">            test_images. test_gts = test_batch[<span class="number">0</span>].cuda(), test_batch[<span class="number">1</span>].cuda()</span><br><span class="line">            test_preds = model(test_iamges)</span><br><span class="line">            loss = F.cross_entropy(test_preds, test_gts)</span><br><span class="line">            test_epoch_loss += loss * test_loader.batch_size</span><br><span class="line">            <span class="comment"># 计算其他标准</span></span><br><span class="line">        test_loss = test_epoch_loss / (<span class="built_in">len</span>(test_loader.dataset))</span><br><span class="line">    <span class="comment"># .......</span></span><br><span class="line">    <span class="comment"># 根据条件对指定epoch的模型进行保存 将模型序列化到磁盘的pickle包</span></span><br><span class="line">    <span class="keyword">if</span> 精度最高:</span><br><span class="line">        torch.save(model.stat_dict(), <span class="string">f&#x27;<span class="subst">&#123;model_path&#125;</span>_<span class="subst">&#123;time_index&#125;</span>.pth&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="test">Test</h2><p>实际使用时需要将训练好的模型上在输入数据上运行，这里以测试集的数据为例，实际情况下只需要初始化模型之后将视频流中的图像帧作为模型的输入即可。</p><h3 id="torch.no_grad"><strong>torch.no_grad()</strong></h3><p>停止autograd模块的工作，不计算和储存梯度，一般在用训练好的模型跑测试集时使用，因为测试集时不需要计算梯度更不会更新梯度。使用后可以加速计算时间，节约gpu的显存</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_dataset = MyDataset(test_dataset_path, transforms)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset,</span><br><span class="line">                                                       batch_size=<span class="number">1</span>,</span><br><span class="line">                                                       shuffle=<span class="literal">False</span>,</span><br><span class="line">                                                       num_workers=<span class="number">2</span>)</span><br><span class="line">model = Network().cuda()</span><br><span class="line"><span class="comment"># 对磁盘上的pickle文件进行解包 将gpu训练的模型加载到cpu上</span></span><br><span class="line">model.load_stat_dict(torch.load(model_path, map_location=torch.device(<span class="string">&#x27;cpu&#x27;</span>)));</span><br><span class="line">mocel.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> test_loader:</span><br><span class="line">        test_images. test_gts = test_batch[<span class="number">0</span>].cuda(), test_batch[<span class="number">1</span>].cuda()</span><br><span class="line">        test_preds = model(test_iamges)</span><br><span class="line">        <span class="comment"># 保存模型输出的图片</span></span><br></pre></td></tr></table></figure><h1 id="补充-deepvacpytorch工程化规范项目">补充: DeepVAC——Pytorch工程化规范项目</h1><blockquote><p>Github源码地址：<a href="https://github.com/deepvac/deepvac">DeepVAC/deepvac: PyTorch Project Specification. (github.com)</a></p></blockquote><h1 id="其他补充">其他补充</h1><blockquote><p><a href="https://zhuanlan.zhihu.com/p/409662511">Pytorch实验代码的亿些小细节 - 知乎 (zhihu.com)</a></p><p>一个<strong>Pytorch框架包</strong>，只要按函数划分往里面填东西就行，还可以自动支持单机多卡，多机多卡的训练：<a href="https://github.com/Lightning-AI/pytorch-lightning?tab=readme-ov-file">Lightning-AI/pytorch-lightning: Pretrain, finetune and deploy AI models on multiple GPUs, TPUs with zero code changes. (github.com)</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CBAM注意力机制</title>
      <link href="/2024/01/23/CBAM%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
      <url>/2024/01/23/CBAM%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文：《CBAM：Convolutional Block Attention Module》</p><p>论文参考样式：Woo S, Park J, Lee J Y, et al. Cbam: Convolutional block attention module[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 3-19.</p><p>论文链接：<a href="https://arxiv.org/pdf/1807.06521.pdf">CBAM：Convolutional Block Attention Module</a></p><p>demo: <a href="https://github.com/Jongchan/attention-module">GitHub - Jongchan/attention-module: Official PyTorch code for "BAM: Bottleneck Attention Module (BMVC2018)" and "CBAM: Convolutional Block Attention Module (ECCV2018)"</a></p></blockquote><blockquote><p>参考网址：</p><p><a href="https://zhuanlan.zhihu.com/p/101590167">CBAM：卷积注意力机制模块 - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/Roaddd/article/details/114646354">【注意力机制】CBAM详解（文末附代码）_cbam注意力-CSDN博客</a></p><p><a href="https://blog.csdn.net/m0_45447650/article/details/123983483">CBAM——即插即用的注意力模块（附代码）_cbam模块-CSDN博客</a></p></blockquote><h1 id="摘要">摘要</h1><p>本文（2018，ECCV）提出了卷积注意力模块——CBAM，这是一种用于前馈卷积神经网络的轻量级的注意力模块。 给定一个中间特征图，CBAM模块会沿着两个独立的维度（通道和空间）依次推断注意力图，然后将注意力图与输入特征图相乘以进行自适应特征优化。 由于CBAM是轻量级的通用模块，因此可以忽略的该模块的开销而将其无缝集成到任何CNN架构中，并且可以与基础CNN一起进行端到端训练。</p><p>论文在 ResNet 和 MobileNet 等经典结构上添加了 CBAM 模块并进行对比分析实验，同时也进行了CAM可视化，发现 CBAM 更关注识别目标物体，这也使得 CBAM 具有更好的解释性。本文验证所用的数据集有 ImageNet-1K，MS COCO检测和VOC 2007检测数据集。 实验表明，使用该模块在各种模型上，并在分类和检测性能方面的持续改进，证明了CBAM的广泛适用性。</p><blockquote><p>关于CAM可视化：<a href="https://aistudio.baidu.com/projectdetail/1655497">一文搞懂卷积网络之五（注意力可视化Grad-CAM） - 飞桨AI Studio星河社区 (baidu.com)</a></p></blockquote><h1 id="模型">模型</h1><p>CBAM模型结构如下所示：</p><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240109153927099.png" alt="image-20240109153927099" /><figcaption aria-hidden="true">image-20240109153927099</figcaption></figure><p>Convolutional Block Attention Module (CBAM) 表示卷积模块的注意力机制模块，可以看到 CBAM 包含2个独立的子模块：<font color=#4eb434>通道注意力模块（CAM，Channel Attention Module)</font> 和<font color=#985fff>空间注意力模块（SAM，Spartial Attention Module)</font> ，分别进行通道和空间的Attention。这样不只能够节约参数和计算力，并且保证了其能够做为即插即用的模块集成到现有的网络架构中去。相比于SENet 只关注<font color=#4eb434>通道（channel）</font>的注意力机制可以取得更好的效果。</p><blockquote><p>通道上的 Attention 机制在 2017 年的 SENet 就被提出，SENet可以参考<a href="https://blog.csdn.net/Roaddd/article/details/111357490">这篇文章</a>。事实上，CAM 与 SENet 相比，只是多了一个并行的 Max Pooling 层，提取到的高层特征更全面，更丰富。至于为何如此更改，论文也给出了解释和实验数据支持。</p></blockquote><p>由上图，CBAM由四个模块组成，分别是：输入特征、通道注意力模块、空间注意力模块和输出的精制特征图。整体流程大致描述如下</p><ol type="1"><li>输入特征<span class="math inline">\(F\in R^{C\times H\times W}\)</span>会先通过一个通道注意力模块，进行一维卷积<span class="math inline">\(M_c\in R^{C\times 1\times 1}\)</span>，将卷积结果乘原特征图<span class="math inline">\(F\)</span>得到加权结果<span class="math inline">\(F&#39;\)</span>。</li><li><span class="math inline">\(F&#39;\)</span>会再经过一个空间注意力模块，进行二维卷积<span class="math inline">\(M_s\in R^{1\times H\times W}\)</span>，再次将卷积结果与<span class="math inline">\(F&#39;\)</span>相乘，最终得到加权结果<span class="math inline">\(F&#39;&#39;\)</span>。</li></ol><p>用公式表示为 <span class="math display">\[F&#39;=M_c\left( F \right) \otimes F,\]</span> <span class="math display">\[F&#39;&#39;=M_s\left( F&#39; \right) \otimes F&#39;,\]</span></p><h2 id="channel-attention-modulecam">Channel Attention Module（CAM）</h2><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240109155441087.png" alt="image-20240109155441087" /><figcaption aria-hidden="true">image-20240109155441087</figcaption></figure><blockquote><p><strong>通道注意力模块</strong>：<strong>通道维度不变，压缩空间维度</strong>。该模块关注输入图片中<strong>有意义的信息</strong>(分类任务就关注因为什么分成了不同类别)。</p></blockquote><p>通道注意力模块CAM如上图所示。</p><ol type="1"><li>将输入的特征图feature map <span style="background:#dad5e9;">F</span>，分别经过并行的基于width和height的global max pooling 和global average pooling，将特征图从C×H×W变为C×1×1的大小，然后分别经过MLP。</li><li>在MLP中，它先将通道数压缩为原来的1/r（Reduction，减少率）倍，再扩张到原通道数，经过ReLU激活函数得到两个激活后的结果。</li><li>将MLP输出的两个结果进行基于element-wise的加和操作（即逐元素相加），再经过sigmoid激活操作，生成最终的channel attention feature-map <span style="background:#dad5e9;">M_c</span>。</li><li>将该channel attention feature-map和原来的input feature-map做element-wise乘法操作（乘完后变回C×H×W的大小），生成Spatial attention模块需要的输入特征 <span style="background:#dad5e9;">F'</span>。</li></ol><p>以上是通道注意力机制的步骤。</p><p>换一个角度考虑，通道注意力机制（Channel Attention Module）是将特征图在空间维度上进行压缩，得到一个一维矢量后再进行操作。在空间维度上进行压缩时，不仅考虑到了平均值池化（Average Pooling）还考虑了最大值池化（Max Pooling）。平均池化和最大池化可用来聚合特征映射的空间信息，送到一个共享网络，压缩输入特征图的空间维数，逐元素求和合并，以产生通道注意力图。单就一张图来说，通道注意力，关注的是这张图上哪些内容是有重要作用的。<u>平均值池化对特征图上的每一个像素点都有反馈，而最大值池化在进行梯度反向传播计算时，只有特征图中响应最大的地方有梯度的反馈。</u>通道注意力机制可以表达为： <span class="math display">\[\begin{aligned}    M_c\left( F \right) &amp;=\sigma \left( MLP\left( AvgPool\left( F \right) \right) +MLP\left( MaxPool\left( F \right) \right) \right)\\    &amp;=\sigma \left( W_1\left( W_0\left( F_{avg}^{c} \right) \right) +W_1\left( W_0\left( F_{\max}^{c} \right) \right) \right)\\\end{aligned}\]</span></p><blockquote><p>在channel attention中，表1对于pooling的使用进行了实验对比，发现avg &amp; max的并行池化的效果要更好。这里也有可能是池化丢失的信息太多，avg&amp;max的并行连接方式比单一的池化丢失的信息更少，所以效果会更好一点。</p></blockquote><h2 id="spatial-attention-modulesam">Spatial Attention Module（SAM）</h2><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240109163244599.png" alt="image-20240109163244599" /><figcaption aria-hidden="true">image-20240109163244599</figcaption></figure><blockquote><p><strong>空间注意力模块</strong>：<strong>空间维度不变，压缩通道维度</strong>。该模块关注的是<strong>目标的位置信息</strong>。</p></blockquote><p>空间注意力模块如上图所示。将Channel attention模块输出的特征图 <span style="background:#dad5e9;">F‘</span> 作为本模块的输入特征图。首先做一个基于channel的global max pooling 和global average pooling，得到两个1×H×W 的特征图，然后将这2个特征图基于channel 做concat操作（通道拼接）。然后经过一个7×7卷积操作（7×7比3×3效果要好），降维为1个channel的特征图，即1×H×W。再经过sigmoid生成spatial attention feature <span style="background:#dad5e9;">M_s</span>。最后将该feature和该模块的输入feature做乘法（乘之后变回C×H×W的大小），得到最终生成的特征 <span style="background:#dad5e9;">F’‘</span>。</p><p>同样，空间注意力机制（Spatial Attention Module）是对通道进行压缩，在通道维度分别进行了平均值池化和最大值池化。MaxPool的操作就是在通道上提取最大值，提取的次数是高乘以宽；AvgPool的操作就是在通道上提取平均值，提取的次数也是是高乘以宽；接着将前面所提取到的特征图（通道数都为1）合并得到一个2通道的特征图。空间注意力机制可以表达为： <span class="math display">\[\begin{aligned}    M_s\left( F \right) &amp;=\sigma \left( f^{7\times 7}\left( \left[ AvgPool\left( F \right) ;MaxPool\left( F \right) \right] \right) \right)\\    &amp;=\sigma \left( f^{7\times 7}\left( \left[ F_{avg}^{s};F_{\max}^{s} \right] \right) \right)\\\end{aligned}\]</span> 其中，σ 为sigmoid操作，7×7表示卷积核的大小，7×7的卷积核比3×3的卷积核效果更好。</p><h1 id="实验">实验</h1><p>本文中，进行了较多的对比实验，旨在验证注意力模块的积极作用。</p><p>首先，对比了通道、空间以及通道&amp;空间，不同注意力机制的效果。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240109164403936.png"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">仅使用Channel Attention，对比的是使用AvgPool、MaxPool以及都使用时的性能</div></center><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240109170246533.png"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">仅使用Spatial Attention，对比的是不同Avg，Max以及kernel_size的性能差异</div></center><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240109170551461.png"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">CBAM模块，对比的是不同顺序的性能差异</div></center><p>关于图3，通道注意力和空间注意力这两个模块能够以并行或者串行顺序的方式组合在一块儿，关于通道和空间上的串行顺序和并行作者进行了实验对比，可以看到的是，先使用Channel（AvgPool&amp;MaxPool），再使用Spatial（avg&amp;max，k=7）——即先通道后空间的性能是最优的。</p><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240109170744084.png" alt="image-20240109170744084" /><figcaption aria-hidden="true">image-20240109170744084</figcaption></figure><p>上图给出了ImageNet-1K数据集上，训练误差的曲线。同样的证明了，CBAM模块在训练集合验证集上，相比于Baseline和SE注意力机制，都有一定的提升。</p><p>最后，是使用Grad-CAM进行了可视化，以来证明CBAM是真正地提取出了积极有效的特征：利用 Grad-CAM 对不一样的网络进行可视化后，能够发现，引入 CBAM 后，特征覆盖到了待识别物体的更多部位，而且最终判别物体的几率也更高，这代表注意力机制的确让网络学会了关注重点信息。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240109171005799.png"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">CBAM，SE，Baseline等最后一层卷积层输出使用 grad-CAM 进行可视化的对比图</div></center><h1 id="总结">总结</h1><ol type="1"><li><p>论文提出了一种基于注意力机制的轻量型结构 CBAM ，基本上可以添加到所有常规的卷积层中。</p></li><li><p>文中验证了 Channel Attention Module 中 avg 与 max 并行的方式最好，接下来通过实验验证了 Channel Attention Module 和 Spatial Attention Module 的最佳先后顺序是先通道后空间，还对比了 CBAM 与 SENet 的性能。</p></li><li><p>文章还在实验中应用grad-CAM可视化了 CBAM 的关注区域（在图像分类任务中可以观察feature map的特征，解释了为什么模型将原图分类到某一类的结果），使得 CBAM 具有更好的解释性。</p></li><li><p>加入CBAM模块不一定会给网络带来性能上的提升，受自身网络还有数据等其他因素影响，甚至会下降。如果网络模型的泛化能力已经很强，而你的数据集不是benchmarks而是自己采集的数据集的话，不建议加入CBAM模块。CBAM性能虽然改进的比SE高了不少，但绝不是无脑加入到网络里就能有提升的。也要根据自己的数据、网络等因素综合考量。</p></li></ol><h1 id="代码">代码</h1>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初步认识 torch.nn</title>
      <link href="/2024/01/23/%E8%AE%A4%E8%AF%86%20torch.nn/"/>
      <url>/2024/01/23/%E8%AE%A4%E8%AF%86%20torch.nn/</url>
      
        <content type="html"><![CDATA[<blockquote><p>搬运链接：[<a href="https://blog.csdn.net/HiWangWenBing/article/details/120614234">Pytorch系列-30]：神经网络基础 - torch.nn库五大基本功能：nn.Parameter、nn.Linear、nn.functioinal、nn.Module、nn.Sequentia-CSDN博客</a></p><p>官方链接1：<a href="https://pytorch.org/docs/1.2.0/">PyTorch documentation — PyTorch master documentation</a></p><p>官方链接2：<a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#torchnn">torch.nn - PyTorch中文文档 (pytorch-cn.readthedocs.io)</a></p></blockquote><blockquote><p>nn是Neural Network的简称，帮助程序员方便执行如下的与神经网络相关的行为：创建、训练、保存和恢复神经网络。</p></blockquote><h1 id="一torch.nn简介">一、torch.nn简介</h1><h2 id="相关库的导入">相关库的导入</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#环境准备</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np              <span class="comment"># numpy数组库</span></span><br><span class="line"><span class="keyword">import</span> math                     <span class="comment"># 数学运算库</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 画图库</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> torch             <span class="comment"># torch基础库</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn    <span class="comment"># torch神经网络库</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><h1 id="二nn.linear类全连接层">二、nn.Linear类（全连接层）</h1><blockquote><p>nn.Linear本身并不包含激活函数（激活函数在Functional中)</p></blockquote><h2 id="函数说明">函数说明</h2><figure><img src="https://img-blog.csdnimg.cn/20191102164419608.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyMDc5Njg5,size_16,color_FFFFFF,t_70#pic_center" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><ul><li><strong>in_features</strong>: 输入的二维张量的大小。Y = WX + b, X的维度就是in_features，X的维度决定的W的维度， 总的参数个数 = in_features + 1。</li><li><strong>out_features：</strong>输出的二维张量的大小。</li></ul><p>从输入输出的张量的shape角度来理解，相当于一个输入为[batch_size, in_features]的张量变换成了[batch_size, out_features]的输出张量。<strong>在 nn.Linear 中，输入张量和输出张量只能是二维的</strong>，否则可以选择在 <code>nn.Linear()</code> 层前加上 <code>nn.Flatten()</code> 操作将输入张量展开为二维的（如 [4,3,2,2]-&gt;[4,12]）。</p><h2 id="使用nn.linear类创建全连接层">使用nn.Linear类创建全连接层</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nn.Linear</span></span><br><span class="line"><span class="comment"># 建立单层的多输入、多输出全连接层</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># in_features由输入张量的形状决定，out_features则决定了输出张量的形状</span></span><br><span class="line">full_connect_layer = nn.Linear(in_features=<span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>, out_features=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;full_connect_layer:&quot;</span>, full_connect_layer)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;parameters        :&quot;</span>, full_connect_layer.parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假定输入的图像形状为[32,32,3]</span></span><br><span class="line">x_input = torch.randn(<span class="number">4</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>) <span class="comment">#或 x_input = x_input.view(1, -1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将四维张量转换为二维张量之后，才能作为全连接层的输入，此处用的是 Tensor.view() 方法</span></span><br><span class="line">x_input = x_input.view(<span class="number">4</span>, <span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input.shape:&quot;</span>, x_input.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用全连接层</span></span><br><span class="line">y_output = full_connect_layer(x_input)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_output.shape:&quot;</span>, y_output.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_output:&quot;</span>, y_output)</span><br></pre></td></tr></table></figure><p>full_connect_layer: Linear(in_features=3072, out_features=2, bias=True) parameters : &lt;bound method Module.parameters of Linear(in_features=3072, out_features=2, bias=True)&gt; x_input.shape: torch.Size([4, 3072]) y_output.shape: torch.Size([4, 2]) y_output: tensor([[ 0.0727, 0.6203], [ 0.6674, 0.4605], [-0.0745, -0.4161], [-0.0399, -0.1105]], grad_fn=<AddmmBackward0>)</p><h1 id="三nn.functional常见函数">三、nn.functional（常见函数）</h1><h2 id="概述">概述</h2><p>nn.functional（通常按惯例导入到 F 命名空间中）定义了创建神经网络所需要的一些常见的处理函数。如<strong>激活函数</strong>、<strong>损失函数</strong>、<strong>正则化函数</strong>和<strong>非状态（non-stateful）版本的层（如卷积层和线性层）</strong>等。</p><figure><img src="https://img-blog.csdnimg.cn/20211005202351328.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_14,color_FFFFFF,t_70,g_se,x_16" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="激活函数">激活函数</h2><p>1.<strong>Sigmoid 型函数</strong>：Sigmoid 型函数是指一类S型曲线函数，为两端饱和函数。常用的 Sigmoid 型函数有 Logistic 函数和 Tanh 函数，其数学表达式分别为</p><ul><li>Logistic 函数：<span class="math inline">\(\sigma(z)= \frac{1}{1+\exp(-z)}\)</span></li><li>Tanh 函数：<span class="math inline">\(tanh(z)=\frac{\exp(z)-\exp(-z)}{\exp(z)+\exp(-z)}\)</span></li></ul><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240617143941116.png" alt="image-20240617143941116" style="zoom:50%;" /></p><p>在Pytorch中，可以通过调用<code>torch.sigmoid / nn.Sigmoid()</code>和<code>torch.nn.functional.tanh(z)</code>实现对张量的Logistic和Tanh计算。</p><p>2.<strong>ReLU型函数</strong>：常见的ReLU函数有ReLU（在一定程度上改善 Sigmoid 的梯度消失问题）和Leaky ReLU（可避免ReLU的“死亡ReLU”问题），数学表达式分别为</p><ul><li>ReLU 函数：<span class="math inline">\(ReLU(z)=\max(0,z)\)</span></li><li>LeakyReLU 函数：<span class="math inline">\(LeakyReLU(z)=\max(0,z)+\lambda \min(0,z)\)</span> , <span class="math inline">\(\lambda\)</span> 为超参数</li></ul><p><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240617144435576.png" alt="image-20240617144435576" style="zoom:50%;" /></p><p>在pytorch中，可以通过调用<code>torch.nn.functional.relu(z) / nn.ReLU()</code>和<code>torch.nn.functional.leaky_relu(z) / nn.LeakyReLU()</code>完成ReLU与LeakyReLU的计算。</p><p>值得注意的是，Leaky ReLU 虽然能改善“死亡 ReLU”问题，但是由于 Leaky ReLU 中，<span class="math inline">\(x&lt;0\)</span> 时的斜率默认只有0.01，所以反向传播时，随着网络层数的加深，梯度值越来越小。如果想要改善这一现象，将 Leaky ReLU 中，<span class="math inline">\(x&lt;0\)</span> 时的斜率调大即可。</p><h2 id="损失函数">损失函数</h2><p>1.nn.BCELoss() / nn.BCEWithLogitsLoss() ：常作为二分类中的损失函数</p><p>2.<strong>交叉熵损失</strong>：nn.CrossEntropyLoss()。常用于作为多分类问题中的损失函数。如果使用CrossEntropyLoss损失函数，一般最后一层不需要激活函数。</p><h2 id="优化器">优化器</h2><p>1.torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</p><h2 id="数据加载">数据加载</h2><p>1.torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, num_workers=0)</p><h1 id="四nn.xxx和nn.functional.xxx比较">四、nn.Xxx和nn.functional.xxx比较</h1><h2 id="相同点">相同点</h2><p><code>nn.Xxx</code>和<code>nn.functional.xxx</code>的实际功能是相同的，即<code>nn.Conv2d</code>和<code>nn.functional.conv2d</code> 都是进行卷积，<code>nn.Dropout</code> 和<code>nn.functional.dropout</code>都是进行dropout，。。。。。；</p><ul><li>运行效率也是近乎相同。</li></ul><h2 id="不同点">不同点</h2><ul><li>形式看：<code>nn.functional.xxx</code>是<strong>小写</strong>字母开头，nn.Xxx中的函数是<strong>大写</strong>字母开头。</li><li><code>nn.functional.xxx</code>是API函数接口，而<code>nn.Xxx</code>是对原始API函数<code>nn.functional.xxx</code>的<strong>类封装</strong>。</li><li>所有<strong><code>nn.Xxx</code>都继承于于共同祖先<code>nn.Module</code>。</strong>这一点导致<code>nn.Xxx</code>除了具有<code>nn.functional.xxx</code>功能之外，内部附带了<code>nn.Module</code>相关的属性和方法，例如<code>train(), eval(),load_state_dict, state_dict</code>等。</li><li><code>nn.Xxx</code>继承于<code>nn.Module</code>， 能够很好的与<code>nn.Sequential</code>结合使用， 而<code>nn.functional.xxx</code>无法与<code>nn.Sequential</code>结合使用。</li><li><code>nn.Xxx</code> 需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据。<code>nn.functional.xxx</code>同时传入输入数据和weight, bias等其他参数 。</li><li><code>nn.Xxx</code>不需要你自己定义和管理weight；而<code>nn.functional.xxx</code>需要你自己定义weight，每次调用的时候都需要手动传入weight, 不利于代码复用。</li></ul><h1 id="五nn.parameter类">五、nn.Parameter类</h1><h2 id="nn.parameter概述">nn.Parameter概述</h2><p><code>Parameter</code>也是<code>Tensor</code>（或者说Tensor的包装器wrapper），也就是说是一个多维矩阵，是Variable类中的一个特殊子类。它告诉 <code>Module</code> 它具有在反向传播期间需要更新的权重。 只更新具有 <code>requires_grad</code> 属性的 <code>tensor</code>。</p><p>当我们创建一个Module时，nn会自动创建相应的参数parameter，并会自动累加到模型的Parameter成员列表中。</p><h4 id="语法">语法</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.nn.parameter.Parameter(data=<span class="literal">None</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li>data (Tensor) – parameter tensor. —— 输入得是一个<code>tensor</code></li><li>requires_grad (bool, optional) – if the parameter requires gradient. See Locally disabling gradient computation for more details. <strong>Default: True</strong> —— 这个不用解释，<strong>需要注意的是<code>nn.Parameter()</code>默认有梯度</strong>。</li></ul><p><code>torch.nn.Parameter()</code>将一个不可训练的tensor转换成可以训练的类型parameter，并将这个parameter绑定到这个module里面。即在定义网络时这个tensor就是一个可以训练的参数了。使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。</p><figure><img src="https://img-blog.csdnimg.cn/20211005202810135.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_13,color_FFFFFF,t_70,g_se,x_16" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="单个全连接层中参数的个数">单个全连接层中参数的个数</h2><blockquote><p><strong>in_features的数量，决定的参数的个数 Y = WX + b, X的维度就是in_features，X的维度决定的W的维度， 总的参数个数 = in_features + 1</strong></p><p><strong>out_features的数量，决定了全连接层中神经元的个数，因为每个神经元只有一个输出。</strong></p><p>多少个输出，就需要多少个神经元 (一个 WX + b 就是一个神经元)！</p></blockquote><p>总的W参数的个数= <strong>in_features * out_features</strong></p><p>总的b参数的个数= <strong>1 * out_features</strong></p><p>总的参数（W和B）的个数= (<strong>in_features + 1) * out_features</strong></p><h2 id="使用参数创建全连接层代码例子">使用参数创建全连接层代码例子</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nn.functional.linear( )</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x_input = torch.Tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input.shape:&quot;</span>, x_input.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input      :&quot;</span>, x_input)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">Weights1 = nn.Parameter(torch.rand(<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights.shape:&quot;</span>, Weights1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights      :&quot;</span>, Weights1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">Bias1 = nn.Parameter(torch.rand(<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bias.shape:&quot;</span>, Bias1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bias      :&quot;</span>, Bias1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">Weights2 = nn.Parameter(torch.Tensor(<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights.shape:&quot;</span>, Weights2.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights      :&quot;</span>, Weights2)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nfull_connect_layer&quot;</span>)</span><br><span class="line">full_connect_layer = nn.functional.linear(x_input, Weights1)</span><br><span class="line"><span class="built_in">print</span>(full_connect_layer)</span><br></pre></td></tr></table></figure><p>x_input.shape: torch.Size([3]) x_input : tensor([1., 1., 1.])</p><p>Weights.shape: torch.Size([3]) Weights : Parameter containing: tensor([0.8948, 0.0114, 0.3688], requires_grad=True)</p><p>Bias.shape: torch.Size([1]) Bias : Parameter containing: tensor([0.8087], requires_grad=True)</p><p>Weights.shape: torch.Size([3]) Weights : Parameter containing: tensor([1.4013e-45, 0.0000e+00, 0.0000e+00], requires_grad=True)</p><p>full_connect_layer tensor(1.2750, grad_fn=<DotBackward>)</p><h1 id="六nn.mudule类">六、nn.Mudule类</h1><p>创建一个可调用的对象，其行为类似于一个函数，但也可以包含状态（例如神经网络层权重）。 它知道它包含哪些参数，并且可以将所有梯度归零，循环遍历它们更新权重等。</p><figure><img src="https://img-blog.csdnimg.cn/2021100520265815.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_14,color_FFFFFF,t_70,g_se,x_16" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h1 id="七利用nn.sequential类创建神经网络继承于nn.module类">七、利用nn.Sequential类创建神经网络（继承于nn.Module类）</h1><p>nn.Sequential是一个有序的容器，该类将按照传入构造器的顺序，依次创建相应的函数，并记录在Sequential类对象的数据结构中。同时，以神经网络模块为元素的有序字典也可以作为传入参数。</p><p>因此，Sequential可以看成是有多个函数运算对象，串联成的神经网络，其返回的是Module类型的神经网络对象。</p><h2 id="以列表的形式串联函数运算构建串行执行的神经网络">以列表的形式，串联函数运算，构建串行执行的神经网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;利用系统提供的神经网络模型类：Sequential,以参数列表的方式来实例化神经网络模型对象&quot;</span>)</span><br><span class="line"><span class="comment"># A sequential container. Modules will be added to it in the order they are passed in the constructor.</span></span><br><span class="line"><span class="comment"># Example of using Sequential</span></span><br><span class="line">model_c = nn.Sequential(nn.Linear(<span class="number">28</span> * <span class="number">28</span>, <span class="number">32</span>), </span><br><span class="line">                        nn.ReLU(), </span><br><span class="line">                        nn.Linear(<span class="number">32</span>, <span class="number">10</span>), </span><br><span class="line">                        nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">                       )</span><br><span class="line"><span class="built_in">print</span>(model_c)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n显示网络模型参数&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_c.parameters)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n定义神经网络样本输入&quot;</span>)</span><br><span class="line">x_input = torch.randn(<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line">x_input = x_input.view(x_input.size()[<span class="number">0</span>], -<span class="number">1</span>)  <span class="comment">#将输入转为二维张量</span></span><br><span class="line"><span class="built_in">print</span>(x_input.size())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用神经网络进行预测&quot;</span>)</span><br><span class="line">y_pred = model_c.forward(x_input)</span><br><span class="line"><span class="built_in">print</span>(y_pred)  <span class="comment"># torch.Size([2, 10])</span></span><br></pre></td></tr></table></figure><p>利用系统提供的神经网络模型类：Sequential,以参数列表的方式来实例化神经网络模型对象 Sequential( (0): Linear(in_features=784, out_features=32, bias=True) (1): ReLU() (2): Linear(in_features=32, out_features=10, bias=True) (3): Softmax(dim=1) )</p><p>显示网络模型参数 &lt;bound method Module.parameters of Sequential( (0): Linear(in_features=784, out_features=32, bias=True) (1): ReLU() (2): Linear(in_features=32, out_features=10, bias=True) (3): Softmax(dim=1) )&gt;</p><p>定义神经网络样本输入 torch.Size([2, 28, 28, 1]) torch.Size([2, 784])</p><p>使用神经网络进行预测 tensor([[0.0773, 0.0843, 0.1366, 0.0933, 0.1107, 0.1086, 0.0721, 0.1129, 0.1267, 0.0777], [0.0875, 0.0570, 0.1179, 0.0981, 0.1261, 0.1177, 0.1174, 0.0735, 0.0932, 0.1116]], grad_fn=<SoftmaxBackward>)</p><h2 id="以字典的形式串联函数运算构建串行执行的神经网络">以字典的形式，串联函数运算，构建串行执行的神经网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;利用系统提供的神经网络模型类：Sequential,以字典的方式来实例化神经网络模型对象&quot;</span>)</span><br><span class="line">model = nn.Sequential(OrderedDict([(<span class="string">&#x27;h1&#x27;</span>, nn.Linear(<span class="number">28</span> * <span class="number">28</span>, <span class="number">32</span>)),</span><br><span class="line">                                   (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">                                   (<span class="string">&#x27;out&#x27;</span>, nn.Linear(<span class="number">32</span>, <span class="number">10</span>)),</span><br><span class="line">                                   (<span class="string">&#x27;softmax&#x27;</span>, nn.Softmax(dim=<span class="number">1</span>))])</span><br><span class="line">                     )</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n显示网络模型参数&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model.parameters)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n定义神经网络样本输入&quot;</span>)</span><br><span class="line">x_input = torch.randn(<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用神经网络进行预测&quot;</span>)</span><br><span class="line">y_pred = model.forward(x_input.view(x_input.size()[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(y_pred)   <span class="comment"># torch.Size([2, 10])</span></span><br></pre></td></tr></table></figure><p>利用系统提供的神经网络模型类：Sequential,以字典的方式来实例化神经网络模型对象 Sequential( (h1): Linear(in_features=784, out_features=32, bias=True) (relu1): ReLU() (out): Linear(in_features=32, out_features=10, bias=True) (softmax): Softmax(dim=1) )</p><p>显示网络模型参数 &lt;bound method Module.parameters of Sequential( (h1): Linear(in_features=784, out_features=32, bias=True) (relu1): ReLU() (out): Linear(in_features=32, out_features=10, bias=True) (softmax): Softmax(dim=1) )&gt;</p><p>定义神经网络样本输入 torch.Size([2, 28, 28, 1])</p><p>使用神经网络进行预测 tensor([[0.0967, 0.1006, 0.0714, 0.0752, 0.1209, 0.1088, 0.1215, 0.1156, 0.0879, 0.1015], [0.0912, 0.1031, 0.0914, 0.0962, 0.1124, 0.0999, 0.0885, 0.1082, 0.0953, 0.1137]], grad_fn=<SoftmaxBackward>)</p><h1 id="八-自定义神经网络模型类继承于module类">八、 自定义神经网络模型类（继承于Module类）</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义网络模型：带relu的两层全连接神经网络</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;自定义新的神经网络模型的类&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#一般写法，即使用 torch.nn 的写法，该写法更为常见</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NetC</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 定义神经网络</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">        <span class="built_in">super</span>(NetC, self).__init__()  <span class="comment"># 或 nn.Module.__init__(self)</span></span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Linear(n_feature, n_hidden),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(n_hidden, n_output),</span><br><span class="line">            nn.Softmax(dim=<span class="number">1</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向运算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_input</span>):</span><br><span class="line">        out = self.model(x_input)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">#用torch.nn.functional引入激活函数的写法</span></span><br><span class="line"><span class="string">class NetC(nn.Module):</span></span><br><span class="line"><span class="string">    # 定义神经网络</span></span><br><span class="line"><span class="string">    def __init__(self, n_feature, n_hidden, n_output):</span></span><br><span class="line"><span class="string">        super(NetC, self).__init__()  # 或 nn.Module.__init__(self)</span></span><br><span class="line"><span class="string">        self.h1 = nn.Linear(n_feature, n_hidden)  #subModule: Linear</span></span><br><span class="line"><span class="string">        self.out = nn.Linear(n_hidden, n_output)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # 定义前向运算</span></span><br><span class="line"><span class="string">    def forward(self, x_input):</span></span><br><span class="line"><span class="string">        h1 = F.relu(self.h1(x_input))</span></span><br><span class="line"><span class="string">        out = F.softmax(self.out(h1),dim=1)</span></span><br><span class="line"><span class="string">        return out</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n实例化神经网络模型对象&quot;</span>)</span><br><span class="line">model = NetC(<span class="number">28</span> * <span class="number">28</span>, <span class="number">32</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n显示网络模型参数&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model.parameters)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n定义神经网络样本输入&quot;</span>)</span><br><span class="line">x_input = torch.randn(<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line"><span class="comment"># 得到的数据格式torch.Size([64, 1, 28, 28])需要转变为（64,784）</span></span><br><span class="line">x_input = x_input.view(x_input.size()[<span class="number">0</span>], -<span class="number">1</span>)  <span class="comment"># -1表示自动匹配</span></span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用神经网络进行预测&quot;</span>)</span><br><span class="line">y_pred = model.forward(x_input)</span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br></pre></td></tr></table></figure><p><strong>1.torch.nn 写法的输出</strong></p><p>自定义新的神经网络模型的类</p><p>实例化神经网络模型对象 NetC( (model): Sequential( (0): Linear(in_features=784, out_features=32, bias=True) (1): ReLU() (2): Linear(in_features=32, out_features=10, bias=True) (3): Softmax(dim=1) ) )</p><p>显示网络模型参数 &lt;bound method Module.parameters of NetC( (model): Sequential( (0): Linear(in_features=784, out_features=32, bias=True) (1): ReLU() (2): Linear(in_features=32, out_features=10, bias=True) (3): Softmax(dim=1) ) )&gt;</p><p>定义神经网络样本输入 torch.Size([2, 28, 28, 1]) torch.Size([2, 784])</p><p>使用神经网络进行预测 tensor([[0.0845, 0.1166, 0.0685, 0.0738, 0.0798, 0.0713, 0.1834, 0.0916, 0.1180, 0.1125], [0.0985, 0.0992, 0.0937, 0.0788, 0.0890, 0.1179, 0.1220, 0.0894, 0.1117, 0.1000]], grad_fn=<SoftmaxBackward0>)</p><p><strong>2.torch.nn.functional 写法的输出</strong></p><p>自定义新的神经网络模型的类</p><p>实例化神经网络模型对象 NetC( (h1): Linear(in_features=784, out_features=32, bias=True) (out): Linear(in_features=32, out_features=10, bias=True) )</p><p>显示网络模型参数 &lt;bound method Module.parameters of NetC( (h1): Linear(in_features=784, out_features=32, bias=True) (out): Linear(in_features=32, out_features=10, bias=True) )&gt;</p><p>定义神经网络样本输入 torch.Size([2, 28, 28, 1]) torch.Size([2, 784])</p><p>使用神经网络进行预测 tensor([[0.0884, 0.0733, 0.0890, 0.1088, 0.1589, 0.0944, 0.0861, 0.1191, 0.0876, 0.0944], [0.1134, 0.0963, 0.0595, 0.1051, 0.0881, 0.1059, 0.0627, 0.1023, 0.1605, 0.1063]], grad_fn=<SoftmaxBackward>)</p><h1 id="九实战mnist手写数字分类">九、实战：MNIST手写数字分类</h1><p>见：<a href="file:///C:/Users/LENOVO/Nutstore/2/这是一个文件夹/课程资料/深度学习/老师发的/4.前馈神经网络1.html">4.前馈神经网络1</a></p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对象和类 学习笔记</title>
      <link href="/2024/01/08/%E7%B1%BB%E5%92%8C%E5%AF%B9%E8%B1%A1%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2024/01/08/%E7%B1%BB%E5%92%8C%E5%AF%B9%E8%B1%A1%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<blockquote><p>部分内容参考网址：<a href="https://www.cnblogs.com/jialexu/articles/14188861.html">【Python学习】对象和类 - xujiale - 博客园 (cnblogs.com)</a></p></blockquote><h1 id="什么是面向对象编程">什么是面向对象编程</h1><h2 id="背景知识">背景知识</h2><p>20世纪70年代中期，人们开始撰写文章来介绍这种编程方法的优点。大概在同一时间， 编程语言SmallTalk（施乐研究中心）和CLU（麻省理工学院）为这种思想提供了语言上 的支持。</p><p>1967年5月20日，挪威奥斯陆计算中心的科学家Ole-Johan Dahl和Kristen Nygaard正式 发布了Simula 67语言。它被认为是最早的 面向对象程序设计 语言，首次引入类、对象、 继承、动态绑定等重要概念。因此，1983年共同获得图灵奖。</p><p>SmallTalk语言起源于施乐帕罗奥多研究中心（Xerox PARC）的一项研究项目。它被公认 为历史上第二个面向对象的程序设计语言和第一个真正的集成开发环境 (IDE) 。</p><p>但C++和Java的出现才真正使这种思想成为现实。</p><h2 id="具体概念">具体概念</h2><blockquote><p>传统的程序设计，即<strong>面向过程的编程</strong>主张将程序看作一系列函数的集合，或者直接就是一系列对计算机下达的指令。</p></blockquote><p><strong>面向对象程序设计（Object Oriented Programming, OOP）</strong>可以看作一种<strong>在程序中包含各种独立而又互相调用的对象</strong>的思想。面向对象编程的一个重要特点就是<span style="background:#f9eda6;color:red"><strong>数据封装</strong></span>。</p><p><strong>对象（Object）</strong>则指的是<strong>类（Class）</strong>的实例。 在面向对象程序编程里，计算机程序将<strong>对象</strong>作为程序的基本单元，将程序和数据封装其中，以提高软件的重用性、灵活性和扩展性。</p><p>面向对象程序设计中的每一个对象都应该能够接受数据、处理数据并将数据传达给其它对象，因此它们都可以被看作一个小型的“机器”，即<strong>对象</strong>。<u>Python里面所有的东西都是对象(<em>objects</em>)，连同一个整数也是一种对象</u>，该语法设计可以巧妙的隐藏诸多细节。面向对象程序设计推广了程序的灵活性和可维护性，并且<strong>在大型项目设计中广为应用</strong>。</p><p>面向对象不仅指一种程序设计方法。它更多意义上是一种程序开发方式。许多流行的编程语言是面向对象的: <span style="background:#fbd4d0;">Python、C++、Objective-C、Java、Swift、C#、Perl、Ruby 与 PHP</span>等。</p><p><strong>Python支持面向过程、面向对象等编程方式。</strong>Python不强制使用任何一种编程方式，可以使用面向过程方式编写任何程序，但在中大型项目中，面向对象会带来很多优势。</p><h3 id="一个栗子">一个栗子</h3><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231218105617559.png" alt="image-20231218105617559" /><figcaption aria-hidden="true">image-20231218105617559</figcaption></figure><h1 id="什么是对象和类">什么是对象和类</h1><h2 id="对象的概念">对象的概念</h2><p><strong>对象是封装的数据抽象：对象是一个数据和操作的封装体。</strong>封装的目的就是<u>阻止非法的访问</u>，因此<strong>对象实现了信息的隐藏</strong>，外部只能通过操作接口访 问对象数据。</p><p>对象包含<strong>属性（attribute）</strong>和<strong>方法（method）</strong>。当你要创建一个新的object时，就必须先定义一个新的类，用它来清楚规范其类别可以创造出来的对象有什么样的属性与方法。</p><p><font color=orange>对象</font>像名词，<font color=blue>方法</font>就像个动词。<font color=orange>对象</font>代表一个独立的事物，<font color=blue>方法</font>用来定义它如何与其他<font color=orange>对象</font>相互作用。与模块不同的是，你可以同时创建多个同类别的<font color=orange>对象</font>，他们之间的属性值可能各有不同。</p><blockquote><p><strong><u>抽象数据类型</u></strong>是一个由对象以及对象上的操作组成的集合，对象和操作被捆绑为一个整体，可以从程序的一个部分传递到另一个部分。</p><p>这些操作的规范定义了抽象数据类型和程序其他部分之间的<strong><u>接口</u></strong>。</p><p><u>接口定义了操作的行为，即它们做什么，但没有说明如何去做。</u>于是，接口建立了一个<strong><u>抽象边界</u></strong>，将程序的其他部分与实现类型抽象的数据结构、算法和代码隔离开来。</p></blockquote><h2 id="类的概念">类的概念</h2><p>类和变量之间存在着一定的联系，类型是模板，而变量则是具有这种模板的一个实体。<strong>类（class）是对客观世界中事物的抽象，对象是类的实例(Instance)。</strong></p><p>从本质上说，对象是一组<u>数据</u>以及<u>操作这些数据的函数</u>。之前介绍的数字、字符串、列表、 字典和函数都是Python提供的内置对象。<strong>要创建新型对象，必须先创建类</strong>。类就类似于内置数据类型，可用于创建特定类型的对象。 <u>类指定了对象将包含哪些数据和函数，还指定了对象与其他类的关系</u>。</p><blockquote><p>一个重要的OOP功能是<strong>继承</strong>：创建新类时，可让其继承父类的数据和函数。使用好继承可避免重新编写代码，还可让程序更容易理解。对象和</p></blockquote><h1 id="对象和类">对象和类</h1><h2 id="定义">定义</h2><h3 id="定义类">定义类</h3><p>Python使用<font color=red>class关键字</font>定义一个类，类名首字符一般要大写。类定义中存在一个函数定义时，被定义的函数称为<strong>方法</strong>，并与这个类相关联。这些方法有时称为类的<strong>方法属性</strong>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Leiming</span>(<span class="title class_ inherited__">继承的父类名</span>)：</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,xx</span>):</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Xinfangfa</span>(<span class="params">xx</span>):</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">XXinfangfa</span>(<span class="params">self,xx</span>):</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><h3 id="在类中定义对象">在类中定义对象</h3><p>创建对象的过程称为<strong>实例化</strong>。</p><p>当一个对象被创建之后，包含3方面的特性：对象的<strong>标识</strong>、<strong>属性</strong>和<strong>方法</strong>。对象的标识用于区分不同的对象，当对象被创建之后，该对象会获取一块存储空间，<strong>存储空间的地址</strong>即为对象的标识。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自定义一个Person()</span></span><br><span class="line"><span class="comment"># __init__为定义属性部分</span></span><br><span class="line"><span class="comment"># self为object自己</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Person</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, email</span>): <span class="comment">#类的构造函数，用来初始化对象</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.email = email</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">say</span>(<span class="params">self, tem</span>):  <span class="comment">#类中定义的函数，也成为方法</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;I am &#x27;</span> + self.name + tem</span><br><span class="line"></span><br><span class="line">hunter = Person(<span class="string">&#x27;Elmer Fudd&#x27;</span>, <span class="string">&quot;QQ@WW.tw&quot;</span>)   <span class="comment">#实例化</span></span><br><span class="line"></span><br><span class="line">Husky = Person(<span class="string">&#x27;Hsuky&#x27;</span>, <span class="string">&quot;XDD@WW.tw&quot;</span>)</span><br><span class="line">Husky.ff = “oo”  <span class="comment"># 定义新对象后可自定义不在类中的新属性（属性的动态添加）</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(Husky.ff)</span><br><span class="line"><span class="comment"># print(hunter.ff)  报错&#x27;Person&#x27; object has no attribute &#x27;ff&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(hunter.name)</span><br><span class="line"><span class="built_in">print</span>(hunter.email)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(hunter.say(<span class="string">&#x27;!!!&#x27;</span>))  <span class="comment">#方法引用：通过点标记法，访问与类关联的方法</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(Husky.name)</span><br><span class="line"><span class="built_in">print</span>(Husky.email)</span><br></pre></td></tr></table></figure><p>oo Elmer Fudd QQ@WW.tw I am Elmer Fudd!!! Hsuky XDD@WW.tw</p><p>值得注意的是，当在类中定义函数时，若函数的第一个参数不是self，则该函数方法只能被类所使用。<span style="background:#fbd4d0;">因此在类中定义一般对象使用的方法时（<strong>除了静态方法外</strong>），第一个参数必须写<strong>self</strong>！</span>下面是个栗子</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">test</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sayhi</span>():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Hi&quot;</span>)</span><br><span class="line"></span><br><span class="line">gg = test()</span><br><span class="line"><span class="comment">#gg.sayhi() #报错 test.sayhi() takes 0 positional arguments but 1 was given</span></span><br><span class="line">test.sayhi() <span class="comment">#Hi</span></span><br></pre></td></tr></table></figure><h2 id="类的属性">类的属性</h2><p>Python并没有真正的私有化支持，但可用下划线得到伪私有。</p><h3 id="protected类型">protected类型</h3><p><font color=#ef042a>"单下划线 "</font> 开始的成员变量叫做保护变量，意思是只有<font color=#df8400><strong>类实例</strong></font>和<font color=#df8400><strong>子类实例</strong></font>能访问到这些变量， 需通过类提供的接口进行访问；不能用'from module import *'导入</p><h3 id="private类型">private类型</h3><p>在Python中，若<u>不希望类中的属性在类外被直接访问</u>，可以将实例的变量名改为<font color=#ef042a>以__（双下划线）开头</font>，就变成了一个私有类型（<font color=#ef042a>private</font>），只有内部可以访问，外部不能访问。</p><p>双下划线开头的类型是不是一定不能从外部访问呢？其实也不是。不能直接访问__xxx是因为Python解释器对外把__xxx变量/方法改成了_Class__xxx，所以，仍然可以通过<font color=#ef042a><strong>instance._Class__xxx</strong></font>来访问__xxx变量/方法。Python提供了直接访问私有属性的方式，可用于程序的测试和调试。当不知道类名时，只有<font color=#df8400><strong>类对象自己内部</strong></font>能访问，连子类对象也不能访问到这个数据。</p><h4 id="使用属性对特性进行访问和设置">使用属性对特性进行访问和设置</h4><p>在其他语言中，可以设置getter 和 setter來确保私有属性的读写。但是在python一切都是公开的，可以通过property()来达到python风格的写法，即可將属性值藏起來，不用通过调用每个getter()和setter()来达到改变私有变量。</p><p>若沒有给定setter函数，则无法通过property()来改变属性值，当然前提是在別人不知道实际存储变量的属性名称是什么。</p><blockquote><p>关于property()的用法请看这里：<a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017502538658208">使用@property - 廖雪峰的官方网站 (liaoxuefeng.com)</a>，里面说得很详细啦！</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Duck</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        self.hidden_name = input_name</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#取的 name 的函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_name</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用get函数---&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.hidden_name + <span class="string">&#x27;!!&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#设定 name 的函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_name</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用set函数---&#x27;</span>)</span><br><span class="line">        self.hidden_name = input_name + <span class="string">&#x27;??&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#使用property(get,set)来包裝，让使用上更方便</span></span><br><span class="line">    name = <span class="built_in">property</span>(get_name, set_name)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义Object为Duck类，并给定name，从头到尾都沒有直接抄作hidden_name來改变属性值</span></span><br><span class="line">fowl = Duck(<span class="string">&#x27;Howard&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;提取名称时，则调用get函数&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n设定名称时，则调用set函数&#x27;</span>)</span><br><span class="line">fowl.name = <span class="string">&#x27;Daffy&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;nname被改成Daffy&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n当然也可以通过原始的set_name()与get_name()进行修改私有属性&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.get_name())</span><br><span class="line">fowl.set_name(<span class="string">&#x27;Daffyyyy&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.get_name())</span><br></pre></td></tr></table></figure><p>提取名称时，则调用get函数 ---使用get函数--- Howard!!</p><p>设定名称时，则调用set函数 ---使用set函数--- nname被改成Daffy ---使用get函数--- Daffy??!!</p><p>当然也可以通过原始的set_name()与get_name()进行修改私有属性 ---使用get函数--- Daffy??!! ---使用set函数--- ---使用get函数--- Daffyyyy??!!</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#当然可以通过装饰器decorator，来写得更漂亮!!!</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Duck</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        self.hidden_name = input_name</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">name</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用get函数---&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.hidden_name</span><br><span class="line"></span><br><span class="line"><span class="meta">    @name.setter</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">name</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用set函数---&#x27;</span>)</span><br><span class="line">        self.hidden_name = input_name</span><br><span class="line">        </span><br><span class="line"><span class="comment">#定义Object为Duck类，并给定name</span></span><br><span class="line">fowl = Duck(<span class="string">&#x27;Howard&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;提取名称时，则调用get函数&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n设定名称时，则调用set函数&#x27;</span>)</span><br><span class="line">fowl.name = <span class="string">&#x27;Daffy&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;nname被改成Daffy&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br></pre></td></tr></table></figure><p>提取名称时，则调用get函数 ---使用get函数--- Howard</p><p>设定名称时，则调用set函数 ---使用set函数--- nname被改成Daffy ---使用get函数--- Daffy</p><h4 id="使用名称重整保持私有性推荐">使用名称重整保持私有性（推荐）</h4><p>前面的用法如果被知道实际储存属性的名称为什么，也是可以对其修改 所以可以通过名称重整来把实际储存的名称改写</p><p>在属性名称前面加上( __ )来重整名称，虽然不能完全的防止修改私有属性，但可以通过有效的方法降低有意或无意的修改</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Duck</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        self.__name = input_name</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">name</span>(<span class="params">self</span>):  <span class="comment">#该函数名不一定为name，也可以改为names等其他名字</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用get函數---&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.__name</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @name.setter  </span><span class="comment">#这里相应的也要改改为names等其他名字，同理访问时也是</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">name</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用set函數---&#x27;</span>)</span><br><span class="line">        self.__name = input_name</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">fowl = Duck(<span class="string">&#x27;Howard&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br><span class="line">fowl.name = <span class="string">&#x27;Donald&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br><span class="line"></span><br><span class="line"><span class="comment">#fowl.__name        #直接访问或修改会错误</span></span><br><span class="line"><span class="comment">#fowl._Duck__name   #重整完的名称</span></span><br><span class="line"><span class="comment">#print(fowl.__name) # 报错</span></span><br><span class="line"><span class="comment">#print(fowl._Duck__name) # OK</span></span><br><span class="line"></span><br><span class="line">fowl.__name=<span class="string">&quot;gg&quot;</span>  <span class="comment">#相当于属性的动态添加，即添加了一个名为__name的新属性</span></span><br><span class="line"><span class="built_in">print</span>(fowl.__name)      <span class="comment">#gg</span></span><br><span class="line"><span class="built_in">print</span>(fowl._Duck__name) <span class="comment">#Donald</span></span><br><span class="line"><span class="built_in">print</span>(fowl.name)        <span class="comment">#仍然为Donald，说明使用双下划线后无法轻易地改变类内的属性</span></span><br><span class="line"></span><br><span class="line">fowl._Duck__name=<span class="string">&quot;GGBond&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)  <span class="comment">#只有知道类名，才能改变该属性</span></span><br></pre></td></tr></table></figure><p>---使用get函數--- Howard ---使用set函數--- ---使用get函數--- Donald gg Donald ---使用get函數--- Donald</p><p>---使用get函數--- GGBond</p><h3 id="public类型">public类型</h3><p>C++有定义属性的关键字（public、private、protect），而Python没有这类关键字，默认情况下所有的属性都是“公有的”，对公有属性的访问没有任何限制，且都会被子类继承，也能从子类中进行访问。</p><h3 id="特殊类型">特殊类型</h3><h4 id="特殊变量">特殊变量</h4><p>在Python中，变量名类似<font color=#4eb434>__xxx__</font>的，也就是<font color=#4eb434>以双下划线开头，并且以双下划线结尾</font>的，是<font color=#4eb434>特殊变量</font>，特殊变量是可以直接访问的，不是private变量，如 init（）代表类的构造函数。</p><h4 id="特殊方法">特殊方法</h4><p>在python中，存在一些特殊方法( special method )或者称为( magic method )。我们可以通过<strong>在类中重写特殊方法</strong>，使编程更加便捷！ 这些方法为<span style="background:#fbd4d0;">双下划线( __ )开头与结尾</span>的用法。前面介绍过的( <strong>init</strong> )就是一个特殊方法，<u>只要一个类被实例化，就会调用该类中定义的__init__方法</u>。<u>__init__方法的第一个参数永远都是self</u>，表示创建实例本身，在__init__方法内部，可以把各种属性绑定到self，因为形参self指向创建的实例实参本身。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#---------------采用一般方法写法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Word</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, text</span>):</span><br><span class="line">        self.text = text</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">equals</span>(<span class="params">self, word2</span>):</span><br><span class="line">        <span class="keyword">return</span> self.text.lower() == word2.text.lower()</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建3个字符Object</span></span><br><span class="line">first = Word(<span class="string">&#x27;ha&#x27;</span>)</span><br><span class="line">second = Word(<span class="string">&#x27;HA&#x27;</span>)</span><br><span class="line">third = Word(<span class="string">&#x27;eh&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#进行比较</span></span><br><span class="line"><span class="built_in">print</span>(first.equals(second))  <span class="comment">#True</span></span><br><span class="line"><span class="built_in">print</span>(first.equals(third))   <span class="comment">#False</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#---------------采用特殊方法写法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Word</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, text</span>):</span><br><span class="line">        self.text = text</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__eq__</span>(<span class="params">self, word2</span>):   <span class="comment">#在Word类中重写__eq__方法</span></span><br><span class="line">        <span class="keyword">return</span> self.text.lower() == word2.text.lower()</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建3个字符对象</span></span><br><span class="line">first = Word(<span class="string">&#x27;ha&#x27;</span>)</span><br><span class="line">second = Word(<span class="string">&#x27;HA&#x27;</span>)</span><br><span class="line">third = Word(<span class="string">&#x27;eh&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#进行比较</span></span><br><span class="line"><span class="built_in">print</span>(first == second)  <span class="comment">#True</span></span><br><span class="line"><span class="built_in">print</span>(first == third)   <span class="comment">#False</span></span><br></pre></td></tr></table></figure><p>由上可见，特殊方法将使输出更加漂亮!</p><p>我们再来看一些常见的特殊方法与普通方法对比：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Element</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,name,symbol,number</span>):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.symbol = symbol</span><br><span class="line">        self.number = number</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dump</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;name=&#123;&#125;,\nsymbol=&#123;&#125;,\nnumber=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(self.name,self.symbol,self.number))</span><br><span class="line">        <span class="comment"># 若将 print 改为 return，输出为：&#x27;name=GGBond,\nsymbol=G,\nnumber=666&#x27;</span></span><br><span class="line">    </span><br><span class="line">hydrogen = Element(<span class="string">&#x27;GGBond&#x27;</span>,<span class="string">&#x27;G&#x27;</span>,<span class="number">666</span>)</span><br><span class="line">hydrogen.dump()</span><br></pre></td></tr></table></figure><p>name=GGBond, symbol=G, number=666</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Element</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,name,symbol,number</span>):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.symbol = symbol</span><br><span class="line">        self.number = number</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):  <span class="comment">#相当于重写__str__方法</span></span><br><span class="line">        <span class="keyword">return</span>(<span class="string">&#x27;name=&#123;&#125;,\nsymbol=&#123;&#125;,\nnumber=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(self.name,self.symbol,self.number))</span><br><span class="line">        <span class="comment"># 若将 return 改为 print, 报错：TypeError: __str__ returned non-string (type NoneType)</span></span><br><span class="line">        </span><br><span class="line">hydrogen = Element(<span class="string">&#x27;GGBond&#x27;</span>,<span class="string">&#x27;G&#x27;</span>,<span class="number">888</span>)</span><br><span class="line"><span class="built_in">print</span>(hydrogen)</span><br></pre></td></tr></table></figure><p>name=GGBond, symbol=G, number=888</p><blockquote><p>关于__str__和__repr__的区别：<a href="https://www.cnblogs.com/huhuxixi/p/10619289.html">__rept__和<strong>str</strong> - 呼呼嘻嘻 - 博客园 (cnblogs.com)</a></p><p>我们在用print输出任何东西的时候，都会有一个渲染步骤，而且默认的就是用str进行渲染，因为任何一样东西都可以看做一个对象，那么它必有一个类型，如果它的类里面没有定义str和repr也没关系，object里面定义了str和repr，object是一切类的父类，所以输出的对象一定会是渲染过的。这个类里面自己写了str和repr，它覆盖了object里面的str和repr，相当于print的重定向。</p><p>调用 print(i1) （#等同与print(str(i1))）的时候，解释器第一个寻找的就是i1这个类的方法里面有没有<u>重新定义str</u>，如果没有，那么它第二步会去寻找这个类里面有没有<u>重新定义repr</u>，如果有则会用类方法的重新定义的repr，如果还没有，那么解释器会找这个类的上一层父类，按同样的规则进行寻找。</p><p>调用print（repr（i1））的时候就不一样了，repr只会调用repr方法，当自定义的类中没有重写repr方法的时候，它会直接找上一级的父类中有没有repr方法，而不会考虑调用str方法。</p></blockquote><p>常用的特殊方法整理如下：</p><p><strong>非常常用</strong></p><table><thead><tr class="header"><th>方法名</th><th>使用</th></tr></thead><tbody><tr class="odd"><td>__init__(self)</td><td>定义新对象时</td></tr><tr class="even"><td>__str__(self)</td><td>print(对象名) 或 print(str(对象名))</td></tr><tr class="odd"><td>__repr__(self)</td><td>print(对象名) 或 print(repr(对象名))</td></tr><tr class="even"><td>__len__(self)</td><td>len(对象名)</td></tr><tr class="odd"><td>__type__(self)</td><td>type(对象名) == 类名</td></tr></tbody></table><p><strong>比较用</strong></p><table><thead><tr class="header"><th>方法名称</th><th>使用</th></tr></thead><tbody><tr class="odd"><td>__eq__(self,other)</td><td>对象名 == other</td></tr><tr class="even"><td>__ne__(self,other)</td><td>对象名 != other</td></tr><tr class="odd"><td>__lt__(self,other)</td><td>对象名 &lt; other</td></tr><tr class="even"><td>__gt__(self,other)</td><td>对象名 &gt; other</td></tr><tr class="odd"><td>__le__(self, other)</td><td>对象名 &lt;= other</td></tr><tr class="even"><td>__ge__(self, other)</td><td>对象名 &gt;= other</td></tr></tbody></table><p><strong>数学用</strong></p><table><thead><tr class="header"><th>方法名</th><th>使用</th></tr></thead><tbody><tr class="odd"><td>__add__(self, other)</td><td>对象名 + other</td></tr><tr class="even"><td>__sub__(self, other)</td><td>对象名 - other</td></tr><tr class="odd"><td>__mul__(self, other)</td><td>对象名 * other</td></tr><tr class="even"><td>__floordiv__(self, other)</td><td>对象名 // other</td></tr><tr class="odd"><td>__truediv__(self, other)</td><td>对象名 / other</td></tr><tr class="even"><td>__mod__(self, other)</td><td>对象名 % other</td></tr><tr class="odd"><td>__pow__(self, other)</td><td>对象名**other</td></tr></tbody></table><blockquote><p>完整清单见官方清单：<a href="https://docs.python.org/3/reference/datamodel.html#special-method-names">3. Data model — Python 3.12.1 documentation</a></p></blockquote><h3 id="其他类型">其他类型</h3><h4 id="类变量静态变量"><font color=#985fff>类变量/静态变量</font></h4><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231218165934920.png" alt="image-20231218165934920" /><figcaption aria-hidden="true">image-20231218165934920</figcaption></figure><p>C++中有一类特殊的属性称为静态变量。静态变量可以被类直接调用，而不被实例化对象调用。当创建新的实例化对象后，静态变量并不会获取新的内存空间，而是使用类创建的内存空间。因此，静态变量能够被多个实例化对象共享。<font color=#ef042a>在Python中静态变量称为类变量，类变量可以在该类的所有实例中被共享</font>。</p><blockquote><p><strong>有实例属性的变量：</strong>实例属性是以<font color=#ef042a>self</font>为前缀的属性，没有该前缀的属性是普通的局部变量。</p><p>如上图中的普通局部变量zone只有在__init__()方法中能被访问到，而有实例属性的变量color则能在外部被实例调用访问。</p></blockquote><h4 id="静态方法和类方法"><font color=#985fff>静态方法和类方法</font></h4><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231218165535528.png" alt="image-20231218165535528" /><figcaption aria-hidden="true">image-20231218165535528</figcaption></figure><blockquote><p>类的方法也分为公有方法和私有方法。私有方法不能被模块外的类或方法调用，私有方法也不能被外部的类或函数调用。</p></blockquote><h5 id="静态方法"><font color=#4eb434>静态方法</font></h5><p>Python使用<font color=#ef042a>函数staticmethod()</font>或 <font color=#ef042a>@ staticmethod修饰器</font>将普通的函数转换为静态方法。在开发中，我们常常需要定义一些方法，这些方法跟类有关，但在实现时并不需要引用类或者实例，例如，设置环境变量，修改另一个类的变量，等。这个时候，我们可以使用静态方法。</p><blockquote><p>静态方法一定要加函数staticmethod()或@ staticmethod修饰器，否则会被识别为没有self参数的普通方法，进而只能被类调用！</p></blockquote><ol type="1"><li>静态方法可以使用<font color=#df8400><strong>类调用</strong></font>也可以使<font color=#df8400><strong>用对象调用</strong></font>。</li><li>一般不需要传参数self。</li><li>Python的静态方法并没有和类的实例进行名称绑定，只是名义上归类管理，<u>实际上在静态方法里面访问不了类或者实例的任何属性</u>。</li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231218165603714.png" alt="image-20231218165603714" /><figcaption aria-hidden="true">image-20231218165603714</figcaption></figure><h5 id="类方法"><font color=#4eb434>类方法</font></h5><p>使用<font color=#ef042a>函数classmethod()</font>或 <font color=#ef042a>@ classmethod修饰器</font>，没有被调用的类中其余参数不会加载进内存中。</p><ol type="1"><li><font color=#df8400><strong>只能访问类变量</strong></font>，不能访问实例变量。需要有参数。类方法能被<u>类本身</u>和<u>类的实例</u>调用。</li><li>类方法的第一个参数长什么样不重要，它指向的都是类本身。</li><li>在类方法中，可以调用类里面的类属性和普通方法/静态方法。</li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231218165618888.png" alt="image-20231218165618888" /><figcaption aria-hidden="true">image-20231218165618888</figcaption></figure><h5 id="静态方法和类方法的栗子"><font color=#4eb434>静态方法和类方法的栗子</font></h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>():</span><br><span class="line">    count = <span class="number">0</span>           <span class="comment">#类属性</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,name</span>):</span><br><span class="line">        A.count += <span class="number">1</span>    <span class="comment">#修改类属性,修改时必须用A.调用</span></span><br><span class="line">        self.name = name</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">exclaim</span>():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;I&#x27;m an A!&quot;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod        </span><span class="comment">#类方法(methond)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kids</span>(<span class="params">cls</span>):</span><br><span class="line">        cls.exclaim()  <span class="comment">#类方法能调用普通方法/静态方法，但在调用实例方法时会报错</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;A has&quot;</span>, cls.count, <span class="string">&quot;little objects.&quot;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="meta">    @classmethod        </span><span class="comment">#类方法(methond)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kids2</span>(<span class="params">pp</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;A has&quot;</span>, A.count, <span class="string">&quot;little objects.&quot;</span>)</span><br><span class="line"></span><br><span class="line">easy_a = A(<span class="string">&quot;easy&quot;</span>)</span><br><span class="line">breezy_a = A(<span class="string">&quot;breezy&quot;</span>)</span><br><span class="line">wheezy_a = A(<span class="string">&quot;wheezy&quot;</span>)</span><br><span class="line">A.kids()</span><br><span class="line">A.kids2()</span><br><span class="line">easy_a.kids()  <span class="comment">#类方法也能被类的实例所调用 </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CoyoteWeapon</span>():</span><br><span class="line">    yyo = <span class="number">1</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">commercial</span>():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;This CoyoteWeapon has been brought to you by Acme&#x27;</span>)</span><br><span class="line">        <span class="comment"># print(yyo) 报错，在静态方法中访问不了类的任何实例和变量</span></span><br><span class="line">        </span><br><span class="line">CoyoteWeapon.commercial()</span><br><span class="line">ppp = CoyoteWeapon()</span><br><span class="line">ppp.commercial()</span><br></pre></td></tr></table></figure><p>I'm an A! A has 3 little objects. A has 3 little objects. I'm an A! A has 3 little objects. This CoyoteWeapon has been brought to you by Acme This CoyoteWeapon has been brought to you by Acme</p><h2 id="属性和方法的动态添加">属性和方法的动态添加</h2><blockquote><p>动态语言目前非常具有活力。例如JavaScript， PHP 、 Ruby 、 Python 等。而 C 、 C++ 等语言则不属于动态语言。</p></blockquote><h3 id="属性的动态添加">属性的动态添加</h3><ol type="1"><li>运行过程中可以给<strong>对象/类</strong>添加属性</li><li>语法格式：xx.new_attribute = attri_value</li></ol><h3 id="方法的动态添加">方法的动态添加</h3><ol type="1"><li>语法格式：class_name.method_name=function_name</li><li>可以动态添加/更改类的方法，将某个已经定义的函数添加到类中。当method_name表示已经存在的方法名，function_name表示1个已经存在的函数，该赋值表达式表示将函数的内容更新到方法。</li></ol><h2 id="继承与多态">继承与多态</h2><blockquote><p>这一章有个例子，感兴趣的可以搜一下 Undercut游戏~</p></blockquote><h3 id="继承">继承</h3><p>不同的类型中有许多通用的属性。例如，list类型和str类型都具有len函数，意义也完全 一样，这是因为它们继承了同一个父类。<strong>“继承”</strong>是面向对象编程 (OOP) 语言的一个主要功能。</p><p>在编写类时，如果发现已经有前人开发过，那就可以不用整段赋值，<strong>可以采用<font color=#985fff>继承</font>的方法取得他的属性与方法</strong>。 <strong>并且补充自己会用的功能</strong>，一方面可以减少去改已有的类的辛苦，也可以省去复制粘贴的功夫。<font color=#985fff>原始的类称为父类或超类（Base class、Super class），新类称为子类（Subclass）</font>。Python在类名后使用一对括号表示继承关系，括号中即为父类。</p><blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219152748146.png" alt="image-20231219152748146" /><figcaption aria-hidden="true">image-20231219152748146</figcaption></figure><p>如果子类中除了pass没有其他的内容，则pass必写；否则，报错下一行的缩进问题</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">math</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;add:&quot;</span>, a + b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">mean</span>(<span class="title class_ inherited__">math</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">ab = mean()</span><br><span class="line">ab.add(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">ac = math()</span><br><span class="line">ac.add(<span class="number">1</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>add: 4 add: 6</p><p>继承的过程，就是从一般到特殊的过程。object类在最顶层。因为在Python中，存在于运行时的一切都是对象。</p><h4 id="构造函数的继承">构造函数的继承</h4><p>python中<u>如果子类有自己的构造函数</u>，<u>不会自动调用父类的构造函数</u>，<font color=#ef042a>如果需要用到父类的构造函数，则需要在子类的构造函数中显式地调用，且调用代码应位于第一行</font>。</p><p>如果子类需要扩展父类的行为，可以添加__init__方法的参数。一种方法如图，即<code>Fruit.__init__(self,color)</code>; 另一种也可以用<code>super().__init__(color)</code>。</p><p>如果子类没有自己的构造函数，则会直接从父类继承构造函数。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219151850198.png" alt="image-20231219151850198" /><figcaption aria-hidden="true">image-20231219151850198</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219152043676.png" alt="image-20231219152043676" /><figcaption aria-hidden="true">image-20231219152043676</figcaption></figure><blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219152242748.png" alt="image-20231219152242748" /><figcaption aria-hidden="true">image-20231219152242748</figcaption></figure><p>为什么P4&lt;P1能够执行，而P1&lt;P4不行？</p><ul><li>P4&lt;P1相当于调用P4.__lt__(P1)，相当于使用与P4对象相关的Person的方法，根据人的姓名做比较，因此可正常执行；</li><li>P1&lt;P4相当于调用P1.__ lt __(P4)，相当于使用与P1对象相关的MITPeron的方法，根据人的IDNum做比较，而P4是一个Person，没有IDNum，因此无法比较。</li></ul></blockquote><blockquote><p>说明:函数isinstance是内置在Python中的一个常用函数,其中第一个参数可以是任何对象,但第二个参数必须是一个type类型的对象。函数当且仅当第一个参数是第二个 参数的一个实例时,才返回True。</p><p>例如,isinstance([1,2],list)的值是True。</p><p>常见用法： def isStudent(self): ​ return isinstance(self, Student)</p><p>请注意，isinstance(p6, Student)与type(p6) == Student在意义上是截然不同的。与p6绑定的对象类型是UnderGraduate，不是Student， 但因为UnderGraduate是Student的子类，所以p6绑定的对象被认为是Student类的一个实例（也是MITPerson类和Person类的实例）。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">math</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;add:&quot;</span>, a + b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">mean</span>(<span class="title class_ inherited__">math</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;add:&quot;</span>, a + b)</span><br><span class="line">    </span><br><span class="line">ab = mean()</span><br><span class="line">ac = math()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(ab)==mean) <span class="comment">#True</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(ab)==math) <span class="comment">#False</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">isinstance</span>(ab,mean)) <span class="comment">#True</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">isinstance</span>(ab,math)) <span class="comment">#True</span></span><br></pre></td></tr></table></figure></blockquote><h4 id="覆盖">覆盖</h4><p>覆盖——也就是替换——超类中的方法。</p><p><font color=#0091ff>如果一个方法被覆盖，那么调用这个方法时使用的版本就要根据调用这个方法的对象来确定。</font>如果这个对象的类型是子类，那么就使用定义在子类中的方法版本；如果对象的 类型是超类，那么就使用超类中的版本。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">math</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;add:&quot;</span>, a + b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">mean</span>(<span class="title class_ inherited__">math</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;add:&quot;</span>, a + b + b)</span><br><span class="line">    </span><br><span class="line">ab = mean()</span><br><span class="line">ab.add(<span class="number">1</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>add: 7</p><h4 id="添加新的属性或方法">添加新的属性或方法</h4><p>添加新的属性。例如，子类MITPerson中新增了类变量nextIdNum、实例变量idNum 和方法getIdNum。我们也可以在新的类中加入新的方法：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class math():</span><br><span class="line">    def add(self, a, b):</span><br><span class="line">        print(&quot;add:&quot;, a + b)</span><br><span class="line"></span><br><span class="line">class mean(math):</span><br><span class="line">    def less(self, a, b):</span><br><span class="line">        print(&quot;add:&quot;, a - b)</span><br><span class="line">    </span><br><span class="line">ab = mean()</span><br><span class="line">ab.add(1, 3)</span><br><span class="line">ab.less(1, 3)</span><br><span class="line"></span><br><span class="line">ac = math()</span><br><span class="line">ac.add(1, 5)</span><br></pre></td></tr></table></figure><p>add: 4 add: -2 add: 6</p><h3 id="多态">多态</h3><p>继承机制说明子类具有父类的公有属性和方法，而且子类可以扩展自身的功能，添加新的属性和方法。因此，子类可以替代父类对象，这种特性称为<strong>多态性</strong>。</p><p><strong>多态（polymorphism）</strong>是指父类的同一个方法在不同的子类对象中具有不同的表现和 行为。 （<u>事实上，即使是同一个类的对象，也可以有不同的属性</u>）</p><p>子类继承了父类的属性和方法之后，还会增加某些特定的属性和方法，同时还会对继承来的某些方法进行改变，都是多态的表现形式。</p><p>Python大多数运算符可以作用于不同类型的操作数，且对不不同类型的操作数往往有不 同的表现，这本身就是多态，是通过重写特殊方法与运算符重载实现的。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219160503820.png" alt="image-20231219160503820" /><figcaption aria-hidden="true">image-20231219160503820</figcaption></figure><p>多态的好处就是，当我们需要传入Dog、Cat、Tortoise……时，我们只需要接收Animal类型就可以了，因为Dog、Cat、Tortoise……都是Animal类型，然后，按照Animal类型进行操作即可。由于Animal类型有run()方法，因此，传入的任意类型，只要是Animal类或者子 类，就会自动调用实际类型的run()方法，这就是多态的意思。</p><p>对于一个变量，我们只需要知道它是Animal类型，无需确切地知道它的子类型，就可以放心地调用run()方法，而具体调用的run()方法是作用在Animal、Dog、Cat还是Tortoise对象上，由运行时该对象的确切类型决定，这就是多态真正的威力：</p><p><font color=#985fff>调用方只管调用，不管细节，而当我们新增一种Person的子类时，只要确保新方法编写正确，而不用管原来的代码。这就是著名的<strong>“开闭”原则</strong>：</font></p><p><font color=#985fff><strong>对扩展开放（Open for extension）：允许子类重写方法函数</strong></font> <font color=#985fff><strong>对修改封闭（Closed for modification）：不重写，直接继承父类方法函数</strong></font></p><h3 id="多重继承">多重继承</h3><p>Python支持多重继承，即一个类可以继承多个父类。多重继承的语法格式： <code>class_name(parent_class1, parent_class2…)</code>。其中class_name是类名，parent_class1和parent_class2是父类名。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219160947542.png" alt="image-20231219160947542" /><figcaption aria-hidden="true">image-20231219160947542</figcaption></figure><h4 id="多重继承关系中的构造函数">多重继承关系中的构造函数</h4><p>子类从多个父类派生，而子类又没有自己的构造函数时， （1）按顺序继承，哪个父类在最前面且它又有自己的构造函数，就继承它的构造函数； （2）如果最前面第一个父类没有构造函数，则继承第2个的构造函数，第2个没有的话， 再往后找，以此类推。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219161134386.png" alt="image-20231219161134386" /><figcaption aria-hidden="true">image-20231219161134386</figcaption></figure><h2 id="抽象基类">抽象基类</h2><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/kkk" alt="image-20231218165633785" /><figcaption aria-hidden="true">image-20231218165633785</figcaption></figure><ol type="1"><li>抽象基类是对一类事物的特征行为的抽象，由抽象方法组成。在Python3中可以使用<font color=#985fff>abc模块</font>，该模块中有一个<font color=#985fff>元类ABCMeta</font>和<font color=#985fff>修饰器 @ abstractmethod</font>。抽象基类不能被直接实例化。</li><li><strong>继承抽象类的子类必须重写抽象函数</strong>。</li></ol><h2 id="组合">组合</h2><p>如果要新建的类有相似的类可以继承的話就可以采用继承来取得父类的所有， 但若两个类差异太大，或是沒有关系，我们就可以采用组合來合并这些类</p><p>例如，鸭子是鸟的一种，所以可以继承鸟的类， 但是嘴巴和尾巴不是鸟的一种，而是鸭子的组成。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Bill</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, description</span>):</span><br><span class="line">        self.description = description</span><br><span class="line">        </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tail</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, length</span>):</span><br><span class="line">        self.length = length</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Duck</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, bill, tail</span>):</span><br><span class="line">        self.bill = bill</span><br><span class="line">        self.tail = tail</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">about</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;这只鸭子有一个&#x27;</span>, bill.description, <span class="string">&#x27;嘴巴，然后有&#x27;</span>, tail.length, <span class="string">&#x27;长的尾巴&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">bill = Bill(<span class="string">&#x27;红色的&#x27;</span>)</span><br><span class="line">tail = Tail(<span class="string">&#x27;白色，15cm&#x27;</span>)</span><br><span class="line"></span><br><span class="line">duck = Duck(bill, tail)</span><br><span class="line">duck.about()</span><br></pre></td></tr></table></figure><p>这只鸭子有一个 红色的 嘴巴，然后有 白色，15cm 长的尾巴</p><h2 id="何时使用类和对象而不是模块">何时使用类和对象而不是模块</h2><p>有一些方法可以帮助你決定是把你的代码封裝到类里还是模块里。</p><ul><li>当你需要许多具有相似行为（方法），但不同状态（特性）的实例时，使用对象是最好的选择。</li><li>类支持继承，但模块不支持。</li><li>如果你想要保证实例的唯一性，使用模块是最好的选择。不管模块在程序中被引用多少次，始终只有一个实例被加载。</li><li>如果你有一系列包含多个值的变量，并且他们能作为参数传入不同的函数，那么最好將它们封裝到类里面。举个例子，你可能会使用以大小和颜色为键的字典代表一张 彩色图片。你可以在程序中为每张图片创建不同的字典，并把它们作为参数传递给像规模（）或者变换（）之类的函数。但这么做的话，一旦你想要添加其他的键或者函数会变得非常麻烦。为了保证统一性，应该定义一个图片类，把大小和颜色作为特性，把规模（）和变换（）定义为方法。这么一来，关于一张图片的所有数据和可执行的操作都存储在了统一的位置。</li><li>用最简单的方式解決问题。使用字典，列表和元組往往要比使用模块更加简单，简介且快速。而使用类则更为复杂。</li></ul><h3 id="命名tuplenamed-tuple">命名Tuple(named tuple)</h3><p>可以用来创造可以用名称访问的Tuple子类</p><p>跟Tuple一样，不可被改变，但是可以透过替换來产生新的命名Tuple</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple <span class="comment">#引入函数库</span></span><br><span class="line"></span><br><span class="line">Duck = namedtuple(<span class="string">&#x27;Duck&#x27;</span>, <span class="string">&#x27;bill tail&#x27;</span>) <span class="comment">#定义为命名Tuple，並且有bill和tail两种名称</span></span><br><span class="line">duck = Duck(<span class="string">&#x27;wide orange&#x27;</span>, <span class="string">&#x27;long&#x27;</span>)     <span class="comment">#赋值</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(duck)</span><br><span class="line"><span class="built_in">print</span>(duck.bill)</span><br><span class="line"><span class="built_in">print</span>(duck.tail)</span><br><span class="line"></span><br><span class="line">parts = &#123;<span class="string">&#x27;bill&#x27;</span>: <span class="string">&#x27;wide orange&#x27;</span>, <span class="string">&#x27;tail&#x27;</span>: <span class="string">&#x27;long&#x27;</span>&#125;  <span class="comment">#使用dictionary赋值</span></span><br><span class="line">duck2 = Duck(**parts)  <span class="comment">#键名相同，因此可以用**提取字典内容导入</span></span><br><span class="line"><span class="built_in">print</span>(duck2)</span><br><span class="line"></span><br><span class="line">duck3 = duck2._replace(tail=<span class="string">&#x27;magnificent&#x27;</span>, bill=<span class="string">&#x27;crushing&#x27;</span>)  <span class="comment">#替换內容</span></span><br><span class="line"><span class="built_in">print</span>(duck3)</span><br></pre></td></tr></table></figure><p>Duck(bill='wide orange', tail='long') wide orange long Duck(bill='wide orange', tail='long') Duck(bill='crushing', tail='magnificent')</p><blockquote><p>学习资料链接：</p><p>链接：https://pan.baidu.com/s/1fg10feuxUbm4bdk5faX9MA?pwd=9k78 提取码：9k78 --来自百度网盘超级会员V1的分享</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> course </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计学习及监督学习概论第一章 读书笔记</title>
      <link href="/2023/12/25/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/12/25/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="统计学习">统计学习</h1><p><strong>统计学习(statistical learning)</strong>，也称统计机器学习(statistical machine learning)，是关于计算机基于<u>数据</u>构建<u>概率统计模型</u>并运<u>用模型对数据进行预测与分析</u>的一门学科。现在,当人们提及机器学习时,往往是指统计机器学习。所以可以认为本书介绍的是机器学习方法。</p><p>统计学习研究的对象是<strong>数据(data)</strong>。它从数据出发,提取数据的特征,抽象出数 据的模型,发现数据中的知识,又回到对数据的分析与预测中去。统计学习关于数据的基本假设是<u>同类数据具有一定的统计规律性</u>,这是统计学习的前提。比如,<em>可以用随机变量描述数据中的特征,用概率分布描述数据的统计规律</em>。在统计学习中,以变量或变量组表示数据。数据分为由<u>连续变量</u><u>和表示的类型</u>。本书以讨论离散变量的方法为主。另外,本书只涉及利用数据构建模型及利用模型对数据进行分析与预测,对数据的观测和收集等问题不作讨论。</p><p>统计学习的方法是<u>基于数据构建概率统计模型从而对数据进行预测与分析</u>。统计学习由<strong>监督学习(supervised learning)</strong>、<strong>无监督学习(unsupervised learning)</strong>和<strong>强化学习(reinforcement learning)</strong>等组成。其中，监督学习、无监督学习方法是最主要的统计学习方法。</p><p>统计学习方法可以概括如下:从<u>给定的、有限的、用于学习</u>的训练数据(training data)集合出发,假设数据是独立同分布产生的;并且假设要学习的模型属于某个函数的集合,称为<font color=#df8400>假设空间(hypothesis space)</font>;应用某个<font color=#0091ff>评价准则(evaluation criterion)</font>,从假设空间中选取一个最优模型,使它对已知的训练数据及未知的测试数据(test data)在给定的评价准则下有最优的预测; 最优模型的选取由<font color=#4eb434>算法</font>实现。这样,统计学习方法包括<font color=#df8400>模型的假设空间</font>、<font color=#0091ff>模型选择的准则</font>以及<font color=#4eb434>模型学习的算法</font>。称其为统计学习方法的三要素,简称为<font color=#df8400><strong>模型(model)</strong></font>、<font color=#0091ff><strong>策略(strategy)</strong></font>和<font color=#4eb434><strong>算法(algorithm)</strong></font>。</p><p><strong>实现统计学习方法的步骤如下:</strong> (1) 得到一个有限的<u>训练数据集合</u>; (2)确定包含所有可能的<u>模型的假设空间</u>,即学习模型的集合; (3)确定模型选择的准则,即<u>学习的策略/选择最优模型的评价准则</u>; (4)实现求解最优模型的算法,即<u>学习的算法/选取最优模型的方法</u>; (5)通过学习方法选择最优模型; (6)利用学习的最优模型对新数据进行预测或分析。</p><p>本书第1篇介绍监督学习方法,主要包括用于<u>分类、标注与回归问题的方法</u>。这些方法在<u>自然语言处理、信息检索、文本数据挖掘等领域</u>中有着极其广泛的应用。统计学习研究一般包括统计学习方法、统计学习理论及统计学习应用三个方面。统计学习是计算机科学发展的一个重要组成部分。可以认为计算机科学由三维组成: <u>系统、计算、信息</u>。统计学习主要属于<u>信息</u>这一维,并在其中起着核心作用。</p><h2 id="统计学习的分类">统计学习的分类</h2><h3 id="基本分类">基本分类</h3><p>统计学习或机器学习一般包括<u>监督学习</u>、<u>无监督学习</u>、<u>强化学习</u>。有时还包括半 监督学习、主动学习。</p><h4 id="监督学习">监督学习</h4><p>监督学习(supervised learning)的本质是学习输入到输出的映射的统计规律。</p><hr /><p>几个概念：</p><ul><li><strong>输入空间(input space)</strong>：输入所有可能取值的集合</li><li><strong>输出空间(output space)</strong>：输出所有可能取值的集合</li><li><strong>实例(instance)</strong>：每个具体的输入是一个实例，通常由特征向量(feature vector)表示。</li><li><strong>特征空间(feature space)</strong>：所有特征向量存在的空间称为特征空间。特征空间的每一维对应于一个特征。模型实际上都是定义在特征空间上的。</li></ul><hr /><p>在监督学习中,将输入与输出看作是定义在输入(特征)空间与输出空间上的随机变量的取值。输入输出变量用大写字母表示,习惯上输入变量写作X,输出变量写作Y。输入输出变量的取值用小写字母表示,输入变量的取值写作x,输出变量的取值写作y。变量可以是标量或向量,都用相同类型字母表示。除特别声明外,本书中向量均为列向量。</p><p>输入变量X和输出变量Y有不同的类型,可以是连续的,也可以是离散的。人们根据输入输出变量的不同类型,对预测任务给予不同的名称：</p><ul><li><strong>回归问题</strong>：输入变量与输出变量均为连续变量的预测问题;</li><li><strong>分类问题</strong>：输出变量为有限个离散变量的预测问题;</li><li><strong>标注问题</strong>：输入变量与输出变量均为变量序列的预测问题。</li></ul><h4 id="无监督学习">无监督学习</h4><p>无监督学习(unsupervised learning)是指从无标注数据中学习预测模型的机器学习问题。无标注数据是自然得到的数据,预测模型表示数据的类别、转换或概率。无监督学习的本质是学习数据中的统计规律或潜在结构。</p><h4 id="强化学习">强化学习</h4><p>强化学习(reinforcement learning)是指智能系统在与环境的连续互动中学习 最优行为策略的机器学习问题。假设智能系统与环境的互动基于马尔可夫决策过 程(Markov decision process),智能系统能观测到的是与环境互动得到的数据序列。 强化学习的本质是学习最优的序贯决策。</p><h4 id="半监督学习">半监督学习</h4><p>半监督学习(semi-supervised learning)是指利用标注数据和未标注数据学习预 测模型的机器学习问题。通常有少量标注数据、大量未标注数据,因为标注数据的构 建往往需要人工,成本较高,未标注数据的收集不需太多成本。半监督学习旨在利用 未标注数据中的信息,辅助标注数据,进行监督学习,以较低的成本达到较好的学习 效果。</p><h4 id="主动学习">主动学习</h4><p>主动学习(active learning)是指机器不断主动给出实例让教师进行标注,然后利 用标注数据学习预测模型的机器学习问题。通常的监督学习使用给定的标注数据,往 往是随机得到的,可以看作是“被动学习”,主动学习的目标是找出对学习最有帮助的 实例让教师标注,以较小的标注代价,达到较好的学习效果。</p><blockquote><p>半监督学习和主动学习更接近监督学习。</p></blockquote><h3 id="按模型分类">按模型分类</h3><h4 id="概率模型与非概率模型">概率模型与非概率模型</h4><p>统计学习的模型可以分为概率模型(probabilistic model)和非概率模型(nonprobabilistic model)或者确定性模型(deterministic model)。</p><h4 id="线性模型与非线性模型">线性模型与非线性模型</h4><p>统计学习模型,特别是非概率模型,可以分为线性模型(linear model)和非线性模型(non-linear model)。如果函数y=f(x)或z=g(x)是线性函数,则称模型是线 性模型,否则称模型是非线性模型。</p><h4 id="参数化模型与非参数化模型">参数化模型与非参数化模型</h4><p>统计学习模型又可以分为参数化模型(parametric model)和非参数化模型(nonparametric model)。参数化模型假设模型参数的维度固定,模型可以由有限维参数完全刻画;非参数化模型假设模型参数的维度不固定或者说无穷大,随着训练数据量的增加而不断增大。</p><p>参数化模型适合问题简单的情况,现实中问题往往比较复杂,非参数化模型更加有效。</p><h3 id="按算法分类">按算法分类</h3><p>统计学习根据算法,可以分为在线学习(online learning)与批量学习(batch learning)。在线学习是指每次接受一个样本,进行预测,之后学习模型,并不断重复该操作的机器学习。与之对应,批量学习一次接受所有数据,学习模型,之后进行预测。有些实际应用的场景要求学习必须是在线的。比如,数据依次达到无法存储,系统需要及时做出处理;数据规模很大,不可能一次处理所有数据;数据的模式随时间动态变化,需要算法快速适应新的模式(不满足独立同分布假设)。</p><p>在线学习通常比批量学习更难,很难学到预测准确率更高的模型,因为每次模型更新中,可利用的数据有限。</p><h3 id="按技巧分类">按技巧分类</h3><h4 id="贝叶斯学习">贝叶斯学习</h4><p>贝叶斯学习(Bayesian learning), 又称为贝叶斯推理(Bayesian inference), 是 统计学、机器学习中重要的方法。其主要想法是, <u>在概率模型的学习和推理中, 利用贝</u> <u>叶斯定理, 计算在给定数据条件下模型的条件概率,即后验概率, 并应用这个原理进</u><u>行模型的估计, 以及对数据的预测</u>。将模型、未观测要素及其参数用变量表示, 使用模型的先验分布是贝叶斯学习的特点。贝叶斯学习中也使用基本概率公式(图1.4)。</p><p>本书介绍的<u>朴素贝叶斯、潜在狄利克雷分配的学习</u>属于贝叶斯学习。</p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>制作电脑纹理壁纸</title>
      <link href="/2023/12/16/%E8%87%AA%E5%88%B6%E7%94%B5%E8%84%91%E7%BA%B9%E7%90%86%E5%A3%81%E7%BA%B8/"/>
      <url>/2023/12/16/%E8%87%AA%E5%88%B6%E7%94%B5%E8%84%91%E7%BA%B9%E7%90%86%E5%A3%81%E7%BA%B8/</url>
      
        <content type="html"><![CDATA[<h1 id="查看电脑壁纸尺寸">查看电脑壁纸尺寸</h1><p>在桌面右键，选择<font color=orange>显示设置</font></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216200845959.png" alt="image-20231216200845959" /><figcaption aria-hidden="true">image-20231216200845959</figcaption></figure><p>便可查看到目前电脑的分辨率是：<font color=orange>1920×1080</font></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216201136929.png" alt="image-20231216201136929" /><figcaption aria-hidden="true">image-20231216201136929</figcaption></figure><h1 id="准备纹理图片">准备纹理图片</h1><blockquote><p>补充于2023/12/17</p><p>如果选择的纹理图片和电脑显示器尺寸一样，可以跳过这一步哦~</p></blockquote><blockquote><p>纹理图片可参考网站：</p><ol type="1"><li><a href="https://texturelabs.org/?pg=1">Free Textures, Tutorials, and More (texturelabs.org)</a></li><li><a href="https://www.transparenttextures.com/">Transparent Textures</a></li><li><a href="https://www.cgbookcase.com/textures">Textures | cgbookcase.com</a></li><li><a href="https://www.toptal.com/designers/subtlepatterns/">Subtle Patterns | Free textures for your next web project (toptal.com)</a></li><li><a href="https://polyhaven.com/">Poly Haven</a></li><li><a href="https://www.poliigon.com/search?credit=0">Search - Poliigon</a></li></ol></blockquote><p>考虑到纹理图片和电脑显示器图片可能不一致，由此制作壁纸的最终效果可能稍有不完美，所以我们可以<strong>设法先将尺寸不匹配的纹理图片铺满整个屏幕</strong>，再导出作为制作壁纸使用的纹理图片。</p><ol type="1"><li><p>在PS软件中点击<font color=purple>文件-打开</font>，或者<font color=purple>Ctrl+O</font>打开纹理图片</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231217150953342.png" alt="image-20231217150953342" /><figcaption aria-hidden="true">image-20231217150953342</figcaption></figure></li><li><p>点击<font color=blue>编辑-定义图案</font>，给纹理图案起一个好记的名字，点击<font color=blue>确定</font></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231217151110446.png" alt="image-20231217151110446" /><figcaption aria-hidden="true">image-20231217151110446</figcaption></figure></li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/wwwwww.png" alt="wwwwww" /><figcaption aria-hidden="true">wwwwww</figcaption></figure><ol start="3" type="1"><li><p>新建项目，将尺寸设置为电脑显示器大小</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/屏幕截图%202023-12-17%20151328.png" alt="屏幕截图 2023-12-17 151328" /><figcaption aria-hidden="true">屏幕截图 2023-12-17 151328</figcaption></figure></li><li><p>选择<font color=red>油漆桶工具</font>，将填充物设置为<font color=red>图案</font>，<font color=red><strong>在图案里找到我们定义的纹理图案</strong></font>，进行填充</p></li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231217151756597.png" alt="image-20231217151756597" /><figcaption aria-hidden="true">image-20231217151756597</figcaption></figure><ol start="4" type="1"><li>最后导出即可</li></ol><h1 id="制作壁纸">制作壁纸</h1><blockquote><p>这里使用的图案编辑工具是在线PS工具（<a href="https://www.photopea.com/">Photopea | Online Photo Editor</a>），也可以直接在PS软件中编辑</p></blockquote><blockquote><p>颜色设置可参考配色网站：<a href="http://zhongguose.com/">zhongguose － 传统颜色</a></p></blockquote><ol type="1"><li><p>新建项目，设置分辨率为<strong>电脑显示器分辨率</strong>，设置<strong>背景色</strong>为你喜欢的颜色（或者直接打开一张你喜欢的背景图片）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216202126809.png" alt="image-20231216202126809" /><figcaption aria-hidden="true">image-20231216202126809</figcaption></figure></li><li><p>点击<strong><font color=green>文件-打开</font></strong>或者<font color=blue>Ctrl+O</font>打开下载好的纹理图片</p><blockquote><p>这一步用的纹理图案尺寸比电脑显示器尺寸小一些，最后的纹理效果有点放大失真。如想避免这一点可以跳回去看第二步：准备纹理图片</p></blockquote></li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216210041490.png" alt="image-20231216210041490" /><figcaption aria-hidden="true">image-20231216210041490</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216214805133.png" alt="image-20231216214805133" /><figcaption aria-hidden="true">image-20231216214805133</figcaption></figure><ol start="3" type="1"><li><p>在纹理图案界面，点击<font color=red>编辑-定义新的图案</font>，定义成功后回到壁纸界面</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216215020114.png" alt="image-20231216215020114" /><figcaption aria-hidden="true">image-20231216215020114</figcaption></figure></li><li><p>点击图层，右键点击<font color=purple>混合模式</font></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216215322738.png" alt="image-20231216215322738" /><figcaption aria-hidden="true">image-20231216215322738</figcaption></figure></li></ol><p>在出现的界面选择<font color=orange>纹理</font>，然后<strong>选择我们定义的图案</strong>，点击确定</p><blockquote><p>纹理界面的深度为正，代表纹理效果是凸出的；深度为负，表示纹理效果是凹陷的</p></blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216215555128.png" alt="image-20231216215555128" /><figcaption aria-hidden="true">image-20231216215555128</figcaption></figure><ol start="5" type="1"><li><p>最后依次点击<font color=green>文件-导出为-PNG</font>，将图片导出即可</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216215902983.png" alt="image-20231216215902983" /><figcaption aria-hidden="true">image-20231216215902983</figcaption></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> blog搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Butterfly主题优化 </tag>
            
            <tag> PS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Butterfly主题 网页标题崩溃欺骗特效</title>
      <link href="/2023/12/05/Butterfly%E4%B8%BB%E9%A2%98%20%E7%BD%91%E9%A1%B5%E6%A0%87%E9%A2%98%E5%B4%A9%E6%BA%83%E6%AC%BA%E9%AA%97%E7%89%B9%E6%95%88/"/>
      <url>/2023/12/05/Butterfly%E4%B8%BB%E9%A2%98%20%E7%BD%91%E9%A1%B5%E6%A0%87%E9%A2%98%E5%B4%A9%E6%BA%83%E6%AC%BA%E9%AA%97%E7%89%B9%E6%95%88/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考网址：<a href="https://asdfv1929.github.io/2018/01/25/crash-cheat/">Hexo NexT主题中添加网页标题崩溃欺骗搞怪特效 | asdfv1929 's Home</a></p></blockquote><p>在butterfly主题中给网页标题增加一些搞怪特效</p><h4 id="创建js文件-crash_cheat.js">创建js文件 crash_cheat.js</h4><p>打开theme文件夹下的<code>\butterfly\source\js</code>，创建文件<span style="background:#f9eda6;color:red">crash_cheat.js</span>，添加代码：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;!--崩溃欺骗--&gt;</span><br><span class="line"> var OriginTitle = document.title;</span><br><span class="line"> var titleTime;</span><br><span class="line"> document.addEventListener(&#x27;visibilitychange&#x27;, function () &#123;</span><br><span class="line">     if (document.hidden) &#123;</span><br><span class="line">         $(&#x27;[rel=&quot;icon&quot;]&#x27;).attr(&#x27;href&#x27;, &quot;/img/favicon.png&quot;);</span><br><span class="line">         document.title = &#x27;╭(°A°`)╮ 页面崩溃啦 ~&#x27;;</span><br><span class="line">         clearTimeout(titleTime);</span><br><span class="line">     &#125;</span><br><span class="line">     else &#123;</span><br><span class="line">         $(&#x27;[rel=&quot;icon&quot;]&#x27;).attr(&#x27;href&#x27;, &quot;/favicon.png&quot;);</span><br><span class="line">         document.title = &#x27;(ฅ&gt;ω&lt;*ฅ) 噫又好了~&#x27; + OriginTitle;</span><br><span class="line">         titleTime = setTimeout(function () &#123;</span><br><span class="line">             document.title = OriginTitle;</span><br><span class="line">         &#125;, 2000);</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;)</span><br></pre></td></tr></table></figure><h4 id="inject-引用">inject 引用</h4><p>在主题配置文件<code>_config.butterfly.yml</code>的<span style="background:#f9eda6;color:red">inject-bottom</span>下直接引入js文件：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Inject</span><br><span class="line"># Insert the code to head (before &#x27;&lt;/head&gt;&#x27; tag) and the bottom (before &#x27;&lt;/body&gt;&#x27; tag)</span><br><span class="line"># 插入代码到头部 &lt;/head&gt; 之前 和 底部 &lt;/body&gt; 之前</span><br><span class="line">inject:</span><br><span class="line">  head:</span><br><span class="line">    # - &lt;link rel=&quot;stylesheet&quot; href=&quot;/xxx.css&quot;&gt;</span><br><span class="line"></span><br><span class="line">  bottom:</span><br><span class="line">    - &lt;script src=&quot;/js/crash_cheat.js&quot;&gt;&lt;/script&gt;     #页面崩溃欺骗特效</span><br><span class="line">    # - &lt;script src=&quot;xxxx&quot;&gt;&lt;/script&gt;     主题/source/js文件夹中的.js文件</span><br></pre></td></tr></table></figure><h4 id="解决引入js文件时出现inject不生效的问题">解决引入js文件时出现inject不生效的问题</h4><p>在上面一通操作后，再hexo三连，发现页面标题并未出现变化。经过多方查找解决方案，得到如下方法：</p><ol type="1"><li><p>在网站按<code>F12</code>检查，发现控制台报错：<span style="background:#fbd4d0;">Uncaught ReferenceError: $ is not defined</span></p></li><li><p>再次查找，发现原因是没有引用jquery库的jquery.min.js文件</p></li><li><p>引入jquery库：一种是从本地项目路径引用；另一种是通过网页链接引入（<span style="background:#f9eda6;color:red">无论哪种引用库的方式，都要把jquery库的引用放到第一个&lt;s<!--断开script关键字-->cript&gt;引用的前面，这样才能使后面的js文件顺序执行时被成功识别</span>）</p><p>如果我们的项目是https安全域名，那么引入代码为：</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://libs.baidu.com/jquery/2.1.4/jquery.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p>反之：</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;http://libs.baidu.com/jquery/2.1.4/jquery.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> blog搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Butterfly主题优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to write an outline 笔记</title>
      <link href="/2023/11/09/How%20to%20write%20an%20outline%20%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/11/09/How%20to%20write%20an%20outline%20%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="how-to-write-an-outline-笔记">How to write an outline 笔记</h1><h2 id="outline-定义">outline 定义</h2><p>Outline是你的写作地图。写作者使用outline来以一种有逻辑的、清晰的顺序展示思想，并有目的地组织话题及支撑的细节。</p><p>它展示了每一段或每一节将包含的信息，以及它们之间的顺序。outline可以被用来辨别并减少文章中可能存在的缺点或缺少的重点。</p><h2 id="outline-的功能">outline 的功能</h2><p>撰写outline将帮助你集中于眼前的任务，并避免不必要的切题、逻辑谬误和段落的不完善。</p><p>在进行大型研究项目时，很容易混淆资源来源，或者在研究过程中忘记阅读到的内容。为了让你对阅读的内容保持清晰，阅读时可以在页边空白处或另一张纸上记录reverse outline。这个reverse outline可以让你轻松地浏览你的资源，突出显示那些可能有助于你研究的信息。</p><h2 id="outline-的类型和结构">outline 的类型和结构</h2><p><strong>Topic outline</strong> 由简短的短语组成。当您处理的问题可能会以各种不同的方式出现在您的论文中时，这种方法非常有用。</p><p><strong>Sentence outline</strong> 为完整句子。当您的论文侧重于复杂问题的细节时，这种方法非常有用。</p><p>两种outline都遵循严格的格式，使用罗马数字和阿拉伯数字以及字母表中的大写和小写字母。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/654cdb69c458853aefce1aba.png" alt="image-20231109124822463" /><figcaption aria-hidden="true">image-20231109124822463</figcaption></figure><h2 id="outlining-步骤">outlining 步骤</h2><ol type="1"><li><strong>初步工作</strong>：写下主题，然后开始头脑风暴。</li><li><strong>确定研究问题</strong>：研究问题是outline其余部分的重点。试着用一句话或短语来概括你的论文要点。它也是决定论文标题的关键。</li><li><strong>确定主要类别</strong>：你将分析哪些要点？导言（Introduction）描述了你所有的要点；你论文的其余部分可以用来阐述这些要点。</li><li><strong>创建第一个类别</strong>：您要介绍的第一点是什么？如果文章围绕一个复杂的术语展开，那么可以从定义开始。对于一篇涉及特定理论的应用和测验的论文来说，提供该理论的一般背景是一个很好的切入点。</li><li><strong>创建子类别</strong> ：完成这些步骤后，在其下方叙述支撑主要论点的材料。使用的类别数量取决于尝试涵盖的信息量。</li></ol><h2 id="tips">Tips</h2><ul><li><p>在开始outlining之前，需要一个清晰的论文陈述或明确的目的或论点，因为outline中的其他内容都将用于支撑主题。</p></li><li><p>论文陈述应该是一个完整的肯定陈述句，而不是疑问句、短语或从属分句。</p></li><li><p>避免混合类型。与 topic outline 或 sentence outline 保持一致。请勿混合使用两种类型。</p></li><li><p>使用平行。每个标题和副标题都应保持与其他标题平行的结构。最明显的是 "topic" 和 "sentence" 的 outline 格式；平行性也指词性和时态。</p></li><li><p>使信息相互关联协调。第一个主标题所提供的信息应该与第二个主标题所提供的信息同等重要。副标题也是如此。</p></li><li><p>学会划分。每个主标题应该被划分为两个或多个部分。换言之，每个主标题至少应有两个副标题。</p></li><li><p>大写问题。在写outline、主标题和副标题时，几乎总是按照正确的句子大写规则书写。</p></li></ul><h2 id="附在读写中-outline-的重要性">附：在读写中 outline 的重要性</h2><p>Outline can help the paper readers sort out the important and unimportant details, so that they can read more effectively.</p><p>When writing, the authors tend to use brainstorming, a useful tool for generating ideas in a free thinking way. Then outlining can help organize all the generated ideas, and help the authors prioritize the important information and eliminate the trivial details.</p><p>Therefore, although outline is not a part of your paper, it is an important tool to facilitate your academic career.</p><p>参考网址：https://www.xuetangx.com/learn/hfut05021002478/hfut05021002478/16907237/video/36265680</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> course </tag>
            
            <tag> 论文写作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/11/06/hello-world/"/>
      <url>/2023/11/06/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
