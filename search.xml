<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2024/01/23/Diffusion%20Model%20%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2024/01/23/Diffusion%20Model%20%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://zhuanlan.zhihu.com/p/576475987">扩散模型 (Diffusion Model) 简要介绍与源码分析 - 知乎 (zhihu.com)</a></p><p><a href="https://www.zhihu.com/question/545764550">(99+ 封私信 / 81 条消息) 怎么理解今年 CV 比较火的扩散模型（DDPM）？ - 知乎 (zhihu.com)</a></p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DreamAvatar 论文笔记</title>
      <link href="/2024/01/23/DreamAvatar/"/>
      <url>/2024/01/23/DreamAvatar/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.bilibili.com/video/BV1SN411i7d8/?spm_id_from=333.337.search-card.all.click&amp;vd_source=124ec79ebd3e16b0f454a3994a468f98">2023082【跨模态学习驱动的三维理解与生成】韩锴：Text-and-Shape Guided 3D Human Avatar Generation……_哔哩哔哩_bilibili</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> diffusion论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积神经网络</title>
      <link href="/2024/01/23/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/"/>
      <url>/2024/01/23/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/</url>
      
        <content type="html"><![CDATA[<blockquote><p>原文链接：<a href="https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/index.html">6. 卷积神经网络 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p></blockquote><h1 id="从全连接层到卷积">从全连接层到卷积</h1><blockquote><p>本节主要讲述卷积层的原理。卷积的本质是<strong>有效提取相邻像素间的相关特征</strong>。</p><p>之前我们将softmax回归模型和多层感知机模型应用于Fashion-MNIST数据集中的服装图片。 为了能够应用softmax回归和多层感知机，我们首先将每个大小为28×28的图像展平为一个784维的固定长度的一维向量，然后用全连接层对其进行处理。 而现在，若我们掌握了卷积层的处理方法，我们就<strong>可以在图像中保留空间结构</strong>。 同时，用卷积层代替全连接层的另一个好处是：<strong>模型更简洁、所需的参数更少</strong>。</p></blockquote><p>多层感知机十分适合处理表格数据，其中行对应样本，列对应特征。 对于表格数据，我们寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构。 此时，多层感知机可能是最好的选择，然而对于高维感知数据（如图片数据），这种缺少结构的网络可能会变得不实用。</p><p>图像中本就拥有丰富的结构，而这些结构可以被人类和机器学习模型使用。 <strong>卷积神经网络（convolutional neural networks，CNN）是机器学习利用自然图像中一些已知结构的创造性方法</strong>。</p><h2 id="不变性">不变性</h2><p>假设我们想从一张图片中找到某个物体。 合理的假设是：<strong>无论哪种方法找到这个物体，都应该和物体的位置无关</strong>。 不妨假设我们要在人群中寻找小明的位置，由于小明的外表不取决于他所处的位置，因此我们可以使用一个“小明检测器“扫描图像。该检测器将图像分割为多个区域，并为每个区域包含小明的可能性打分。卷积神经网络正是将<strong>空间不变性（spatial invariance）</strong>的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。</p><p>现在，我们将上述想法总结一下，从而帮助我们设计适合于计算机视觉的神经网络架构。</p><ol type="1"><li><p>平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</p></li><li><p>局部性（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</p></li></ol><h2 id="多层感知机的限制">多层感知机的限制</h2><h3 id="平移不变性">平移不变性</h3><h3 id="局部性">局部性</h3><h2 id="卷积">卷积</h2><p>在数学中，两个函数（如<span class="math inline">\(f,g:R^d\rightarrow R\)</span>）之间的“卷积”被定义为 <span class="math display">\[\left( f*g \right) \left( \mathbf{x} \right) =\int{f\left( \mathbf{z} \right)}g\left( \mathbf{x}-\mathbf{z} \right) d\mathbf{z}.\]</span> 也就是说，卷积是当把一个函数“翻转”并移位<span class="math inline">\(\mathbf{x}\)</span>时，测量<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>之间的重叠。 当为离散对象时，积分就变成求和。对于二维张量，则为<span class="math inline">\(f\)</span>的索引(<span class="math inline">\(a\)</span>,<span class="math inline">\(b\)</span>)和<span class="math inline">\(g\)</span>的索引(<span class="math inline">\(i-a\)</span>,<span class="math inline">\(j-b\)</span>)上的对应加和： <span class="math display">\[\left( f*g \right) \left( i,j \right) =\sum_a{\sum_b{f\left( a,b \right) g\left( i-a,j-b \right)}}.\]</span></p><h2 id="通道">通道</h2><p>回到上面的找小明游戏，让我们看看它到底是什么样子。卷积层根据<strong>滤波器</strong><span class="math inline">\(V\)</span> <span style="background:#daf5e9;">（filter, 即卷积核 convolution kernel, 亦或简单地称之为该卷积层的<em>权重</em>，通常该权重是可学习的参数）</span>选取给定大小的窗口，并加权处理图片。我们的目标是学习一个模型，以便探测出“小明”最可能出现的地方。</p><p>然而这种方法有一个问题：我们忽略了图像一般包含三个通道/三种原色（红色、绿色和蓝色）。 实际上，<strong>图像不是二维张量，而是一个由高度、宽度和颜色组成的三维张量</strong>，比如包含1024×1024×3个像素。 前两个轴与像素的空间位置有关，而第三个轴可以看作每个像素的多维表示。因此，我们将输入<span class="math inline">\(X\)</span>索引为<span class="math inline">\([X]_{i,j,k}\)</span>。由此卷积相应地调整为<span class="math inline">\([V]_{a,b,c}\)</span>，而不是<span class="math inline">\([V]_{a,b}\)</span>。</p><p>此外，由于输入图像是三维的，我们的隐藏表示<span class="math inline">\(H\)</span>也最好采用三维张量。 换句话说，对于每一个空间位置，我们想要采用一组而不是一个隐藏表示。这样一组隐藏表示可以想象成一些互相堆叠的二维网格。 因此，我们可以把隐藏表示想象为一系列具有二维张量的<strong>通道（channel）</strong>。 这些通道有时也被称为<strong>特征映射（feature maps）</strong>，因为每个通道都向后续层提供一组空间化的学习特征。 直观上可以想象在靠近输入的底层，一些通道专门识别边缘，而一些通道专门识别纹理。</p><p>为了支持输入<span class="math inline">\(X\)</span>和隐藏表示<span class="math inline">\(H\)</span>中的多个通道，我们可以在<span class="math inline">\(V\)</span>中添加第四个坐标，即<span class="math inline">\([V]_{a,b,c,d}\)</span>。综上所述， <span class="math display">\[[\mathsf{H} ]_{i,j,d}=\sum_{a=-\Delta}^{\Delta}{\sum_{b=-\Delta}^{\Delta}{\sum_c{[}}}\mathsf{V} ]_{a,b,c,d}[\mathsf{X} ]_{i+a,j+b,c},\]</span> 其中隐藏表示<span class="math inline">\(H\)</span>中的索引<span class="math inline">\(d\)</span>表示输出通道，而随后的输出将继续以三维张量<span class="math inline">\(H\)</span>作为输入进入下一个卷积层。 所以，上式可以定义具有多个通道的卷积层，而其中<span class="math inline">\(V\)</span>是该卷积层的权重。</p><h2 id="从全连接层到卷积总结">从全连接层到卷积总结</h2><ul><li>图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。</li><li>局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。</li><li>在图像处理中，卷积层通常比全连接层需要更少的参数，但依旧获得高效用的模型。</li><li>卷积神经网络（CNN）是一类特殊的神经网络，它可以包含多个卷积层。</li><li>多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。</li></ul><h1 id="图像卷积">图像卷积</h1><blockquote><p>由于卷积神经网络的设计是用于探索图像数据，本节以图像为例讲述卷积的实际应用。</p></blockquote><h2 id="互相关运算">互相关运算</h2><p>严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是<em>互相关运算</em>（cross-correlation），而不是卷积运算。 根据<u>第一节</u>中的描述，在卷积层中，输入张量和核张量通过互相关运算产生输出张量。</p><p>首先，我们暂时忽略通道（第三维）这一情况，看看如何处理二维图像数据和隐藏表示。在下图中，输入是高度为3、宽度为3的二维张量（即形状为3×3）。卷积核的高度和宽度都是2，而卷积核窗口（或卷积窗口）的形状由内核的高度和宽度决定（即2×2）。</p><figure><img src="https://zh-v2.d2l.ai/_images/correlation.svg" alt="zh-v2.d2l.ai/_images/correlation.svg" /><figcaption aria-hidden="true">zh-v2.d2l.ai/_images/correlation.svg</figcaption></figure><p>二维互相关运算。阴影部分是第一个输出元素，以及用于计算输出的输入张量元素和核张量元素： <span class="math display">\[0\times0+1\times1+3\times2+4\times3=19\]</span> 在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。 在如上例子中，输出张量的四个元素由二维互相关运算得到，输出高度为2、宽度为2。</p><blockquote><p>注意，输出大小略小于输入大小。这是因为卷积核的宽度和高度大于1， 而卷积核只与图像中每个大小完全适合的位置进行互相关运算。 所以，输出大小等于输入大小<span class="math inline">\(n_h×n_w\)</span>减去卷积核大小<span class="math inline">\(k_h×k_w\)</span>，即： <span class="math display">\[(n_h-k_h+1) \times (n_w-k_w+1).\]</span> 这是因为我们需要足够的空间在图像上“移动”卷积核。稍后，我们将看到如何通过在图像边界周围填充零来保证有足够的空间移动卷积核，从而保持输出大小不变。</p></blockquote><p>接下来，我们在<code>corr2d</code>函数中实现如上过程，该函数接受输入张量<code>X</code>和卷积核张量<code>K</code>，并返回输出张量<code>Y</code>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">X, K</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算二维互相关运算&quot;&quot;&quot;</span></span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i:i + h, j:j + w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;验证上述二维互相关运算的输出&#x27;&#x27;&#x27;</span></span><br><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">K = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">corr2d(X, K)</span><br></pre></td></tr></table></figure><p>tensor([[19., 25.], [37., 43.]])</p><h2 id="卷积层">卷积层</h2><blockquote><p>高度和宽度分别为<span class="math inline">\(ℎ\)</span>和<span class="math inline">\(w\)</span>的卷积核可以被称为<span class="math inline">\(h×w\)</span>卷积或<span class="math inline">\(h×w\)</span>卷积核。 我们也将带有<span class="math inline">\(h×w\)</span>卷积核的卷积层称为<span class="math inline">\(h×w\)</span>卷积层。</p></blockquote><p>卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。 所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。 就像我们之前随机初始化全连接层一样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。</p><p>基于上面定义的<code>corr2d</code>函数实现二维卷积层。在<code>__init__</code>构造函数中，将<code>weight</code>和<code>bias</code>声明为两个模型参数<code>Parameter</code>。前向传播函数调用<code>corr2d</code>函数并添加偏置。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, kernel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#赋初值，nn.Parameter为可更新的参数（requires_grad=True）</span></span><br><span class="line">        self.weight = nn.Parameter(torch.rand(kernel_size))  <span class="comment">#随机初始化</span></span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure><h2 id="图像中目标的边缘检测">图像中目标的边缘检测</h2><p>如下是卷积层的一个简单应用：通过找到像素变化的位置，来检测图像中不同颜色的边缘。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.首先，我们构造一个6×8像素的黑白图像。中间四列为黑色（0），其余像素为白色（1）。</span></span><br><span class="line">X = torch.ones((<span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">X</span></span><br><span class="line"><span class="string">tensor([[1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.接下来，我们构造一个高度为1、宽度为2的卷积核K。当进行互相关运算时，如果水平相邻的两元素相同，则输出为零，否则输出为非零。</span></span><br><span class="line">K = torch.tensor([[<span class="number">1.0</span>, -<span class="number">1.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.现在，我们对参数X（输入）和K（卷积核）执行互相关运算。 如下所示，输出Y中的1代表从白色到黑色的边缘，-1代表从黑色到白色的边缘，其他情况的输出为0。</span></span><br><span class="line">Y = corr2d(X, K)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Y</span></span><br><span class="line"><span class="string">tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.最后，我们将输入的二维图像转置，再进行如上的互相关运算。 其输出如下，之前检测到的垂直边缘消失了。 不出所料，这个卷积核K只可以检测垂直边缘，无法检测水平边缘。</span></span><br><span class="line">corr2d(X.t(), K)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="学习卷积核">学习卷积核</h2><p>如果我们只需寻找黑白边缘，那么以上<code>[1, -1]</code>的边缘检测器足以。然而，当有了更复杂数值的卷积核，或者连续的卷积层时，我们不可能手动设计滤波器。因此考虑是否可以通过<code>仅查看“输入-输出”对</code>来学习由<code>X</code>生成<code>Y</code>的卷积核？</p><p>我们先构造一个卷积层，并将其卷积核初始化为随机张量。接下来，在每次迭代中，我们比较<code>Y</code>与卷积层输出的平方误差，然后计算梯度来更新卷积核。为了简单起见，我们在此使用内置的二维卷积层<code>nn.Conv2d</code>，并忽略偏置。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造一个二维卷积层，它具有1个输入通道、1个输出通道和形状为（1，2）的卷积核</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），</span></span><br><span class="line"><span class="comment"># 其中批量大小和通道数都为1</span></span><br><span class="line">X = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">Y = Y.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>))</span><br><span class="line">lr = <span class="number">3e-2</span>  <span class="comment"># 学习率0.03</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    loss = (Y_hat - Y) ** <span class="number">2</span></span><br><span class="line">    conv2d.zero_grad()</span><br><span class="line">    loss.<span class="built_in">sum</span>().backward()</span><br><span class="line">    <span class="comment"># 迭代卷积核</span></span><br><span class="line">    conv2d.weight.data[:] -= lr * conv2d.weight.grad</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:<span class="comment">#偶数行输出</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l.<span class="built_in">sum</span>():<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>epoch 2, loss 6.422 epoch 4, loss 1.225 epoch 6, loss 0.266 epoch 8, loss 0.070 epoch 10, loss 0.022</p><p>在10次迭代之后，误差已经降到足够低。现在我们来看看我们所学的卷积核的权重张量。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv2d.weight.data.reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>tensor([[ 1.0010, -0.9739]])</p><p>发现我们学习到的卷积核权重非常接近我们之前定义的卷积核<code>K</code>。</p><h2 id="互相关和卷积">互相关和卷积</h2><blockquote><p>为了与深度学习文献中的标准术语保持一致，我们将继续把“互相关运算”称为卷积运算，尽管严格地说，它们略有不同。 此外，对于卷积核张量上的权重，我们称其为<em>元素</em>。</p></blockquote><h2 id="特征映射和感受野">特征映射和感受野</h2><p>如前文所述，输出卷积层有时被称为<strong>特征映射</strong>（feature map），因为它可以被视为一个输入映射到下一层的空间维度的转换器。 在卷积神经网络中，对于某一层的任意元素<span class="math inline">\(x\)</span>，其<strong>感受野</strong>（receptive field）是指在前向传播期间可能影响<span class="math inline">\(x\)</span>计算的所有元素（来自所有先前层）。</p><blockquote><p>注意，感受野可能大于输入的实际大小。</p><p>因此，当一个特征图中的任意元素需要检测更广区域的输入特征时，我们可以构建一个更深的网络。</p></blockquote><h2 id="图像卷积总结">图像卷积总结</h2><ul><li>二维卷积层的核心计算是二维互相关运算。最简单的形式是，对二维输入数据和卷积核执行互相关操作，然后添加一个偏置。</li><li>我们可以设计一个卷积核来检测图像的边缘。</li><li>我们可以从数据中学习卷积核的参数。</li><li>学习卷积核时，无论用严格卷积运算或互相关运算，卷积层的输出不会受太大影响。</li><li>当需要检测输入特征中更广区域时，我们可以构建一个更深的卷积网络。</li></ul><h1 id="填充padding-和步幅stride">填充padding 和步幅stride</h1><p>正如我们在第二节中所概括的那样，假设输入形状为<span class="math inline">\(n_h×n_w\)</span>，卷积核形状为<span class="math inline">\(k_h×k_w\)</span>，那么输出形状将是<span class="math inline">\((n_h-k_h+1)×(n_w-k_w+1)\)</span>。可见，<font color=#4eb434>卷积的输出形状取决于输入形状和卷积核的形状。</font></p><p>还有什么因素会影响输出的大小呢？本节我们将介绍<font color=#985fff><strong>填充</strong>（padding）</font>和<font color=#df8400><strong>步幅</strong>（stride）</font>。假设以下情景： <font color=#985fff>有时，在应用了连续的卷积之后，我们最终得到的输出远小于输入大小。这是由于卷积核的宽度和高度通常大于1所导致的。比如，一个240×240像素的图像，经过10层5×5的卷积后，将减少到200×200像素。如此一来，原始图像的边界丢失了许多有用信息。而<em>填充</em>是解决此问题最有效的方法</font>； <font color=#df8400>有时，我们可能希望大幅降低图像的宽度和高度。例如，如果我们发现原始的输入分辨率十分冗余。<em>步幅</em>则可以在这类情况下提供帮助。</font></p><h2 id="填充">填充</h2><p><strong>填充</strong>（padding）：在输入图像的边界填充元素（通常填充元素是0）。</p><p>例如，在下图中，我们将3×3输入填充到5×5，那么它的输出就增加为4×4。阴影部分是第一个输出元素以及用于输出计算的输入和核张量元素： 0×0+0×1+0×2+0×3=0。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/conv-pad.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">带填充的二维互相关</div></center><p>通常，如果我们添加<span class="math inline">\(p_h\)</span>行填充（大约一半在顶部，一半在底部）和<span class="math inline">\(p_w\)</span>列填充（大约左侧一半，右侧一半），则输出形状将为 <span class="math display">\[(n_h-k_h+1+p_h)\times(n_w-k_w+1+p_w)。\]</span> 这意味着输出的高度和宽度将分别增加<span class="math inline">\(p_h\)</span>和<span class="math inline">\(p_w\)</span>。</p><p><strong>在许多情况下，我们需要设置<span class="math inline">\(p_h=k_h-1\)</span>和<span class="math inline">\(p_w=k_w-1\)</span>，使输入和输出具有相同的高度和宽度。</strong> 这样可以在构建网络时更容易地预测每个图层的输出形状。假设<span class="math inline">\(k_h\)</span>是奇数，我们将在高度的两侧填充<span class="math inline">\(p_h/2\)</span>行。 如果<span class="math inline">\(k_h\)</span>是偶数，则一种可能性是在输入顶部填充<span class="math inline">\(\lceil p_h/2\rceil\)</span>行，在底部填充<span class="math inline">\(\lfloor p_h/2\rfloor\)</span>行。同理，我们填充宽度的两侧。</p><p><span style="background:#daf5e9;">卷积神经网络中卷积核的高度和宽度通常为奇数，例如1、3、5或7。 </span>选择奇数的好处是，保持空间维度的同时，我们可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。</p><p>此外，使用奇数的核大小和填充大小也提供了书写上的便利。对于任何二维张量<code>X</code>，当满足： 1. 卷积核的大小是奇数； 2. 所有边的填充行数和列数相同； 3. 输出与输入具有相同高度和宽度 则可以得出：输出<code>Y[i, j]</code>是通过以输入<code>X[i, j]</code>为中心，与卷积核进行互相关计算得到的。</p><p>在下面的例子中，我们创建一个高度和宽度为3的二维卷积层，并在所有侧边填充1个像素。给定高度和宽度为8的输入，则输出的高度和宽度也是8；<span style="background:#daf5e9;">当卷积核的高度和宽度不同时，我们可以填充不同的高度和宽度，使输出和输入具有相同的高度和宽度。</span>在如下示例中，我们使用高度为5，宽度为3的卷积核，高度和宽度两边的填充分别为2和1。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了方便起见，我们定义了一个计算卷积层的函数。</span></span><br><span class="line"><span class="comment"># 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span><br><span class="line">    <span class="comment"># 这里的（1，1）表示批量大小和通道数都是1</span></span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>) + X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="comment"># 省略前两个维度：批量大小和通道</span></span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([8, 8])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([8, 8])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="步幅">步幅</h2><p>在计算互相关时，卷积窗口从输入张量的左上角开始，向下、向右滑动。 在前面的例子中，我们默认每次滑动一个元素。 但是，有时候<span style="background:#daf5e9;">为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素</span>。</p><p>我们将每次滑动元素的数量称为<strong>步幅（stride）</strong>。到目前为止，我们只使用过高度或宽度为1的步幅，那么如何使用较大的步幅呢？下图是垂直步幅为3，水平步幅为2的二维互相关运算。 着色部分是输出元素以及用于输出计算的输入和内核张量元素：0×0+0×1+1×2+2×3=8、0×0+6×1+0×2+0×3=6。</p><p>可以看到，为了计算输出中第一列的第二个元素和第一行的第二个元素，卷积窗口分别向下滑动三行和向右滑动两列。但是，当卷积窗口继续向右滑动两列时，没有输出，因为输入元素无法填充窗口（除非我们添加另一列填充）。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/conv-stride.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">垂直步幅为3，水平步幅为2的二维互相关运算</div></center><p>通常，当垂直步幅为<span class="math inline">\(s_h\)</span>、水平步幅为<span class="math inline">\(s_w\)</span>时，输出形状为 <span class="math display">\[\lfloor(n_h-k_h+s_h+p_h)/s_h\rfloor \times \lfloor(n_w-k_w+s_w+p_w)/s_w\rfloor.\]</span> <strong>如果我们设置了<span class="math inline">\(p_h=k_h-1\)</span>和<span class="math inline">\(p_w=k_w-1\)</span>，则输出形状将简化为<span class="math inline">\(\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor\)</span>。 更进一步，如果输入的高度和宽度可以被垂直和水平步幅整除，则输出形状将为<span class="math inline">\((n_h/s_h) \times (n_w/s_w)\)</span>。</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们将高度和宽度的步幅设置为2，从而将输入的高度和宽度减半。</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([4, 4])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 看一个稍微复杂的例子</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), stride=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([2, 2])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>为了简洁起见，当输入高度和宽度两侧的填充数量分别为<span class="math inline">\(p_h\)</span>和<span class="math inline">\(p_w\)</span>时，我们称之为填充<span class="math inline">\((p_h, p_w)\)</span>。当<span class="math inline">\(p_h = p_w = p\)</span>时，填充是<span class="math inline">\(p\)</span>。同理，当高度和宽度上的步幅分别为<span class="math inline">\(s_h\)</span>和<span class="math inline">\(s_w\)</span>时，我们称之为步幅<span class="math inline">\((s_h, s_w)\)</span>。特别地，当<span class="math inline">\(s_h = s_w = s\)</span>时，我们称步幅为<span class="math inline">\(s\)</span>。默认情况下，填充为0，步幅为1。在实践中，我们很少使用不一致的步幅或填充，也就是说，我们通常有<span class="math inline">\(p_h = p_w\)</span>和<span class="math inline">\(s_h = s_w\)</span>。</p><h2 id="填充和步幅总结">填充和步幅总结</h2><ul><li>填充可以增加输出的高度和宽度。在许多情况下，我们需要设置<span class="math inline">\(p_h=k_h-1\)</span>和<span class="math inline">\(p_w=k_w-1\)</span>，用来使输出与输入具有相同的高和宽。</li><li>步幅可以减小输出的高和宽，例如在设置了填充的情况下，若输入的高度和宽度可以被步幅s整除，输出的高和宽仅为输入的高和宽的1/s（步幅s是一个大于1的整数）。</li><li>填充和步幅可用于有效地调整输出数据的维度。</li></ul><h1 id="多输入多输出通道">多输入多输出通道</h1><p>虽然我们在第一节中描述了构成每个图像的多个通道和多层卷积层。例如彩色图像具有标准的RGB通道来代表红、绿和蓝。 但是到目前为止，我们仅展示了单个输入和单个输出通道的简化例子。 这使得我们可以将输入、卷积核和输出看作二维张量。</p><p>当我们添加通道时，我们的输入和隐藏的表示都变成了三维张量。例如，每个RGB输入图像具有<span class="math inline">\(3×ℎ×w\)</span>的形状。我们将这个大小为3的轴称为<strong>通道（channel）</strong>维度。本节将更深入地研究具有多输入和多输出通道的卷积核。</p><h2 id="多输入通道">多输入通道</h2><p><span style="background:#daf5e9;">当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便与输入数据进行互相关运算。</span>若输入和卷积核都有<span class="math inline">\(c_i\)</span>个通道，我们可以对每个通道输入的二维张量和卷积核的二维张量进行互相关运算，再对通道求和（将<span class="math inline">\(c_i\)</span>的结果相加）得到二维张量。这是多通道输入和多输入通道卷积核之间进行二维互相关运算的结果。</p><p>在下图中，我们演示了一个具有两个输入通道的二维互相关运算的示例。阴影部分是第一个输出元素以及用于计算这个输出的输入和核张量元素：(1×1+2×2+4×3+5×4)+(0×0+1×1+3×2+4×3)=56。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/conv-multi-in.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">两个输入通道的互相关计算</div></center><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 再来看下具体实现，我们实现一下多输入通道互相关运算。 简而言之，我们所做的就是对每个通道执行互相关操作，然后将结果相加。</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 先遍历“X”和“K”的第0个维度（通道维度），再把它们加在一起</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(d2l.corr2d(x, k) <span class="keyword">for</span> x, k <span class="keyword">in</span> <span class="built_in">zip</span>(X, K))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们可以构造与上图中的值相对应的输入张量X和核张量K，以验证互相关运算的输出。</span></span><br><span class="line">X = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]],</span><br><span class="line">               [[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]]])</span><br><span class="line">K = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]], [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]]])</span><br><span class="line"></span><br><span class="line">corr2d_multi_in(X, K)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 56.,  72.],</span></span><br><span class="line"><span class="string">        [104., 120.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>zip()</strong> 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的zip对象： &gt;&gt;&gt; a = [1,2,3] &gt;&gt;&gt; b = [4,5,6] &gt;&gt;&gt; zipped = zip(a,b) # 返回一个对象 &gt;&gt;&gt; zipped &lt;zip object at 0x103abc288&gt; &gt;&gt;&gt; list(zipped) # list() 转换为列表 [(1, 4), (2, 5), (3, 6)]</p><p>对于zip(args)这个函数，Python还提供了一种逆操作： &gt;&gt;&gt;origin = zip(<em>result) #前面加</em>号，事实上*号也是一个特殊的运算符，叫解包运算符。</p></blockquote><h2 id="多输出通道">多输出通道</h2><p>到目前为止，不论有多少输入通道，我们还只有一个输出通道。然而，正如我们在第一节中所讨论的，每一层有多个输出通道是至关重要的。在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。直观地说，我们可以将每个通道看作对不同特征的响应。而现实可能更为复杂一些，因为<u>每个通道不是独立学习的，而是为了共同使用而优化的</u>。因此，<u>多输出通道并不仅是学习多个单通道的检测器</u>。</p><p>用<span class="math inline">\(c_i\)</span>和<span class="math inline">\(c_o\)</span>分别表示输入和输出通道的数目，并让<span class="math inline">\(k_h\)</span>和<span class="math inline">\(k_w\)</span>为卷积核的高度和宽度。为了获得多个通道的输出，我们可以<span style="background:#daf5e9;">为每个输出通道创建一个形状为<span class="math inline">\(c_i\times k_h\times k_w\)</span>的卷积核张量</span>，这样卷积核的形状是<span class="math inline">\(c_o\times c_i\times k_h\times k_w\)</span>。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如下，我们实现一个计算多个通道的输出的互相关函数。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。</span></span><br><span class="line">    <span class="comment"># 最后将所有结果都叠加在一起</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过将核张量K与K+1（K中每个元素加1）和K+2连接起来，构造了一个具有3个输出通道的卷积核。</span></span><br><span class="line">K = torch.stack((K, K + <span class="number">1</span>, K + <span class="number">2</span>), <span class="number">0</span>)</span><br><span class="line">K.shape</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([3, 2, 2, 2])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面，我们对输入张量X与卷积核张量K执行互相关运算。现在的输出包含3个通道，第一个通道的结果与先前输入张量X和多输入单输出通道的结果一致。</span></span><br><span class="line">corr2d_multi_in_out(X, K)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[ 56.,  72.],</span></span><br><span class="line"><span class="string">         [104., 120.]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 76., 100.],</span></span><br><span class="line"><span class="string">         [148., 172.]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 96., 128.],</span></span><br><span class="line"><span class="string">         [192., 224.]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>输入张量X与具有3个输出通道的卷积核K执行互相关运算，得到输出的过程如下图：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nnn.jpg" alt="nnn" /><figcaption aria-hidden="true">nnn</figcaption></figure></blockquote><h2 id="卷积层-1">1×1卷积层</h2><p>1×1卷积，即<span class="math inline">\(k_h = k_w = 1\)</span>，看起来似乎没有多大意义。 毕竟，<span style="background:#daf5e9;">卷积的本质是有效提取相邻像素间的相关特征</span>，而1×1卷积显然没有此作用。 尽管如此，1×1仍然十分流行，经常包含在复杂深层网络的设计中。下面，让我们详细地解读一下它的实际作用。</p><p>因为使用了最小窗口，1×1卷积失去了卷积层的特有能力——在高度和宽度维度上，识别相邻元素间相互作用的能力。 其实1×1卷积的唯一计算发生在<strong>通道</strong>上。</p><p>下图展示了使用1×1卷积核与3个输入通道和2个输出通道的互相关计算。 这里输入和输出具有相同的高度和宽度，输出中的每个元素都是从输入图像中同一位置的元素的线性组合。 我们可以将1×1卷积层看作在每个像素位置应用的全连接层，以<span class="math inline">\(c_i\)</span>个输入值转换为<span class="math inline">\(c_o\)</span>个输出值。 因为这仍然是一个卷积层，所以跨像素的权重是一致的。 同时，1×1卷积层需要的权重维度为<span class="math inline">\(c_o\times c_i\)</span>，再额外加上一个偏置。</p><blockquote><p>这里的“权重”理解为“卷积核”</p></blockquote><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/conv-1x1.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">互相关计算使用了具有3个输入通道和2个输出通道的1×1卷积核。其中，输入和输出具有相同的高度3和宽度3</div></center><p>下面，我们使用全连接层实现1×1卷积。 请注意，我们需要对输入和输出的数据形状进行调整。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out_1x1</span>(<span class="params">X, K</span>):</span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]</span><br><span class="line">    X = X.reshape((c_i, h * w))</span><br><span class="line">    K = K.reshape((c_o, c_i))</span><br><span class="line">    <span class="comment"># 全连接层中的矩阵乘法</span></span><br><span class="line">    Y = torch.matmul(K, X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape((c_o, h, w))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当执行1×1卷积运算时，上述函数相当于先前实现的互相关函数corr2d_multi_in_out。让我们用一些样本数据来验证这一点。</span></span><br><span class="line">X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">K = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">Y1 = corr2d_multi_in_out_1x1(X, K)</span><br><span class="line">Y2 = corr2d_multi_in_out(X, K)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">float</span>(torch.<span class="built_in">abs</span>(Y1 - Y2).<span class="built_in">sum</span>()) &lt; <span class="number">1e-6</span></span><br></pre></td></tr></table></figure><h2 id="多输入多输出通道总结">多输入多输出通道总结</h2><ul><li>多输入多输出通道可以用来扩展卷积层的模型。</li><li>当以每像素为基础应用时，1×1卷积层相当于全连接层。</li><li>1×1卷积层通常用于调整网络层的通道数量和控制模型复杂性。</li></ul><h1 id="池化层-pooling">池化层 pooling</h1><p>本节将介绍<em>池化</em>（pooling）层，它具有双重目的：<strong>1.降低对空间降采样表示的敏感性</strong>，<font color=#ef042a><strong>2.降低卷积层对位置的敏感性</strong></font>。</p><ol type="1"><li><p>我们的机器学习任务通常会跟<u>全局图像的问题</u>有关（例如，“图像是否包含一只猫呢？”），所以我们<u>最后一层的神经元应该对整个输入的全局敏感</u>。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。因此，当我们处理图像时，我们希望<u>逐渐降低隐藏表示的空间分辨率、聚集信息</u>，这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野（输入）就越大。</p></li><li><p>此外，当检测较底层的特征时（例如第二节中所讨论的边缘），我们通常<u>希望这些特征保持某种程度上的平移不变性</u>。例如，如果我们拍摄黑白之间轮廓清晰的图像<code>X</code>，并将整个图像向右移动一个像素，即<code>Z[i, j] = X[i, j + 1]</code>，则新图像<code>Z</code>的输出可能大不相同。而<u>在现实中，随着拍摄角度的移动，任何物体几乎不可能发生在同一像素上</u>。即使用三脚架拍摄一个静止的物体，由于快门的移动而引起的相机振动，可能会使所有物体左右移动一个像素（除了高端相机配备了特殊功能来解决这个问题）。</p></li></ol><h2 id="最大池化层和平均池化层">最大池化层和平均池化层</h2><p>与卷积层类似，池化层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口（有时称为<strong>池化窗口</strong>）遍历的每个位置计算一个输出。 然而，<span style="background:#daf5e9;">不同于卷积层中的输入与卷积核之间的互相关计算，池化层不包含参数</span>。 相反，池运算是确定性的，我们通常计算池化窗口中所有元素的最大值或平均值。这些操作分别称为<strong>最大池化层</strong>（maximum pooling）和<strong>平均池化层</strong>（average pooling）。</p><p>在这两种情况下，与互相关运算符一样，池化窗口从输入张量的左上角开始，从左往右、从上往下的在输入张量内滑动。在池化窗口到达的每个位置，它计算该窗口中输入子张量的最大值或平均值。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/pooling.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">池化窗口形状为2×2的最大池化层。着色部分是第一个输出元素，以及用于计算这个输出的输入元素:max(0,1,3,4)=4.</div></center><p>池化窗口形状为<span class="math inline">\(p \times q\)</span>的池化层称为<u><span class="math inline">\(p \times q\)</span>池化层</u>，池化操作称为<u><span class="math inline">\(p \times q\)</span>池化</u>。</p><p><span style="background:#daf5e9;">回到本节开头提到的对象边缘检测示例，现在我们将使用卷积层的输出作为2×2最大池化的输入。 设置卷积层输入为<code>X</code>，池化层输出为<code>Y</code>。 无论<code>X[i, j]</code>和<code>X[i, j + 1]</code>的值相同与否，池化层始终输出<code>Y[i, j] = 1</code>。 也就是说，<u>使用2×2最大池化层，即使在高度或宽度上移动一个元素，卷积层仍然可以识别到模式</u>。</span></p><p>在下面的代码中的<code>pool2d</code>函数，我们实现池化层的前向传播。 这类似于第二节中的<code>corr2d</code>函数。 然而，这里我们没有卷积核中的类似参数，输出为输入中每个区域的最大值或平均值。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们可以构建上图中的输入张量X，验证二维最大池化层的输出。</span></span><br><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[4., 5.],</span></span><br><span class="line"><span class="string">        [7., 8.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们还可以验证平均池化层。</span></span><br><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>), <span class="string">&#x27;avg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[2., 3.],</span></span><br><span class="line"><span class="string">        [5., 6.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="填充和步幅">填充和步幅</h2><p>与卷积层一样，池化层也可以改变输出形状。和以前一样，我们可以通过填充和步幅以获得所需的输出形状。 下面，我们用深度学习框架中内置的二维最大池化层，来演示池化层中填充和步幅的使用。 我们首先构造了一个输入张量<code>X</code>，它有四个维度，其中样本数和通道数都是1。</p><blockquote><p>注意！默认情况下，深度学习框架中的步幅与池化窗口的大小相同</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">X</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 0.,  1.,  2.,  3.],</span></span><br><span class="line"><span class="string">          [ 4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="string">          [ 8.,  9., 10., 11.],</span></span><br><span class="line"><span class="string">          [12., 13., 14., 15.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认情况下，深度学习框架中的步幅与池化窗口的大小相同。 因此，如果我们使用形状为(3, 3)的池化窗口，那么默认情况下，我们得到的步幅形状为(3, 3)。</span></span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[10.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 填充和步幅可以手动设定。</span></span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 5.,  7.],</span></span><br><span class="line"><span class="string">          [13., 15.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当然，我们可以设定一个任意大小的矩形池化窗口，并分别设定填充和步幅的高度和宽度。</span></span><br><span class="line">pool2d = nn.MaxPool2d((<span class="number">2</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 5.,  7.],</span></span><br><span class="line"><span class="string">          [13., 15.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="多个通道">多个通道</h2><p>在处理多通道输入数据时，池化层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总。 这意味着<span style="background:#daf5e9;">池化层的输出通道数与输入通道数相同</span>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们在通道维度上连结张量X和X + 1，以构建具有2个通道的输入。</span></span><br><span class="line">X = torch.cat((X, X + <span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">X</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 0.,  1.,  2.,  3.],</span></span><br><span class="line"><span class="string">          [ 4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="string">          [ 8.,  9., 10., 11.],</span></span><br><span class="line"><span class="string">          [12., 13., 14., 15.]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[ 1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">          [ 5.,  6.,  7.,  8.],</span></span><br><span class="line"><span class="string">          [ 9., 10., 11., 12.],</span></span><br><span class="line"><span class="string">          [13., 14., 15., 16.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如下，池化后输出通道的数量仍然是2。</span></span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 5.,  7.],</span></span><br><span class="line"><span class="string">          [13., 15.]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[ 6.,  8.],</span></span><br><span class="line"><span class="string">          [14., 16.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="池化层总结">池化层总结</h2><ul><li>对于给定输入元素，最大池化层会输出该窗口内的最大值，平均池化层会输出该窗口内的平均值。</li><li>池化层的主要优点之一是<strong>减轻卷积层对位置的过度敏感</strong>。</li><li>使用最大池化层以及大于1的步幅，可减少空间维度（如高度和宽度）。</li><li>我们可以指定池化层的填充和步幅。</li><li>池化层的输出通道数与输入通道数相同。</li></ul><h1 id="卷积神经网络-lenet">卷积神经网络 LeNet</h1><p>本节将介绍LeNet，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。 这个模型是由AT&amp;T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），目的是识别图像中的手写数字。 当时，Yann LeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果。</p><p>当时，LeNet取得了与支持向量机（support vector machines）性能相媲美的成果，成为监督学习的主流方法。 LeNet被广泛用于自动取款机（ATM）机中，帮助识别处理支票的数字。 时至今日，一些自动取款机仍在运行Yann LeCun和他的同事Leon Bottou在上世纪90年代写的代码呢！</p><h2 id="lenet">LeNet</h2><p>总体来看，LeNet（LeNet-5）由两个部分组成：</p><ul><li>卷积编码器：由两个卷积层组成;</li><li>全连接层密集块：由三个全连接层组成。</li></ul><p>该架构如下所示：</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/lenet.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">LeNet中的数据流。输入是手写数字，输出为10种可能结果的概率</div></center><p>每个卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均池化层。请注意，虽然ReLU和最大池化层更有效，但它们在20世纪90年代还没有出现。每个卷积层使用5×5卷积核和一个sigmoid激活函数。这些层将输入映射到多个二维特征输出，通常同时增加通道的数量。第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个2×2池操作（步幅2）通过空间下采样将维数即特征图大小减少4倍。卷积的输出形状由批量大小、通道数、高度、宽度决定。</p><p>为了将卷积块的输出传递给全连接层密集块即稠密块，我们必须在小批量中展平每个样本。<span style="background:#daf5e9;">换言之，我们将这个四维输入转换成全连接层所期望的二维输入。</span>这里的二维表示的<span style="background:#FFCC99;">第一个维度索引小批量中的样本，第二个维度给出每个样本的平面向量表示</span>。LeNet的稠密块有三个全连接层，分别有120、84和10个输出。因为我们在执行分类任务，所以输出层的10维对应于最后输出结果的数量。</p><p>通过下面的LeNet代码，可以看出用深度学习框架实现此类模型非常简单。我们只需要实例化一个<code>Sequential</code>块并将需要的层连接在一起。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>我们对原始模型做了一点小改动，去掉了最后一层的高斯激活。除此之外，这个网络与最初的LeNet-5一致。</p><p>下面，我们将一个大小为28×28的单通道（黑白）图像通过LeNet。通过在每一层打印输出的形状，我们可以检查模型，以确保其操作与我们期望的数据流图一致。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/lenet-vert.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">LeNet 的简化版</div></center><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape: \t&#x27;</span>,X.shape)</span><br><span class="line">    </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Conv2d output shape:         torch.Size([1, 6, 28, 28])</span></span><br><span class="line"><span class="string">Sigmoid output shape:        torch.Size([1, 6, 28, 28])</span></span><br><span class="line"><span class="string">AvgPool2d output shape:      torch.Size([1, 6, 14, 14])</span></span><br><span class="line"><span class="string">Conv2d output shape:         torch.Size([1, 16, 10, 10])</span></span><br><span class="line"><span class="string">Sigmoid output shape:        torch.Size([1, 16, 10, 10])</span></span><br><span class="line"><span class="string">AvgPool2d output shape:      torch.Size([1, 16, 5, 5])</span></span><br><span class="line"><span class="string">Flatten output shape:        torch.Size([1, 400])</span></span><br><span class="line"><span class="string">Linear output shape:         torch.Size([1, 120])</span></span><br><span class="line"><span class="string">Sigmoid output shape:        torch.Size([1, 120])</span></span><br><span class="line"><span class="string">Linear output shape:         torch.Size([1, 84])</span></span><br><span class="line"><span class="string">Sigmoid output shape:        torch.Size([1, 84])</span></span><br><span class="line"><span class="string">Linear output shape:         torch.Size([1, 10])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>注意，在整个卷积块中，与上一层相比，每一层特征的高度和宽度都减小了。 第一个卷积层使用2个像素的填充（上下/左右两边就是4个像素），来补偿5×5卷积核导致的特征减少。 相反，第二个卷积层没有填充，因此高度和宽度都减少了4个像素。 随着层叠的上升，通道的数量从输入时的1个，增加到第一个卷积层之后的6个，再到第二个卷积层之后的16个。 同时，每个池化层的高度和宽度都减半。最后，每个全连接层减少维数，最终输出一个维数与结果分类数相匹配的输出。</p><h2 id="模型训练">模型训练</h2><p>现在我们已经实现了LeNet，让我们看看LeNet在Fashion-MNIST数据集上的表现。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br></pre></td></tr></table></figure><p>虽然卷积神经网络的参数较少，但与深度的多层感知机相比，它们的计算成本仍然很高，因为每个参数都参与更多的乘法。 通过使用GPU，可以用它加快训练。</p><p>为了进行评估，我们需要对之前描述的<code>evaluate_accuracy</code>函数进行轻微的修改。 由于完整的数据集位于内存中，因此在模型使用GPU计算数据集之前，我们需要将其复制到显存中。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 设置为评估模式</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    <span class="comment"># 正确预测的数量，总预测的数量</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">                <span class="comment"># BERT微调所需的（之后将介绍）</span></span><br><span class="line">                X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                X = X.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            metric.add(d2l.accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>为了使用GPU，我们还需要一点小改动。 与之前定义的<code>train_epoch_ch3</code>不同，在进行正向和反向传播之前，我们需要将每一小批量数据移动到我们指定的设备（例如GPU）上。</p><p>如下所示，训练函数<code>train_ch6</code>也类似于之前定义的<code>train_ch3</code>。 由于我们将实现多层神经网络，因此我们将主要使用高级API。 以下训练函数假定从高级API创建的模型作为输入，并进行相应的优化。 我们使用在之前介绍的Xavier随机初始化模型参数。 与全连接层一样，我们使用交叉熵损失函数和小批量随机梯度下降。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch6</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用GPU训练模型(在第六章定义)&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)<span class="comment">#小批量随机梯度下降</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()<span class="comment">#交叉熵损失函数</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                            legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># 训练损失之和，训练准确率之和，样本数</span></span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()</span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>现在，我们训练和评估LeNet-5模型。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">loss 0.469, train acc 0.823, test acc 0.779</span></span><br><span class="line"><span class="string">55296.6 examples/sec on cuda:0</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><figure><img src="https://zh-v2.d2l.ai/_images/output_lenet_4a2e9e_67_1.svg" alt="zh-v2.d2l.ai/_images/output_lenet_4a2e9e_67_1.svg" /><figcaption aria-hidden="true">zh-v2.d2l.ai/_images/output_lenet_4a2e9e_67_1.svg</figcaption></figure><h2 id="卷积神经网络-lenet总结">卷积神经网络 LeNet总结</h2><ul><li>卷积神经网络（CNN）是一类使用卷积层的网络。</li><li>在卷积神经网络中，我们组合使用卷积层、非线性激活函数和池化层。</li><li>为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。</li><li>在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。</li><li>LeNet是最早发布的卷积神经网络之一。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CBAM注意力机制</title>
      <link href="/2024/01/23/CBAM%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
      <url>/2024/01/23/CBAM%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文：《CBAM：Convolutional Block Attention Module》</p><p>论文参考样式：Woo S, Park J, Lee J Y, et al. Cbam: Convolutional block attention module[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 3-19.</p><p>论文链接：<a href="https://arxiv.org/pdf/1807.06521.pdf">CBAM：Convolutional Block Attention Module</a></p><p>demo: <a href="https://github.com/Jongchan/attention-module">GitHub - Jongchan/attention-module: Official PyTorch code for "BAM: Bottleneck Attention Module (BMVC2018)" and "CBAM: Convolutional Block Attention Module (ECCV2018)"</a></p></blockquote><blockquote><p>参考网址：</p><p><a href="https://zhuanlan.zhihu.com/p/101590167">CBAM：卷积注意力机制模块 - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/Roaddd/article/details/114646354">【注意力机制】CBAM详解（文末附代码）_cbam注意力-CSDN博客</a></p><p><a href="https://blog.csdn.net/m0_45447650/article/details/123983483">CBAM——即插即用的注意力模块（附代码）_cbam模块-CSDN博客</a></p></blockquote><h1 id="摘要">摘要</h1><p>本文（2018，ECCV）提出了卷积注意力模块——CBAM，这是一种用于前馈卷积神经网络的轻量级的注意力模块。 给定一个中间特征图，CBAM模块会沿着两个独立的维度（通道和空间）依次推断注意力图，然后将注意力图与输入特征图相乘以进行自适应特征优化。 由于CBAM是轻量级的通用模块，因此可以忽略的该模块的开销而将其无缝集成到任何CNN架构中，并且可以与基础CNN一起进行端到端训练。</p><p>论文在 ResNet 和 MobileNet 等经典结构上添加了 CBAM 模块并进行对比分析实验，同时也进行了CAM可视化，发现 CBAM 更关注识别目标物体，这也使得 CBAM 具有更好的解释性。本文验证所用的数据集有 ImageNet-1K，MS COCO检测和VOC 2007检测数据集。 实验表明，使用该模块在各种模型上，并在分类和检测性能方面的持续改进，证明了CBAM的广泛适用性。</p><blockquote><p>关于CAM可视化：<a href="https://aistudio.baidu.com/projectdetail/1655497">一文搞懂卷积网络之五（注意力可视化Grad-CAM） - 飞桨AI Studio星河社区 (baidu.com)</a></p></blockquote><h1 id="模型">模型</h1><p>CBAM模型结构如下所示：</p><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240109153927099.png" alt="image-20240109153927099" /><figcaption aria-hidden="true">image-20240109153927099</figcaption></figure><p>Convolutional Block Attention Module (CBAM) 表示卷积模块的注意力机制模块，可以看到 CBAM 包含2个独立的子模块：<font color=#4eb434>通道注意力模块（CAM，Channel Attention Module)</font> 和<font color=#985fff>空间注意力模块（SAM，Spartial Attention Module)</font> ，分别进行通道和空间的Attention。这样不只能够节约参数和计算力，并且保证了其能够做为即插即用的模块集成到现有的网络架构中去。相比于SENet 只关注<font color=#4eb434>通道（channel）</font>的注意力机制可以取得更好的效果。</p><blockquote><p>通道上的 Attention 机制在 2017 年的 SENet 就被提出，SENet可以参考<a href="https://blog.csdn.net/Roaddd/article/details/111357490">这篇文章</a>。事实上，CAM 与 SENet 相比，只是多了一个并行的 Max Pooling 层，提取到的高层特征更全面，更丰富。至于为何如此更改，论文也给出了解释和实验数据支持。</p></blockquote><p>由上图，CBAM由四个模块组成，分别是：输入特征、通道注意力模块、空间注意力模块和输出的精制特征图。整体流程大致描述如下</p><ol type="1"><li>输入特征<span class="math inline">\(F\in R^{C\times H\times W}\)</span>会先通过一个通道注意力模块，进行一维卷积<span class="math inline">\(M_c\in R^{C\times 1\times 1}\)</span>，将卷积结果乘原特征图<span class="math inline">\(F\)</span>得到加权结果<span class="math inline">\(F&#39;\)</span>。</li><li><span class="math inline">\(F&#39;\)</span>会再经过一个空间注意力模块，进行二维卷积<span class="math inline">\(M_s\in R^{1\times H\times W}\)</span>，再次将卷积结果与<span class="math inline">\(F&#39;\)</span>相乘，最终得到加权结果<span class="math inline">\(F&#39;&#39;\)</span>。</li></ol><p>用公式表示为 <span class="math display">\[F&#39;=M_c\left( F \right) \otimes F,\]</span> <span class="math display">\[F&#39;&#39;=M_s\left( F&#39; \right) \otimes F&#39;,\]</span></p><h2 id="channel-attention-modulecam">Channel Attention Module（CAM）</h2><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240109155441087.png" alt="image-20240109155441087" /><figcaption aria-hidden="true">image-20240109155441087</figcaption></figure><blockquote><p><strong>通道注意力模块</strong>：<strong>通道维度不变，压缩空间维度</strong>。该模块关注输入图片中<strong>有意义的信息</strong>(分类任务就关注因为什么分成了不同类别)。</p></blockquote><p>通道注意力模块CAM如上图所示。</p><ol type="1"><li>将输入的特征图feature map <span style="background:#dad5e9;">F</span>，分别经过并行的基于width和height的global max pooling 和global average pooling，将特征图从C×H×W变为C×1×1的大小，然后分别经过MLP。</li><li>在MLP中，它先将通道数压缩为原来的1/r（Reduction，减少率）倍，再扩张到原通道数，经过ReLU激活函数得到两个激活后的结果。</li><li>将MLP输出的两个结果进行基于element-wise的加和操作（即逐元素相加），再经过sigmoid激活操作，生成最终的channel attention feature-map <span style="background:#dad5e9;">M_c</span>。</li><li>将该channel attention feature-map和原来的input feature-map做element-wise乘法操作（乘完后变回C×H×W的大小），生成Spatial attention模块需要的输入特征 <span style="background:#dad5e9;">F'</span>。</li></ol><p>以上是通道注意力机制的步骤。</p><p>换一个角度考虑，通道注意力机制（Channel Attention Module）是将特征图在空间维度上进行压缩，得到一个一维矢量后再进行操作。在空间维度上进行压缩时，不仅考虑到了平均值池化（Average Pooling）还考虑了最大值池化（Max Pooling）。平均池化和最大池化可用来聚合特征映射的空间信息，送到一个共享网络，压缩输入特征图的空间维数，逐元素求和合并，以产生通道注意力图。单就一张图来说，通道注意力，关注的是这张图上哪些内容是有重要作用的。<u>平均值池化对特征图上的每一个像素点都有反馈，而最大值池化在进行梯度反向传播计算时，只有特征图中响应最大的地方有梯度的反馈。</u>通道注意力机制可以表达为： <span class="math display">\[\begin{aligned}    M_c\left( F \right) &amp;=\sigma \left( MLP\left( AvgPool\left( F \right) \right) +MLP\left( MaxPool\left( F \right) \right) \right)\\    &amp;=\sigma \left( W_1\left( W_0\left( F_{avg}^{c} \right) \right) +W_1\left( W_0\left( F_{\max}^{c} \right) \right) \right)\\\end{aligned}\]</span></p><blockquote><p>在channel attention中，表1对于pooling的使用进行了实验对比，发现avg &amp; max的并行池化的效果要更好。这里也有可能是池化丢失的信息太多，avg&amp;max的并行连接方式比单一的池化丢失的信息更少，所以效果会更好一点。</p></blockquote><h2 id="spatial-attention-modulesam">Spatial Attention Module（SAM）</h2><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240109163244599.png" alt="image-20240109163244599" /><figcaption aria-hidden="true">image-20240109163244599</figcaption></figure><blockquote><p><strong>空间注意力模块</strong>：<strong>空间维度不变，压缩通道维度</strong>。该模块关注的是<strong>目标的位置信息</strong>。</p></blockquote><p>空间注意力模块如上图所示。将Channel attention模块输出的特征图 <span style="background:#dad5e9;">F‘</span> 作为本模块的输入特征图。首先做一个基于channel的global max pooling 和global average pooling，得到两个1×H×W 的特征图，然后将这2个特征图基于channel 做concat操作（通道拼接）。然后经过一个7×7卷积操作（7×7比3×3效果要好），降维为1个channel的特征图，即1×H×W。再经过sigmoid生成spatial attention feature <span style="background:#dad5e9;">M_s</span>。最后将该feature和该模块的输入feature做乘法（乘之后变回C×H×W的大小），得到最终生成的特征 <span style="background:#dad5e9;">F’‘</span>。</p><p>同样，空间注意力机制（Spatial Attention Module）是对通道进行压缩，在通道维度分别进行了平均值池化和最大值池化。MaxPool的操作就是在通道上提取最大值，提取的次数是高乘以宽；AvgPool的操作就是在通道上提取平均值，提取的次数也是是高乘以宽；接着将前面所提取到的特征图（通道数都为1）合并得到一个2通道的特征图。空间注意力机制可以表达为： <span class="math display">\[\begin{aligned}    M_s\left( F \right) &amp;=\sigma \left( f^{7\times 7}\left( \left[ AvgPool\left( F \right) ;MaxPool\left( F \right) \right] \right) \right)\\    &amp;=\sigma \left( f^{7\times 7}\left( \left[ F_{avg}^{s};F_{\max}^{s} \right] \right) \right)\\\end{aligned}\]</span> 其中，σ 为sigmoid操作，7×7表示卷积核的大小，7×7的卷积核比3×3的卷积核效果更好。</p><h1 id="实验">实验</h1><p>本文中，进行了较多的对比实验，旨在验证注意力模块的积极作用。</p><p>首先，对比了通道、空间以及通道&amp;空间，不同注意力机制的效果。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240109164403936.png"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">仅使用Channel Attention，对比的是使用AvgPool、MaxPool以及都使用时的性能</div></center><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240109170246533.png"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">仅使用Spatial Attention，对比的是不同Avg，Max以及kernel_size的性能差异</div></center><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240109170551461.png"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">CBAM模块，对比的是不同顺序的性能差异</div></center><p>关于图3，通道注意力和空间注意力这两个模块能够以并行或者串行顺序的方式组合在一块儿，关于通道和空间上的串行顺序和并行作者进行了实验对比，可以看到的是，先使用Channel（AvgPool&amp;MaxPool），再使用Spatial（avg&amp;max，k=7）——即先通道后空间的性能是最优的。</p><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240109170744084.png" alt="image-20240109170744084" /><figcaption aria-hidden="true">image-20240109170744084</figcaption></figure><p>上图给出了ImageNet-1K数据集上，训练误差的曲线。同样的证明了，CBAM模块在训练集合验证集上，相比于Baseline和SE注意力机制，都有一定的提升。</p><p>最后，是使用Grad-CAM进行了可视化，以来证明CBAM是真正地提取出了积极有效的特征：利用 Grad-CAM 对不一样的网络进行可视化后，能够发现，引入 CBAM 后，特征覆盖到了待识别物体的更多部位，而且最终判别物体的几率也更高，这代表注意力机制的确让网络学会了关注重点信息。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240109171005799.png"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">CBAM，SE，Baseline等最后一层卷积层输出使用 grad-CAM 进行可视化的对比图</div></center><h1 id="总结">总结</h1><ol type="1"><li><p>论文提出了一种基于注意力机制的轻量型结构 CBAM ，基本上可以添加到所有常规的卷积层中。</p></li><li><p>文中验证了 Channel Attention Module 中 avg 与 max 并行的方式最好，接下来通过实验验证了 Channel Attention Module 和 Spatial Attention Module 的最佳先后顺序是先通道后空间，还对比了 CBAM 与 SENet 的性能。</p></li><li><p>文章还在实验中应用grad-CAM可视化了 CBAM 的关注区域（在图像分类任务中可以观察feature map的特征，解释了为什么模型将原图分类到某一类的结果），使得 CBAM 具有更好的解释性。</p></li><li><p>加入CBAM模块不一定会给网络带来性能上的提升，受自身网络还有数据等其他因素影响，甚至会下降。如果网络模型的泛化能力已经很强，而你的数据集不是benchmarks而是自己采集的数据集的话，不建议加入CBAM模块。CBAM性能虽然改进的比SE高了不少，但绝不是无脑加入到网络里就能有提升的。也要根据自己的数据、网络等因素综合考量。</p></li></ol><h1 id="代码">代码</h1>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>认识 torch.nn</title>
      <link href="/2024/01/23/%E8%AE%A4%E8%AF%86%20torch.nn/"/>
      <url>/2024/01/23/%E8%AE%A4%E8%AF%86%20torch.nn/</url>
      
        <content type="html"><![CDATA[<blockquote><p>搬运链接：[<a href="https://blog.csdn.net/HiWangWenBing/article/details/120614234">Pytorch系列-30]：神经网络基础 - torch.nn库五大基本功能：nn.Parameter、nn.Linear、nn.functioinal、nn.Module、nn.Sequentia-CSDN博客</a></p><p>官方链接1：<a href="https://pytorch.org/docs/1.2.0/">PyTorch documentation — PyTorch master documentation</a></p><p>官方链接2：<a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#torchnn">torch.nn - PyTorch中文文档 (pytorch-cn.readthedocs.io)</a></p></blockquote><blockquote><p>nn是Neural Network的简称，帮助程序员方便执行如下的与神经网络相关的行为：创建、训练、保存和恢复神经网络。</p></blockquote><h1 id="一torch.nn简介">一、torch.nn简介</h1><h2 id="相关库的导入">相关库的导入</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#环境准备</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np              <span class="comment"># numpy数组库</span></span><br><span class="line"><span class="keyword">import</span> math                     <span class="comment"># 数学运算库</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 画图库</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> torch             <span class="comment"># torch基础库</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn    <span class="comment"># torch神经网络库</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><h1 id="二nn.linear类全连接层">二、nn.Linear类（全连接层）</h1><blockquote><p>nn.Linear本身并不包含激活函数（激活函数在Functional中)</p></blockquote><h2 id="函数说明">函数说明</h2><figure><img src="https://img-blog.csdnimg.cn/20191102164419608.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyMDc5Njg5,size_16,color_FFFFFF,t_70#pic_center" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><ul><li><strong>in_features</strong>: 输入的二维张量的大小。Y = WX + b, X的维度就是in_features，X的维度决定的W的维度， 总的参数个数 = in_features + 1。</li><li><strong>out_features：</strong>输出的二维张量的大小。</li></ul><p>从输入输出的张量的shape角度来理解，相当于一个输入为[batch_size, in_features]的张量变换成了[batch_size, out_features]的输出张量。</p><h2 id="使用nn.linear类创建全连接层">使用nn.Linear类创建全连接层</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nn.Linear</span></span><br><span class="line"><span class="comment"># 建立单层的多输入、多输出全连接层</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># in_features由输入张量的形状决定，out_features则决定了输出张量的形状</span></span><br><span class="line">full_connect_layer = nn.Linear(in_features=<span class="number">64</span> * <span class="number">64</span> * <span class="number">3</span>, out_features=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;full_connect_layer:&quot;</span>, full_connect_layer)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;parameters        :&quot;</span>, full_connect_layer.parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假定输入的图像形状为[64,64,3]</span></span><br><span class="line">x_input = torch.randn(<span class="number">1</span>, <span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>) <span class="comment">#或 x_input = x_input.view(1, -1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将四维张量转换为二维张量之后，才能作为全连接层的输入</span></span><br><span class="line">x_input = x_input.view(<span class="number">1</span>, <span class="number">64</span> * <span class="number">64</span> * <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input.shape:&quot;</span>, x_input.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用全连接层</span></span><br><span class="line">y_output = full_connect_layer(x_input)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_output.shape:&quot;</span>, y_output.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_output:&quot;</span>, y_output)</span><br></pre></td></tr></table></figure><p>full_connect_layer: Linear(in_features=12288, out_features=2, bias=True) parameters : &lt;bound method Module.parameters of Linear(in_features=12288, out_features=2, bias=True)&gt; x_input.shape: torch.Size([1, 12288]) y_output.shape: torch.Size([1, 2]) y_output: tensor([[-0.5883, -0.4527]], grad_fn=<AddmmBackward>)</p><h1 id="三nn.functional常见函数">三、nn.functional（常见函数）</h1><h2 id="nn.functional-概述">nn.functional 概述</h2><p>nn.functional（通常按惯例导入到 F 命名空间中）定义了创建神经网络所需要的一些常见的处理函数。如<strong>激活函数</strong>、<strong>损失函数</strong>、<strong>正则化函数</strong>和<strong>非状态（non-stateful）版本的层（如卷积层和线性层）</strong>等。</p><figure><img src="https://img-blog.csdnimg.cn/20211005202351328.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_14,color_FFFFFF,t_70,g_se,x_16" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h1 id="四nn.xxx和nn.functional.xxx比较">四、nn.Xxx和nn.functional.xxx比较</h1><h2 id="相同点">相同点</h2><p><code>nn.Xxx</code>和<code>nn.functional.xxx</code>的实际功能是相同的，即<code>nn.Conv2d</code>和<code>nn.functional.conv2d</code> 都是进行卷积，<code>nn.Dropout</code> 和<code>nn.functional.dropout</code>都是进行dropout，。。。。。；</p><ul><li>运行效率也是近乎相同。</li></ul><h2 id="不同点">不同点</h2><ul><li>形式看：<code>nn.functional.xxx</code>是<strong>小写</strong>字母开头，nn.Xxx中的函数是<strong>大写</strong>字母开头。</li><li><code>nn.functional.xxx</code>是API函数接口，而<code>nn.Xxx</code>是对原始API函数<code>nn.functional.xxx</code>的<strong>类封装</strong>。</li><li>所有<strong><code>nn.Xxx</code>都继承于于共同祖先<code>nn.Module</code>。</strong>这一点导致<code>nn.Xxx</code>除了具有<code>nn.functional.xxx</code>功能之外，内部附带了<code>nn.Module</code>相关的属性和方法，例如<code>train(), eval(),load_state_dict, state_dict</code>等。</li><li><code>nn.Xxx</code>继承于<code>nn.Module</code>， 能够很好的与<code>nn.Sequential</code>结合使用， 而<code>nn.functional.xxx</code>无法与<code>nn.Sequential</code>结合使用。</li><li><code>nn.Xxx</code> 需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据。<code>nn.functional.xxx</code>同时传入输入数据和weight, bias等其他参数 。</li><li><code>nn.Xxx</code>不需要你自己定义和管理weight；而<code>nn.functional.xxx</code>需要你自己定义weight，每次调用的时候都需要手动传入weight, 不利于代码复用。</li></ul><h1 id="五nn.parameter类">五、nn.Parameter类</h1><h2 id="nn.parameter概述">nn.Parameter概述</h2><p><code>Parameter</code>也是<code>Tensor</code>（或者说Tensor的包装器wrapper），也就是说是一个多维矩阵，是Variable类中的一个特殊子类。它告诉 <code>Module</code> 它具有在反向传播期间需要更新的权重。 只更新具有 <code>requires_grad</code> 属性的 <code>tensor</code>。</p><p>当我们创建一个Module时，nn会自动创建相应的参数parameter，并会自动累加到模型的Parameter成员列表中。</p><h4 id="语法">语法</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.nn.parameter.Parameter(data=<span class="literal">None</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li>data (Tensor) – parameter tensor. —— 输入得是一个<code>tensor</code></li><li>requires_grad (bool, optional) – if the parameter requires gradient. See Locally disabling gradient computation for more details. <strong>Default: True</strong> —— 这个不用解释，<strong>需要注意的是<code>nn.Parameter()</code>默认有梯度</strong>。</li></ul><p><code>torch.nn.Parameter()</code>将一个不可训练的tensor转换成可以训练的类型parameter，并将这个parameter绑定到这个module里面。即在定义网络时这个tensor就是一个可以训练的参数了。使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。</p><figure><img src="https://img-blog.csdnimg.cn/20211005202810135.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_13,color_FFFFFF,t_70,g_se,x_16" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="单个全连接层中参数的个数">单个全连接层中参数的个数</h2><blockquote><p><strong>in_features的数量，决定的参数的个数 Y = WX + b, X的维度就是in_features，X的维度决定的W的维度， 总的参数个数 = in_features + 1</strong></p><p><strong>out_features的数量，决定了全连接层中神经元的个数，因为每个神经元只有一个输出。</strong></p><p>多少个输出，就需要多少个神经元 (一个 WX + b 就是一个神经元)！</p></blockquote><p>总的W参数的个数= <strong>in_features * out_features</strong></p><p>总的b参数的个数= <strong>1 * out_features</strong></p><p>总的参数（W和B）的个数= (<strong>in_features + 1) * out_features</strong></p><h2 id="使用参数创建全连接层代码例子">使用参数创建全连接层代码例子</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nn.functional.linear( )</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x_input = torch.Tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input.shape:&quot;</span>, x_input.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input      :&quot;</span>, x_input)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">Weights1 = nn.Parameter(torch.rand(<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights.shape:&quot;</span>, Weights1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights      :&quot;</span>, Weights1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">Bias1 = nn.Parameter(torch.rand(<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bias.shape:&quot;</span>, Bias1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bias      :&quot;</span>, Bias1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">Weights2 = nn.Parameter(torch.Tensor(<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights.shape:&quot;</span>, Weights2.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights      :&quot;</span>, Weights2)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nfull_connect_layer&quot;</span>)</span><br><span class="line">full_connect_layer = nn.functional.linear(x_input, Weights1)</span><br><span class="line"><span class="built_in">print</span>(full_connect_layer)</span><br></pre></td></tr></table></figure><p>x_input.shape: torch.Size([3]) x_input : tensor([1., 1., 1.])</p><p>Weights.shape: torch.Size([3]) Weights : Parameter containing: tensor([0.8948, 0.0114, 0.3688], requires_grad=True)</p><p>Bias.shape: torch.Size([1]) Bias : Parameter containing: tensor([0.8087], requires_grad=True)</p><p>Weights.shape: torch.Size([3]) Weights : Parameter containing: tensor([1.4013e-45, 0.0000e+00, 0.0000e+00], requires_grad=True)</p><p>full_connect_layer tensor(1.2750, grad_fn=<DotBackward>)</p><h1 id="六nn.mudule类">六、nn.Mudule类</h1><p>创建一个可调用的对象，其行为类似于一个函数，但也可以包含状态（例如神经网络层权重）。 它知道它包含哪些参数，并且可以将所有梯度归零，循环遍历它们更新权重等。</p><figure><img src="https://img-blog.csdnimg.cn/2021100520265815.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_14,color_FFFFFF,t_70,g_se,x_16" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h1 id="七利用nn.sequential类创建神经网络继承于nn.module类">七、利用nn.Sequential类创建神经网络（继承于nn.Module类）</h1><p>nn.Sequential是一个有序的容器，该类将按照传入构造器的顺序，依次创建相应的函数，并记录在Sequential类对象的数据结构中。同时，以神经网络模块为元素的有序字典也可以作为传入参数。</p><p>因此，Sequential可以看成是有多个函数运算对象，串联成的神经网络，其返回的是Module类型的神经网络对象。</p><h2 id="以列表的形式串联函数运算构建串行执行的神经网络">以列表的形式，串联函数运算，构建串行执行的神经网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;利用系统提供的神经网络模型类：Sequential,以参数列表的方式来实例化神经网络模型对象&quot;</span>)</span><br><span class="line"><span class="comment"># A sequential container. Modules will be added to it in the order they are passed in the constructor.</span></span><br><span class="line"><span class="comment"># Example of using Sequential</span></span><br><span class="line">model_c = nn.Sequential(nn.Linear(<span class="number">28</span> * <span class="number">28</span>, <span class="number">32</span>), </span><br><span class="line">                        nn.ReLU(), </span><br><span class="line">                        nn.Linear(<span class="number">32</span>, <span class="number">10</span>), </span><br><span class="line">                        nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">                       )</span><br><span class="line"><span class="built_in">print</span>(model_c)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n显示网络模型参数&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_c.parameters)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n定义神经网络样本输入&quot;</span>)</span><br><span class="line">x_input = torch.randn(<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line">x_input = x_input.view(x_input.size()[<span class="number">0</span>], -<span class="number">1</span>)  <span class="comment">#将输入转为二维张量</span></span><br><span class="line"><span class="built_in">print</span>(x_input.size())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用神经网络进行预测&quot;</span>)</span><br><span class="line">y_pred = model_c.forward(x_input)</span><br><span class="line"><span class="built_in">print</span>(y_pred)  <span class="comment"># torch.Size([2, 10])</span></span><br></pre></td></tr></table></figure><p>利用系统提供的神经网络模型类：Sequential,以参数列表的方式来实例化神经网络模型对象 Sequential( (0): Linear(in_features=784, out_features=32, bias=True) (1): ReLU() (2): Linear(in_features=32, out_features=10, bias=True) (3): Softmax(dim=1) )</p><p>显示网络模型参数 &lt;bound method Module.parameters of Sequential( (0): Linear(in_features=784, out_features=32, bias=True) (1): ReLU() (2): Linear(in_features=32, out_features=10, bias=True) (3): Softmax(dim=1) )&gt;</p><p>定义神经网络样本输入 torch.Size([2, 28, 28, 1]) torch.Size([2, 784])</p><p>使用神经网络进行预测 tensor([[0.0773, 0.0843, 0.1366, 0.0933, 0.1107, 0.1086, 0.0721, 0.1129, 0.1267, 0.0777], [0.0875, 0.0570, 0.1179, 0.0981, 0.1261, 0.1177, 0.1174, 0.0735, 0.0932, 0.1116]], grad_fn=<SoftmaxBackward>)</p><h2 id="以字典的形式串联函数运算构建串行执行的神经网络">以字典的形式，串联函数运算，构建串行执行的神经网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;利用系统提供的神经网络模型类：Sequential,以字典的方式来实例化神经网络模型对象&quot;</span>)</span><br><span class="line">model = nn.Sequential(OrderedDict([(<span class="string">&#x27;h1&#x27;</span>, nn.Linear(<span class="number">28</span> * <span class="number">28</span>, <span class="number">32</span>)),</span><br><span class="line">                                   (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">                                   (<span class="string">&#x27;out&#x27;</span>, nn.Linear(<span class="number">32</span>, <span class="number">10</span>)),</span><br><span class="line">                                   (<span class="string">&#x27;softmax&#x27;</span>, nn.Softmax(dim=<span class="number">1</span>))])</span><br><span class="line">                     )</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n显示网络模型参数&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model.parameters)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n定义神经网络样本输入&quot;</span>)</span><br><span class="line">x_input = torch.randn(<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用神经网络进行预测&quot;</span>)</span><br><span class="line">y_pred = model.forward(x_input.view(x_input.size()[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(y_pred)   <span class="comment"># torch.Size([2, 10])</span></span><br></pre></td></tr></table></figure><p>利用系统提供的神经网络模型类：Sequential,以字典的方式来实例化神经网络模型对象 Sequential( (h1): Linear(in_features=784, out_features=32, bias=True) (relu1): ReLU() (out): Linear(in_features=32, out_features=10, bias=True) (softmax): Softmax(dim=1) )</p><p>显示网络模型参数 &lt;bound method Module.parameters of Sequential( (h1): Linear(in_features=784, out_features=32, bias=True) (relu1): ReLU() (out): Linear(in_features=32, out_features=10, bias=True) (softmax): Softmax(dim=1) )&gt;</p><p>定义神经网络样本输入 torch.Size([2, 28, 28, 1])</p><p>使用神经网络进行预测 tensor([[0.0967, 0.1006, 0.0714, 0.0752, 0.1209, 0.1088, 0.1215, 0.1156, 0.0879, 0.1015], [0.0912, 0.1031, 0.0914, 0.0962, 0.1124, 0.0999, 0.0885, 0.1082, 0.0953, 0.1137]], grad_fn=<SoftmaxBackward>)</p><h1 id="八-自定义神经网络模型类继承于module类">八、 自定义神经网络模型类（继承于Module类）</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义网络模型：带relu的两层全连接神经网络</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;自定义新的神经网络模型的类&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">#原始写法</span></span><br><span class="line"><span class="string">class NetC(torch.nn.Module):</span></span><br><span class="line"><span class="string">    # 定义神经网络</span></span><br><span class="line"><span class="string">    def __init__(self, n_feature, n_hidden, n_output):</span></span><br><span class="line"><span class="string">        super(NetC, self).__init__()</span></span><br><span class="line"><span class="string">        self.h1 = nn.Linear(n_feature, n_hidden)  #subModule: Linear</span></span><br><span class="line"><span class="string">        self.relu1 = nn.ReLU()</span></span><br><span class="line"><span class="string">        self.out = nn.Linear(n_hidden, n_output)</span></span><br><span class="line"><span class="string">        self.softmax = nn.Softmax(dim=1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # 定义前向运算</span></span><br><span class="line"><span class="string">    def forward(self, x_input):</span></span><br><span class="line"><span class="string">        h1 = self.h1(x_input)</span></span><br><span class="line"><span class="string">        a1 = self.relu1(h1)</span></span><br><span class="line"><span class="string">        out = self.out(a1)</span></span><br><span class="line"><span class="string">        a_out = self.softmax(out)</span></span><br><span class="line"><span class="string">        return a_out</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#用torch.nn.functional引入激活函数的写法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NetC</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 定义神经网络</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">        <span class="built_in">super</span>(NetC, self).__init__()</span><br><span class="line">        self.h1 = nn.Linear(n_feature, n_hidden)  <span class="comment">#subModule: Linear</span></span><br><span class="line">        self.out = nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向运算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_input</span>):</span><br><span class="line">        h1 = F.relu(self.h1(x_input))</span><br><span class="line">        out = F.softmax(self.out(h1),dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n实例化神经网络模型对象&quot;</span>)</span><br><span class="line">model = NetC(<span class="number">28</span> * <span class="number">28</span>, <span class="number">32</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n显示网络模型参数&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model.parameters)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n定义神经网络样本输入&quot;</span>)</span><br><span class="line">x_input = torch.randn(<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line"><span class="comment"># 得到的数据格式torch.Size([64, 1, 28, 28])需要转变为（64,784）</span></span><br><span class="line">x_input = x_input.view(x_input.size()[<span class="number">0</span>], -<span class="number">1</span>)  <span class="comment"># -1表示自动匹配</span></span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用神经网络进行预测&quot;</span>)</span><br><span class="line">y_pred = model.forward(x_input)</span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br></pre></td></tr></table></figure><p>自定义新的神经网络模型的类</p><p>实例化神经网络模型对象 NetC( (h1): Linear(in_features=784, out_features=32, bias=True) (out): Linear(in_features=32, out_features=10, bias=True) )</p><p>显示网络模型参数 &lt;bound method Module.parameters of NetC( (h1): Linear(in_features=784, out_features=32, bias=True) (out): Linear(in_features=32, out_features=10, bias=True) )&gt;</p><p>定义神经网络样本输入 torch.Size([2, 28, 28, 1]) torch.Size([2, 784])</p><p>使用神经网络进行预测 tensor([[0.0884, 0.0733, 0.0890, 0.1088, 0.1589, 0.0944, 0.0861, 0.1191, 0.0876, 0.0944], [0.1134, 0.0963, 0.0595, 0.1051, 0.0881, 0.1059, 0.0627, 0.1023, 0.1605, 0.1063]], grad_fn=<SoftmaxBackward>)</p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Typora 搜狗自定义短语设置</title>
      <link href="/2024/01/23/Typora%20%E6%90%9C%E7%8B%97%E5%BF%AB%E6%8D%B7%E9%94%AE%E8%AE%BE%E7%BD%AE/"/>
      <url>/2024/01/23/Typora%20%E6%90%9C%E7%8B%97%E5%BF%AB%E6%8D%B7%E9%94%AE%E8%AE%BE%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考网址：<a href="https://blog.csdn.net/kllo__/article/details/122494151">Typora：改变字体的背景颜色_typora文字背景色_kllo__的博客-CSDN博客</a></p></blockquote><blockquote><p>在Typora中没有快捷设置字体颜色的方式，因此我们想到在搜狗输入法的自定义短语中定义快捷输入html代码来设置字体颜色。本文给出了一些常用的自定义短语。</p></blockquote><p>[TOC]</p><h2 id="设置方法">设置方法</h2><p>打开<code>搜狗-属性设置-高级-候选扩展-自定义短语</code>，点击<code>添加新定义</code>，即可定义新的自定义短语。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231206093545264.png" alt="image-20231206093545264" /><figcaption aria-hidden="true">image-20231206093545264</figcaption></figure><h2 id="设置字体颜色">设置字体颜色</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#常见颜色</span><br><span class="line">&lt;font color=red&gt;红色字体&lt;/font&gt;  #缩写red</span><br><span class="line">&lt;font color=blue&gt;蓝色字体&lt;/font&gt;  #缩写blu</span><br><span class="line">&lt;font color=green&gt;绿色字体&lt;/font&gt;  #缩写gree</span><br><span class="line">&lt;font color=orange&gt;橙色字体&lt;/font&gt;#缩写oran</span><br><span class="line">&lt;font color=purple&gt;紫色字体&lt;/font&gt;#缩写purp</span><br><span class="line">&lt;font color=yellow&gt;黄色字体&lt;/font&gt;#缩写yel</span><br><span class="line"></span><br><span class="line">&lt;font color=#1ec4d3&gt;藻蓝色字体&lt;/font&gt;</span><br></pre></td></tr></table></figure><h4 id="样式预览">样式预览：</h4><p><font color=red>红色字体</font><br /><font color=blue>蓝色字体</font><br /><font color=green>绿色字体</font><br /><font color=orange>橙色字体</font><br /><font color=purple>紫色字体</font><br /><font color=yellow>黄色字体</font></p><p><font color=#1ec4d3>藻蓝色字体</font></p><p>另外附上我从mubu上扒下来的字体颜色设置：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;font color=#ef042a&gt;红色字体&lt;/font&gt;  #缩写red</span><br><span class="line">&lt;font color=#0091ff&gt;蓝色字体&lt;/font&gt;  #缩写blu</span><br><span class="line">&lt;font color=#4eb434&gt;绿色字体&lt;/font&gt;  #缩写gree</span><br><span class="line">&lt;font color=#df8400&gt;橙色字体&lt;/font&gt;#缩写oran</span><br><span class="line">&lt;font color=#985fff&gt;紫色字体&lt;/font&gt;#缩写purp</span><br></pre></td></tr></table></figure><h4 id="样式预览-1">样式预览：</h4><p><font color=#df8400>这是橙色</font> <font color=#ef042a>这是红色</font> <font color=#4eb434>这是绿色</font> <font color=#0091ff>这是蓝色</font> <font color=#985fff>这是紫色</font></p><h2 id="给字体加底色">给字体加底色</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;span style=&quot;background:#eef0f4;&quot;&gt;浅灰色背景&lt;/span&gt;#缩写bggrey</span><br><span class="line">&lt;span style=&quot;background:#fbd4d0;&quot;&gt;浅红色背景&lt;/span&gt;#缩写bgred</span><br><span class="line">&lt;span style=&quot;background:#d4e9d5;&quot;&gt;浅绿色背景&lt;/span&gt;#缩写bggre</span><br><span class="line">&lt;span style=&quot;background:#dad5e9;&quot;&gt;浅紫色背景&lt;/span&gt;#缩写bgpur</span><br><span class="line">&lt;span style=&quot;background:#f9eda6;&quot;&gt;浅黄色背景&lt;/span&gt;#缩写bgyel</span><br><span class="line">&lt;span style=&quot;background:#FFCC99;&quot;&gt;浅橘色背景&lt;/span&gt;#缩写bgora</span><br><span class="line"></span><br><span class="line">&lt;span style=&quot;background:#daf5e9;&quot;&gt;亮绿色背景&lt;/span&gt;</span><br><span class="line">&lt;span style=&quot;background:#fff1c9;&quot;&gt;亮黄色背景&lt;/span&gt;  </span><br></pre></td></tr></table></figure><h4 id="样式预览-2">样式预览</h4><p><span style="background:#eef0f4;">浅灰色背景</span> <span style="background:#fbd4d0;">浅红色背景</span> <span style="background:#d4e9d5;">浅绿色背景</span> <span style="background:#dad5e9;">浅紫色背景</span> <span style="background:#f9eda6;">浅黄色背景</span> <span style="background:#FFCC99;">浅橘色背景</span></p><p><span style="background:#daf5e9;">亮绿色背景</span> <span style="background:#fff1c9;">亮黄色背景</span></p><blockquote><p>配色参考网站：</p><p><a href="https://likexia.gitee.io/tools/peise/index.html">网页设计常用色彩搭配表 - 配色表 (gitee.io)</a></p><p><a href="https://encycolorpedia.cn/">十六进制颜色代码表，图表及调色板 - Encycolorpedia</a></p></blockquote><h2 id="给字体加底色且改颜色">给字体加底色且改颜色</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;span style=&quot;background:#f9eda6;color:red&quot;&gt;浅黄色背景红字&lt;/span&gt;</span><br></pre></td></tr></table></figure><p>如：<span style="background:#f9eda6;color:red">浅黄色背景</span></p><h2 id="设置图片题注">设置图片题注</h2><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">center</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">img</span> <span class="attr">style</span>=<span class="string">&quot;border-radius: 0.3125em;</span></span></span><br><span class="line"><span class="string"><span class="tag">    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);&quot;</span> </span></span><br><span class="line"><span class="tag">    <span class="attr">src</span>=<span class="string">&quot;这里输入图片地址&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">&quot;color:orange; border-bottom: 1px solid #d9d9d9;</span></span></span><br><span class="line"><span class="string"><span class="tag">    display: inline-block;</span></span></span><br><span class="line"><span class="string"><span class="tag">    color: #999;</span></span></span><br><span class="line"><span class="string"><span class="tag">    padding: 2px;&quot;</span>&gt;</span>这里输入题注<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">center</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> blog搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2023/12/31/DVGO%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/12/31/DVGO%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p><strong>这篇文章的主要贡献点在于两点：</strong></p><ol type="1"><li><p><strong>在体素密度直接优化中，采用了两个先验算法来避免几何陷入局部最优解：</strong>直接优化密度体素网格会导致超快收敛，但容易出现次优解，所提出方法在自由空间分配"云"，并试图将光度损失与云拟合，而不是搜索具有更好多视图一致性的几何。对这个问题的解决方案简单而有效。<u>首先</u>，初始化密度体素网格，以产生非常接近于零的不透明度，以避免几何解决方案偏向于相机的附近平面。<u>其次</u>，给较少视图可见的体素一个较低的学习率，可以避免仅为解释少量视图的观察而分配的冗余体素。所提出的解决方案可以成功地避免次优几何，并在五个数据集上表现良好。</p></li><li><p><strong>提出了先插值后激活的体素网格插值，它可以在较低的网格分辨率下实现清晰的边界建模：</strong>之前的工作要么对激活的不透明度进行体素网格插值，要么使用最近邻插值，从而在每个网格单元中产生光滑的表面。从数学和经验上证明，所提出的后激活可以在单个网格单元内建模(超越)尖锐的线性表面。因此，可以使用更少的体素来实现更好的质量——具有160^3个密集体素的方法在大多数情况下已经优于NeRF。</p></li></ol><p><strong>框架：</strong></p><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240101105729136.png" alt="image-20240101105729136" /><figcaption aria-hidden="true">image-20240101105729136</figcaption></figure><p>softplus激活函数：</p><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240101125119035.png" alt="image-20240101125119035" /><figcaption aria-hidden="true">image-20240101125119035</figcaption></figure><p><strong>inward-facing</strong></p><blockquote><p><a href="https://immortalqx.github.io/2022/08/22/nerf-notes-3/#!">(ฅ&gt;ω&lt;*ฅ) 噫？又好了~ (immortalqx.github.io)</a></p></blockquote><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240101144152110.png" alt="image-20240101144152110" /><figcaption aria-hidden="true">image-20240101144152110</figcaption></figure><p>文献34</p><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240101112426521.png" alt="image-20240101112426521" /><figcaption aria-hidden="true">image-20240101112426521</figcaption></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>复现代码指南</title>
      <link href="/2023/12/27/%E5%A4%8D%E7%8E%B0%E4%BB%A3%E7%A0%81%E6%8C%87%E5%8D%97/"/>
      <url>/2023/12/27/%E5%A4%8D%E7%8E%B0%E4%BB%A3%E7%A0%81%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<h1 id="复现代码指南">复现代码指南</h1><p>安装环境时需要注意<strong>Python版本</strong>、<strong>Pytorch版本</strong>和<strong>CUDA版本</strong>是否兼容</p><h2 id="python修改pip源">Python修改pip源</h2><h3 id="一步到位版">一步到位版</h3><p>参考网址：<a href="https://www.jianshu.com/p/b2412f7fc93f">pip换源一行命令直接搞定 - 简书 (jianshu.com)</a></p><p>打开cmd,输入：<code>pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</code></p><p>或者：<code>pip install 安装包名 -i https://pypi.tuna.tsinghua.edu.cn/simple/</code></p><p>如果临时使用的话，可以使用：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install +库名 -i +源</span><br><span class="line">eg:    pip install numpy -i http://mirrors.aliyun.com/pypi/simple/</span><br></pre></td></tr></table></figure><h3 id="几个国内源">几个国内源</h3><p>阿里云 <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fmirrors.aliyun.com%2Fpypi%2Fsimple%2F">http://mirrors.aliyun.com/pypi/simple/</a> 中国科技大学 <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fpypi.mirrors.ustc.edu.cn%2Fsimple%2F">https://pypi.mirrors.ustc.edu.cn/simple/</a> 豆瓣(douban) <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fpypi.douban.com%2Fsimple%2F">http://pypi.douban.com/simple/</a> 清华大学 <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fpypi.tuna.tsinghua.edu.cn%2Fsimple%2F">https://pypi.tuna.tsinghua.edu.cn/simple/</a> 中国科学技术大学 <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fpypi.mirrors.ustc.edu.cn%2Fsimple%2F">http://pypi.mirrors.ustc.edu.cn/simple/</a></p><h2 id="离线安装pytorch">离线安装Pytorch</h2><p>参考网址：<a href="https://blog.csdn.net/weixin_47142735/article/details/113684365">离线安装Pytorch 最简单 高效的方法_pytorch离线安装_正在学习的浅语的博客-CSDN博客</a></p><blockquote><p>在线安装的一些指令：</p><ul><li>安装torch1.10.1: pip --default-timeout=1000 install torch==1.10.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html</li><li>安装 torch_scatter：pip install torch_scatter==2.0.9 --extra-index-url https://download.pytorch.org/whl/cu111</li><li>同时安装多个包：pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116</li></ul></blockquote><p>在线下载安装包，特别是比较大的安装包，很容易因为网络原因失败:</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231202203943.png" alt="image-20231202203938666" /><figcaption aria-hidden="true">image-20231202203938666</figcaption></figure><p>所以我们考虑先将需要的包下载后，再离线安装。下载地址：<span style="background:#fbd4d0;"><a href="http://download.pytorch.org/whl/torch_stable.html">download.pytorch.org/whl/torch_stable.html</a></span></p><blockquote><p>附上pytorch其他相关包的下载地址：</p><ul><li><p><a href="https://pytorch-geometric.com/whl/">pytorch-geometric.com/whl/</a> （下载 torch_scatter）</p></li><li><p><a href="https://mmcv.readthedocs.io/zh-cn/latest/get_started/installation.html">安装 MMCV — mmcv 2.1.0 文档</a></p></li></ul></blockquote><h3 id="下载离线安装包">下载离线安装包</h3><p>点进去后有pytorch安装包、torchaudio安装包和torchvision安装包等，可以通过<span style="background:#eef0f4;">Ctrl + F</span>寻找需要下载的包。主要有两种pytorch安装包（<span style="background:#f9eda6;">一般只使用gpu版本的安装包</span>）：</p><ol type="1"><li><p>cpu版本pytorch,<font color=red>开头为cpu</font>;</p></li><li><p>gpu版本pytorch,<font color=red>开头为cu</font>，如cu111表示gpu版本pytorch，且该pytorch的cuda版本为11.1;</p></li></ol><p>cp表示python版本，linux/window 表示系统版本。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231202204318.png" alt="cpu" /><figcaption aria-hidden="true">cpu</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231202204355.png" alt="gpu" /><figcaption aria-hidden="true">gpu</figcaption></figure><p><strong>注意，我们要根据自己cuda的版本和系统版本来下载安装包，且一定要使Python版本、Pytorch版本和CUDA版本三者兼容！！！</strong>下面附一张版本对应关系表（也可以去问chatgpt）： <img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231202212206.png" alt="cuda_pytorch" /></p><blockquote><ol type="1"><li>torchvision 与 pytorch 版本对应关系（如果直接安装torchvision，可能会自动安装最新的版本，同时也把torch升级到最新）</li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240118150625277.png" alt="image-20240118150625277" /><figcaption aria-hidden="true">image-20240118150625277</figcaption></figure><ol start="2" type="1"><li>torch_scatter 与 pytorch版本对应关系：见<a href="https://pytorch-geometric.com/whl/">pytorch-geometric.com/whl/</a></li></ol></blockquote><h3 id="离线安装以pycharm安装为例">离线安装（以Pycharm安装为例）</h3><p>打开Pycharm中的终端，切换到需要安装包的环境，先使用<span style="background:#dad5e9;">cd指令</span>跳转到下载文件夹；然后<span style="background:#d4e9d5;">pip install 安装包名.后缀</span>进行安装；最后可使用<span style="background:#f9eda6;">conda list 或 python - import torch - torch.cuda.is_available()（需要完成cuda+cudnn+torch配套安装）</span>进行查看。<span style="background:#fbd4d0;">大功告成！</span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231202212635162.png" alt="image-20231203191239667" /><figcaption aria-hidden="true">image-20231203191239667</figcaption></figure><p><strong>值得注意的是，一定要使用纯英文的路径，并且安装包的名字不饿能有任何变动，这些问题都会导致安装的失败。</strong>如果在终端中直接安装，别忘了先用<span style="background:#eef0f4;">conda activate</span>先激活环境。</p><h2 id="安装cuda">安装cuda</h2><h3 id="安装过程">安装过程</h3><p>cuda官网下载地址：<span style="background:#fbd4d0;"><a href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA Toolkit Archive | NVIDIA Developer</a></span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231203184824.png" alt="image-20231203184822148" /><figcaption aria-hidden="true">image-20231203184822148</figcaption></figure><p>按照图中白色位置选择下载包，然后选择自定义安装。</p><p>在文件资源管理器中创建一个名为cuxx的文件夹，用来存放cuda；然后设置安装路径</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203190515923.png" alt="image-20231203190515923" /><figcaption aria-hidden="true">image-20231203190515923</figcaption></figure><p>检查完系统兼容性后，点击<span style="background:#d4e9d5;">同意并继续</span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203190837475.png" alt="image-20231202212635162" /><figcaption aria-hidden="true">image-20231202212635162</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203190858663.png" alt="image-20231203190858663" /><figcaption aria-hidden="true">image-20231203190858663</figcaption></figure><p>勾选<span style="background:#d4e9d5;">自定义</span>，点击<span style="background:#d4e9d5;">下一步</span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203191209483.png" alt="image-20231203191209483" /><figcaption aria-hidden="true">image-20231203191209483</figcaption></figure><p>第一次安装需要把组件都勾选上</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203191239667.png" alt="image-20231203190837475" /><figcaption aria-hidden="true">image-20231203190837475</figcaption></figure><p><span style="background:#fbd4d0;"><strong>选择安装位置，并记住安装路径：</strong></span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203191504491.png" alt="image-20231203192352716" /><figcaption aria-hidden="true">image-20231203192352716</figcaption></figure><p>等待安装完成</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203191624776.png" alt="image-20231203193341199" /><figcaption aria-hidden="true">image-20231203193341199</figcaption></figure><h3 id="验证是否安装成功">验证是否安装成功</h3><p>打开<span style="background:#dad5e9;">设置 - 系统 - 系统信息 - 高级系统设置 - 高级 - 环境变量</span>，出现橙色框中的两个环境变量（也可在cmd中输入<code>set cuda</code>查看cuda设置的环境变量）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203192352716.png" alt="image-20231203191624776" /><figcaption aria-hidden="true">image-20231203191624776</figcaption></figure><p><span style="background:#f9eda6;">重启电脑！！（否则nvcc -V会找不到指令）</span>进入cmd界面，输入 <code>nvcc -V / nvcc --version</code> 查看版本号，出现如下界面，说明 cuda 安装成功啦！</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203193341199.png" alt="image-20231203191504491" /><figcaption aria-hidden="true">image-20231203191504491</figcaption></figure><h2 id="安装cudnn">安装cudnn</h2><p>注：cudnn是用于配置深度学习使用，相当于cuda的一个专为深度学习运算进行优化的补丁</p><h3 id="安装过程-1">安装过程</h3><p>cudnn 官方下载地址：<span style="background:#d4e9d5;"><a href="https://developer.nvidia.com/rdp/cudnn-download">cuDNN Download | NVIDIA Developer</a></span></p><p>进入cudnn下载时需要注册或登录账号，然后选择需要对应版本下载安装包。这里下载的是压缩后的安装包。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203193624549.png" alt="image-20231203193813587" /><figcaption aria-hidden="true">image-20231203193813587</figcaption></figure><p>解压后出现三个文件夹和License</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203193813587.png" alt="image-20231203193624549" /><figcaption aria-hidden="true">image-20231203193624549</figcaption></figure><p><span style="background:#fbd4d0;">找到cuda的安装路径，如：<strong>C:FilesGPU Computing Toolkit1.1</strong></span>。将cudnn三个文件夹里的内容分别替换到对应的文件夹里。</p><p>打开<span style="background:#dad5e9;">设置 - 系统 - 系统信息 - 高级系统设置 - 高级 - 环境变量</span>，再找到<span style="background:#d4e9d5;">系统变量 - path</span>，将以下三个变量添加进去，完成安装(<font color=red>注意修改对应变量名喔</font>)</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\bin</span><br><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\include</span><br><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\lib</span><br><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\libnvvp</span><br></pre></td></tr></table></figure><h3 id="验证是否安装成功-1">验证是否安装成功</h3><p>在cmd中使用<span style="background:#d4e9d5;">cd命令</span>进入cuda的安装路径找到测试工具，如：<span style="background:#f9eda6;">C:FilesGPU Computing Toolkit1.1_suite</span>，执行如下两个.exe文件</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203195334390.png" alt="image-20231203195334390" /><figcaption aria-hidden="true">image-20231203195334390</figcaption></figure><p>执行：<code>deviceQuery.exe</code>查询本机的GPU设备和<code>bandwidthTest.exe</code>测试带宽，如果结果都为PASS，说明运行正常</p><h1 id="问题汇总">问题汇总：</h1><h2 id="cuda-driver-version-is-insufficient-for-cuda-runtime-version"><strong>CUDA driver version is insufficient for CUDA runtime version</strong></h2><p>cuda驱动程序版本和cuda运行时版本不匹配：问题可大可小，可能是程序装错了，也可能你的电脑根本配不了cuda环境（如amd显卡）</p><h2 id="failed-to-initialize-nvml-unknown-error"><strong>Failed to initialize NVML: Unknown Error</strong></h2><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216143546376.png" alt="image-20231216143546376" /><figcaption aria-hidden="true">image-20231216143546376</figcaption></figure><p>在复现代码时，配置完cuda后，想验证<code>torch.cuda.is_available()</code>，结果过了很久也没有输出；然后又输入<code>nvidia-smi</code>查看显卡配置，报错！</p><p>在网上查找后发现了几个方法：</p><h3 id="win11下运行nvidia-smi报错failed-to-initialize-nvml-unknown-error-csdn博客"><strong><a href="https://blog.csdn.net/qq_39499680/article/details/134855395">WIN11下运行nvidia-smi报错Failed to initialize NVML: Unknown Error-CSDN博客</a></strong></h3><p>使用Everything(或电脑自带的文件搜索)去查找nvidia-smi的位置，然后进入文件目录（如：<span style="background:#d4e9d5;">C:_dispig.inf_amd64_49aadc39d4f73881</span>），点击运行 <span style="background:#f9eda6;color:red">setup.exe</span> 重新安装即可。</p><p>尝试方法一，在点击 setup.exe 后报错：<span style="background:#fb6172d0;color:white">所需文件丢失</span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216144502672.png" alt="image-20231216144502672" /><figcaption aria-hidden="true">image-20231216144502672</figcaption></figure><h3 id="考虑重新安装nvidia驱动"><strong>考虑重新安装nvidia驱动</strong></h3><h4 id="卸载驱动">2.1 卸载驱动</h4><p>打开<font color=purple>设备管理器</font>，点击<span style="background:#dad5e9;">显卡适配器-驱动程序-卸载设备</span>，然后重启电脑</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216151900323.png" alt="image-20231216151900323" /><figcaption aria-hidden="true">image-20231216151900323</figcaption></figure><p>发现这时显卡已经变为<font color=purple>Microsoft基本显示适配器</font>：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216152946150.png" alt="image-20231216152946150" /><figcaption aria-hidden="true">image-20231216152946150</figcaption></figure><h4 id="重新安装驱动">2.2 重新安装驱动</h4><p>打开nvidia驱动下载地址：<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn#">官方驱动 | NVIDIA</a>，按自己的电脑型号选择合适的驱动（笔记本产品系列是<font color=red>notebooks</font>），开始下载</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216153411364.png" alt="image-20231216153411364" /><figcaption aria-hidden="true">image-20231216153411364</figcaption></figure><p>完成下载后双击，在解压提示框中点击OK，等待进度完成（此处默认安装路径是：<font color=orange>C:\546.33_Win10-DCH_64</font>）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154244543.png" alt="image-20231216154244543" /><figcaption aria-hidden="true">image-20231216154244543</figcaption></figure><p>等待检查系统兼容性完成，后续工作见图</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154626870.png" alt="image-20231216154626870" /><figcaption aria-hidden="true">image-20231216154626870</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154713103.png" alt="image-20231216154713103" /><figcaption aria-hidden="true">image-20231216154713103</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154731465.png" alt="image-20231216154731465" /><figcaption aria-hidden="true">image-20231216154731465</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154826092.png" alt="image-20231216154826092" /><figcaption aria-hidden="true">image-20231216154826092</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154918798.png" alt="image-20231216154918798" /><figcaption aria-hidden="true">image-20231216154918798</figcaption></figure><blockquote><p>安装快完成时会提示<strong>是否立即重启计算机完成安装</strong>，点击确定前记得保存未保存的文件</p></blockquote><p>耐心等待安装完成后，再次打开<strong>设备管理器</strong>，即可发现安装成功！</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216155635451.png" alt="image-20231216155635451" /><figcaption aria-hidden="true">image-20231216155635451</figcaption></figure><p>我们打开终端，输入<code>nvidia-smi</code>,成功输出显卡驱动信息</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216155757435.png" alt="image-20231216155757435" /><figcaption aria-hidden="true">image-20231216155757435</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 环境搭建 </tag>
            
            <tag> cuda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3D Gaussian Splatting for Real-Time Radiance Field Rendering 论文笔记</title>
      <link href="/2023/12/20/3D%20Gaussian%20Splatting%20for%20Real-Time%20Radiance%20Field%20Rendering%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/12/20/3D%20Gaussian%20Splatting%20for%20Real-Time%20Radiance%20Field%20Rendering%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p><strong>数据集</strong>：Mip-NeRF360 数据集、Deep Blending 数据集[Hedman et al. 2018]</p><h1 id="abstract">Abstract</h1><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231220144503261.png" alt="image-20231220144503261" /><figcaption aria-hidden="true">image-20231220144503261</figcaption></figure><p>优化时间与<strong>InstantNGP</strong>相当 优化质量与<strong>光学体素</strong>相当 训练时间达到51分钟时，我们的质量达到最好，甚至比Mip-NeRF更胜一筹</p><p>mip_splatting</p><p>我们引入了三个关键元素，使我们能够在保持竞技训练时间的同时实现最先进的视觉质量，重要的是允许在1080p分辨率下实现高质量的实时(≥30 fps)新视图合成。<strong>首先</strong>，从相机标定过程中产生的稀疏点云出发，用三维高斯模型表示场景，该模型保留了场景优化所需的连续体亮度场特性，同时避免了在空空间中进行不必要的计算; <strong>其次</strong>，我们对三维高斯模型进行了交错优化/密度控制，特别是优化了各向异性协方差，以实现对场景的准确表示; <strong>最后</strong>，我们开发了一个快速的可视性感知的渲染算法，支持各向异性喷溅，既加速训练过程，又允许实时渲染。我们在几个已建立的数据集上展现出了最先进的视觉质量和实时渲染。</p><p>其他关键词和短语: <strong>新视图合成(novel view synthesis)，亮度场(radiance fields)，三维高斯(3D gaussians)，实时渲染(real-time rendering)</strong></p><h1 id="introduction">Introduction</h1><p>网格和点是最常见的3D场景表示，因为它们是显式的，并且非常适合基于GPU/ cuda的快速栅格化。相比之下，最近的神经辐射场(NeRF)方法建立在连续场景表示的基础上，通常使用体积射线行进优化多层感知器(MLP)，以获得捕获场景的新视图合成。类似地，迄今为止最有效的亮度场解决方案建立在连续表示的基础上，通过插值存储在体素[fridovic - keil和Yu等人，2022]或哈希[Müller等人，2022]网格或点[Xu等人，2022]中的值。虽然这些方法的连续性质有助于优化，但渲染所需的随机采样代价高昂，并可能导致噪声。我们引入了一种新方法，它结合了两个方面的优点: 我们的3D高斯表示允许优化最先进的(SOTA)视觉质量和有竞争性的训练时间，而我们的基于瓦片的喷溅解决方案确保在几个之前发布的数据集上以1080p分辨率的SOTA质量进行实时渲染[Barron等，2022; Hedman等人2018年;Knapitsch等人，2017](见图1)。</p><p>我们的目标是允许对多张照片捕获的场景进行实时渲染，并在优化时间内创建典型真实场景的表示，速度与之前最有效的方法一样快。最近的方法实现了快速训练[fridovic - keil和Yu等人，2022;Müller等人，2022]，但很难达到目前SOTA NeRF方法，即Mip-NeRF360 [Barron等人，2022]获得的视觉质量，这需要多达48小时的训练时间。快速但质量较低的亮度场方法可以根据场景实现<u>交互渲染</u>时间(10-15帧/秒)，但在高分辨率的实时渲染上达不到要求。</p><blockquote><p>fridovic - keil和Yu等人，2022;Müller等人，2022</p><p>InstantNGP <strong>光学体素</strong></p></blockquote><p>我们的解决方案构建在三个主要组件上。<font color=#985fff>我们首先介绍三维高斯模型作为一种灵活和富有表现力的场景表示。</font>我们从与之前类似nerf的方法相同的输入开始，即使用结构从运动(SfM)校准的相机[Snavely等人，2006]，并使用SfM过程中免费生成的稀疏点云初始化3D高斯模型。与大多数需要多视图立体视觉(MVS)数据的基于点的解决方案相比[Aliev等人，2020;Kopanas等人2021年;Rückert et al. 2022]，我们仅以SfM点作为输入就获得了高质量的结果。注意，<u>对于nerf合成数据集，我们的方法即使在随机初始化的情况下也能实现高质量</u>。我们表明，3D Gaussians是一个非常好的选择，因为它们是可微分的体积表示，但它们也可以非常有效地栅格化，通过将它们投影到2D，并应用标准的𝛼-blending，使用等效的图像生成模型作为NeRF。</p><p><font color=#df8400>我们方法的第二部分</font>是优化<u>三维高斯函数的性质</u>—<span style="background:#fbd4d0;">三维位置，不透明度𝛼，各向异性协方差，和 球面调和(SH)系数</span>—与<u>自适应密度控</u>制步骤交错，在优化过程中我们添加和偶尔删除三维高斯函数。优化过程生成了一个相当紧凑的、非结构化的、精确的场景表示(测试的所有场景的1-5百万高斯值)。</p><p><font color=#4eb434>我们方法的第三个也是最后一个部分是我们的实时渲染解决方案</font>，它使用了快速的GPU排序算法，这是受到基于贴图的栅格化的启发，遵循最近的工作[Lassner和Zollhofer 2021]。然而，由于我们的3D高斯表示，我们可以执行各向异性溅射，它遵循可见性排序——这要归功于排序和𝛼blending——并通过跟踪所需的多个排序splats (splats是排序好的) 的遍历来实现快速和准确的向后遍历。</p><blockquote><p>可以不提供真实相机参数, 运行colmap，可以估计图像的相机参数</p><p>3DGS只有两个数据加载器一个是Blender另一个是colmap数据加载器; Blender数据集用的是随机点云，不是从SFM生成的, blender数据集用的是box内随机生成点云</p><p>抛雪球算法：splatting</p></blockquote><p><strong>我们的贡献主要如下:</strong></p><ul><li><p>引入各向异性三维高斯函数作为高质量、非结构化的亮度场表示。</p></li><li><p>3D高斯属性的优化方法，与自适应密度控制交织，为捕获的场景创建高质量的表示。</p></li><li><p>GPU的一种快速、可微分的渲染方法，它是可见感知的，允许各向异性喷溅和快速反向传播，以实现高质量的新视图合成。</p></li></ul><p>我们在之前发布的数据集上的结果表明，我们可以从多视图捕获的图像中优化3D高斯，并获得与之前的隐式亮度场方法相同或更好的质量。我们还可以达到与最快的方法相似的训练速度和质量，重要的是，为新视图合成提供第一次高质量的实时渲染。</p><p>我们首先简要概述了传统的重建，然后讨论了基于点的渲染和亮度场工作，讨论了它们的相似性;亮度场是一个很大的区域，所以我们只关注直接相关的工作。有关该领域的完整报道，请参阅最近的优秀调查[Tewari等人。2022;谢等。2022]。</p><h2 id="traditional-scene-reconstruction-and-rendering">2.1 Traditional Scene Reconstruction and Rendering</h2><p>第一个新视觉合成方法是基于光场，第一个密集采样[Gortler等人，1996;Levoy和Hanrahan 1996]然后允许非结构化捕获[Buehler等人2001]。结构从运动(SfM)的出现[Snavely等人，2006]使一个全新的领域，一组照片可以用来合成新的视图。SfM在相机标定过程中估计了一个稀疏的点云，最初用于简单的三维空间可视化。随后的多视图立体视觉(MVS)在过去几年里产生了令人印象深刻的完整的三维重建算法[Goesele等人，2007]，使几个视图合成算法的发展成为可能[Chaurasia等人，2013;Eisemann等人2008年;Hedman等人2018年;Kopanas等人，2021]。所有这些方法都将输入图像重新投影和混合到新的视图相机中，并使用几何图形来引导这种重新投影。这些方法在许多情况下都产生了很好的效果，但是当MVS生成不存在的几何时，通常无法从未重建区域或“过度重建”区域完全恢复。最近的神经渲染算法[Tewari等人，2022]大大减少了这些工件，并避免了在GPU上存储所有输入图像的巨大成本，在大多数方面都优于这些方法。</p><blockquote><p>工件 (artifacts, 理解为人为在软件运行过程中造出的阶段性产物，可以是软件代码、文档、图纸、数据等非天然存在的资产，<a href="https://www.zhihu.com/question/455298119">(99+ 封私信 / 80 条消息) artifact 一词在计算机编程里面表示什么意思？ - 知乎 (zhihu.com)</a>)</p></blockquote><h2 id="neural-rendering-and-radiance-fields">2.2 Neural Rendering and Radiance Fields</h2><p>深度学习技术很早就被用于新视角合成[Flynn等人，2016;周等2016;cnn被用于估计混合权重[Hedman等人2018]，或用于纹理空间解决方案[Riegler和Koltun 2020;Thies等。2019]。使用基于MVS的几何是这些方法的一个主要缺点; 此外，使用CNNs进行最终渲染经常会导致时间闪烁(temporal flickering)。</p><p>新颖视图合成的体积表示由Soft3D发起[Penner和Zhang 2017]; 随后提出了深度学习技术和体积射线推进技术[Henzler等人，2019; Sitzmann等人。2019]基于连续可微密度场来表示几何。由于查询体积需要大量的样本，使用体积射线行进进行渲染的成本非常高。神经辐射场(NeRFs) [Mildenhall等人，2020]引入了重要性采样和位置编码来提高质量，但使用了大型多层感知器，对速度产生了负面影响。NeRF的成功导致了后续方法的爆炸式增长，这些后续方法通常通过引入正则化策略来解决质量和速度问题; 当前最先进的新型视图合成图像质量是Mip-NeRF360 [Barron等人，2022]。虽然渲染质量卓越，训练和渲染时间仍然非常高; 在提供快速训练和实时渲染的同时，我们能够达到甚至在某些情况下超过这个质量。</p><p>最近的方法主要通过三种设计选择来实现更快的训练和/或渲染: <font color=#4eb434>使用空间数据结构来存储(神经)特征，这些特征随后在体积射线行进过程、不同的编码和MLP容量中被插值，</font>。这些方法包括空间离散化的不同变体[Chen et al. 2022b,a; fridovic - keil和Yu等人，2022年;Garbin等人2021年; Hedman等人，2021年; Reiser等人，2021年; Takikawa等人，2021年; Wu等人，2022; Yu等人，2021年]，码本[Takikawa等人，2022年]，<font color=#ef042a>编码如哈希表</font>[Müller等人，2022年]，<font color=#0091ff>完全允许使用较小的MLP或先前提到的神经网络</font>[fridovic - keil和Yu等人，2022年; Sun等，2022]。</p><blockquote><p>Plenoxels [fridovic - keil和Yu等人，2022] 即 光学体素</p><p>360场景合成——处理图片并生成带背景的场景</p><p>3dgs用到了两个cuda扩展包，主体还是pytorch</p></blockquote><p>这些方法中最值得注意的是 <strong>InstantNGP</strong> [Müller等人。2022]，它使用哈希网格和占用网格来加速计算，并使用较小的 MLP 来表示密度和外观; 和 <strong>Plenoxels</strong> [fridovic - keil和Yu等人，2022]使用稀疏体素网格插值连续密度场，并能够完全放弃神经网络。<u>两者都依赖于球面谐波: 前者直接表示方向效果，后者将其输入编码到颜色网络</u>。虽然这两种方法都提供了出色的结果，但这些方法仍然难以有效地表示空白空间，这部分取决于场景/捕获类型。此外，图像质量在很大程度上受到用于加速的结构化网格的选择的限制，而渲染速度则因需要为给定的光线推进步骤查询许多样本而受到阻碍。我们使用的非结构化的、显式gpu友好的3D高斯函数，在没有神经组件的情况下，实现了更快的渲染速度和更好的质量。</p><h2 id="point-based-rendering-and-radiance-fields">2.3 Point-Based Rendering and Radiance Fields</h2><p>基于点的方法可以有效地渲染不连接的和非结构化的几何样本(即点云)[Gross和Pfister 2011]。在最简单的形式中，点采样渲染[Grossman和Dally 1998]栅格化一个固定大小的非结构化点集，它可以利用图形API中自然支持的点类型[Sainz和Pajarola 2004]或GPU上的并行软件栅格化[Laine和Karras 2011;Schütz et al. 2022]。虽然对底层数据来说是真实的，但点采样呈现会出现漏洞，导致混叠，并且严格来说是不连续的。基于点的高质量渲染的开创性工作通过“喷溅”大于一个像素的点基元来解决这些问题，例如圆形或椭圆形圆盘、椭球或面元surfels [Botsch et al. 2005;Pfister等人2000年;Ren等人，2002年;Zwicker等人。2001b]。</p><p>最近人们对基于可微点的渲染技术产生了兴趣[Wiles等人，2020;Yifan等，2019。用神经特征增强了点，并使用CNN渲染[Aliev等人，2020;Rückert等人，2022]导致快速甚至实时的视图合成; 然而，他们仍然依赖于MVS得到初始几何，并继承其工件，最明显的过度或重构不足的情况下，如无特征/闪亮的区域或薄结构。</p><p>基于点的𝛼-blending和NeRF风格的体绘制在本质上共享相同的图像生成模型。具体来说，颜色𝐶是由沿着光线的体积渲染给出的: $$</p><p>$$</p><blockquote><p>该公式与volsdf中的公式26下面一个公式类似</p><p>splatting的另外相关论文包括，mip-splatting</p><p>instantngp比mipNeRF跑得好，但是比mipNeRF360差</p></blockquote><h1 id="overview">Overview</h1><p>我们的方法的输入是一组静态场景的图像，以及由SfM [Schönberger和Frahm 2016]校准的相应摄像机，这将产生一个稀疏的点云作为副作用。从这些点出发，我们创建了一组3D高斯(第4节)，由位置(均值)、协方差矩阵和不透明度𝛼定义，这允许一个非常灵活的优化机制。这就产生了一个合理紧凑的3D场景表示，部分原因是高度各向异性的体块可以用来紧凑地表示精细结构。亮度场的方向外观分量(颜色)通过球面谐波(SH)表示，遵循标准实践[fridovic - keil和Yu等人。2022;Müller et al. 2022]。我们的算法通过三维高斯参数的一系列优化步骤，即位置、协方差、𝛼和SH系数与高斯密度的自适应控制操作交织，来创建亮度场表示(第5节)。我们的方法效率的关键是我们的基于tile的光栅化(第6节)，它允许𝛼-blending的各向异性碎片，由于快速排序而尊重可见性顺序。我们的快速光栅化还包括一个快速的向后通过跟踪累积的𝛼值，没有限制高斯的数量，可以接收梯度。我们的方法概述如图2所示。</p><h1 id="differentiable-3d-gaussian-splatting">DIFFERENTIABLE 3D GAUSSIAN SPLATTING</h1><p>我们的目标是优化一个允许高质量的新视图合成的场景表示，从一个没有法线的稀疏集(SfM)点开始。为了做到这一点，我们需要一个原语，它继承了可微体积表示的属性，同时是非结构化和显式的，以允许非常快速的渲染。我们选择3D高斯，这是可微分的，可以很容易地投影到2D splats允许快速𝛼-blending渲染。我们的表示方法与之前使用2D点的方法相似[Kopanas等人，2021;并假设每个点都是一个带【数】法线的平面小圆。由于SfM点的极度稀疏性，很难估计法线。类似地，</p>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维重建 </tag>
            
            <tag> NeRF </tag>
            
            <tag> splatting </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>制作电脑纹理壁纸</title>
      <link href="/2023/12/16/%E8%87%AA%E5%88%B6%E7%94%B5%E8%84%91%E7%BA%B9%E7%90%86%E5%A3%81%E7%BA%B8/"/>
      <url>/2023/12/16/%E8%87%AA%E5%88%B6%E7%94%B5%E8%84%91%E7%BA%B9%E7%90%86%E5%A3%81%E7%BA%B8/</url>
      
        <content type="html"><![CDATA[<h1 id="查看电脑壁纸尺寸">查看电脑壁纸尺寸</h1><p>在桌面右键，选择<font color=orange>显示设置</font></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216200845959.png" alt="image-20231216200845959" /><figcaption aria-hidden="true">image-20231216200845959</figcaption></figure><p>便可查看到目前电脑的分辨率是：<font color=orange>1920×1080</font></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216201136929.png" alt="image-20231216201136929" /><figcaption aria-hidden="true">image-20231216201136929</figcaption></figure><h1 id="准备纹理图片">准备纹理图片</h1><blockquote><p>补充于2023/12/17</p><p>如果选择的纹理图片和电脑显示器尺寸一样，可以跳过这一步哦~</p></blockquote><blockquote><p>纹理图片可参考网站：</p><ol type="1"><li><a href="https://texturelabs.org/?pg=1">Free Textures, Tutorials, and More (texturelabs.org)</a></li><li><a href="https://www.transparenttextures.com/">Transparent Textures</a></li><li><a href="https://www.cgbookcase.com/textures">Textures | cgbookcase.com</a></li><li><a href="https://www.toptal.com/designers/subtlepatterns/">Subtle Patterns | Free textures for your next web project (toptal.com)</a></li><li><a href="https://polyhaven.com/">Poly Haven</a></li><li><a href="https://www.poliigon.com/search?credit=0">Search - Poliigon</a></li></ol></blockquote><p>考虑到纹理图片和电脑显示器图片可能不一致，由此制作壁纸的最终效果可能稍有不完美，所以我们可以<strong>设法先将尺寸不匹配的纹理图片铺满整个屏幕</strong>，再导出作为制作壁纸使用的纹理图片。</p><ol type="1"><li><p>在PS软件中点击<font color=purple>文件-打开</font>，或者<font color=purple>Ctrl+O</font>打开纹理图片</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231217150953342.png" alt="image-20231217150953342" /><figcaption aria-hidden="true">image-20231217150953342</figcaption></figure></li><li><p>点击<font color=blue>编辑-定义图案</font>，给纹理图案起一个好记的名字，点击<font color=blue>确定</font></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231217151110446.png" alt="image-20231217151110446" /><figcaption aria-hidden="true">image-20231217151110446</figcaption></figure></li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/wwwwww.png" alt="wwwwww" /><figcaption aria-hidden="true">wwwwww</figcaption></figure><ol start="3" type="1"><li><p>新建项目，将尺寸设置为电脑显示器大小</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/屏幕截图%202023-12-17%20151328.png" alt="屏幕截图 2023-12-17 151328" /><figcaption aria-hidden="true">屏幕截图 2023-12-17 151328</figcaption></figure></li><li><p>选择<font color=red>油漆桶工具</font>，将填充物设置为<font color=red>图案</font>，<font color=red><strong>在图案里找到我们定义的纹理图案</strong></font>，进行填充</p></li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231217151756597.png" alt="image-20231217151756597" /><figcaption aria-hidden="true">image-20231217151756597</figcaption></figure><ol start="4" type="1"><li>最后导出即可</li></ol><h1 id="制作壁纸">制作壁纸</h1><blockquote><p>这里使用的图案编辑工具是在线PS工具（<a href="https://www.photopea.com/">Photopea | Online Photo Editor</a>），也可以直接在PS软件中编辑</p></blockquote><blockquote><p>颜色设置可参考配色网站：<a href="http://zhongguose.com/">zhongguose － 传统颜色</a></p></blockquote><ol type="1"><li><p>新建项目，设置分辨率为<strong>电脑显示器分辨率</strong>，设置<strong>背景色</strong>为你喜欢的颜色（或者直接打开一张你喜欢的背景图片）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216202126809.png" alt="image-20231216202126809" /><figcaption aria-hidden="true">image-20231216202126809</figcaption></figure></li><li><p>点击<strong><font color=green>文件-打开</font></strong>或者<font color=blue>Ctrl+O</font>打开下载好的纹理图片</p><blockquote><p>这一步用的纹理图案尺寸比电脑显示器尺寸小一些，最后的纹理效果有点放大失真。如想避免这一点可以跳回去看第二步：准备纹理图片</p></blockquote></li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216210041490.png" alt="image-20231216210041490" /><figcaption aria-hidden="true">image-20231216210041490</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216214805133.png" alt="image-20231216214805133" /><figcaption aria-hidden="true">image-20231216214805133</figcaption></figure><ol start="3" type="1"><li><p>在纹理图案界面，点击<font color=red>编辑-定义新的图案</font>，定义成功后回到壁纸界面</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216215020114.png" alt="image-20231216215020114" /><figcaption aria-hidden="true">image-20231216215020114</figcaption></figure></li><li><p>点击图层，右键点击<font color=purple>混合模式</font></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216215322738.png" alt="image-20231216215322738" /><figcaption aria-hidden="true">image-20231216215322738</figcaption></figure></li></ol><p>在出现的界面选择<font color=orange>纹理</font>，然后<strong>选择我们定义的图案</strong>，点击确定</p><blockquote><p>纹理界面的深度为正，代表纹理效果是凸出的；深度为负，表示纹理效果是凹陷的</p></blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216215555128.png" alt="image-20231216215555128" /><figcaption aria-hidden="true">image-20231216215555128</figcaption></figure><ol start="5" type="1"><li><p>最后依次点击<font color=green>文件-导出为-PNG</font>，将图片导出即可</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216215902983.png" alt="image-20231216215902983" /><figcaption aria-hidden="true">image-20231216215902983</figcaption></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> blog搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Butterfly主题优化 </tag>
            
            <tag> PS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>footer 和 侧边栏优化</title>
      <link href="/2023/12/16/footer%20%E5%92%8C%20%E4%BE%A7%E8%BE%B9%E6%A0%8F%E4%BC%98%E5%8C%96/"/>
      <url>/2023/12/16/footer%20%E5%92%8C%20%E4%BE%A7%E8%BE%B9%E6%A0%8F%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h1 id="footer美化">footer美化</h1>]]></content>
      
      
      <categories>
          
          <category> blog搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Butterfly主题优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Butterfly主题 网页标题崩溃欺骗特效</title>
      <link href="/2023/12/05/Butterfly%E4%B8%BB%E9%A2%98%20%E7%BD%91%E9%A1%B5%E6%A0%87%E9%A2%98%E5%B4%A9%E6%BA%83%E6%AC%BA%E9%AA%97%E7%89%B9%E6%95%88/"/>
      <url>/2023/12/05/Butterfly%E4%B8%BB%E9%A2%98%20%E7%BD%91%E9%A1%B5%E6%A0%87%E9%A2%98%E5%B4%A9%E6%BA%83%E6%AC%BA%E9%AA%97%E7%89%B9%E6%95%88/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考网址：<a href="https://asdfv1929.github.io/2018/01/25/crash-cheat/">Hexo NexT主题中添加网页标题崩溃欺骗搞怪特效 | asdfv1929 's Home</a></p></blockquote><p>在butterfly主题中给网页标题增加一些搞怪特效</p><h4 id="创建js文件-crash_cheat.js">创建js文件 crash_cheat.js</h4><p>打开theme文件夹下的<code>\butterfly\source\js</code>，创建文件<span style="background:#f9eda6;color:red">crash_cheat.js</span>，添加代码：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;!--崩溃欺骗--&gt;</span><br><span class="line"> var OriginTitle = document.title;</span><br><span class="line"> var titleTime;</span><br><span class="line"> document.addEventListener(&#x27;visibilitychange&#x27;, function () &#123;</span><br><span class="line">     if (document.hidden) &#123;</span><br><span class="line">         $(&#x27;[rel=&quot;icon&quot;]&#x27;).attr(&#x27;href&#x27;, &quot;/img/favicon.png&quot;);</span><br><span class="line">         document.title = &#x27;╭(°A°`)╮ 页面崩溃啦 ~&#x27;;</span><br><span class="line">         clearTimeout(titleTime);</span><br><span class="line">     &#125;</span><br><span class="line">     else &#123;</span><br><span class="line">         $(&#x27;[rel=&quot;icon&quot;]&#x27;).attr(&#x27;href&#x27;, &quot;/favicon.png&quot;);</span><br><span class="line">         document.title = &#x27;(ฅ&gt;ω&lt;*ฅ) 噫又好了~&#x27; + OriginTitle;</span><br><span class="line">         titleTime = setTimeout(function () &#123;</span><br><span class="line">             document.title = OriginTitle;</span><br><span class="line">         &#125;, 2000);</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;)</span><br></pre></td></tr></table></figure><h4 id="inject-引用">inject 引用</h4><p>在主题配置文件<code>_config.butterfly.yml</code>的<span style="background:#f9eda6;color:red">inject-bottom</span>下直接引入js文件：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Inject</span><br><span class="line"># Insert the code to head (before &#x27;&lt;/head&gt;&#x27; tag) and the bottom (before &#x27;&lt;/body&gt;&#x27; tag)</span><br><span class="line"># 插入代码到头部 &lt;/head&gt; 之前 和 底部 &lt;/body&gt; 之前</span><br><span class="line">inject:</span><br><span class="line">  head:</span><br><span class="line">    # - &lt;link rel=&quot;stylesheet&quot; href=&quot;/xxx.css&quot;&gt;</span><br><span class="line"></span><br><span class="line">  bottom:</span><br><span class="line">    - &lt;script src=&quot;/js/crash_cheat.js&quot;&gt;&lt;/script&gt;     #页面崩溃欺骗特效</span><br><span class="line">    # - &lt;script src=&quot;xxxx&quot;&gt;&lt;/script&gt;     主题/source/js文件夹中的.js文件</span><br></pre></td></tr></table></figure><h4 id="解决引入js文件时出现inject不生效的问题">解决引入js文件时出现inject不生效的问题</h4><p>在上面一通操作后，再hexo三连，发现页面标题并未出现变化。经过多方查找解决方案，得到如下方法：</p><ol type="1"><li><p>在网站按<code>F12</code>检查，发现控制台报错：<span style="background:#fbd4d0;">Uncaught ReferenceError: $ is not defined</span></p></li><li><p>再次查找，发现原因是没有引用jquery库的jquery.min.js文件</p></li><li><p>引入jquery库：一种是从本地项目路径引用；另一种是通过网页链接引入（<span style="background:#f9eda6;color:red">无论哪种引用库的方式，都要把jquery库的引用放到第一个&lt;s<!--断开script关键字-->cript&gt;引用的前面，这样才能使后面的js文件顺序执行时被成功识别</span>）</p><p>如果我们的项目是https安全域名，那么引入代码为：</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://libs.baidu.com/jquery/2.1.4/jquery.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p>反之：</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;http://libs.baidu.com/jquery/2.1.4/jquery.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> blog搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Butterfly主题优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>学术英语写作与沟通 笔记</title>
      <link href="/2023/11/25/%E5%AD%A6%E5%A0%82%E5%9C%A8%E7%BA%BF%20%E5%AD%A6%E6%9C%AF%E8%8B%B1%E8%AF%AD%E5%86%99%E4%BD%9C%E4%B8%8E%E6%B2%9F%E9%80%9A/"/>
      <url>/2023/11/25/%E5%AD%A6%E5%A0%82%E5%9C%A8%E7%BA%BF%20%E5%AD%A6%E6%9C%AF%E8%8B%B1%E8%AF%AD%E5%86%99%E4%BD%9C%E4%B8%8E%E6%B2%9F%E9%80%9A/</url>
      
        <content type="html"><![CDATA[<p>课程链接：https://www.xuetangx.com/learn/hfut05021002478/hfut05021002478/16907237/video/36265667</p><p><strong>本文只作学习交流使用！</strong></p><h1 id="ch1-title-author-affiliation-论文题目-作者姓名-单位">Ch1 Title + Author + Affiliation 论文题目 + 作者姓名 + 单位</h1><h2 id="how-to-write-the-title-of-academic-paper">1.1 How to Write the Title of Academic Paper</h2><p>Titles are succinct descriptive labels of texts and are meant to fulfifil different purposes, such as to individualize a publication, summarize its content and appeal to its audience among others. They are ideally relevant to present the content of a study and, in general, they are self-explanatory to their readers. This topic will cover 5 sections; they are function, basic requirements, classification, syntax and tips.</p><h2 id="ex-1.1">ex 1.1</h2><p>1.Which type does this title belong to?</p><p>Effect of non-pharmaceutical interventions to contain COVID-19 in China</p><p><font color=red>Noun phrase title </font></p><p>2.Analyze the grammatical construction of the two following titles from top academic journals?</p><p>An investigation of transmission control measures during the first 50 days of the covid-19 epidemic in China ------ From <em>Science</em></p><p><font color=red>Nominal group construction</font></p><h2 id="author-affiliation">1.2 Author + Affiliation</h2><p>Author and affiliation provide important information of a published paper so we need to write them correctly in English. This section will talk about how to write author and affiliation for an academic paper from four aspects, namely definition, function, layout and writing tips.</p><h2 id="ex-1.2">ex 1.2</h2><p>1.Affiliations lie just above authors’ names in published papers, which usually contain such information as authors’ institutions or addresses.</p><p><font color=red>false</font></p><p>2.Corresponding author has the authority to act on behalf of all authors and is the contact person for the research paper.</p><p><font color=red>true</font></p><p>3.In terms of author’s affiliated information, we tend to put bigger unit first then followed by smaller one.</p><p><font color=red>false</font></p><p>4.We usually use the word “and” or the sign “&amp;” to connect the last two authors’ names.</p><p><font color=red>true</font></p><h1 id="ch2-outline-大纲">Ch2 Outline 大纲</h1><h2 id="ex-2">ex 2</h2><p>1.Which of the following is not true about the reverse outlining?</p><p><font color=red>It is useful only when your paper focuses on complex issues in detail.</font></p><p>2.Analyze the sample outline, and try to tell the type of the sample outline.</p><p><strong>Thesis:</strong> Explorers who went to conquer Mt. Everest have achieved much in many ways, but their expeditions also have exerted impacts, negative as well as positive, on Mt. Everest and the local community.</p><ol type="I"><li><pre><code>Background Information</code></pre></li></ol><p>​ A. Location of Mt. Everest</p><p>​ B. Geography of the Surrounding Area</p><p>​ C. Facts about Mt. Everest</p><p>​ 1. Height of the Mountain</p><p>​ 2. How the Mountain Was Named</p><p>​ a. Peak XV</p><p>​ b. Jomolungma (Tibetan name)</p><p>​ c. Sagarmatha (Nepalese name)</p><p>​ 3. The Number of People Who Have Climbed Everest to Date</p><ol start="2" type="I"><li><pre><code>Major Explorers Covered in this Paper</code></pre></li></ol><p>​ A. Sir Edmund Hillary</p><p>​ 1. First to Reach the Summit (1953)</p><p>​ 2. Leader of a Team of Experienced Mountain Climbers Who Worked Together</p><p>​ B. Tenzing Norgay and the Sherpas</p><p>​ 1. Norgay the Experienced Climber and Guide Who Accompanied Hillary</p><p>​ 2. Sherpas Still Used to Guide Expeditions</p><p>​ C. Rob Hall</p><p>​ 1. Leader of the Failed 1996 Expedition</p><p>​ 2. Leading Group of Tourists with Little Mountain Climbing Experience</p><ol start="3" type="I"><li><pre><code>The Impact Expeditions have had on Mt. Everest and Local Community</code></pre></li></ol><p>​ A. Ecological Effects</p><p>​ 1. Loss of Trees Due to High Demand for Wood for Cooking and Heating for Tourists.</p><p>​ 2. Piles of Trash Left by Climbing Expeditions</p><p>​ B. Economic Effects</p><p>​ 1. Expedition Fees Providing Income for the Country</p><p>​ 2. Expeditions Providing Work for the Sherpas, Contributing to the Local Economy.</p><p>​ C. Cultural Effects</p><p>​ 1. Introduction of Motor Vehicles</p><p>​ 2. Introduction of Electricity</p><p><font color=red>The topic outline</font></p><h2 id="importantce-of-an-outline-in-reading-and-writing">Importantce of an outline in reading and writing</h2><p><em>Outline can help the paper readers sort out the important and unimportant details, so that they can read more effectively.</em></p><p><em>When writing, the authors tend to use brainstorming, a useful tool for generating ideas in a free thinking way. Then outlining can help organize all the generated ideas, and help the authors prioritize the important information and eliminate the trivial details.</em></p><p><em>Therefore, although outline is not a part of your paper, it is an important tool to facilitate your academic career.</em></p><h2 id="outline-巩固题">outline 巩固题</h2><p>1.A thesis statement should be _____.</p><p><font color=red>a sentence</font></p><p>2.What is the role of thesis statement in an outline?</p><p><font color=red>the writer’s position about the topic</font></p><h1 id="ch3-abstract-摘要">Ch3 Abstract 摘要</h1><h2 id="ex-3.1">ex 3.1</h2><p>1.An abstract is a ____ summary of your published or unpublished research paper, usually about a paragraph long.</p><p><font color=red>short</font></p><p>2.An abstract lets readers get the gist or essence of your paper or article quickly, in order todecide whether to read the ____ paper.</p><p><font color=red>full</font></p><p>3.An abstract is a short statement about a paper designed to give a complete yet concise understanding of its research and finding.</p><p><font color=red>true</font></p><p>4.An abstract should include a ____ but ____ statement of the problem or issue, a description of the research method and design, the major findings and their significance, and the conclusion.</p><p><font color=red>brief</font> <font color=red>precise</font></p><p>5.The function of an abstract is to ___ .</p><p><font color=red>let the reader understand the main content of the paper </font></p><p><font color=red>provide convenience for the construction and maintenance of scientific and technological literature retrieval database</font></p><p>6.The features of an abstract is ___ .</p><p><font color=red>concise、objective、consistent、complete</font></p><h2 id="ex-3.2">ex 3.2</h2><p>1.An abstract prepares readers to follow the ____ information, analyses, and arguments in your full paper.</p><p><font color=red>detailed</font></p><p>2.An abstract helps readers remember ____ from your paper.</p><p><font color=red>key points</font></p><p>3.An abstract is often composed of____.</p><p><font color=red>Background or purpose、Methods、Results、Conclusion</font></p><p>4.The premises, objectives and tasks of the research work, and the scope of the topics covered will be introduced in the abstract.</p><p><font color=red>true</font></p><p>5.Methods are going to define how do you get answers to your research question, so ____, conditions, ____, means, ____, procedures employed in the paper will be illustrated define what material, what method, and what procedures are to be used.</p><p><font color=red>theories、materials、equipment</font></p><p>6.You may choose ____ as keywords.</p><p><font color=red>3-5 highly relevant terms</font></p><h2 id="samples-of-abstract">Samples of Abstract</h2><h3 id="descriptive-abstract"><strong>Descriptive abstract</strong></h3><p>Translation is not only a linguistic transference, but also an intercultural communication. For quite a long time, translation studies have been concentrated on the prescription of translation methods, with scant attention paid to the description of macro-cultural factors involved in the translating. In this paper, the writer contends that the study of the macro-cultural factors will surely enlarge the scope and enrich the content of the translation studies. The paper is largely a rudimentary step, both in theory and in practice, to expose some of the factors influencing Mr. Fu’s translation of <em>Gone with the Wind</em>, with a historic and descriptive approach employed.</p><h3 id="informative-abstract"><strong>Informative abstract</strong></h3><p>This study investigated the role of "signaling" in helping good readers comprehend expository text. As the existing literature on signaling, reviewed in the last issue of the Journal, pointed to deficiencies in previous studies' methodologies, one goal of this study was to refine prose research methods. Two passages were designed in one of eight signaled versions each. The design was constructed to assess the individual and combined effect of headings, previews, and logical connectives. The study also assessed the effect of passage length, familiarity and difficulty. The results showed that signals do improve a reader's comprehension, particularly comprehension two weeks after the reading of a passage and comprehension of subordinate and superordinate inferential information. This study supports the hypothesis that signals can influence retention of text-based information, particularly with long, unfamiliar, or difficult passages.</p><h1 id="ch4-data-collection-and-evaluation-数据收集与评价">Ch4 Data Collection and Evaluation 数据收集与评价</h1><h2 id="ex-4">ex 4</h2><p>1.There are ____ formats of data collection and evaluation.</p><p><font color=red>3</font></p><p>2.When introducing background information, if the content of a sentence is a general fact that is not affected by time, apply_____.</p><p><font color=red>the present tense</font></p><p>3.Data evaluation refers to the process of ____.</p><p><font color=red>critically reading and evaluating the sources</font></p><p>4.The titles of tables or diagrams are usually expressed in ____.</p><p><font color=red>phrases</font></p><h2 id="tips-for-data-collection-and-evaluation">Tips for Data Collection and Evaluation</h2><h3 id="i.-data-collection"><strong>I</strong>. <strong>Data Collection</strong></h3><p><strong>Collecting primary sources</strong></p><p><strong>Texts</strong>: Once the main arguments are in mind, the text should be re-read while highlighting, and underlining, scribbling in the margins, or using sticky notes to pick out what is needed.</p><p><strong>Interviews</strong>: Locate someone through friends and family networks.</p><p><strong>Collecting secondary sources</strong></p><p>Searching the Web for government documents. Government records may be helpful but in most cases secondary-source research begins at the library.</p><p>The library is the main source of data collection, where readers can access the relevant literature, books, periodicals, audio and video recording, radio, television and the Internet.</p><p><strong>As to the types of information</strong>, in the library there are data in print and electronic resources, including encyclopedias, almanacs (年鉴)，indexes (索引), abstracts, dictionaries, reference literature, compilations, bibliographic catalogs, online databases, web sites, online communities, search engines and so on.</p><p>Researchers often follow the following four steps in collecting data from the library:</p><p>l Search encyclopedias.</p><p>l Search biographical references, yearbooks, atlases（地图册），gazettes（公报，报纸）and professional dictionaries relevant to the topic.</p><p>l Search library catalogs to find appropriate books. Search by author, title or subject.</p><p>l Retrieve journal articles citation from journals or newspapers index with the relevant terms found in encyclopedias, dictionaries and other reference directories (目录).</p><h3 id="ii.-data-evaluation"><strong>II. Data Evaluation</strong></h3><p>Data evaluation refers to the process of critically reading and evaluating the sources. The gist of being critical is not just to criticize, but to question and, not take anything read at face value.</p><p>Structure, purpose, audience and author are four important dimensions of the text to pay close attention to in critical thinking and reading.</p><p><strong>Structure</strong></p><p>If starting with a book, look at the table of contents. See the shape of what is to come and identify places where the thesis or question might be most directly addressed. Notice the subsections. Is there anything very obviously missing?</p><p>Glance at any appendices, diagrams, tables, or figures and see what kinds of things make it into the Endnotes section if there is one.</p><p>Look at the topics listed in the Index at the back. Which of the entries has the most page numbers listed next to it? This will give you an indication of the subjects that contribute to the real scope of the book.</p><p><strong>Purpose</strong></p><p>Examine the title and first few paragraphs. What is the author trying to do? What is his or her bias? Any assumptions to be challenged?</p><p><strong>Audience</strong></p><p>Who is the intended audience? How narrow or broad is it? To answer this, look at stylistic choices such as diction and tone. Who is the target audience?</p><p><strong>Author</strong></p><p>Who is the author? Is it someone a professor has mentioned or one that you come across in the course of other reading? Has the person been mentioned in other texts or bibliographies of other texts? Is the person a teacher or researcher from a reputable academic institution?</p><p>Does the person have considerable knowledge of what he or she is talking about? Is the author respected and well-received（深受好评的）?</p><p><strong>Evaluating Web Pages</strong></p><p>Authorship</p><p>Who wrote this?</p><p>The publishing body</p><p>Is the name of any organization given on the document? Are there headers (页眉) , footers, or a distinctive watermark that show the document to be part of an official academic or scholarly web site?</p><p>Point of view or bias</p><p>Referral to or display of knowledge of the literature</p><p>Accuracy or verifiability of details</p><p>Currency: the date of publication</p><p>All information needs to be evaluated by readers for authority, appropriateness and other personal criteria for value. Never use information that cannot be verified. Establishing and learning criteria to filter information found on the Internet is a good beginning for becoming a critical consumer of information in all forms.</p><p>Learn to be skeptical and then learn to trust your instincts.</p><h2 id="useful-expressions-and-sentence-patterns">Useful Expressions and Sentence Patterns</h2><ol type="1"><li><h3 id="useful-expressions">Useful Expressions</h3></li></ol><ol type="1"><li><p>开头 图表类型：table（表格）、chart（图表）、graph（多指曲线图）、diagram（图标）、column chart（柱状图）、pie graph（饼图）、tree diagram（树形图） 描述：show, describe, illustrate, can be seen from, clear, apparent, reveal, represent 内容：figure, statistic, number, percentage, proportion</p></li><li><p>表示数据变化的单词或者词组 rapid/rapidly 迅速的/地，飞快的/地，险峻的/地 dramatic/dramatically 戏剧性的/地</p></li></ol><p>significant/significantly 有意义的/地，重大的/地 sharp/sharply 锐利的/地，急剧的/地 steep/steeply 急剧升降的/地</p><p>gradual/gradually 渐进的/地，逐渐的/地 slow/slowly 缓慢的/地 slight/slightly 稍微的/略微地</p><p>stable/stably 稳定的/地</p><p>steady/steadily 稳固的/地，坚定不移的/地</p><ol start="3" type="1"><li>其它在描述中的常用到的词</li></ol><p>grow 增长 distribute 分布 unequally 不相等地</p><p>measure n. 方法，措施 v. 估量，调节 forecast n.先见，预见 v. 猜测</p><p>significant changes 图中一些较大变化 noticeable trend 明显趋势 during the same period 在同一时期 in the case of 在……的情况下 in terms of/in respect of/regarding 在……方面 in contrast 相反，大不相同</p><ol start="2" type="1"><li><h3 id="useful-sentence-patterns">Useful Sentence Patterns</h3></li></ol><ol type="1"><li><p>The table shows the changes in the number of…over the period from…to…. 该表格描述了在……年至……年间……数量的变化。</p></li><li><p>The bar chart illustrates that…. 该柱状图展示了……</p></li><li><p>The graph provides some interesting data regarding…. 该图为我们提供了有关……有趣数据。</p></li><li><p>The diagram shows（that）…. 该图向我们展示了……</p></li><li><p>The pie graph depicts （that）…. 该圆形图揭示了……</p></li><li><p>This is a cure graph which describes the trend of…. 这个曲线图描述了……的趋势。</p></li><li><p>The figures/statistics show （that）…. 数据（字）表明……</p></li><li><p>The tree diagram reveals how…. 该树型图向我们揭示了如何……</p></li><li><p>The data/statistics show （that）…. 该数据（字）可以这样理解……</p></li><li><p>The data/statistics/figures lead us to the conclusion that….</p></li><li><p>From the table/chart/diagram/figure，we can see clearly that……or it is clear/apparent from the chart that…. 从图表我们可以很清楚（明显）看到……</p></li><li><p>This is a graph which illustrates…. 这个图表向我们展示了……</p></li><li><p>This table shows the changing proportion of a and b from……to…. 该表格描述了……年到……年间a与b的比例关系。</p></li><li><p>The graph，presented in a pie chart，shows the general trend in…. 该图以圆形图形式描述了……总的趋势。</p></li><li><p>This is a column chart showing…. 这是一个柱型图，描述了……</p></li><li><p>As can be seen from the graph，the two curves show the fluctuation of…. 如图所示，两条曲线描述了……的波动情况。</p></li></ol><h1 id="ch5-summary-概要">Ch5 Summary 概要</h1><h2 id="ex-5">ex 5</h2><p>1.A good summary is an accurate reflection of the author’s viewpoint.</p><p><font color=red>true</font></p><p>2.When you write a summary, you need to________.</p><p><font color=red>Write in your own words</font></p><p>3.How would you avoid plagiarism when you want to cite source materials?</p><p><font color=red>Summarize、Quote、Paraphrase</font></p><p>4.Most summaries start with a sentence containing two elements:____ and ____.</p><p><font color=red>the source、the main idea</font></p><h1 id="ch6-literature-review-文献综述">Ch6 Literature Review 文献综述</h1><h2 id="ex-6.1">ex 6.1</h2><p>1.Is it plagiarism if I use big parts of someone’s literature review for the introduction section of my paper?</p><p><font color=red>Yes</font></p><p>2.The most important reason for you to write literature review in an article is to ____.</p><p><font color=red>find a gap or something contradictory in the previous studies to justify your research</font></p><p>3.Which of the following title looks like a literature review?</p><p><font color=red>Background subtraction techniques: a review.</font></p><h2 id="summarize-vs-synthesize">Summarize VS Synthesize</h2><p><strong>Summarizing and synthesizing information from multiple sources is an indispensable step for writing a literature review.</strong> <strong>But are you really clear about their differences?</strong></p><ul><li><p><strong>A summary reiterates what the study is about.</strong></p><p><strong>Summary is important in a literature review because some research may not be familiar to the reader. It helps the reader develop an understanding of the subject matter. But a literature review goes beyond retelling what the data points out.</strong></p></li><li><p><strong>Synthesis pulls several sources together and explains through the writer’s words what his interpretation of the data means to the writer in his own voice.</strong></p><p><strong>For example, you have five research studies and they all point to the same conclusion.</strong></p></li></ul><h2 id="ex-6.2">ex 6.2</h2><p>1.We can use past tense when describing an action in a research beginning in the past and continuing to the present.</p><p><font color=red>false</font></p><p>2.The literature review is a synthesis and analysis of research on your topic in your own words. Most ideas can be and should be paraphrased. Paraphrase is a preferred choice over direct quotations on most occasions.</p><p><font color=red>true</font></p><h1 id="ch7-proposal-开题报告">Ch7 Proposal 开题报告</h1><h2 id="ex-7.1">ex 7.1</h2><p>1.The research proposal can __________.</p><ul><li><font color=red> clearly and systematically present the research problem and objective</font></li><li><font color=red>indicate the significance and introduce the specific methodology and research procedures synthesize current knowledge, seek gaps and formulate a plan to address the problem</font></li><li><font color=red>synthesize current knowledge, seek gaps and formulate a plan to address the problem</font></li><li><font color=red>provide a timetable for the study and a budget of the investigation or experiments</font></li></ul><p>2.A research proposal is intended to_________.</p><ul><li><font color=red>convince the readers that you are ready to do a research</font></li><li><font color=red>demonstrate that you have the knowledge, full understanding and the expertise to complete the project</font></li><li><font color=red>show your competency in a particular area of study</font></li><li><font color=red>serve as a planning tool</font></li></ul><p>3.The main elements of a research proposal include __________.</p><p><font color=red>what、why、how、expected result</font></p><p>4.Examine carefully the following to determine to what extent the topic chosen meets the criteria for a proposal:</p><ul><li><font color=red>It must be interesting to you.</font></li><li><font color=red>It must be within your competence.</font></li><li><font color=red> It must be feasible.</font></li><li><font color=red>It must be sufficiently delimited.</font></li><li><font color=red>It must have the potential to make a contribution to knowledge or practice in the appropriate area.</font></li></ul><p>5.<strong>The potential supervisors use research proposals to assess</strong></p><ul><li><font color=red>The quality and originality of your ideas</font></li><li><font color=red>Your skills in critical thinking</font></li><li><font color=red>The feasibility of the research project</font></li></ul><h2 id="ex-7.2">ex 7.2</h2><p>1.The budget of a research proposal consists of __________.</p><p><font color=red>a section details the amount of cost of equipment and service</font></p><p><font color=red>a section provides the justification for the funding requested for reviewers to check and see if it is reasonable</font></p><p>2.A thesis statement is a simple sentence that formulates both your topic and _________ toward it.</p><p><font color=red>point of view</font></p><p>3.In exploring data for a research proposal, sources can be divided into ____________.</p><p><font color=red>primary source and secondary source</font></p><p>4.Sometimes the literature review section is incorporated into ( ) section.</p><p><font color=red> Introduction</font></p><p>5.Most students’ literature reviews suffer from the following problems:</p><ul><li><font color=red>Lacking organization and structure</font></li><li><font color=red>Lacking focus, unity and coherence</font></li><li><font color=red>Failing to cite influential papers</font></li><li><font color=red>Failing to critically evaluate cited papers</font></li><li><font color=red>Depending too much on secondary sources</font></li></ul><h2 id="ex-7.3">ex 7.3</h2><p>1.In a successful writing of a research proposal, we should __________.</p><ul><li><font color=red>use accepted scientific terms</font></li><li><font color=red> try to use full forms instead of abbreviations and avoid contractions</font></li><li><font color=red>use formal word and phrases instead of nonstandard or informal expressions</font></li><li><font color=red>use impersonal expressions and passive voice in a proper manner</font></li></ul><p>2.In proposal, the discussion section will include:</p><ul><li><font color=red>An analysis of sources of error in the data</font></li><li><font color=red>Integration with what was previously known</font></li><li><font color=red>Implications for future study</font></li></ul><h1 id="ch8-how-to-write-a-research-paper-introduction">Ch8 How to Write a Research Paper Introduction</h1><h2 id="ex-8">ex 8</h2><p>1.Generally speaking, what will be covered in Introduction Part?</p><ul><li><font color=red>Significance and necessity of the study</font></li><li><font color=red>Background, scope of the issue being study </font></li><li><font color=red>Clear definitions of key term involved in the study</font></li><li><font color=red>Theoretical foundations for the study</font></li><li><font color=red>Objectives of the present study</font></li><li><font color=red>Brief literature review and comment on the previous studies</font></li></ul><h1 id="ch9-material-and-methods-材料与方法">Ch9 Material and Methods 材料与方法</h1><h2 id="ex-9">ex 9</h2><p>1.Among the following research methods, which one is cheaper and easier?</p><p><font color=red>Opinion-based research method.</font></p><p>2.Compared with qualitative research method, quantitative research method_________.</p><p><font color=red>focuses on specific and narrow area.</font></p><p>3.In the materials and methods section, the ____ tense is more natural since you are describing work that is already completed at the time of writing, and the____ voice is preferable since this section focuses more on research than on researcher.</p><p><font color=red>past、passive</font></p><h1 id="ch10-resultsfindings-figures-tables-结果图表">Ch10 Results/Findings + Figures &amp; Tables 结果+图表</h1><p>The Results/Findings describes the statistical results/ findings of a research, which directly answers the research questions raised previously. It is important as it is the section where authors present new information and make new knowledge claims. The results/ findings can be given in the form of numerical data, verbal description or the combination of the above two.</p><h2 id="ex-10.1">ex 10.1</h2><p>1.In which form can the Results/ Findings section be given? ( )</p><p><font color=red>numerical data、verbal description、the combination of B and C</font></p><p>2.What are the three main functions of Results/ Findings section? ( )</p><p><font color=red>locating results、reporting results、explaining results</font></p><p>3.What are the three writing principles for the Results/ Findings section?</p><p><font color=red>faithfulness、innovation、generalization</font></p><p>4.How can the main findings be presented in the Results/ Findings section?</p><ul><li><font color=red>in a certain logical order</font></li><li><font color=red>in chronological order</font></li><li><font color=red>in order of importance</font></li></ul><h2 id="tips-for-writing-the-results-findings">Tips for Writing the Results/ Findings</h2><h3 id="structure-of-the-resultsfindings-section"><strong>1) structure of the Results/Findings section</strong></h3><p><strong>The first subdivision</strong> in this section is to provide preparatory information for the presentation of the results and it functions as a reminder and connector between the Methods section and the Results section. Authors often introduce source of data such as the type of data, the size of data, and the data collection method to prepare for the presentation of the significant results.</p><p><strong>The second subdivision</strong> in this section is to present results. Authors present the results of the study with relevant evidence such as statistics and examples. To report results is obligatory while to locate results and to explain results are optional.</p><h3 id="writing-principles-for-the-results-findings"><strong>2) writing principles for the Results/ Findings</strong></h3><p>There are some writing principles to follow here. They are faithfulness, innovation and generalization, or FIG.</p><p><strong>Principle of faithfulness.</strong> Whether research results can hold water depends on whether they can be tested repeatedly. Never add or delete the research results subjectively. Conflicting results sometimes lead to more meaningful further hypotheses and even more scientific conclusions. So be faithful in presenting your results.</p><p><strong>Principle of innovation.</strong> It is not necessary to cite too much of others' research results here. Therefore, when this section is written, the content of authors’ own original findings should be highlighted.</p><p><strong>Principle of generalization.</strong> Authors need to generalize essential facts and summarize key results in this section. No need to report all specific raw data here. State the main findings in a certain logical order, say, in chronological order or in order of importance.</p><h2 id="ex-10.2">ex 10.2</h2><p><em>Tables and figures are very important in academic writing and communication. They provide visual ways of presenting data and each type has its own advantages and disadvantages.</em></p><p>1.Tables and figures provide visual ways of presenting data and each type has its own advantages and disadvantages.</p><p><font color=red>true</font></p><p>2.Figures can display exact data or statistical information.</p><p><font color=red>false</font></p><p>3.There is no need to classify, process or select data in tables or figures.</p><p><font color=red>false</font></p><p>4.Which one is a flow chart?</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/ba2ccc22-193a-4164-8a21-f41d0087b960.png" alt="chart 4.png" /><figcaption aria-hidden="true">chart 4.png</figcaption></figure><h2 id="tips-for-various-tables-and-figures">Tips for Various Tables and Figures</h2><p>A table is an arrangement of data in rows and columns, or possibly in a more complex structure. Tables are widely used in communication, research and data analysis. They can display exact data or statistical information.</p><p>Figures have many various types. Here we just focus on pie chart, bar chart, line chart and flow chart.</p><ol type="1"><li>A pie chart is a circular statistical graphic which is divided into slices to illustrate numerical proportions. But sometimes it is not easy to compare different sections of a given pie chart or compare data across different pie charts.</li><li>A bar chart is a chart to present grouped data with rectangular bars whose lengths are proportional to the values they present. It can show comparisons among various categories.</li><li>A line chart displays information as a series of data points connected by straight line segments. It is often used to visualize a trend in data changing with the time or condition.</li><li>A flow chart is a diagram to show the order of operations or sequence of tasks for solving a problem or managing a complex project.</li></ol><h1 id="ch11-discussion-and-conclusion">Ch11 Discussion and Conclusion</h1><h2 id="ex-11.1">ex 11.1</h2><p>1.The results show that those who reported driving unsupervised 1-12 times were 80% more likely to be involved in crash than those who reported never having done this.</p><p><font color=red>Reporting results</font></p><p>2.The research has some potential limitations.</p><p><font color=red>Indicating limitations</font></p><p>3.We bring everything together in this part by discussing the ______ of our findings and its relationship to previous research in the area.</p><p><font color=red>significance</font></p><p>4.Each result reported should be followed by a proper ______.</p><p><font color=red>discussion</font></p><h2 id="tips-for-discussion">Tips for Discussion</h2><ol type="1"><li>summarized the main results.</li><li>interpreted (not described) the results.</li><li>discussed the significance of the results.</li><li>explained whether the results prove or disprove the hypothesis.</li><li>discussed the results in the light of previous research.</li><li>explained the wider implications of the work.</li><li>discussed any problems with or limitations of the study.</li><li>made suggestions for improvements.</li><li>suggested directions for future research.</li></ol><h2 id="ex-11.2">ex 11.2</h2><p>1.Given that a significant portion of learner license holders report driving unsupervised and those that violate this condition the most are more likely to crash, improving compliance with learner license supervised driving conditions varrants increased attention.</p><p><font color=red>Interpreting results</font></p><p>2.Evaluations of the initiatives designed to increase compliance supervised driving condition should be a research priority.</p><p><font color=red>Recommending further research</font></p><p>3.Though components of a conclusion may vary, a conclusion generally contains a restatement of the thesis in the ________, a summary of key points in the body, and a broad statement.</p><p><font color=red>introduction </font></p><p>4.This exploratory case study ________the stated beliefs and actual instructional practices of two experienced teachers of English language in a primary school in Singapore.</p><p><font color=red>investigated </font></p><h2 id="tips-for-conclusion">Tips for Conclusion</h2><p>The structure of a conclusion generally follows a pattern, moving from specific to general.</p><ol type="1"><li>Restatement of the main premise.</li><li>Summary of key points in the essay.</li><li>Broad statement.</li></ol><h1 id="ch12-how-to-write-acknowledgements-致谢">Ch12 How to Write Acknowledgements 致谢</h1><h2 id="ex-12">ex 12</h2><p>1.请翻译：国家自然科学基金</p><p><font color=red>National Natural Science Foundation of China</font></p><p>2.Whom will we thank in Acknowledgement Part?</p><ul><li><font color=red>Those who have helped the author's scientific research</font></li><li><font color=red>Reviewers and editors </font></li><li><font color=red>Funding / program</font></li></ul><h1 id="ch13-references-参考文献">Ch13 References 参考文献</h1><h2 id="ex-13">ex 13</h2><p>1.The functions of the references section include the following points except:</p><p><font color=red>It concludes the whole researc</font></p><p>2.The following two samples are documented in ____ style.</p><p>Wysocki, Anne Frances, et al. <em>Writing New Media: Theory and Applications for Expanding the Teaching of Composition</em>. Logan, UT: Utah State UP, 2004. Print.</p><p>Foucault, Michel. <em>Madness and Civilization: A History of Insanity in the Age of Reason</em>. Trans. Richard Howard. New York: Vintage-Random House, 1988. Print.</p><p><font color=red>MLA</font></p><p>3.Which of the following is a documentation tool?</p><p><font color=red>Latex、Endnote、Bibtex（Trados 是翻译软件）</font></p><h2 id="samples-of-apa-and-mla-style">Samples of APA and MLA style</h2><p>Anderson, A. K., Christoff, K., Panitz, D., De Rosa, E., &amp; Gabrieli, J. D. E. (2003). Neural correlates of the automatic processing of threat facial signals. <em>Journal of Neuroscience, 23,5627-5633.</em></p><p>Chow T. W., &amp; Cummings, J. L. (2000). The amygdala and Alzheimer<em>’</em>s disease. In J. P. Aggleton (Ed.), <em>The amygdala: A functional analysis</em> (pp. 656–680). Oxford,England: Oxford University Press.</p><p>Shipley, W. C. (1986). <em>Shipley Institute of Living Scale</em>. Los Angeles, CA: Western Psychological Services.</p><p>Wheeler, D.P., &amp; Bragin, M. (2007). Bringing it all back home: Social work and the challenge of returning veterans. Health and Social Wor, 32, 297-300. Retrieved from <a href="http://www/">http://www</a>. naswpressonline. org</p><p>Samples of MLA style</p><p>Wysocki, Anne Frances, et al. <em>Writing New Media: Theory and Applications for Expanding the Teaching of Composition</em>. Logan, UT: Utah State UP, 2004. Print.</p><p>Foucault, Michel. <em>Madness and Civilization: A History of Insanity in the Age of Reason</em>. Trans. Richard Howard. New York: Vintage-Random House, 1988. Print.</p><p>Crowley, Sharon, and Debra Hawhee. <em>Ancient Rhetorics for Contemporary Students</em>. 3rd ed. New York: Pearson/Longman, 2004. Print.</p><p><strong>The New Jerusalem Bible</strong>. Ed. Susan Jones. New York: Doubleday, 1985. Print.</p><p>Bishop, Karen Lynn. <em>Documenting Institutional Identity: Strategic Writing in the IUPUI Comprehensive Campaign</em>. Diss. Purdue University, 2002. Ann Arbor: UMI, 2004. Print.</p><p>Poniewozik, James. "TV Makes a Too-Close Call." <em>Time</em> 20 Nov. 2000: 70-71. Print.</p><p>Buchman, Dana. "A Special Education." <em>Good Housekeeping</em> Mar. 2006: 143-48. Print.</p><p>"Of Mines and Men." Editorial. <em>Wall Street Journal</em> east. ed. 24 Oct. 2003: A14. Print.</p><p>Hamer, John. Letter. <em>American Journalism Review</em> Dec. 2006/Jan. 2007: 7. Print.</p><p>Bagchi, Alaknanda. "Conflicting Nationalisms: The Voice of the Subaltern in Mahasweta Devi's Bashai Tudu." <em>Tulsa</em> <em>Studies in Women's Literature</em> 15.1 (1996): 41-50. Print.</p><p>Aristotle. <em>Poetics</em>. Trans. S. H. Butcher. <em>The Internet Classics Archive</em>. Web Atomic and Massachusetts Institute of Technology, 13 Sept. 2007. Web. 4 Nov. 2008. <a href="http://classics.mit.edu/Aristotle.html" class="uri">http://classics.mit.edu/Aristotle.html</a>.</p><h2 id="references-作业">references 作业</h2><p>1.If writers do not give credit for borrowed ideas or words, they make a serious error, i.e. ____.</p><p><font color=red>plagiarism</font></p><p>2.When you use a direct quotation in APA style, you must state the following elements except _____.</p><p><font color=red>the edition of the journal</font></p><h2 id="文献引用的三种格式">文献引用的三种格式</h2><p>文献引用一般有三种形式。（参考链接：<a href="https://blog.csdn.net/programmer_jiang/article/details/118557929#:~:text=Author%20Prominent%20Citation**：一般过去时**%20关注完成研究的人。%20一般以作者姓放在句首做主语，%20其后括号标注引注年份%20，汇报动词（reporting%20verb）做谓语动词。,higher%20in%20order%20to%20yield%20acceptable%20sliceability.%204">文献英语期末_hedging模糊语有哪些-CSDN博客</a>）</p><ul><li>Information Prominent Citation<strong>：一般现在时</strong></li></ul><p>关注前人研究的内容。一般陈述研究内容，引注将作者和年份放在句末括号里。 如：Scientific paper writing skill is usually adopted with learning by doing and formal training (Auvinen, 2015).</p><ul><li>Author Prominent Citation<strong>：一般过去时</strong></li></ul><p>关注完成研究的人。一般以作者姓放在句首做主语，<strong>其后括号标注引注年份</strong>，汇报动词（reporting verb）做谓语动词。 如：Theno et al. (1978) concluded that salt content had to be 2% or higher in order to yield acceptable sliceability. 4</p><ul><li>Weak Author Prominent Citation<strong>：现在完成时</strong></li></ul><p>关注一系列研究的相似结果。一般以 many researchers，many scholars 等一群人作为句子主语，谓语动词用完成时，引注将诸多作者和年份排列在句末括号里。 如： Researchers have noted that resolving this debate hinges on understanding the relationship between yield and biodiversity, the likelihood of land being spared, and external consequences of practices raising yield, such as agrochemical runoff (Grau et al., 2013; Green et al., 2005; Phalan et al., 2011).</p><h1 id="ch14-academic-poster-学术海报">Ch14 Academic Poster 学术海报</h1><h2 id="ex-14.1">ex 14.1</h2><p>1.Important information should be ____ from about 10 feet away.</p><p><font color=red>readable</font></p><p>2.Title is ____ and draws interest.</p><p><font color=red>short</font></p><p>3.Academic posters could be an ____way of communicating concisely, visually and attractively.</p><p><font color=red>effective</font></p><p>4.Use ____ language to present your work. Avoid ____ unless you're really positive that yours will be a specialist-only audience.</p><p><font color=red>plain、jargon and acronyms</font></p><p>5.An effective poster lets ____ tell the story, uses ____ sparingly, and keeps the sequence well-ordered and obvious.</p><p><font color=red>graphs and images、text</font></p><p>6.Academic posters summarize information or research concisely and attractively, to help publicize information or research and generate discussion.</p><p><font color=red>true</font></p><p>7.What roles does an academic poster play in academic communication？</p><ul><li><font color=red>Posters are widely used in the academic community.</font></li><li><font color=red>Most conferences include poster presentations in their program. </font></li><li><font color=red>Academic posters may be displayed at national or international conferences.</font></li><li><font color=red>They may also be published online as part of conference proceedings.</font></li><li><font color=red>Academic Posters can be used as assessment at university.</font></li></ul><h2 id="ex-14.2">ex 14.2</h2><p>1.An effective poster can make a ____impact, so it's worth developing your poster planning skills.</p><p><font color=red>strong</font></p><p>2.In some courses, ____ and ____ may be weighted at 60%, with ____ and ____weighted at 40%.</p><p><font color=red>content、structure、visual organization、presentation</font></p><p>3.If you are reporting on a piece of research with an academic poster , you can turn to the some language signals to ____, ____, ____, and ____.</p><p><font color=red>introduce the poster、locate a point on the poster、answer directly、handle complex questions</font></p><p>4.Academic posters need to show evidence of reading and research, so you must always include Literature cited.</p><p><font color=red>true</font></p><p>5.Acknowledgments section is to thank individuals for specific contributions to the project Also include in this section explicit disclosures for any conflicts of interest and conflicts of commitment.</p><p><font color=red>true</font></p><h2 id="poster">poster</h2><p>• An effective poster is a visual communications tool;</p><p>• An effective poster will get your main point(s) across to as many people as possible;</p><p>• An effective poster is focused on a single message; lets graphs and images tell the story; uses text sparingly; keeps the sequence well-ordered and obvious.</p><h1 id="ch15-linguistic-features-of-academic-english-学术英语的语言特点">Ch15 Linguistic Features of Academic English 学术英语的语言特点</h1><h2 id="ex-15.1">ex 15.1</h2><p>1.In writing academic paper, we should avoid expressing____ arising out of intuition, feeling, prejudice or your own experience.</p><p><font color=red>personal opinions</font></p><p>2.Another common feature of academic writing is nominalization, whereby verbs become _____.</p><p><font color=red>nouns</font></p><p>3.We can make tentative statements by applying ____ into our academic writing.</p><p><font color=red>models、verbs、adverbs、adjectives</font></p><h2 id="tips-for-academic-writing">Tips for Academic Writing</h2><p><strong>Why do we need a formal language style?</strong></p><p>To fulfil the expectations of academic readers.</p><p>It is important to follow the specific genre requirements in academic writing. This is a defining feature of academic writing.</p><p>Term papers and research reports are generally formal.</p><p><strong>How to achieve formality?</strong></p><p>Advanced and academic vocabulary</p><p>Long and complex sentences</p><p>The minimized use of 1st - and 2nd –person pronouns</p><p><strong>The use of advanced and academic vocabulary</strong></p><p>Avoid using simple and colloquial words</p><p><strong>Simple words</strong> <strong>Advanced words</strong></p><p>eat consume</p><p>try attempt/endeavor</p><p>keep remain/maintain</p><p>good (for) beneficial</p><p>bad(for) harmful/detrimental</p><p><strong>Avoid using phrasal verbs</strong></p><p>It would be better to use a single verb instead of a phrasal verb.</p><p><strong>Phrasal verbs</strong> <strong>One-word verbs</strong></p><p>look into investigate</p><p>find out discover</p><p>cut down reduce</p><p>go up increase</p><p>get rid of eliminate</p><p><strong>Avoid using shortened forms of words/contractions</strong></p><p><strong>Contractions Full forms</strong></p><p>won’t will not</p><p>didn’t did not</p><p>can’t cannot</p><p>it’s it is</p><p>you’re you are</p><p>​</p><p><strong>Academic Word List (AWL)</strong></p><p>Developed by Averil Coxhead, a scholar in New Zealand</p><p>570 word families</p><p>Formal, advanced and academic</p><p><strong>The use of long and complex sentences</strong></p><p>Subordinate clauses: add extra information to the main clause</p><p>Subordinate conjunctions 从属连词</p><p>Some youngsters admit getting restless if their phones is not nearby.</p><p>Relative pronouns 关系代词</p><p>It is advisable to write an outline, which will help you organize the essay in a more logical way.</p><p><strong>Subordinate conjunctions</strong> <strong>Relative pronouns</strong></p><p>Once after until that who whose</p><p>Provided that although when</p><p>Rather than as whenever which whoever whosever</p><p>Since because where</p><p>So that before whereas whichever whom whomever than even if wherever whether if</p><p>Though while unless why</p><p>l Do not always use too long and complex sentences in your essay, because readers will find them difficult to understand.</p><p>l The best way is to use a combination of simple and complex sentences.</p><p><strong>The minimized use of 1st - and 2nd –person pronouns</strong></p><p>First-person pronouns: I, we, us, my, our</p><p>second-person pronouns: you and your</p><p>E.g. Recently, we have discussed COVID-19 and China-US relations a lot.</p><p>→ Recently, COVID-19 and China-US relations have been much discussed.</p><p>Tip: Use third-person and passive voice.</p><h2 id="useful-expressions-in-results-and-tables-figures">Useful Expressions in Results and Tables &amp; Figures</h2><h3 id="useful-expressions-in-results"><strong>1.</strong> <strong>Useful Expressions in Results</strong></h3><p><strong>1.1 Active voice VS Passive voice</strong></p><p>Table 1 presents…and Table 2 presents…</p><p>Our hypothesis predicted….</p><p>Information…was obtained….</p><p>…this information was collected when</p><p><strong>1.2 Suitable reporting verbs</strong> show, indicate, reveal, report,</p><p>describe, explain, display, present…</p><p><strong>1.2.1 Locating the data</strong></p><p>As can be seen from Table 1…</p><p>… are shown/given/provided/ summarized in Table 1.</p><p>Table 1 demonstrates/</p><p>indicates/suggests….</p><p><strong>1.2.2 Highlighting the data</strong></p><p>…is exactly/approximately/almost the same as</p><p>… is completely/entirely/quite different from…</p><p>The main difference between…and… is that…</p><p><strong>1.2.3 Discussing the data</strong></p><p>The data clarify the relationship</p><p>between… and…</p><p>There is some evidence in the data to support our hypothesis, which proposed that…</p><p>This particular result may be</p><p>attributed to the influence of …</p><p><strong>1.3 Phrases of generality</strong></p><p>Overall</p><p>In general</p><p>On the whole</p><p>In the main</p><p>With …exception(s)</p><p>Overall, the results indicate that students performed above the 12th-grade level.</p><p>The overall results indicate…</p><p>The results indicate, overall, that…</p><p>In general, the experimental samples resisted…</p><p>With one exception, the experimental samples resisted…</p><h3 id="useful-expressions-in-tables-and-figures"><strong>2.</strong> <strong>Useful Expressions in</strong> <strong>Tables and Figures</strong></h3><p><strong>2.1 Very frequent and appropriate verbs</strong></p><p>reported, show, characterized, suggests, used, intended, contradict, suggest, prevail, focused, enables, speculate, maintain, compared, focused, claimed, shows, tend, represent</p><p><strong>2.2 Tense of verbs</strong></p><p>The tables show that those who reported driving unsupervised 1-12 times were 80% more likely to be involved in crash than those who reported never having done this.</p><h2 id="ex-15.2">ex 15.2</h2><p>1.Regardless of the placement, each figure must be _____ consecutively and complete with caption.</p><p><font color=red>numbered</font></p><p>2.What kind of tense do we usually use if we start a sentence with words like “Table 1 or Table n”?</p><p><font color=red>The present tense</font></p><p>3.We can place figures and tables within ____, or we can include them at____.</p><p><font color=red>the text of the results、the end of the report</font></p><h2 id="additional-material-for-describing-tables-and-figures-in-academic-language">Additional material for describing tables and figures in academic language</h2><h3 id="图形种类及概述法">1、 图形种类及概述法：</h3><p>泛指一份数据图表： a data graph(曲线图)/chart/diagram/illustration/table 饼图：pie chart 直方图或柱形图：bar chart/histogram 趋势曲线图：line chart/curve diagram 表格图：table 流程图或过程图：flow chart/sequence diagram 程序图：processing/procedures diagram</p><h3 id="常用的描述用法">2、常用的描述用法</h3><p>The table/chart diagram/graph shows （that） According to the table/chart diagram/graph As （is） shown in the table/chart diagram/graph As can be seen from the table/chart/diagram/graph/figures， figures/statistics shows （that）…… It can be seen from the figures/statistics We can see from the figures/statistics It is clear from the figures/statistics It is apparent from the figures/statistics table/chart/diagram/graph figures （that） …… table/chart/diagram/graph shows/describes/illustrates</p><h3 id="图表中的数据data具体表达法">3、图表中的数据（Data）具体表达法</h3><p>数据（Data）在某一个时间段固定不变：fixed in time 在一系列的时间段中转变：changes over time 持续变化的data在不同情况下： 增加：increase/raise/rise/go up …… 减少：decrease/grow down/drop/fall …… 波动：fluctuate/rebound/undulate/wave …… 稳定：remain stable/stabilize/level off ……</p><h3 id="二相关常用词组">二、相关常用词组</h3><h4 id="主章开头">1、主章开头</h4><p>图表类型：table（表格）、chart（图表）、diagram（图标）、graph（多指曲线图）、column chart（柱状图）、pie graph（饼图）、tree diagram（树形图） 描述：show、describe、illustrate、can be seen from、clear、apparent、reveal、represent 内容：figure、statistic、number、percentage、proportion</p><h4 id="表示数据变化的单词或者词组">2、表示数据变化的单词或者词组</h4><p>rapid/rapidly 迅速的，飞快的，险峻的 dramatic/dramatically 戏剧性的，生动的 significant/significantly 有意义的，重大的，重要的 sharp/sharply 锐利的，明显的，急剧的 steep/steeply 急剧升降的 steady/steadily 稳固的，坚定不移的 gradual/gradually 渐进的，逐渐的 slow/slowly 缓慢的，不活跃的 slight/slightly 稍微的、略微地 stable/stably 稳定的</p><h4 id="其它在描述中的常用到的词">3、其它在描述中的常用到的词</h4><p>significant changes 图中一些较大变化 noticeable trend 明显趋势 during the same period 在同一时期 grow/grew 增长 distribute 分布 unequally 不相等地 in the case of 在……的情况下 in terms of/in respect of/regarding 在……方面 in contrast 相反，大不相同 government policy 政府政策 market forces <a href="https://www.baidu.com/s?wd=市场力量&amp;tn=SE_PcZhidaonwhc_ngpagmjz&amp;rsv_dl=gh_pc_zhidao">市场力量</a> measure n.尺寸，方法，措施 v.估量，调节 forecast n.先见，预见 v.猜测</p><h3 id="三图表描述套句精选">三、图表描述套句精选</h3><p>1.The table shows the changes in the number of…over the period from…to…. 该表格描述了在……年之……年间……数量的变化。 2.The bar chart illustrates that…. 该柱状图展示了…… 3.The graph provides some interesting data regarding…. 该图为我们提供了有关……有趣数据。 4.The diagram shows （that）…. 该图向我们展示了…… 5.The pie graph depicts （that）…. 该圆形图揭示了…… 6.This is a cure graph which describes the trend of…. 这个曲线图描述了……的趋势。 7.The figures/statistics show （that）…. 数据（字）表明……</p><p>8.The tree diagram reveals how….</p><p>该树型图向我们揭示了如何……</p><p>9.The data/statistics show （that）….</p><p>该数据（字）可以这样理解……</p><p>10.The data/statistics/figures lead us to the conclusion that….</p><p>这些数据资料令我们得出结论…… 11.As is shown/demonstrated/exhibited in the diagram/graph/chart/table…. 如图所示…… 12.According to the chart/figures…. 根据这些表（数字）…… 13.As is shown in the table…. 如表格所示…… 14.As can be seen from the diagram，great changes have taken place in…. 从图中可以看出，……发生了巨大变化。 15.From the table/chart/diagram/figure，we can see clearly that……or it is clear/apparent from the chart that…. 从图表我们可以很清楚（明显）看到…… 16.This is a graph which illustrates…. 这个图表向我们展示了…… 17.This table shows the changing proportion of a &amp; b from……to…. 该表格描述了……年到……年间a与b的比例关系。 18.The graph，presented in a pie chart，shows the general trend in…. 该图以圆形图形式描述了……总的趋势。 19.This is a column chart showing…. 这是一个柱型图，描述了…… 20.As can be seen from the graph，the two curves show the fluctuation of…. 如图所示，两条曲线描述了……的波动情况。 21.Over the period from…to…, the…remained level. 在……至……期间，……基本不变。 22.In the year between…and…. 在……年到……期间…… 23.In the 3 years spanning from 1995 through 1998…. 1995年至1998三年里…… 24.From then on/from this time onwards…. 从那时起…… 25.The number of…remained steady/stable from （month/year） to （month/year）. ……月（年）至……月（年）……的数量基本不变。 26.The number sharply went up to…. 数字急剧上升至…… 27.The percentage of…stayed the same between…and…. ……至……期间……的比率维持不变。 28.The figures peaked at…in（month/year）. ……的数目在……月（年）达到顶点，为…… 29.The percentage remained steady at…. 比率维持在…… 30.The percentage of…is slightly larger/smaller than that of…. ……的比例比……的比例略高（低）。 31.There is not a great deal of difference between…and… ……与……的区别不大。 32.The graphs show a threefold increase in the number of…. 该图表表明……的数目增长了三倍。 33…decreased year by year while…increased steadily. ……逐年减少，而……逐步上升。 34.The situation reached a peak（a high point at）of …%. ……的情况（局势）到达顶（高）点，为……百分点。 35.The figures/situation bottomed out in…. 数字（情况）在……达到底部。</p><p>36.The figures reached the bottom/a low point/hit a trough.</p><p>数字（情况）达到底部（低谷）。</p><p>37.A is …times as much/many as b.</p><p>a是b的……倍。 38.A increased by…. a增长了…… 39.A increased to…. a增长到……</p><p>40.high/low/great/small/ percentage.</p><p>比率高（低） 41.There is an upward trend in the number of…. ……数字呈上升趋势。 42.Aconsiderable increase/decrease occurred from…to…. ……到……发生急剧上升。 43.From…to…the rate of decrease slow down. 从……到……，下降速率减慢。 44.from this year on，there was a gradual decline/ reduction in the…，reaching a figure of…. 从这年起，……逐渐下降至…… 45.be similar to… 与……相似 46.be the same as… 与……相同 47.There are a lot similarities/differences between…and…. ……与……之间有许多相似（不同）之处 48.A has something in common with b. a与b有共同之处。 49.The difference between a and b lies in…. a与b之间的差别在于…… 50.…（year）witnessed/saw a sharp rise//in….</p><p>……年……急剧上升。</p><h2 id="ex-15.3">ex 15.3</h2><p>1.We analyzed ____ variety of tissue samples.</p><p><font color=red>a</font></p><p>2.____colors affect our perception of reality.</p><p><font color=red>x</font></p><p>3.Those interested in donating the kidney should notify the hospital’s donation committee.</p><p><font color=red>false</font></p><p>4.The cheetah is the quickest of the land animals.</p><p><font color=red>true</font></p><h2 id="samples-for-articles">Samples for Articles</h2><ol type="1"><li><p>Becoming an expert takes a lot of ✗ experience.</p></li><li><p>The Cheetah is the quickest of the land animals.</p></li></ol><h2 id="culture-and-ethics-练习">culture and ethics 练习</h2><p>1.Ethics in academic writing exclude the following action(s):</p><p><font color=red>academic theft</font></p><p>2.Scientific writing is objective, impersonal and detached, so cultural difference cannot be detected in the academic field.</p><p><font color=red>false</font></p><h2 id="ex-15.4">ex 15.4</h2><p>1.In the paper discussed in this lecture, the word “Creator” should not be used because it ____.</p><p><font color=red>makes references to Creationism</font></p><p>2.Academic Integrity and ethics includes:</p><ul><li><font color=red>honesty</font></li><li><font color=red>no fabrication, falsification, or plagiarism</font></li><li><font color=red>honoring property rights</font></li></ul><h1 id="ch16-academic-correspondence-with-the-editors-与编辑的学术联系">Ch16 Academic Correspondence with the Editors 与编辑的学术联系</h1><h2 id="ex-16.1">ex 16.1</h2><p>1.You write a cover letter to______.</p><ul><li><font color=red>Introduce your paper to the editor</font></li><li><font color=red>Recommend reviewers to the editor</font></li><li><font color=red>Oppose reviewers to the editor</font></li></ul><p>2.What is a cover letter?</p><p><font color=red>A cover letter also called submission letter is a letter of transmittal to the editor of the journal for possible publication.</font></p><p>3.In order to choose your target journal, you need to find a journal without any peer review.</p><p><font color=red>false</font></p><h2 id="ex-16.2">ex 16.2</h2><p>1.If you want to know whether your paper is accepted or not, you need to write______.</p><p><font color=red>An inquiry letter</font></p><p>2.Peer review will provide you with comments and suggestions from ____and editors.</p><p><font color=red>reviewers</font></p><p>3.You can write a rebuttal letter to accuse the reviewers of bias or incompetence.</p><p><font color=red>false</font></p><h1 id="ch17-international-academic-conference-presentation-skills-国际学术会议宣讲技巧">Ch17 International Academic Conference Presentation Skills 国际学术会议宣讲技巧</h1><h2 id="ex-17">ex 17</h2><p>1.A presentation is the act of effective _____ communication with an audience.</p><p><font color=red>oral</font></p><p>2.Presentation skills consist of three major parts: ____,____ , and____ .</p><p><font color=red>audience analysis、delivery、managing stage fright</font></p><p>3.Which of the following should NOT be done in international academic conference presentation?</p><p><font color=red> speak towards the screen</font></p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> course </tag>
            
            <tag> 论文写作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to write an outline 笔记</title>
      <link href="/2023/11/09/How%20to%20write%20an%20outline%20%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/11/09/How%20to%20write%20an%20outline%20%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="how-to-write-an-outline-笔记">How to write an outline 笔记</h1><h2 id="outline-定义">outline 定义</h2><p>Outline是你的写作地图。写作者使用outline来以一种有逻辑的、清晰的顺序展示思想，并有目的地组织话题及支撑的细节。</p><p>它展示了每一段或每一节将包含的信息，以及它们之间的顺序。outline可以被用来辨别并减少文章中可能存在的缺点或缺少的重点。</p><h2 id="outline-的功能">outline 的功能</h2><p>撰写outline将帮助你集中于眼前的任务，并避免不必要的切题、逻辑谬误和段落的不完善。</p><p>在进行大型研究项目时，很容易混淆资源来源，或者在研究过程中忘记阅读到的内容。为了让你对阅读的内容保持清晰，阅读时可以在页边空白处或另一张纸上记录reverse outline。这个reverse outline可以让你轻松地浏览你的资源，突出显示那些可能有助于你研究的信息。</p><h2 id="outline-的类型和结构">outline 的类型和结构</h2><p><strong>Topic outline</strong> 由简短的短语组成。当您处理的问题可能会以各种不同的方式出现在您的论文中时，这种方法非常有用。</p><p><strong>Sentence outline</strong> 为完整句子。当您的论文侧重于复杂问题的细节时，这种方法非常有用。</p><p>两种outline都遵循严格的格式，使用罗马数字和阿拉伯数字以及字母表中的大写和小写字母。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/654cdb69c458853aefce1aba.png" alt="image-20231109124822463" /><figcaption aria-hidden="true">image-20231109124822463</figcaption></figure><h2 id="outlining-步骤">outlining 步骤</h2><ol type="1"><li><strong>初步工作</strong>：写下主题，然后开始头脑风暴。</li><li><strong>确定研究问题</strong>：研究问题是outline其余部分的重点。试着用一句话或短语来概括你的论文要点。它也是决定论文标题的关键。</li><li><strong>确定主要类别</strong>：你将分析哪些要点？导言（Introduction）描述了你所有的要点；你论文的其余部分可以用来阐述这些要点。</li><li><strong>创建第一个类别</strong>：您要介绍的第一点是什么？如果文章围绕一个复杂的术语展开，那么可以从定义开始。对于一篇涉及特定理论的应用和测验的论文来说，提供该理论的一般背景是一个很好的切入点。</li><li><strong>创建子类别</strong> ：完成这些步骤后，在其下方叙述支撑主要论点的材料。使用的类别数量取决于尝试涵盖的信息量。</li></ol><h2 id="tips">Tips</h2><ul><li><p>在开始outlining之前，需要一个清晰的论文陈述或明确的目的或论点，因为outline中的其他内容都将用于支撑主题。</p></li><li><p>论文陈述应该是一个完整的肯定陈述句，而不是疑问句、短语或从属分句。</p></li><li><p>避免混合类型。与 topic outline 或 sentence outline 保持一致。请勿混合使用两种类型。</p></li><li><p>使用平行。每个标题和副标题都应保持与其他标题平行的结构。最明显的是 "topic" 和 "sentence" 的 outline 格式；平行性也指词性和时态。</p></li><li><p>使信息相互关联协调。第一个主标题所提供的信息应该与第二个主标题所提供的信息同等重要。副标题也是如此。</p></li><li><p>学会划分。每个主标题应该被划分为两个或多个部分。换言之，每个主标题至少应有两个副标题。</p></li><li><p>大写问题。在写outline、主标题和副标题时，几乎总是按照正确的句子大写规则书写。</p></li></ul><h2 id="附在读写中-outline-的重要性">附：在读写中 outline 的重要性</h2><p>Outline can help the paper readers sort out the important and unimportant details, so that they can read more effectively.</p><p>When writing, the authors tend to use brainstorming, a useful tool for generating ideas in a free thinking way. Then outlining can help organize all the generated ideas, and help the authors prioritize the important information and eliminate the trivial details.</p><p>Therefore, although outline is not a part of your paper, it is an important tool to facilitate your academic career.</p><p>参考网址：https://www.xuetangx.com/learn/hfut05021002478/hfut05021002478/16907237/video/36265680</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> course </tag>
            
            <tag> 论文写作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/11/06/hello-world/"/>
      <url>/2023/11/06/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
