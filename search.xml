<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>长江后浪扑前浪，NeRF现在沙滩上</title>
      <link href="/2024/02/17/%E4%B8%80%E4%BA%9B%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D%E7%B4%A2%E5%BC%95/"/>
      <url>/2024/02/17/%E4%B8%80%E4%BA%9B%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D%E7%B4%A2%E5%BC%95/</url>
      
        <content type="html"><![CDATA[<h1 id="透射比-transmittance">透射比 transmittance</h1><blockquote><p>参考链接：<a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODA3MDkyMA==&amp;mid=2247485148&amp;idx=1&amp;sn=4e6acc1b358e624ac2301729d35cc89d&amp;chksm=cf81f0bbf8f679ad3f990f91d5c84fcc67cc6948c3cbc0c34c59874f7795e9263f2382979414&amp;token=400584181&amp;lang=zh_CN#rd">NeRF入门之体渲染 (Volume Rendering) (qq.com)</a> 强烈推荐！</p></blockquote><p><strong>透射比</strong>（transmittance）的公式如下： <span class="math display">\[T(s)=\frac{I_{o}}{I_{i}}=\exp \left(-\int_{i}^{o} \tau_{a}(t) d t\right)\]</span> 其中，<span class="math inline">\(I_o\)</span>表示入射光辐射强度/颜色，<span class="math inline">\(I_i\)</span>表示出射光辐射强度/颜色。由上述公式知，<strong>透射比表示粒子群某一点的透明度，数值越大，说明粒子群越透明，光线衰减的幅度就越小</strong>。</p><p>而透明度本身是关于$_a( t ) $的方程， $_a( t ) <span class="math inline">\(越大，\)</span>T(s)$就越小。由公式 <span class="math inline">\(\tau _a\left( t \right) =\rho \left( t \right) A\)</span> 可以知道，它是由粒子密度<span class="math inline">\(\rho \left( t \right)\)</span>和粒子垂直光线方向的投影面积<span class="math inline">\(A\)</span>决定的。这在直觉上也很好理解，如果粒子密度大，粒子本身也比较大，那么遮住光线的概率也会相应提升，自然透明度也就下降了。</p><p><span class="math inline">\(\tau _a\left( t \right)\)</span>也被称为<strong>光学厚度 (optical depth)</strong>。下图为不同光学厚度下，光线透射度对比。</p><figure><img src="https://mmbiz.qpic.cn/mmbiz_jpg/bdpnCavfx2pcmvCruJHc0c3l8penibibHHg7jWW7W01Vb3hOHbB7UCjBJgQSdGX4j6GiaclqUQ0aIMsqibMhKNZ0lw/640?wx_fmt=jpeg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="不同光学厚度下，光线透射度对比" /><figcaption aria-hidden="true">不同光学厚度下，光线透射度对比</figcaption></figure><h1 id="体渲染-volume-rendering">体渲染 Volume Rendering</h1><p>见<a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODA3MDkyMA==&amp;mid=2247485148&amp;idx=1&amp;sn=4e6acc1b358e624ac2301729d35cc89d&amp;chksm=cf81f0bbf8f679ad3f990f91d5c84fcc67cc6948c3cbc0c34c59874f7795e9263f2382979414&amp;token=400584181&amp;lang=zh_CN#rd">NeRF入门之体渲染 (Volume Rendering) (qq.com)</a></p><h1 id="衡量指标">衡量指标</h1><h2 id="psnr">PSNR</h2><p>峰值信噪比（Peak Signal-to-Noise Ratio，PSNR）是一种用于衡量图像或视频质量的指标。它通过比较原始信号（通常是无失真的参考信号）和经过压缩或其他失真处理后的信号之间的差异来评估质量。</p><p>PSNR的计算公式如下： PSNR = 10 * log10((R^2) / MSE) 其中，R表示像素值的最大可能值（比如对于8位灰度图像，R=255），MSE表示均方误差（Mean Squared Error），它是原始信号和失真信号之间差异的平方的平均值。</p><p><span style="background:#daf5e9;">PSNR的值通常以分贝（dB）为单位，数值越高表示质量越好</span>。常见的PSNR阈值为30dB以上，30-40dB为较好的质量，40-50dB为很好的质量，50dB以上为极好的质量。</p><h2 id="ssim">SSIM</h2><p>结构相似性（Structural Similarity，简称SSIM）是一种用于衡量两幅图像相似程度的指标。与传统的均方误差（Mean Squared Error，MSE）相比，SSIM更能反映人眼对图像质量的感知。</p><p>SSIM指标结合了亮度、对比度和结构三个方面的信息，通过计算这些信息的相似性来评估图像的相似度。<span style="background:#daf5e9;">SSIM的取值范围在[0, 1]之间，值越接近1表示两幅图像越相似</span>。</p><p>SSIM的计算公式如下：</p><p><span class="math inline">\(SSIM(x, y)\)</span> = (2 * <span class="math inline">\(\mu_x\)</span> <em><span class="math inline">\(\mu_y\)</span> + <span class="math inline">\(c_1\)</span>) </em> (2 * <span class="math inline">\(\sigma_{xy}\)</span> + <span class="math inline">\(c_2\)</span>) / (<span class="math inline">\({\mu_x}^2\)</span> + <span class="math inline">\({\mu_y}^2\)</span> + c1) * (<span class="math inline">\({σ_x}^2\)</span> + <span class="math inline">\({σ_y}^2\)</span> + <span class="math inline">\(c_2\)</span>)</p><p>其中，x和y分别表示两幅图像，<span class="math inline">\(\mu\)</span> 表示图像的均值，<span class="math inline">\(\sigma\)</span> 表示图像的标准差，<span class="math inline">\(\sigma_{xy}\)</span> 表示图像的协方差，<span class="math inline">\(c_1\)</span> 和 <span class="math inline">\(c_2\)</span> 是常数，用于避免分母为0。</p><h2 id="lpips">LPIPS</h2><p>学习感知图像块相似度 （Learned Perceptual Image Patch Similarity，简称LPIPS）是一种用于计算图像质量和相似度的指标。它基于深度学习模型，通过学习人类感知图像质量的方式来评估图像之间的相似性。</p><p>LPIPS使用预训练的卷积神经网络（如VGG网络）来提取图像的特征表示，然后通过比较这些特征表示之间的差异来计算图像之间的相似度。与传统的像素差异度量方法（如均方误差）相比，LPIPS更能反映人类感知的差异。<span style="background:#daf5e9;">LPIPS 值越小越好</span>。</p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> implicit rendering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>长江后浪扑前浪，NeRF现在沙滩上</title>
      <link href="/2024/02/17/NeRF/"/>
      <url>/2024/02/17/NeRF/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考链接：</p><p><a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODA3MDkyMA==&amp;mid=2247485459&amp;idx=1&amp;sn=3228ed101ae9ece2f65021cfb3aeccd4&amp;chksm=cf81fe74f8f67762b2d02998a6d9f76c862b13b78ff808655639f27441f567721f7ccf72e24d&amp;cur_album_id=2737490560171376640&amp;scene=189#wechat_redirect">NeRF入门之神经辐射场全貌 (qq.com)</a> （本文大多内容直接抄的这个大大的，建议自己亲自看一遍，强烈推荐！！！本文仅做记录）</p><p>[<a href="https://blog.csdn.net/g11d111/article/details/118959540">NeRF]代码+逻辑详细分析_nerf代码解读 csdn-CSDN博客</a></p></blockquote><p>NeRF 全称是 Neural Radiance Field (神经辐射场)，它是想做这样一件事情：</p><p><strong>给定一个场景，输入相机 (或者观察者) 的位置和朝向后，输出这个视角下的视图——新视图合成</strong></p><figure><img src="https://mmbiz.qpic.cn/mmbiz_png/bdpnCavfx2q6scter5jkxUwIbO64s49tSZkgvU8U1tvek0fs5nJXWxNB5d6kVpPdibl4xbQRCCjtKiaB7kn33bmg/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="640 (825×534) (qpic.cn)" /><figcaption aria-hidden="true">640 (825×534) (qpic.cn)</figcaption></figure><p>更详细地说，NeRF 是一种用神经网络来渲染三维场景的技术。它可以在提供某几个角度的图像的前提下，生成其他角度下的图像，间接重建出场景的三维信息。</p><p>为什么要学习这个东西呢？</p><p>已故计算机视觉先驱 David Marr 曾经把计算机视觉终极问题定义为：<strong>从二维图像重建出三维物体的位置和形状。</strong></p><p>在 Marr 看来，只有当计算机可以学习到三维信息，才能说明计算机已经学会了视觉。</p><p>按照他的理论，CV 中的分类、检测、分割等任务，只能称为「模式识别」问题，而像 NeRF 这类三维重建任务，才能称为真正的计算机视觉。</p><p>NeRF 早在 2020 年就出现了，并在当年的 ECCV 会议上被提名最佳论文。由于 NeRF 优秀的 3D 建模能力，它可以做很多事情。</p><p>计算景深：</p><figure><img src="https://mmbiz.qpic.cn/mmbiz_gif/bdpnCavfx2rGebHOX9csKUCXewgYqxKaoNB05vX64l9p4JjERZb1JaHMPSBdsEIwkic1p9nqqol1m5OuALIick6A/640?wx_fmt=gif&amp;wxfrom=5&amp;wx_lazy=1" alt="640 (502×192) (qpic.cn)" /><figcaption aria-hidden="true">640 (502×192) (qpic.cn)</figcaption></figure><p>设置对焦：</p><figure><img src="https://mmbiz.qpic.cn/mmbiz_gif/bdpnCavfx2rGebHOX9csKUCXewgYqxKaneOFGERm8aCdhdrpHEeG42jIBV7f0Tymy9iaAgd7iaFBMQBREgqmYvsA/640?wx_fmt=gif&amp;wxfrom=5&amp;wx_lazy=1" alt="640 (480×320) (qpic.cn)" /><figcaption aria-hidden="true">640 (480×320) (qpic.cn)</figcaption></figure><p>重建三维场景（不是真正意义上，仅仅提供新视图）：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_gif/bdpnCavfx2rGebHOX9csKUCXewgYqxKalXuwpR0BnRD0N5JOvWmCw99QWoB3QIwbaOQBHa4iccpNswjx1EUznog/640?wx_fmt=gif&amp;wxfrom=5&amp;wx_lazy=1" alt="mmbiz.qpic.cn/mmbiz_gif/bdpnCavfx2rGebHOX9csKUCXewgYqxKalXuwpR0BnRD0N5JOvWmCw99QWoB3QIwbaOQBHa4iccpNswjx1EUznog/640?wx_fmt=gif&amp;wxfrom=5&amp;wx_lazy=1" /><br /></p><p>设置光影变换： <img src="https://mmbiz.qpic.cn/mmbiz_gif/bdpnCavfx2rGebHOX9csKUCXewgYqxKahjqdqPwzGKWeHm6HFgHjHrQ1e5P46y4kAn0d8bOsI0MH2ESpEqshaw/640?wx_fmt=gif&amp;wxfrom=5&amp;wx_lazy=1" alt="640 (480×320) (qpic.cn)" /></p><h1 id="体渲染-volume-rendering">体渲染 Volume Rendering</h1><p>体渲染能够对光线进行建模，计算出像素的光线强度/颜色值。</p><p>不管是相机还是人眼，像素的诞生都是光线打在感光器件 (视锥细胞) 上的结果。以最经典的小孔成像为例，光线穿过孔洞后，打在屏幕上形成像素：</p><figure><img src="https://mmbiz.qpic.cn/mmbiz_png/bdpnCavfx2q6scter5jkxUwIbO64s49tsouXpNCpUV58bWPRPsO8YMj17B3nP8sPAxfNe5fj4lzPqBiaz4dicmyA/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="640 (1080×737) (qpic.cn)" /><figcaption aria-hidden="true">640 (1080×737) (qpic.cn)</figcaption></figure><p>因此，只要对光线渲染过程进行建模，计算出最终的像素值就可以了。当然啦，真实的相机中为了获得更高的信噪比，用的是镜头成像，不过，在 NeRF 中，小孔成像模型就足够建模整个渲染过程了。</p><p>而对光线进行建模，正好是<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODA3MDkyMA==&amp;mid=2247485148&amp;idx=1&amp;sn=4e6acc1b358e624ac2301729d35cc89d&amp;chksm=cf81f0bbf8f679ad3f990f91d5c84fcc67cc6948c3cbc0c34c59874f7795e9263f2382979414&amp;scene=21#wechat_redirect">体渲染</a>的拿手好戏。</p><p>体渲染把每一条光路建模成由一个个粒子构成。光子在光路传播中，可能因为粒子的遮挡损失能量，导致入射光减弱 (比如气体会遮挡部分光线，不透明固体则会遮挡所有光线)。光路中的粒子也可能本身会发光，抑或是周围光路的光子被弹射到当前光路，导致光线增强。</p><figure><img src="https://mmbiz.qpic.cn/mmbiz_png/bdpnCavfx2q6scter5jkxUwIbO64s49tbFwEzN8QzukIt8n6e4aXkNPOsSicTFXNyVUZCG3ksmRlboNtQr5JJZw/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="640 (436×373) (qpic.cn)" /><figcaption aria-hidden="true">640 (436×373) (qpic.cn)</figcaption></figure><p>在计算机中，由于计算资源限制，不可能建模出所有粒子的状态，因此通常会在每条光路上采样一些点，由这些采样点上的粒子来代表整条光线。<strong>最终每条光线渲染出来的颜色值都可以用下面的公式表示</strong> (该公式的详细说明与推导见：<a href="https://mp.weixin.qq.com/s?__biz=Mzg4ODA3MDkyMA==&amp;mid=2247485148&amp;idx=1&amp;sn=4e6acc1b358e624ac2301729d35cc89d&amp;chksm=cf81f0bbf8f679ad3f990f91d5c84fcc67cc6948c3cbc0c34c59874f7795e9263f2382979414&amp;token=400584181&amp;lang=zh_CN#rd">NeRF入门之体渲染 (Volume Rendering) (qq.com)</a>)： <span class="math display">\[\hat{C}(\mathbf{r})=\sum_{i=1}^{N} T_{i}\left(1-\exp \left(-\sigma_{i} \delta_{i}\right)\right) \mathbf{c}_{i}, \text { where } T_{i}=\exp \left(-\sum_{j=1}^{i-1} \sigma_{j} \delta_{j}\right)\tag{1}\label{eq1}\]</span> 这里的<span class="math inline">\(\sigma_i\)</span>可以理解为是第 i 个采样点处粒子的密度，<span class="math inline">\(\delta_i\)</span>是相邻采样点的距离，<span class="math inline">\(c_i\)</span>表示采样点处粒子发射的光线强度 (也可以理解为颜色)，<span class="math inline">\(T_i\)</span>则表示从第 i 个采样点到光线终点的透射比。从公式可知一共采样了<span class="math inline">\(N\)</span>个点。</p><p>公式计算结果<span class="math inline">\(\hat{C}(\mathbf{r})\)</span>就是这条光线最终渲染得到的像素值。</p><p>以上就是体渲染的精髓了。<strong>在传统的计算机图形学中，我们需要先知道整个场景中，每条光线上的每个采样点的粒子状态</strong> (即<span class="math inline">\(\sigma_i\)</span>这些数值，在NeRF中是颜色<span class="math inline">\(RGB\)</span>和密度<span class="math inline">\(\sigma\)</span>)，<strong>才能渲染出整个画面</strong>。但如何计算这些粒子的状态，本身是一件很复杂的事情。</p><p>那 NeRF 做了什么呢？</p><p>没错，它就是用来计算这些数值的。</p><h1 id="神经辐射场">神经辐射场</h1><p>在 NeRF 中，作者做了一个这样的尝试，既然体渲染中这些粒子状态这么复杂，那有没有可能让神经网络自己去学出来呢？</p><p>事实证明，这是可以的，而且效果相当好。</p><p>这个过程是这样的：</p><p><strong>给定相机位置和朝向后，我们可以确定出当前的成像平面。然后，将相机的位置坐标和平面上的某个像素相连，就确定了一条光线 (也即确定了光线的方向)。接着用网络预测出光线上每个采样点的粒子信息，就可以确定像素颜色。这个过程重复下去，直到每个像素都渲染完为止。</strong></p><p>这些排列整齐的光线，构成了类似磁场一样的东西，而光线本身就是一种辐射，因此叫辐射场。而每条光线上的粒子信息又都是由神经网络预测的，因此作者又给整个过程命名为<strong>神经辐射场</strong>。</p><figure><img src="https://mmbiz.qpic.cn/mmbiz_gif/bdpnCavfx2q6scter5jkxUwIbO64s49tWM1nZf131FiaA5nD5rrfmLMWmXn4QGHoWwK7fN9zRqh9XA1dAs42suw/640?wx_fmt=gif&amp;wxfrom=5&amp;wx_lazy=1" alt="640 (854×480) (qpic.cn)" /><figcaption aria-hidden="true">640 (854×480) (qpic.cn)</figcaption></figure><h1 id="nerf-全貌">NeRF 全貌</h1><figure><img src="https://mmbiz.qpic.cn/mmbiz_jpg/bdpnCavfx2q6scter5jkxUwIbO64s49tXicZyqYwMNbr2zfYdND1jNRlOnEZzjMjZSBqlSJlstiaa6tVYIkTOdkw/640?wx_fmt=jpeg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="640 (1080×343) (qpic.cn)" /><figcaption aria-hidden="true">640 (1080×343) (qpic.cn)</figcaption></figure><blockquote><p>NeRF 的训练流程图</p></blockquote><p>首先，选择一个特定的场景 (比如上图的乐高玩具)。</p><p>然后，在这个场景的四周摆放一些相机，并确定好相机位置<span class="math inline">\((x,y,z)\)</span>和光线方向 <span class="math inline">\((\theta,\phi)\)</span>。</p><p>沿着光线方向乘以不同的采样距离，可以确定光线上每个采样点的位置。<strong>注意，此时每个采样点对应的方向向量会转换成笛卡尔坐标系 (x', y', z')</strong>。</p><p>将相机位置以及方向向量送入网络后，让网络预测出光线上采样点的粒子信息<span class="math inline">\((RGB,\sigma)\)</span>，根据公式 <span class="math inline">\(\eqref{eq1}\)</span> 渲染出整个画面。</p><p>将模型渲染的结果和相机捕捉的真实结果计算损失 (均方误差)，由于公式<span class="math inline">\(\eqref{eq1}\)</span>是可导的，因此梯度可以正常回传，从而训练整个网络。</p><p>在预测的时候，我们直接将相机位置等参数输入网络，让网络计算出每根光线上的粒子信息后，便可以渲染出任意视角下的画面了。</p><p>以上便是 NeRF 的整个算法流程，是一种很经典的<strong>神经渲染</strong>方法。</p><blockquote><p><span style="background:#daf5e9;"><strong>需要注意一点，原始的 NeRF 只能针对一个三维场景进行重建</strong></span>，之后有不少论文对这一点做了改进，我们以后再聊。</p></blockquote><p>下面再介绍一下 NeRF 所使用的网络结构，以及其中用到的两个技巧。</p><h1 id="网络结构">网络结构</h1><p>NeRF 的网络结构是一个很简单的全连接网络。</p><p>作者把密度<span class="math inline">\(\sigma\)</span>和颜色<span class="math inline">\(RGB\)</span>分为两部分输出，这么做的考量在于，粒子密度是跟三维场景本身更强相关的属性，不管观察的方向怎么变，它都不会有太大的变化 (即论文中提到的 <u>multiview consistent</u>)，而颜色值在不同观察方向下，受光照影响，可能会发生大的变动，即它受相机位置和观察方向的影响都更大。</p><p>基于此，论文先把相机位置<span class="math inline">\((x,y,z)\)</span>经过位置编码 Positional encoding (下文会提及) 后拓展成 60 维的向量，送入一堆全连接网络 (FC 层, Full Connect) 后，分叉出两路，其中一路经过一个 FC 层和 ReLU 激活后输出<span class="math inline">\(\sigma\)</span>值；另一路和方向向量经过位置编码的向量 concat 后，一起送入之后的全连接网络，最后经过 Sigmoid 激活输出颜色值 <span class="math inline">\(RGB\)</span>。</p><blockquote><p>相机位置向量 <span class="math inline">\(\gamma(x)\)</span> 和方向向量 <span class="math inline">\(\gamma(d)\)</span> 都是三维。由于位置编码选择的参数<span class="math inline">\(L\)</span>不同，因此位置向量 <span class="math inline">\(\gamma(x)\)</span> 经过 positional encoding 后是 60 维，方向向量 <span class="math inline">\(\gamma(d)\)</span> 在 encoding 后则是 24 维。</p></blockquote><figure><img src="https://mmbiz.qpic.cn/mmbiz_jpg/bdpnCavfx2q6scter5jkxUwIbO64s49tOV8pv7iax6M4o1BryKj9tyZJBtEAYkcYyH5Z0k6Cp3tvgGKAJSSA0hg/640?wx_fmt=jpeg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="640 (1080×629) (qpic.cn)" /><figcaption aria-hidden="true">640 (1080×629) (qpic.cn)</figcaption></figure><p>网络输出的<span class="math inline">\(\sigma\)</span>和<span class="math inline">\(RGB\)</span>就是一条光线上一个采样点对应的粒子密度和颜色值。在整个训练过程中，我们会收集所有光线上所有采样点的粒子密度和颜色，再根据体渲染得到预测的投影视图，然后和真实的投影视图计算 loss 来训练网络。</p><p>不过，仅靠这个网络训练的结果并不够逼真，论文使用了两个 trick 进一步优化。</p><h1 id="trick-1-位置编码-positional-encoding">trick 1: 位置编码 positional encoding</h1><p>第一个 trick 是位置编码 (positional encoding)。</p><p>它来自之前一些研究的实验发现：<strong>如果给网络输入的数据维度越高 (越高频)，那网络也能输出更加高频的信号 (即图像清晰度越好)</strong>。</p><p>说直白点就是，<strong>由于<span class="math inline">\((x,y,z)\)</span>这类位置和方向向量只有三维</strong>，如果直接将它们投喂给网络，那网络也只能回馈给你低维度的信号，而如果能把输入拓展成更高维，那网络的输出信号会包含更高维的信息，图像内容会更丰富。</p><figure><img src="https://mmbiz.qpic.cn/mmbiz_png/bdpnCavfx2q6scter5jkxUwIbO64s49tOqrlq9cJibThH9mv1rXnuvjHvPnKDpzdkL5M0ZnYhzhqcllwiaI276sQ/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="640 (601×318) (qpic.cn)" /><figcaption aria-hidden="true">640 (601×318) (qpic.cn)</figcaption></figure><blockquote><p>图：使用 positional encoding 的结果对比</p></blockquote><p>论文使用三角函数对输入向量的每个元素进行扩展 <span class="math display">\[\gamma(p)=\left(\sin \left(2^{0} \pi p\right), \cos \left(2^{0} \pi p\right), \cdots, \sin \left(2^{L-1} \pi p\right), \cos \left(2^{L-1} \pi p\right)\right)\tag{2}\]</span> 这个公式可以使用以下 pytorch 代码实现：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">positional_encoding</span>(<span class="params">inputs, L=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    inputs: 输入向量，包含(x,y,z)三个坐标</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    L = <span class="number">10</span></span><br><span class="line">    <span class="comment"># freq_bands: [2^0, 2^1, ..., 2^(L-1)]</span></span><br><span class="line">    freq_bands = <span class="number">2</span> ** torch.linespace(<span class="number">0</span>, L-<span class="number">1</span>, L)</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> freq <span class="keyword">in</span> freq_bands:</span><br><span class="line">        <span class="comment"># [sin(2^f \pi x), sin(2^f \pi y), sin(2^f \pi z)]</span></span><br><span class="line">        outputs += [torch.sin(freq * inputs)]</span><br><span class="line">        <span class="comment"># [cos(2^f \pi x), cos(2^f \pi y), cos(2^f \pi z)]</span></span><br><span class="line">        outputs += [torch.cos(freq * inputs)]</span><br><span class="line">    <span class="comment"># [sin(2^0 \pi x), sin(2^0 \pi y), sin(2^0 \pi z), cos(2^0 \pi x), cos(2^0 \pi y), cos(2^0 \pi z), ...]</span></span><br><span class="line">    outputs = torch.cat(outputs, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><p>对于位置向量<span class="math inline">\((x,y,z)\)</span>，论文选取的<span class="math inline">\(L=10\)</span>，即每个元素会拓展成 20 维，所以在之前的网络输入中，输入是 3*20 即 60 维的向量。而方向向量的<span class="math inline">\(L=4\)</span>，每个元素会拓展为 8 维，对应的位置编码 positional encoding 则是 24 维 (3*8)。</p><p>NeRF 采用的位置编码和 transformer 中的位置编码几乎一样，不过二者的目的不同：transformer 的位置编码是为了给模型提供序列中每个元素的位置信息，而 NeRF 纯粹是为了帮助网络拟合更高清的图像。</p><h1 id="trick-2-分层采样-hierarchical-sampling">trick 2: 分层采样 hierarchical sampling</h1><p>分层采样 (hierarchical sampling) 是另一个重要的 trick。</p><p>前面提到，NeRF 会对每根光线进行采样，然后用网络对每个采样点进行预测。而由于资源的限制，采样不可能做到很密集。<strong>在实际情况中，粒子在空间中的分布也是不均匀的，有些采样点可能粒子密度很高，有些则密度几乎为 0</strong>。</p><p>因此，在密度高的地方多放一些采样点是比较合适的，即做重要性采样 importance sampling。论文为此设计了一种由粗到细的分层采样方法。</p><p>其设计思路非常简单：首先，先在每条光线上均匀采样若干点，让网络预测出这些点的粒子密度，然后，根据这些密度信息，重新在密度更高的采样点附近，多采样一些点，再让网络预测一轮，两轮预测的结果分别计算 loss。这样一来，网络便可以在密度高的区域学到更多有效的信息。</p><p>具体实施流程：</p><p>首先是<strong>粗粒度采样</strong>。论文在每根光线上<u>均匀采样</u> 64 个点，然后让网络预测出每个点的信息，包括颜色值 <span class="math inline">\(c_i\)</span> 、粒子密度 <span class="math inline">\(\sigma_i\)</span> 和透射比 <span class="math inline">\(T_i\)</span> 。其中，<span class="math inline">\(\sigma_i\)</span> 和 <span class="math inline">\(T_i\)</span> 可以整合成一个权重 <span class="math inline">\(w_i\)</span> : <span class="math display">\[\hat{C}_{c}(\mathbf{r})=\sum_{i=1}^{N_{c}} w_{i} c_{i}, \quad w_{i}=T_{i}\left(1-\exp \left(-\sigma_{i} \delta_{i}\right)\right)\tag{3}\]</span></p><blockquote><p>体渲染公式可以理解为采样点颜色的累积。</p></blockquote><p>怎么理解这里的 <span class="math inline">\(w_i\)</span> 呢？在之前介绍体渲染的文章中，我们知道，光线最终的辐射强度 (或者说颜色) 其实是由光线上每个点的粒子辐射<strong>累加</strong>而成的。既然是累加，那就可以简单地把 <span class="math inline">\(w_i\)</span> 理解为是每个采样点的粒子对最终成像的贡献度，或者说粒子的浓度。 <span class="math inline">\(w_i\)</span> 大的地方，证明这个区域粒子很浓，信息量大，需要重点采样。</p><figure><img src="https://mmbiz.qpic.cn/mmbiz_png/bdpnCavfx2q6scter5jkxUwIbO64s49tI3sIEkBtdKE16vHPFTCJEuy0LxKM7gNfXhBUVYHuhBRUnsGdp3IgkQ/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="640 (938×778) (qpic.cn)" /><figcaption aria-hidden="true">640 (938×778) (qpic.cn)</figcaption></figure><blockquote><p>图：粗粒度均匀采样</p></blockquote><p>那下面就是在第一轮预测的基础上，找出 <span class="math inline">\(w_i\)</span> 比较大的地方再重点采样，也即<strong>细粒度采样</strong>。</p><p>这个过程是这样，先将 <span class="math inline">\(w_i\)</span> 归一化：$<em>{i}=w</em>{i} / <em>{j=1}^{N</em>{c}} w_{j} $ （<span class="math inline">\(N_c\)</span> 表示粗粒度采样的点数），这样 <span class="math inline">\(\hat{w}_{i}\)</span> 就可以视为一个概率分布了。然后，为了在 <span class="math inline">\(\hat{w}_{i}\)</span> 数值更大的地方重采样，我们可以把 <span class="math inline">\(\hat{w}_{i}\)</span> 累加起来，求出<code>概率累积直方图 (cdf)</code>，然后<u>在 cdf 的<strong>纵坐标</strong>上再均匀采样</u>，这样，大部分采样点会对应到 <span class="math inline">\(\hat{w}_{i}\)</span> 更大的直方图，这个直方图的横坐标就是细粒度采样点。这个过程也称为<code>逆变换采样 (inverse transform sampling)</code>。</p><figure><img src="https://mmbiz.qpic.cn/mmbiz_jpg/bdpnCavfx2q6scter5jkxUwIbO64s49tqPVGqic0ricODl59BtzT8mHFXmJEHhT956xO7d1ic0vxhAtpxJg29pt4Q/640?wx_fmt=jpeg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="640 (900×750) (qpic.cn)" /><figcaption aria-hidden="true">640 (900×750) (qpic.cn)</figcaption></figure><blockquote><p>逆变换采样</p></blockquote><figure><img src="https://mmbiz.qpic.cn/mmbiz_png/bdpnCavfx2q6scter5jkxUwIbO64s49tbAOrDdxNwCOXKzcjocVIC2JuPEf6bGmFZ7ZibZ4BWLhQzoDcEVEy4PQ/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="640 (1080×336) (qpic.cn)" /><figcaption aria-hidden="true">640 (1080×336) (qpic.cn)</figcaption></figure><blockquote><p>在粗粒度采样的基础上进行细粒度采样</p></blockquote><h2 id="实现整个分层采样的过程">实现整个分层采样的过程</h2><h1 id="实验效果">实验效果</h1><p>NeRF 这类新视图生成任务的主要评价标准，就是看生成的图片逼真度如何。因此评价指标采用的是图像复原中常用的 PSNR 、 SSIM 和 LPIPS 等衡量方法。</p><p>下图是 NeRF 和之前几种方法的对比结果：</p><figure><img src="https://mmbiz.qpic.cn/mmbiz_png/bdpnCavfx2q6scter5jkxUwIbO64s49tCKfXzskT5HiaGkiaSDSS5UucFqu8yQr5kFwFuDTPoHRXhN5ceus5ROdg/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="640 (1080×202) (qpic.cn)" /><figcaption aria-hidden="true">640 (1080×202) (qpic.cn)</figcaption></figure><p>NeRF 之所以诞生后受到很大的关注，就在于它相比之前的方法提升太多了，对于 PSNR 这类指标来说，能提升两三个点证明方法本身已经取得质的突破，更何况 NeRF 在有些数据集上提升了五六个点！</p><p>对比方法中的 LLFF 和 NeRF 是同一个作者，所以人家也是在这个领域默默耕耘了很久。</p><p>效果对比图就不用说了，秒杀般的存在：</p><figure><img src="https://mmbiz.qpic.cn/mmbiz_png/bdpnCavfx2q6scter5jkxUwIbO64s49t5iaezJuRCXtGcN9xalF9eBwVqFJxvI8icCkJjUic9vorVoveK563nNAibw/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="640 (1080×665) (qpic.cn)" /><figcaption aria-hidden="true">640 (1080×665) (qpic.cn)</figcaption></figure><h2 id="深度图">深度图</h2><p>NeRF 有一个副产品是，它可以输出场景的深度图。</p><figure><img src="https://mmbiz.qpic.cn/mmbiz_gif/bdpnCavfx2rGebHOX9csKUCXewgYqxKaoNB05vX64l9p4JjERZb1JaHMPSBdsEIwkic1p9nqqol1m5OuALIick6A/640?wx_fmt=gif&amp;wxfrom=5&amp;wx_lazy=1" alt="640 (502×192) (qpic.cn)" /><figcaption aria-hidden="true">640 (502×192) (qpic.cn)</figcaption></figure><p>深度图代表的是场景中的物体离相机的距离。NeRF 输出深度图的方法是把光线上每个采样点的权重 <span class="math inline">\(w_i\)</span> 根据距离的远近累加起来：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># weights是光线上各个采样点的权重，z_vals是采样点离相机的距离</span></span><br><span class="line">depth_map = torch.<span class="built_in">sum</span>(weights*z_vals, -<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="为什么会有这种效果">为什么会有这种效果？</h3><h1 id="nerf与三维重建">NeRF与三维重建</h1><p>NeRF 可以预测三维场景中每个视角的二维投影信息，因此，它本身也具备了整个三维场景的信息，可以认为是一种三维重建算法。</p><p>但它跟我们以往认知的三维重建又有所区别。</p><p>最关键的一点，<strong>它并没有显示地学习出整个三维场景的结构信息，而是把整个三维场景的信息编码进了神经网络中</strong>。这个神经网络建立了观察视角与三维模型之间的映射，它就像一个查找表，通过输入观察视角，找出对应视角下三维场景的粒子和反射光等信息。</p><p>如果想通过 NeRF 直接获得三维模型，抱歉，办不到，NeRF 只能给你二维投影，需要你自己用这些投影图像去合成真正的三维模型。</p><p>在 NeRF 中存储的是一种很像点云的「雾」，只有在你观察它的时候，它才会给你具体某个投影面的信息，从这个角度想，这团雾又很像是「量子态」的。</p><h1 id="nerf的缺陷">NeRF的缺陷</h1><p>NeRF 相比之前的工作，最大的优势就是渲染出来的图像清晰度更高，更真实。但它也存在几个严重的命门。</p><p><strong>首先，NeRF 的渲染速度极其慢</strong>。假设要渲染一张 1024x1024 的图片，且每根光线的采样点为 128，那总共需要跑 1024x1024x128=134217728 次网络。我的天，家里没矿可不敢这么玩。</p><p><strong>其次，NeRF 只适用于一个场景</strong>，如果换了别的场景，就得重新训练了，这泛化能力约等于没有。</p><p>不过，作为奠基之作，NeRF 已经开了个好头，剩下的问题自然有追随者帮忙解决。在之后的文章中，我们会逐一看到那些精妙的破解之法。</p><h1 id="代码">代码</h1><figure><img src="https://mmbiz.qpic.cn/mmbiz_jpg/bdpnCavfx2q6scter5jkxUwIbO64s49tOV8pv7iax6M4o1BryKj9tyZJBtEAYkcYyH5Z0k6Cp3tvgGKAJSSA0hg/640?wx_fmt=jpeg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="640 (1080×629) (qpic.cn)" /><figcaption aria-hidden="true">640 (1080×629) (qpic.cn)</figcaption></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NeRF</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, D=<span class="number">8</span>, W=<span class="number">256</span>, input_ch=<span class="number">3</span>, input_ch_views=<span class="number">3</span>, output_ch=<span class="number">4</span>, skips=[<span class="number">4</span>], use_viewdirs=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(NeRF, self).__init__()</span><br><span class="line">        self.D = D</span><br><span class="line">        self.W = W</span><br><span class="line">        <span class="comment">## Position Encoding之后的 位置vector通道数（63）</span></span><br><span class="line">        self.input_ch = input_ch  </span><br><span class="line">        <span class="comment">## Position Encoding之后的 direction的vector通道数（27）</span></span><br><span class="line">        self.input_ch_views = input_ch_views</span><br><span class="line">        self.skips = skips   <span class="comment">## 在第4层有跳跃连接</span></span><br><span class="line">        self.use_viewdirs = use_viewdirs</span><br><span class="line">        </span><br><span class="line">        <span class="comment">## 前8层的MLP实现：输入为63，输出为 256</span></span><br><span class="line">        self.pts_linears = nn.ModuleList(</span><br><span class="line">            [nn.Linear(input_ch, W)] + [nn.Linear(W, W) <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> self.skips <span class="keyword">else</span> nn.Linear(W + input_ch, W) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(D-<span class="number">1</span>)])</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### 构建了第9层的输入为 第8层的输出 和 direction 进行concat,输出为128 维</span></span><br><span class="line">        self.views_linears = nn.ModuleList([nn.Linear(input_ch_views + W, W//<span class="number">2</span>)])</span><br><span class="line">   </span><br><span class="line">        <span class="keyword">if</span> use_viewdirs:</span><br><span class="line">            self.feature_linear = nn.Linear(W, W) <span class="comment"># 第9层 输出256维的向量</span></span><br><span class="line">            self.alpha_linear = nn.Linear(W, <span class="number">1</span>) <span class="comment"># 第9层输出 density alpha(1维)</span></span><br><span class="line">            self.rgb_linear = nn.Linear(W//<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.output_linear = nn.Linear(W, output_ch)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        input_pts, input_views = torch.split(x, [self.input_ch, self.input_ch_views], dim=-<span class="number">1</span>)</span><br><span class="line">        h = input_pts</span><br><span class="line">        <span class="keyword">for</span> i, l <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.pts_linears):</span><br><span class="line">            h = self.pts_linears[i](h)</span><br><span class="line">            h = F.relu(h)</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> self.skips:</span><br><span class="line">                h = torch.cat([input_pts, h], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.use_viewdirs:</span><br><span class="line">            alpha = self.alpha_linear(h)</span><br><span class="line">            feature = self.feature_linear(h)</span><br><span class="line">            h = torch.cat([feature, input_views], -<span class="number">1</span>) <span class="comment">#第9层concat direction 向量</span></span><br><span class="line">        </span><br><span class="line">            <span class="keyword">for</span> i, l <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.views_linears):</span><br><span class="line">                h = self.views_linears[i](h)</span><br><span class="line">                h = F.relu(h)</span><br><span class="line"></span><br><span class="line">            rgb = self.rgb_linear(h)  <span class="comment">## 输出rgb 3维度向量</span></span><br><span class="line">            outputs = torch.cat([rgb, alpha], -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            outputs = self.output_linear(h)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs   </span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> implicit rendering </tag>
            
            <tag> 新视图合成 </tag>
            
            <tag> ECCV2020 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>欲要通晓他人码，恰似平地起高楼</title>
      <link href="/2024/02/16/%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81%E5%87%BD%E6%95%B0%E8%AE%B0%E5%BD%95/"/>
      <url>/2024/02/16/%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81%E5%87%BD%E6%95%B0%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="parser.add_argument">parser.add_argument()</h1><blockquote><p>参考链接：<a href="https://blog.csdn.net/qq_34243930/article/details/106517985">python之parser.add_argument()用法——命令行选项、参数和子命令解析器-CSDN博客</a></p></blockquote><h2 id="argparse-介绍">argparse 介绍</h2><blockquote><p>官方文档：<a href="https://docs.python.org/zh-cn/3/library/argparse.html">argparse --- 命令行选项、参数和子命令解析器 — Python 3.12.2 文档</a></p></blockquote><p>argparse 模块是 Python 内置的一个用于命令项选项与参数解析的模块。通过在程序中定义好我们需要的参数，然后 argparse 将会从 sys.argv 解析出这些参数，并在用户给程序传入无效参数时报出错误信息。</p><h2 id="argparse使用说明与示例">argparse使用说明与示例</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;test&#x27;</span>) <span class="comment">#Step 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--sparse&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>, <span class="built_in">help</span>=<span class="string">&#x27;GAT with sparse version or not.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">72</span>, <span class="built_in">help</span>=<span class="string">&#x27;Random seed.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10000</span>, <span class="built_in">help</span>=<span class="string">&#x27;Number of epochs to train.&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">ArgumentParser.add_argument(name or flags...[, action][, nargs][, const][, default][, type][, choices][, required][, help][, metavar][, dest])</span></span><br><span class="line"><span class="string">-name: 有&#x27;--&#x27;指定表示可选参数；不加&#x27;--&#x27;表示必选参数，需在命令行中手动指定，否则即使通过default设置默认参数，也还是会报错</span></span><br><span class="line"><span class="string">-action: 当参数在命令行中出现时使用的动作基本类型，如当其值为&#x27;store_true&#x27;时表示参数作用是存储True值，其默认值为&#x27;False&#x27;，const值为&#x27;True&#x27;(若在parse_args()的括号中被引用，使用&#x27;True&#x27;值，否则使用&#x27;False&#x27;值)</span></span><br><span class="line"><span class="string">-default: 不指定参数时其默认值</span></span><br><span class="line"><span class="string">-type: 参数应当被转换成的类型</span></span><br><span class="line"><span class="string">-help: 所添加参数的简要说明</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3</span></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"><span class="built_in">print</span>(args.sparse)</span><br><span class="line"><span class="built_in">print</span>(args.seed)</span><br><span class="line"><span class="built_in">print</span>(args.epochs)</span><br></pre></td></tr></table></figure><p>三个步骤：</p><ol type="1"><li>创建一个解析器——创建一个 <strong>argparse.ArgumentParser()</strong> 对象，名称为 <code>parser</code></li><li>添加参数——调用<code>parser</code>. <strong>add_argument()</strong>方法添加参数</li><li>解析参数——使用 <code>parser</code>. <strong>parse_args()</strong> 解析检查添加的参数集为<span style="background:#daf5e9;"><strong>args</strong></span>，调用时括号内通常不带参数</li></ol><h1 id="tqdm-python进度条库">tqdm — python进度条库</h1><blockquote><p>参考链接：<a href="https://zhuanlan.zhihu.com/p/163613814">python进度条库tqdm详解 - 知乎 (zhihu.com)</a></p></blockquote><p><strong>tqdm模块是python进度条库，主要分为两种运行方式</strong></p><p>1.基于迭代对象运行：tqdm(iterator)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm, trange</span><br><span class="line"></span><br><span class="line"><span class="comment">#trange(i)是tqdm(range(i))的一种简单写法</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> trange(<span class="number">100</span>):</span><br><span class="line">    time.sleep(<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">100</span>), desc=<span class="string">&#x27;Processing&#x27;</span>):</span><br><span class="line">    time.sleep(<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">dic = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>]</span><br><span class="line">pbar = tqdm(dic)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> pbar:</span><br><span class="line">    pbar.set_description(<span class="string">&#x27;Processing &#x27;</span>+i)</span><br><span class="line">    time.sleep(<span class="number">0.2</span>)</span><br><span class="line"><span class="number">100</span>%|██████████| <span class="number">100</span>/<span class="number">100</span> [<span class="number">00</span>:06&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">16.04</span>it/s]</span><br><span class="line">Processing: <span class="number">100</span>%|██████████| <span class="number">100</span>/<span class="number">100</span> [<span class="number">00</span>:06&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">16.05</span>it/s]</span><br><span class="line">Processing e: <span class="number">100</span>%|██████████| <span class="number">5</span>/<span class="number">5</span> [<span class="number">00</span>:01&lt;<span class="number">00</span>:<span class="number">00</span>,  <span class="number">4.69</span>it/s]</span><br></pre></td></tr></table></figure><p>2.手动进行更新</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tqdm(total=<span class="number">200</span>) <span class="keyword">as</span> pbar:</span><br><span class="line">    pbar.set_description(<span class="string">&#x27;Processing:&#x27;</span>)</span><br><span class="line">    <span class="comment"># total表示总的项目, 循环的次数20*10(每次更新数目) = 200(total)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">        <span class="comment"># 进行动作, 这里是过0.1s</span></span><br><span class="line">        time.sleep(<span class="number">0.1</span>)</span><br><span class="line">        <span class="comment"># 进行进度更新, 这里设置10个</span></span><br><span class="line">        pbar.update(<span class="number">10</span>)</span><br><span class="line">Processing:: <span class="number">100</span>%|██████████| <span class="number">200</span>/<span class="number">200</span> [<span class="number">00</span>:02&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">91.94</span>it/s]</span><br></pre></td></tr></table></figure><h2 id="tqdm模块参数说明">tqdm模块参数说明</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">tqdm</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, iterable=<span class="literal">None</span>, desc=<span class="literal">None</span>, total=<span class="literal">None</span>, leave=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">               file=sys.stderr, ncols=<span class="literal">None</span>, mininterval=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">               maxinterval=<span class="number">10.0</span>, miniters=<span class="literal">None</span>, <span class="built_in">ascii</span>=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">               disable=<span class="literal">False</span>, unit=<span class="string">&#x27;it&#x27;</span>, unit_scale=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">               dynamic_ncols=<span class="literal">False</span>, smoothing=<span class="number">0.3</span>, nested=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">               bar_format=<span class="literal">None</span>, initial=<span class="number">0</span>, gui=<span class="literal">False</span></span>):</span><br></pre></td></tr></table></figure><ul><li>iterable: 可迭代的对象, 在手动更新时不需要进行设置</li><li>desc: 字符串, 左边进度条描述文字description</li><li>total: 总的项目数</li><li>leave: bool值, 迭代完成后是否保留进度条</li><li>file: 输出指向位置, 默认是终端, 一般不需要设置</li><li>ncols: 调整进度条宽度, 默认是根据环境自动调节长度, 如果设置为0, 就没有进度条, 只有输出的信息</li><li>unit: 描述处理项目的文字, 默认是'it', 例如: 100 it/s, 处理照片的话设置为'img' ,则为 100 img/s</li><li>unit_scale: 自动根据国际标准进行项目处理速度单位的换算, 例如 100 000 it/s &gt;&gt; 100k it/s</li></ul><h1 id="torch">torch</h1><h2 id="torch.randperm">torch.randperm</h2><p>torch.randperm(n)：将 <span class="math inline">\(0\)</span> ~ <span class="math inline">\(n-1\)</span>（包括 <span class="math inline">\(0\)</span> 和 <span class="math inline">\(n-1\)</span>）随机打乱后获得的数字序列，函数名是random permutation缩写。'permutation' 译为‘排列组合’。下面是一个例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.randperm(<span class="number">10</span>)</span><br><span class="line"><span class="comment">#输出 tensor([2, 3, 6, 7, 8, 9, 1, 5, 0, 4])</span></span><br></pre></td></tr></table></figure><h2 id="section"></h2>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>岂知辗转复相见，阖应辨得故友来</title>
      <link href="/2024/02/15/%E5%B8%B8%E8%A7%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Pytorch%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E8%A7%84%E5%BE%8B/"/>
      <url>/2024/02/15/%E5%B8%B8%E8%A7%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Pytorch%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E8%A7%84%E5%BE%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>常见Pytorch代码模型结构规律总结</p><p>参考链接：<a href="https://www.zhihu.com/question/406133826">一个完整的Pytorch深度学习项目代码，项目结构是怎样的？ - 知乎 (zhihu.com)</a></p></blockquote><h1 id="常见模型结构">常见模型结构</h1><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">|--project<span class="emphasis">_name/</span></span><br><span class="line"><span class="emphasis">|    |--data/ # 数据</span></span><br><span class="line"><span class="emphasis">|    |--datasets/ # 生成数据集，加载处理数据集</span></span><br><span class="line"><span class="emphasis">|    |    |--data_</span>loader.py</span><br><span class="line">|    |--models/ # 模型的综合训练、测试、验证、预测等</span><br><span class="line">|    |    |--model.py</span><br><span class="line">|    |--configs/ # 配置文件，例如模型参数、优化器参数、训练和测试参数等</span><br><span class="line">|    |    |--config.py</span><br><span class="line">|    |--model<span class="emphasis">_hub/ # 预训练模型权重</span></span><br><span class="line"><span class="emphasis">|    |--utils/ # 大杂烩辅助模块，可以是日志、评价指标等常用接口文件</span></span><br><span class="line"><span class="emphasis">|    |    |--utils.py</span></span><br><span class="line"><span class="emphasis">|    |    |--metrics.py</span></span><br><span class="line"><span class="emphasis">|    |    |--visualization.py</span></span><br><span class="line"><span class="emphasis">|    |--tools/ # 训练，评估，预测的脚本</span></span><br><span class="line"><span class="emphasis">|    |    |--train.py</span></span><br><span class="line"><span class="emphasis">|    |    |--test.py</span></span><br><span class="line"><span class="emphasis">|    |    |--eval.py</span></span><br><span class="line"><span class="emphasis">|    |    |--predict.py</span></span><br><span class="line"><span class="emphasis">|    |--outputs/ # 输出的结果</span></span><br><span class="line"><span class="emphasis">|    |    |--checkpoints/ # 训练好的模型文件</span></span><br><span class="line"><span class="emphasis">|    |    |--logs/ # 日志</span></span><br><span class="line"><span class="emphasis">|    |    |--images/ # 可视化的结果图片</span></span><br><span class="line"><span class="emphasis">|    |--requirements.py # 依赖包列表</span></span><br><span class="line"><span class="emphasis">|    |--README.md # 项目说明</span></span><br><span class="line"><span class="emphasis">|    |--.gitignore #告诉Git忽略哪些文件</span></span><br></pre></td></tr></table></figure><p>关于配置文件configs，可以使用YAML或JSON格式，在<code>train.py</code>或<code>test.py</code>中，可以使用<code>PyYAML</code>库或<code>json</code>库来加载这些配置文件。例如，YAML配置文件可以包括以下内容：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">model:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">model1</span></span><br><span class="line">  <span class="attr">input_size:</span> <span class="number">224</span></span><br><span class="line">  <span class="attr">output_size:</span> <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">optimizer:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">Adam</span></span><br><span class="line">  <span class="attr">lr:</span> <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line"><span class="attr">train:</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">32</span></span><br><span class="line">  <span class="attr">epochs:</span> <span class="number">100</span></span><br><span class="line">  <span class="attr">validation_split:</span> <span class="number">0.2</span></span><br></pre></td></tr></table></figure><h1 id="模型的参数可选">模型的参数(可选)</h1><p>一个深度学习网络有很多的参数可以配置，一般分成以下三类：</p><ul><li>数据集参数（文件路径、batch_size等）</li><li>训练参数（学习率、训练epoch等）</li><li>模型参数（输入的大小，输出的大小）</li></ul><p>这些参数可以写一个类保存，也可以写一个字典，然后使用json保存，这些都是需要自己去实现的，但是这些都是一些细枝末节东西，写了几次，找到一个自己最喜欢的方式就可以，不是深度学习项目中必要的部分。</p><h1 id="network-模型的定义">Network 模型的定义</h1><p>创建一个Network类，继承<code>torch.nn.Module</code>，在构造函数中用初始化成员变量为具体的网络层，在forward函数中使用成员变量搭建网络架构，模型的使用过程中pytorch会自动调用forword进行参数的前向传播，构建计算图。<strong>以下拿一个简单的CNN图像分类模型举例</strong>：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Network, self).__init__()</span><br><span class="line">        <span class="comment"># 灰度图像的channels=1即in_channels=1 输出为10个类别即out_features=10</span></span><br><span class="line">        <span class="comment"># parameter(形参)=argument(实参) 卷积核即卷积滤波器 out_channels=6即6个卷积核 输出6个feature-maps(特征映射)</span></span><br><span class="line">        <span class="comment"># 权重shape 6*1*5*5</span></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">6</span>)  <span class="comment"># 二维批归一化 输入size=6</span></span><br><span class="line">        <span class="comment"># 权重shape 12*1*5*5</span></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">12</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 全连接层：fc or dense or linear out_features即特征(一阶张量)</span></span><br><span class="line">        <span class="comment"># 权重shape 120*192</span></span><br><span class="line">        self.fc1 = nn.Linear(in_features=<span class="number">12</span>*<span class="number">4</span>*<span class="number">4</span>, out_features=<span class="number">120</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm1d(<span class="number">120</span>)  <span class="comment"># 一维批归一化 输入size=120</span></span><br><span class="line">        <span class="comment"># 权重shape 60*120</span></span><br><span class="line">        self.fc2 = nn.Linear(in_features=<span class="number">120</span>, out_features=<span class="number">60</span>)</span><br><span class="line">        <span class="comment"># 权重shape 10*60</span></span><br><span class="line">        self.out = nn.Linear(in_features=<span class="number">60</span>, out_features=<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, t</span>):</span><br><span class="line">        <span class="comment"># (1) input layer</span></span><br><span class="line">        t = t</span><br><span class="line">        <span class="comment"># (2) hidden conv layer</span></span><br><span class="line">        t = F.relu(self.conv1(t))  <span class="comment"># (28-5+0)/1+1=24 输入为b(batch_size)*1*28*28 输出为b*6*24*24 relu后shape不变</span></span><br><span class="line">        t = F.max_pool2d(t, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)  <span class="comment"># (24-2+0)/2+1=12 输出为b*6*12*12</span></span><br><span class="line">        t = self.bn1(t)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (3) hidden conv layer</span></span><br><span class="line">        t = F.relu(self.conv2(t))  <span class="comment"># (12-5+0)/1+1=8 输出为b*12*8*8 relu后shape不变</span></span><br><span class="line">        t = F.max_pool2d(t, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)  <span class="comment"># (8-2+0)/2+1=4 输出为b*12*4*4</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (4) hidden linear layer</span></span><br><span class="line">        t = F.relu(self.fc1(t.reshape(-<span class="number">1</span>, <span class="number">12</span>*<span class="number">4</span>*<span class="number">4</span>)))  <span class="comment"># t.reshape后为b*192 全连接层后输出为b*120 relu后shape不变</span></span><br><span class="line">        t = self.bn2(t)</span><br><span class="line">        <span class="comment"># (5) hidden linear layer</span></span><br><span class="line">        t = F.relu(self.fc2(t))  <span class="comment"># 全连接层后输出为b*60 relu后shape不变</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (6) output layer</span></span><br><span class="line">        t = self.out(t)  <span class="comment"># 全连接层后输出为b*10 relu后shape不变</span></span><br><span class="line">        <span class="keyword">return</span> t</span><br></pre></td></tr></table></figure><h1 id="transforms">Transforms</h1><p>数据处理可以直接使用<code>torchvision.transforms</code>下的处理函数，包括均值，随机旋转，随机裁剪等等，也可以自己实现一些pytorch中没有实现的处理函数，<strong>下面拿一个分割网络的处理函数举例</strong>，可支持同时对传入的Image和GroundTruth进行处理，使用时直接按照顺序构造ProcessImgAndGt即可。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ProcessImgAndGt</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, transforms</span>):</span><br><span class="line">            self.transforms = transforms</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img, label</span>):</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> self.transforms:</span><br><span class="line">            img, label = t(img, label)</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Resize</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, height, width</span>):</span><br><span class="line">        self.height = height</span><br><span class="line">        self.width = width</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img, label</span>):</span><br><span class="line">        img = img.resize((self.width, self.height), Image.BILINEAR)</span><br><span class="line">        label = label.resize((self.width, self.height), Image.NEAREST)</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Normalize</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, mean, std</span>):</span><br><span class="line">        self.mean, self.std = mean, std</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img, label</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            img[:, :, i] -= <span class="built_in">float</span>(self.mean[i])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            img[:, :, i] /= <span class="built_in">float</span>(self.std[i])</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ToTensor</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.to_tensor = torchvision.transforms.ToTensor()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img, label</span>):</span><br><span class="line">        img, label = self.to_tensor(img), self.to_tensor(label).long()</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">transforms = ProcessImgAndGt([</span><br><span class="line">    Resize(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">    Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>]),</span><br><span class="line">    ToTensor()</span><br><span class="line">])</span><br></pre></td></tr></table></figure><h1 id="dataset-数据处理和加载">Dataset 数据处理和加载</h1><p>一般需要我们自己定义：创建一个数据集类，继承torch.utils.data.Dataset，只需重写__init__构造函数，__getitem__迭代器遍历函数以及__len__函数。</p><ul><li>在__init__函数中读取传入的数据集路径下的指定数据文件，<strong>还是拿一个分割网络的dataset流程举例</strong>，其他分类分类模型可以直接将GroundTruth替换为对应label即可，将拼接处理好的图片文件路径和GroundTruth文件路径作为元组存入一个为列表的成员变量file_list中；</li><li>在__getitem__中根据传入的索引从file_list取对应的元素，并且通过Transforms进行处理；</li><li>在__len__中返回len(self.file_list)即可。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset_path, transforms</span>):</span><br><span class="line">        <span class="built_in">super</span>(TrainDataset, self).__init()</span><br><span class="line">        self.dataset_path = dataset_path</span><br><span class="line">        self.transforms = transforms</span><br><span class="line">        <span class="comment"># 根据具体的业务逻辑读取全部数据路径作为加载数据的索引</span></span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">dir</span> <span class="keyword">in</span> os.listdir(dataset_path):</span><br><span class="line">            image_dir = os.path.join(dataset_path, <span class="built_in">dir</span>)</span><br><span class="line">            gt_path = image_dir + <span class="string">&#x27;/GT/&#x27;</span></span><br><span class="line">            img_path = image_dir + <span class="string">&#x27;/Frame/&#x27;</span></span><br><span class="line">            img_list = []</span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> os.listdir(img_path):</span><br><span class="line">                <span class="keyword">if</span> name.endswith(<span class="string">&#x27;.png&#x27;</span>):</span><br><span class="line">                    img_list.append(name)</span><br><span class="line">            self.file_list.extend([(img_path + name, gt_path + name) <span class="keyword">for</span> name <span class="keyword">in</span> img_list])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):   </span><br><span class="line">        img_path, label_path = self.file_list[idx]</span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_path).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">        label = Image.<span class="built_in">open</span>(label_path).convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">        img, label = self.transforms(img, label)</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.file_list)</span><br></pre></td></tr></table></figure><h1 id="train-and-validate-模型的训练和运行">Train and Validate 模型的训练和运行</h1><p><span style="background:#daf5e9;">训练模型一般分成以下几个步骤:</span></p><p>定义网络 定义数据 定义损失函数和优化器 开始训练 <strong>训练网络</strong> <u>将梯度置为0</u> <u>求loss</u> <u>反向传播</u> <u>更新参数</u> <u>更新优化器的学习率（可选）</u> <strong>测试网络</strong> <strong>可视化处理各种指标</strong> <strong>计算在验证集上的指标 （可选）</strong></p><h2 id="optimizer">Optimizer</h2><p>选择优化器进行模型参数更新，要创建优化器必须给它一个可进行迭代优化的包含了全部参数的列表 然后可以指定针对这些参数的学习率（learning_rate），权重衰减（weight_decay），momentum等，</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters(), lr = <span class="number">0.0001</span>)</span><br></pre></td></tr></table></figure><p>或者是可以指定针对哪些参数执行不一样的优化策略，根据不同层的name对不同层使用不同的优化策略。列表中的每一项都可以是一个dict，dict中params对应当前项的参数列表，可以对当前项指定学习率或者是衰减策略。对base_params使用的1e-4的学习率，对finetune_params使用1e-3的学习率，对两者一起使用1e-4的权重衰减</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">base_params = [params <span class="keyword">for</span> name, params <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> (<span class="string">&quot;xxx&quot;</span> <span class="keyword">in</span> name)]</span><br><span class="line">finetune_params = [params <span class="keyword">for</span> name, params <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> (<span class="string">&quot;yyy&quot;</span> <span class="keyword">in</span> name)]</span><br><span class="line">optimizer = optim.Adam([</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: base_params&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: finetune_params, <span class="string">&quot;lr&quot;</span>: <span class="number">1e-3</span>&#125;</span><br><span class="line">], lr=<span class="number">1e-4</span>, weight_decay=<span class="number">1e-4</span>);</span><br></pre></td></tr></table></figure><h2 id="run">Run</h2><p>基础组件都写好了，剩下的就是组成一个完整的模型结构。</p><ol type="1"><li>实例化模型对象，并将其加载到GPU中</li><li>根据需要构建数据预处理对象，传入数据集对象中进行读取数据时的数据处理</li><li>构建训练和测试的数据集对象，并将其传入torch.utils.data.DataLoader，指定batch_size（训练或测试是每次读取多少条数据）、shuffle（读取数据时是否打乱）、num_workers（开启多少线程进行数据加载，为0时(不推荐)用主线程在训练模型时进行数据加载）等参数</li><li>使用torch.optim.Adam构建优化器对象，这里根据不同层的name对不同层使用不同的优化策略</li><li>训练20个epoch，并且每5个epoch在测试集上跑一遍，这里只计算了损失，对于其他评价指标直接计算即可</li><li>根据条件对指定epoch的模型进行保存</li></ol><ul><li><strong>optimizer.zero_grad() # pytorch会积累梯度，在优化每个batch的权重的梯度之前将之前计算出的每个权重的梯度置0</strong><br /></li><li><strong>loss.backward() # 在最后一个张量上调用反向传播方法，在计算图中计算权重的梯度</strong><br /></li><li><strong>optimizer.step() # 使用预先设置的学习率等参数根据当前梯度对权重进行更</strong></li><li><strong>model.train()</strong> # <strong>保证BN层能够继续计算数据的均值和方差并进行更新，保证dropout层会按照设定的参数设置保留激活单元的概率（保留概率=p）</strong></li><li><strong>model.eval()</strong> # <strong>BN层会停止计算均值和方差，直接使用训练时的参数，dropout层利用了训练好的全部网络连接，不随机舍弃激活单元</strong></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Network().cuda()</span><br><span class="line"><span class="comment"># 构建数据预处理</span></span><br><span class="line">transforms = ProcessImgAndGt([</span><br><span class="line">    Resize(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">    Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>]),</span><br><span class="line">    ToTensor()</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 构建Dataset</span></span><br><span class="line">train_dataset = MyDataset(train_dataset_path, transforms)</span><br><span class="line"><span class="comment"># DataLoader</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset,</span><br><span class="line">                                                   batch_size=<span class="number">12</span>,</span><br><span class="line">                                                   shuffle=<span class="literal">True</span>,</span><br><span class="line">                                                   num_workers=<span class="number">4</span>,</span><br><span class="line">                                                   pin_memory=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># TestDataset</span></span><br><span class="line">test_dataset = MyDataset(test_dataset_path, transforms)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset,</span><br><span class="line">                                                  batch_size=<span class="number">4</span>,</span><br><span class="line">                                                  shuffle=<span class="literal">True</span>,</span><br><span class="line">                                                  num_workers=<span class="number">2</span>,</span><br><span class="line">                                                  pin_memory=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># optimizer需要传入全部需要更新的参数名称，这里是对不用的参数执行不同的更新策略 </span></span><br><span class="line">base_params = [params <span class="keyword">for</span> name, params <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> (<span class="string">&quot;xxx&quot;</span> <span class="keyword">in</span> name)]</span><br><span class="line">finetune_params = [params <span class="keyword">for</span> name, params <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> (<span class="string">&quot;yyy&quot;</span> <span class="keyword">in</span> name)]</span><br><span class="line">optimizer = torch.optim.Adam([</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: base_params, <span class="string">&quot;lr&quot;</span>: <span class="number">1e-3</span>, ...&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: finetune_params, <span class="string">&quot;lr&quot;</span>: <span class="number">1e-4</span>, ...&#125;</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> trian_loader:</span><br><span class="line">        images. gts = batch[<span class="number">0</span>].cuda(), batch[<span class="number">1</span>].cuda()</span><br><span class="line">        preds = model(iamges)</span><br><span class="line">        loss = F.cross_entropy(preds, gts)</span><br><span class="line">        optimizer.zero_grad()    <span class="comment"># pytorch会积累梯度，在优化每个batch的权重的梯度之前将之前计算出的每个权重的梯度置0</span></span><br><span class="line">        loss.backward()          <span class="comment"># 在最后一个张量上调用反向传播方法，在计算图中计算权重的梯度 </span></span><br><span class="line">        optimizer.step()         <span class="comment"># 使用预先设置的学习率等参数根据当前梯度对权重进行更新</span></span><br><span class="line">        epoch_loss += loss * trian_loader.batch_size</span><br><span class="line">        <span class="comment"># 计算其他标准</span></span><br><span class="line">    loss = epoch_loss / <span class="built_in">len</span>(train_loader.dataset)</span><br><span class="line">    <span class="comment"># .......</span></span><br><span class="line">    <span class="comment"># 每隔几个epoch在测试集上跑一下</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        test_epoch_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> test_batch <span class="keyword">in</span> test_loader:</span><br><span class="line">            test_images. test_gts = test_batch[<span class="number">0</span>].cuda(), test_batch[<span class="number">1</span>].cuda()</span><br><span class="line">            test_preds = model(test_iamges)</span><br><span class="line">            loss = F.cross_entropy(test_preds, test_gts)</span><br><span class="line">            test_epoch_loss += loss * test_loader.batch_size</span><br><span class="line">            <span class="comment"># 计算其他标准</span></span><br><span class="line">        test_loss = test_epoch_loss / (<span class="built_in">len</span>(test_loader.dataset))</span><br><span class="line">    <span class="comment"># .......</span></span><br><span class="line">    <span class="comment"># 根据条件对指定epoch的模型进行保存 将模型序列化到磁盘的pickle包</span></span><br><span class="line">    <span class="keyword">if</span> 精度最高:</span><br><span class="line">        torch.save(model.stat_dict(), <span class="string">f&#x27;<span class="subst">&#123;model_path&#125;</span>_<span class="subst">&#123;time_index&#125;</span>.pth&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="test">Test</h2><p>实际使用时需要将训练好的模型上在输入数据上运行，这里以测试集的数据为例，实际情况下只需要初始化模型之后将视频流中的图像帧作为模型的输入即可。</p><h3 id="torch.no_grad"><strong>torch.no_grad()</strong></h3><p>停止autograd模块的工作，不计算和储存梯度，一般在用训练好的模型跑测试集时使用，因为测试集时不需要计算梯度更不会更新梯度。使用后可以加速计算时间，节约gpu的显存</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_dataset = MyDataset(test_dataset_path, transforms)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset,</span><br><span class="line">                                                       batch_size=<span class="number">1</span>,</span><br><span class="line">                                                       shuffle=<span class="literal">False</span>,</span><br><span class="line">                                                       num_workers=<span class="number">2</span>)</span><br><span class="line">model = Network().cuda()</span><br><span class="line"><span class="comment"># 对磁盘上的pickle文件进行解包 将gpu训练的模型加载到cpu上</span></span><br><span class="line">model.load_stat_dict(torch.load(model_path, map_location=torch.device(<span class="string">&#x27;cpu&#x27;</span>)));</span><br><span class="line">mocel.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> test_loader:</span><br><span class="line">        test_images. test_gts = test_batch[<span class="number">0</span>].cuda(), test_batch[<span class="number">1</span>].cuda()</span><br><span class="line">        test_preds = model(test_iamges)</span><br><span class="line">        <span class="comment"># 保存模型输出的图片</span></span><br></pre></td></tr></table></figure><h1 id="补充-deepvacpytorch工程化规范项目">补充: DeepVAC——Pytorch工程化规范项目</h1><blockquote><p>Github源码地址：<a href="https://github.com/deepvac/deepvac">DeepVAC/deepvac: PyTorch Project Specification. (github.com)</a></p></blockquote><h1 id="其他补充">其他补充</h1><blockquote><p><a href="https://zhuanlan.zhihu.com/p/409662511">Pytorch实验代码的亿些小细节 - 知乎 (zhihu.com)</a></p><p>一个<strong>Pytorch框架包</strong>，只要按函数划分往里面填东西就行，还可以自动支持单机多卡，多机多卡的训练：<a href="https://github.com/Lightning-AI/pytorch-lightning?tab=readme-ov-file">Lightning-AI/pytorch-lightning: Pretrain, finetune and deploy AI models on multiple GPUs, TPUs with zero code changes. (github.com)</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>群策群力战NeuS，乃知NeRF难自遮</title>
      <link href="/2024/02/06/NeuS/"/>
      <url>/2024/02/06/NeuS/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文名称: <strong>NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction</strong>（NeuS: 通过体积渲染学习神经隐式曲面，用于多视图重建）</p><p>代码地址：<a href="https://github.com/Totoro97/NeuS">Totoro97/NeuS: Code release for NeuS (github.com)</a></p><p>论文主页: <a href="https://lingjie0206.github.io/papers/NeuS/">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction (lingjie0206.github.io)</a></p><p>论文翻译：<a href="https://zhuanlan.zhihu.com/p/577893396">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction - 知乎 (zhihu.com)</a> （来自别的友友的翻译~）</p></blockquote><h1 id="基本信息">基本信息</h1><p>NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction[1]是<strong>2021</strong>年由来自香港大学、Max Planck研究所和德州农工大学的Peng Wang 等人发表在 <strong>NeurIPS (Spotlight)</strong> 会议上的SCI论文。</p><h2 id="论文摘要">论文摘要</h2><p>我们提出了一种新颖的神经表面重建方法，称为NeuS，用于从2D图像输入以高保真度重建对象和场景。</p><p>现有的神经表面重建方法，例如DVR [Niemeyer等人，2020] 和IDR [Yariv等人，2020]，需要前景掩模<span style="background:#eef0f4;">(foreground mask)</span>作为监督，容易被困在局部最小值中，并且因此与具有严重自遮挡<span style="background:#eef0f4;">(self-occlusion)</span>或薄结构的对象的重建作斗争。同时，用于新视图合成的最近的神经方法，例如NeRF [Mildenhall等人，2020] 及其变体，使用体积渲染来产生具有优化鲁棒性的神经场景表示，即使对于高度复杂的对象也是如此。但是，很难从这种学习的隐式表示中提取高质量的曲面，因为表示中没有足够的曲面约束<span style="background:#eef0f4;">(surface constraints)</span>。</p><p>在NeuS中，我们建议将曲面表示为有符号距离函数 (SDF) 的零级集<span style="background:#eef0f4;">(zero-level set)</span>，并开发一种新的体渲染方法来训练神经SDF表示。我们观察到传统的体绘制方法会导致表面重建的固有几何误差 (即偏差 <span style="background:#eef0f4;">bias</span>)，因此提出了一种新的公式，该公式在一阶近似中没有偏差，因此<strong>即使没有掩模监督</strong>，也可以实现更准确的表面重建。在DTU数据集和BlendedMVS数据集上的实验表明，NeuS在高质量的表面重建方面的性能优于现有技术，特别是对于具有复杂结构和自遮挡的对象和场景。</p><h2 id="创新点idea">创新点/IDEA</h2><p>NeRF旨在进行新视图合成而不是表面重建，因此NeRF仅学习体积密度场，因此很难从中提取高质量的表面。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240211204429014.png" alt="image-20240211204429014" /><figcaption aria-hidden="true">image-20240211204429014</figcaption></figure><blockquote><p>图1 : ( a) 表面渲染和体积渲染的图示。(b) 竹花机的玩具实例，其中花机顶部有咬合物。与最先进的方法相比，我们的方法可以处理闭塞并获得更好的重建质量。</p></blockquote><p>针对该问题，该文章提出通过引入由SDF引起的密度分布，应用体渲染方法来学习隐式SDF表示。又因简单地将标准体渲染方法应用于与SDF相关的密度将导致重构表面中的可识别偏差，所以文章提出一种新颖的体绘制方案，以确保在SDF的一阶近似中进行无偏表面重建。</p><p><strong>给定一组3D对象的有位姿图像{<span class="math inline">\(I_k\)</span>}，我们的目标是重建其表面<span class="math inline">\(S\)</span>。表面由神经隐式SDF的零级集表示。为了学习神经网络的权重，我们开发了一种新颖的体渲染方法，用于从隐式SDF渲染图像，并最小化渲染图像与输入图像之间的差异。这种体渲染方法可确保NeuS中的稳健优化，以重建复杂结构的对象。</strong></p><h1 id="相关工作">相关工作</h1><h2 id="classical-multi-view-surface-and-volumetric-reconstruction">Classical Multi-view Surface and Volumetric Reconstruction</h2><ul><li>基于点和表面的重建<ul><li>基于点和表面的重建方法通过利用图像间光度一致性来估计每个像素的深度图，然后将深度图融合为全局密集点云；</li><li>表面重建通常是通过诸如筛选的泊松表面重建之类的方法进行后处理 ；</li><li>总结：重建质量很大程度上依赖于对应匹配的质量，对于没有丰富纹理的对象，匹配对应的困难往往会导致重建结果出现严重的伪影和缺失部分；</li></ul></li><li>体积重建<ul><li>体积重建方法通过从多视图图像中估计体素网格中的占用率和颜色并评估每个体素的颜色一致性来规避显式对应匹配的困难；</li><li>总结：由于可实现的体素分辨率有限，这些方法无法实现高精度；</li></ul></li></ul><h2 id="neural-implicit-representation">Neural Implicit Representation</h2><ul><li>一些方法通过引入归纳偏差来在深度学习框架中强制实现3D理解。这些归纳偏差可以是显式表示，例如体素网格 ，点云 ，网格和隐式表示。</li><li>由于神经网络编码的隐式表示是连续的并且可以实现高空间分辨率，因此最近引起了很多关注。此表示已成功应用于形状表示，新颖的视图合成 和多视图3D重建。</li></ul><h1 id="算法框架">算法框架</h1><h2 id="场景表示-scene-representation">场景表示 Scene representation</h2><p><u>第一步是构造从3D模型到图像的渲染方法</u>（在传统图形学大概可称为<u>光栅化</u>）。定义重建对象的<u>场景</u>由两个函数表示:</p><ul><li><span class="math inline">\(f:\mathbb{R} ^3\rightarrow \mathbb{R}\)</span> 将空间位置<span class="math inline">\(x\in \mathbb{R} ^3\)</span>映射为从<span class="math inline">\(x\)</span>到对象的有符号距离</li><li><span class="math inline">\(c:\mathbb{R} ^3\times \mathbb{S}^2\rightarrow \mathbb{R} ^3\)</span> 对与点<span class="math inline">\(x\in \mathbb{R} ^3\)</span>和视角方向<span class="math inline">\(v\in \mathbb{S} ^2\)</span>相关的颜色进行编码</li></ul><p>这两个函数都由多层感知机MLP编码。重建对象的表面<span class="math inline">\(S\)</span>由SDF的零级集表示（<span style="background:#daf5e9;">SDF函数，即<span class="math inline">\(f(x)\)</span>的零值面表达形状）</span>，即 <span class="math display">\[\tag{1}\label{eq1}S=\left\{ x\in \mathbb{R} ^3|f\left( x \right) =0 \right\} .\]</span> <u>第二步是构造体渲染训练SDF网络。</u>为了应用体渲染方法来训练SDF网络，和传统体渲染不同，引入概率密度函数$_s( f( x ) ) <span class="math inline">\(作为不透明度`opacity`，称为 S-density 。其中\)</span>f(x)<span class="math inline">\(是\)</span>x<span class="math inline">\(的符号距离函数SDF，\)</span>s$是一个<strong>trainable parameter</strong>，外层是逻辑密度分布函数(logistic density function): <span class="math display">\[\phi _s\left( x \right) =se^{-sx}/\left( 1+e^{-sx} \right) ^2 \tag{2}.\]</span> 逻辑密度分布函数是sigmoid函数<span class="math inline">\(\varPhi _s\left( x \right) =\left( 1+e^{-sx} \right) ^{-1}\)</span>的一阶导数。原则上，概率密度函数<span class="math inline">\(\phi _S\left( x \right)\)</span>可以是以0为中心的任何单峰unimodal（即铃形 bell-shaped）密度函数，这里我们选逻辑密度分布是为了计算方便。逻辑密度分布函数<span class="math inline">\(\phi _s\left( x \right)\)</span>的标准差是<span class="math inline">\(1/s\)</span>，也是一个<strong>trainable parameter</strong>，即随着网络收敛<span class="math inline">\(1/s\)</span>趋向于0，同时也意味着此时概率密度函数pdf在射线上的某个空间点达到了峰值。</p><h2 id="渲染-rendering">渲染 Rendering</h2><p>给定一个像素，相机中心为<span class="math inline">\(\mathbf{o}\)</span>，光线方向（即射线ray的单位向量）为<span class="math inline">\(\mathbf{v}\)</span>，从相机中心到该像素的射线上一点为<span class="math inline">\(\mathbf{p}(t)=\mathbf{o}+t\mathbf{v}，t\geqslant0\)</span>。该像素颜色<span class="math inline">\(C\left( \mathbf{o},\mathbf{v} \right)\)</span>的一般渲染通式为： <span class="math display">\[C\left( \mathbf{o},\mathbf{v} \right) =\int_0^{+\infty}{w\left( t \right) c\left( \mathbf{p}\left( t \right) ,\mathbf{v} \right) dt}\tag{3}.\]</span> <span class="math inline">\(c\left( \mathbf{p}\left( t \right) ,\mathbf{v} \right)\)</span>是在射线上一点<span class="math inline">\(\mathbf{p}\)</span>沿着视角方向<span class="math inline">\(\mathbf{v}\)</span>的颜色。<span class="math inline">\(w(t)\)</span>是权重函数，因此要求<span class="math inline">\(w(t)&gt;0\)</span>且<span class="math inline">\(\int_0^{+\infty}w(t)dt=1\)</span>。</p><p>作者认为从2D图像学习精确的SDF表达的关键是<strong>建立一个合适的基于SDF的权重函数 <span class="math inline">\(w(t)\)</span></strong>，有两点要求：</p><ul><li><strong>Unbiased</strong>：对于相机射线<span class="math inline">\(\mathbf{p}(t)\)</span>和表面surface的交点<span class="math inline">\(\mathbf{p}(t^*)\)</span>，<span class="math inline">\(w(t)\)</span>应在<span class="math inline">\(t^*\)</span>处取得局部最大值（即<u>相机射线</u>和<u>SDF零级集zero-level set（表面surface）</u>的交点处的像素贡献最大）。</li><li><strong>Occulusion-aware</strong>：同一条射线上两个深度<span class="math inline">\(t_0\)</span>和<span class="math inline">\(t_1\)</span>，若<span class="math inline">\(f\left( t_0 \right) =f\left( t_1 \right)\)</span>但<span class="math inline">\(t_0&lt;t_1\)</span>，那么应该有<span class="math inline">\(w(t_0)&gt;w(t_1)\)</span>（即考虑自体遮挡，如果一条射线多次交叉表面（穿越多个表面），应该更多地使用最靠近相机的交点的颜色（射线所遇到的第一个表面交点所带来的颜色））。</li></ul><p>作者讨论了两种<span class="math inline">\(w(t)\)</span>表达，最终选择了<span class="math inline">\((b)\)</span>形式。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240212171629587.png" alt="image-20240212171629587" /><figcaption aria-hidden="true">image-20240212171629587</figcaption></figure><blockquote><p>图2</p></blockquote><h3 id="naive的wt权重函数设计">naive的<span class="math inline">\(w(t)\)</span>权重函数设计</h3><p>直接利用先前NeRF中提出的 volume rendering pipeline： <span class="math display">\[w\left( t \right) =T\left( t \right) \sigma \left( t \right)\tag{4}.\]</span> <span class="math inline">\(\sigma \left( t \right)\)</span>是传统体渲染中的体密度volume density，被设置为与S-density相同：<span class="math inline">\(\sigma(t)=S-density=\phi_s(f(\mathbf{p}(t)))\)</span>。<span class="math inline">\(T(t)=\exp(-\int_0^t\sigma(u){\rm d}u)\)</span>是沿射线的累积透射率accumulated transmittance。该方法只能做到occulusion-aware，但是由于它在重建表面引入了固有误差，因此有bias，在射线打到表面之前<span class="math inline">\(w(t)\)</span>已经取得了局部最优，如图2(a)所示（具体推导可参考原文补充材料）。</p><h3 id="作者提出的wt权重函数设计">作者提出的<span class="math inline">\(w(t)\)</span>权重函数设计</h3><p>首先讨论一种straight-forward 方式： <span class="math display">\[w(t) = \frac{ \phi_s(f(\mathbf{p}(t)))}{ \int_0^{+\infty} \phi_s( f( \mathbf{p}(u) ) ) {\rm d}u }. \tag{5}\label{eq5}\]</span> 该方式直接使用归一化的S-density作为权重函数，显然无偏，但是没有occlusion-aware：两个SDF交叉点将在<span class="math inline">\(w(t)\)</span>中产生两个等值的峰。</p><p>接下来，我们需要再对其进行改造，使其能够满足occlusion-aware的性质。自然地，NeRF本身基本的公式就是occlusion-aware，文章还是采用了基本公式的逻辑，但是在其基础上做了微小的改动，即<span style="background:#daf5e9;">自行定义了一个不透明的密度函数<span class="math inline">\(\rho(t)\)</span>（opaque density），对应着传统公式中的<span class="math inline">\(\sigma(t)\)</span></span>，因此，权重函数变为了： <span class="math display">\[w(t)=T(t)\rho(t),\,\,where\,\,T(t)=\exp(-\int_0^t \rho(u){\rm d}u). \tag{6}\label{eq6}\]</span></p><h4 id="rhot的推导思路"><span class="math inline">\(\rho(t)\)</span>的推导思路</h4><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240213173052919.png" alt="image-20240213173052919" /><figcaption aria-hidden="true">image-20240213173052919</figcaption></figure><blockquote><p>图3 多个表面相交情况下，权重分布图示。由图可见我们设计的<span class="math inline">\(w(t)\)</span>保证SDF的估计是一阶无偏的，并且是occlusion-aware的。</p></blockquote><p>首先，我们考虑一种简单情况：即只有一个表面交点且表面是平面，此时从<span class="math inline">\(\eqref{eq5}\eqref{eq6}\)</span>式出发导出<span class="math inline">\(\rho(t)\)</span>以<span class="math inline">\(f(\mathbf{p}(t))\)</span>为输入的表达式，然后再推广到多次表明相交的情况。</p><p>在这种简单的特殊情况下，我们很容易地知道符号距离函数sdf，即<span class="math inline">\(f(\mathbf{p}(t))\)</span>有以下的关系： <span class="math display">\[f(\mathbf{p}(t))=-|cos(\theta)|\cdot(t-t^*),\,\,where\,\, f(\mathbf{p}(t^*))=0 \tag{7}.\]</span> 其中，<span class="math inline">\(\theta\)</span>是视角方向<span class="math inline">\(\mathbf{v}\)</span>与外表面法向量<span class="math inline">\(\mathbf{n}\)</span>之间的夹角。由于表面surface被假定为一个平面plane，因此<span class="math inline">\(|cos(\theta)|\)</span>是一个常量。此时根据<span class="math inline">\(\eqref{eq5}\)</span>式有： <span class="math display">\[\begin{aligned}w(t) &amp; =\lim _{t^{*} \rightarrow+\infty} \frac{\phi_{s}(f(\mathbf{p}(t)))}{\int_{0}^{+\infty} \phi_{s}(f(\mathbf{p}(u))) \mathrm{d} u} \\&amp; =\lim _{t^{*} \rightarrow+\infty} \frac{\phi_{s}(f(\mathbf{p}(t)))}{\int_{0}^{+\infty} \phi_{s}\left(-|\cos (\theta)|\left(u-t^{*}\right)\right) \mathrm{d} u} \\&amp; =\lim _{t^{*} \rightarrow+\infty} \frac{\phi_{s}(f(\mathbf{p}(t)))}{\int_{-t^{*}}^{+\infty} \phi_{s}\left(-|\cos (\theta)| u^{*}\right) \mathrm{d} u^{*}} \\&amp; =\lim _{t^{*} \rightarrow+\infty} \frac{\phi_{s}(f(\mathbf{p}(t)))}{|\cos (\theta)|^{-1} \int_{-|\cos (\theta)| t^{*}}^{+\infty} \phi_{s}(\hat{u}) \mathrm{d} \hat{u}} \\&amp; =|\cos (\theta)| \phi_{s}(f(\mathbf{p}(t))) .\end{aligned} \tag{8}\]</span> 又根据<span class="math inline">\(\eqref{eq6}\)</span>式， <span class="math display">\[T(t)\rho(t)=|\cos (\theta)| \phi_{s}(f(\mathbf{p}(t))).\tag{9}\]</span> 已知累积透射率accumulated transmittance <span class="math inline">\(T(t)=\exp(-\int_0^t \rho(u){\rm d}u)\)</span>，则： <span class="math display">\[T\left( t \right) \rho \left( t \right) =-\frac{dT}{dt}\left( t \right).\tag{10.a}\]</span></p><p><span class="math display">\[|\cos (\theta)| \phi_{s}(f(\mathbf{p}(t)))=-\frac{d\varPhi _s}{dt}\left( f\left( \mathbf{p}\left( t \right) \right) \right) .\tag{10.b}\]</span></p><p>从而， <span class="math display">\[\frac{dT}{dt}\left( t \right)=\frac{d\varPhi _s}{dt}\left( f\left( \mathbf{p}\left( t \right) \right) \right)\]</span> 对两边积分得， <span class="math display">\[T\left( t \right) =\varPhi _s\left( f\left( \mathbf{p}\left( t \right) \right) \right)\tag{11}\]</span> 最后取对数，再对两边积分， <span class="math display">\[\int_{0}^{t} \rho(u) \mathrm{d} u=-\ln \left(\Phi_{s}(f(\mathbf{p}(t)))\right) \\\Rightarrow \rho(t)=\frac{-\frac{\mathrm{d} \Phi_{s}}{\mathrm{~d} t}(f(\mathbf{p}(t)))}{\Phi_{s}(f(\mathbf{p}(t)))}. \tag{12}\]</span> 上式是单个surface的情况，当光线在两个surface之间时会变成负，把它拓展到多surface的情况需要在这时将之设为0，即： <span class="math display">\[\rho(t)=\max \left(\frac{-\frac{\mathrm{d} \Phi_{s}}{\mathrm{~d} t}(f(\mathbf{p}(t)))}{\Phi_{s}(f(\mathbf{p}(t)))}, 0\right) .\tag{13}\label{eq13}\]</span> 在补充材料中我们给出了在单面交和多面交情况下，由<span class="math inline">\(\eqref{eq13}\eqref{eq6}\)</span>式定义的权重函数在SDF的一阶逼近中是无偏的定理证明。</p><h3 id="离散化discretization">离散化Discretization</h3><p>为了获得不透明度opacity和权重函数weight function的离散对应部分，我们采用与NeRF中使用的近似方案，该方案沿着射线对<span class="math inline">\(n\)</span>个点进行采样<span class="math inline">\(\{\mathbf{p}_i=\mathbf{o}+t_i\mathbf{v}|i=1,...,n,t_i&lt;t_{i+1}\}\)</span>，以计算射线的近似像素颜色<span class="math inline">\(\hat{C}\)</span>如下： <span class="math display">\[\hat{C}=\sum_{i=1}^n{T_i\alpha _ic_i}, \]</span> <span class="math display">\[T_i=\prod_{j=1}^{i-1}{\left( 1-\alpha _j \right)},   \tag{14}\label{eq14}\]</span></p><p><span class="math display">\[\alpha _i=1-\exp \left( -\int_{t_i}^{t_{i+1}}{\rho \left( t \right) dt} \right) =\max \left( \frac{\Phi _s(f(\mathbf{p}(t_i)))-\Phi _s(f(\mathbf{p}(t_{i+1})))}{\Phi _s(f(\mathbf{p}(t_i)))},0 \right).\]</span></p><p>其中<span class="math inline">\(T_i\)</span>是离散的累积透射率accumulated transmittance，<span class="math inline">\(\alpha_i\)</span>是离散的不透明度opacity值，关于<span class="math inline">\(\alpha_i\)</span>的详细推导在补充材料中给出。</p><h4 id="离散化的采样方式">离散化的采样方式</h4><p>在真正的实现过程中，实际上有两种采样方式：</p><ol type="1"><li>直接采样射线上的点：<span class="math inline">\(\mathbf{q}_i=\mathbf{o}+t_i\mathbf{v}\)</span></li><li>采样射线上某一小段的中点： <span class="math inline">\(\mathbf{p}_i = \mathbf{o} + \frac{ t_i + t_{i+1} }{2} \mathbf{v}\)</span></li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240213205224343.png" alt="image-20240213205224343" /><figcaption aria-hidden="true">image-20240213205224343</figcaption></figure><blockquote><p>图4 两种采样方式。</p><p>具体而言，当我们<strong>计算<span class="math inline">\(\alpha\)</span>值</strong>时，我们采用section points的方式，计算公式与<span class="math inline">\(\eqref{eq14}\)</span>保持一致，即<span class="math inline">\(\max \left( \frac{\Phi _s(f(\mathbf{q}(t_i)))-\Phi _s(f(\mathbf{q}(t_{i+1})))}{\Phi _s(f(\mathbf{q}(t_i)))},0 \right)\)</span>。当<strong>计算颜色值</strong>时，我们采用mid-point方式。</p></blockquote><h2 id="训练-training">训练 Training</h2><h3 id="损失函数">损失函数</h3><p>随机采样一批(a batch)像素，以及其对应的在世界坐标系下的射线，即： <span class="math display">\[P=\{C_k,M_k,\mathbf{o}_k,\mathbf{v}_k\}\]</span> 其中，<span class="math inline">\(C_k\)</span>是像素颜色，<span class="math inline">\(M_k\in\{0,1\}\)</span>是可选的mask值。采样像素数量（batch size）设为m，一条射线上的采样点数（point sampling size）设为n。损失函数定义为： <span class="math display">\[L=L_{color}+\lambda L_{reg}+\beta L_{mask}.\tag{15}\]</span> 其中，</p><ul><li><span class="math inline">\(L_{color}=\frac{1}{m}\sum_k{R\left( \hat{C}_k,C_k \right)}\)</span> ——渲染图和真实图差异。类似IDR，<span class="math inline">\(R\)</span>采用L1损失，这使得我们的观察对outliers鲁棒且训练稳定</li><li><span class="math inline">\(L_{reg}=\frac{1}{nm}\sum_{k,i}{\left( |\nabla f\left( \mathbf{\hat{p}}_{k,i} \right) |_2-1 \right) ^2}\)</span>——在采样点sample points上加的Eikonal项，用于正则化<span class="math inline">\(f_{\theta}\)</span>的SDF值，即SDF法向量的损失</li><li><span class="math inline">\(L_{mask}=BCE(M_k,\hat{O}_k)\)</span>——可选mask项，BCE（binary cross entropy）即二值交叉熵。其中<span class="math inline">\(\hat{O}_k=\sum_{i=1}^n{T_{k,i}\alpha _k}\)</span>是在相机射线上采样点的权重求和，这里的预测可微分mask就是通过射线上权重<span class="math inline">\(w(t)\)</span>的和求出的。</li></ul><h3 id="分层采样-hierarchical-sampling">分层采样 Hierarchical Sampling</h3><blockquote><p>原来的NeRF采样过程：</p><p>首先均匀采样一组点，然后 coarse importance sampling + fine importance sampling。<strong>coarse sampling</strong> 64个点，其概率值由 <strong>固定的很大的</strong>标准差 <span class="math inline">\(1/s\)</span>定义的<span class="math inline">\(\phi_s(f(x))\)</span>给出；<strong>fine sampling</strong> 64个点，其概率值由 <strong>learned</strong> <span class="math inline">\(1/s\)</span> 定义的 <span class="math inline">\(\phi_s(f(x))\)</span>给出。</p></blockquote><p>基本没怎么改动 NeRF 原来的 stratified sampling 过程（首先，均匀地在射线上进行采样，然后迭代地在粗概率估计峰值处执行重要性采样。）:</p><p>但不同于NeRF同时优化 coarse network和 fine network，这里只训练了一个network。首先均匀采样64个点作为coarse sampling 点，其概率由 <strong>固定的很大的</strong>标准差 <span class="math inline">\(1/s\)</span>定义的S-density <span class="math inline">\(\phi_s(f(x))\)</span>给出；然后利用这组coarse点、其对应的由<strong>固定的很大的</strong>标准差<span class="math inline">\(1/s\)</span>定义的<span class="math inline">\(\phi_s(f(x))\)</span>值和学得的标准差构造pdf，采样64个fine sampling点。</p><h1 id="代码">代码</h1><blockquote><p>代码注释小技巧：在注释代码时，如果想区别原来就有的代码和我们后来加上的代码，可以通过 Ctrl+R 将所有原来就有的注释符号 '#' 改为' '###'。在后面添加的代码注释则都以 '#' 开头。</p></blockquote><h2 id="运行指令">运行指令</h2><ul><li><strong>Training without mask</strong></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python exp_runner.py --mode train --conf ./confs/womask.conf --<span class="keyword">case</span> &lt;case_name&gt;</span><br></pre></td></tr></table></figure><ul><li><strong>Training with mask</strong></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python exp_runner.py --mode train --conf ./confs/wmask.conf --<span class="keyword">case</span> &lt;case_name&gt;</span><br></pre></td></tr></table></figure><ul><li><strong>Extract surface from trained model</strong>——The corresponding mesh can be found in <code>exp/&lt;case_name&gt;/&lt;exp_name&gt;/meshes/&lt;iter_steps&gt;.ply</code>.</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python exp_runner.py --mode validate_mesh --conf &lt;config_file&gt; --<span class="keyword">case</span> &lt;case_name&gt; --is_continue <span class="comment"># use latest checkpoint</span></span><br></pre></td></tr></table></figure><ul><li><strong>View interpolation</strong>——The corresponding image set of view interpolation can be found in <code>exp/&lt;case_name&gt;/&lt;exp_name&gt;/render/</code>.</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python exp_runner.py --mode interpolate_&lt;img_idx_0&gt;_&lt;img_idx_1&gt; --conf &lt;config_file&gt; --<span class="keyword">case</span> &lt;case_name&gt; --is_continue <span class="comment"># use latest checkpoint</span></span><br></pre></td></tr></table></figure><h2 id="代码结构">代码结构</h2><details><summary>confs /配置文件 configurations</summary><p><span style="color: #800000;">thin_structure.conf    /薄结构的情况</span><br /><span style="color: #800000;">wmask.conf    /有mask</span><br /><span style="color: #800000;">womask.conf    /没有mask</span><br /><span style="color: #800000;">womask_dtu_large_roi.conf    /不知道是什么</span></p></details><details><summary>exp /输出的结果 export</summary><div><strong><span style="font-size: 14pt;">data_BlendedMVS</span></strong></div><div style="padding-left: 40px;"><strong><span style="font-size: 12pt;">bmvs_bear</span></strong></div><div style="padding-left: 80px;"><span style="color: #800000;">wmask/有mask</span></div><div style="padding-left: 120px;"><span style="color: #f1c40f;">logs/日志</span></div><div style="padding-left: 120px;"><span style="color: #f1c40f;">recording/训练记录</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">models</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">dataset.py</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">embedder.py</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">fields.py</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">renderer.py</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">config.conf</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">exp_runner.py</span></div><div style="padding-left: 80px;"><span style="color: #800000;">womask_sphere/无mask</span></div><div style="padding-left: 120px;"><span style="color: #f1c40f;">logs</span></div><div style="padding-left: 120px;"><span style="color: #f1c40f;">recording</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">models</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">dataset.py/加载数据</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">embedder.py</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">fields.py</span></div><div style="padding-left: 200px;"><span style="color: #3598db;">renderer.py</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">config.conf</span></div><div style="padding-left: 160px;"><span style="color: #e03e2d;">exp_runner.py</span></div></details><details><summary>models /模型</summary><p><span style="color: #800000;">dataset.py    /生成数据集，加载数据集</span><br /><span style="color: #800000;">embedder.py</span><br /><span style="color: #800000;">fields.py /存放了四个用于训练的网络类</span><br /><span style="color: #800000;">renderer.py</span></p></details><details><summary>preprocess_custom_data</summary><p><span style="font-size: 14pt; color: #800000;">aruco_preprocess</span><br /><span style="color: #3598db;">    calibration.cpp</span><br /><span style="color: #3598db;">    CMakeLists.txt</span><br /><span style="color: #3598db;">    gen_cameras.py</span><br /><span style="color: #3598db;">    run.sh</span><br /><span style="font-size: 14pt; color: #800000;">colmap_preprocess</span><br /><span style="color: #3598db;">    colmap_read_model.py</span><br /><span style="color: #3598db;">    colmap_wrapper.py</span><br /><span style="color: #3598db;">    gen_cameras.py</span><br /><span style="color: #3598db;">    imgs2poses.py</span><br /><span style="color: #3598db;">    pose_utils.py</span><br /><span style="font-size: 14pt; color: #800000;">static</span><br /><span style="color: #3598db;">    aruco_board<span style="color: #3598db;">.png</span><br /><span style="color: #3598db;">    interest_sparse_points.png</span><br /><span style="color: #3598db;">    raw_sparse_points.png</span><br /><span style="font-size: 14pt; color: #800000;">readme.md</span></span></p></details><details><summary>static</summary><p><span style="color: #800000;">intro_1_compressed.gif</span><br /><span style="color: #800000;">intro_2_compressed.gif</span></p></details><p>exp_runner.py /主程序：包含训练、验证、测试和预测<br> README.md /项目说明<br> requirements.txt /需要的python依赖包列表<br> public_data<br> .gitignore /告诉Git忽略哪些文件<br> LICENSE /许可类型<br></p><h2 id="主程序-exp_runner.py">主程序 exp_runner.py</h2><p>主要分为两部分：</p><ul><li><p><strong>class Runner</strong></p><p>|<span style="background:#daf5e9;">def __init__</span> /初始化函数</p><div class="line-block">  |self.Networks /同时将其加入 params_to _train(来自 /models/fields.py)</div><div class="line-block">  | |nerf_outside—NeRF</div><div class="line-block">  | |sdf_network—SDFNetwork</div><div class="line-block">  | |deviation_network—SingleVarianceNetwork</div><div class="line-block">  | |color_network—RenderingNetwork</div><div class="line-block">  |self.renderer /neus渲染器,继承自/models/renderer.py</div><div class="line-block">  | |self.nerf_outside</div><div class="line-block">  | |self.sdf_network</div><div class="line-block">  | |self.deviation_network</div><div class="line-block">  | |self.color_network</div><div class="line-block">  | |**self.conf['model.neus_renderer']</div><p>|<span style="background:#f9eda6;">def train(self)</span> /当mode为'train'时执行训练函数</p><p>|<span style="background:#daf5e9;">def get_image_perm(self)</span> /将图片打乱顺序</p><p>|<span style="background:#f9eda6;">def get_cos_anneal_ratio(self)</span></p><p>|<span style="background:#daf5e9;">def update_learning_rate(self)</span> /更新学习率</p><p>|<span style="background:#f9eda6;">def file_backup(self)</span></p><p>|<span style="background:#daf5e9;">def load_checkpoint(self, checkpoint_name)</span></p><p>|<span style="background:#f9eda6;">def save_checkpoint(self)</span></p><p>|<span style="background:#daf5e9;">def validate_image(self, idx=-1, resolution_level=-1)</span></p><p>|<span style="background:#f9eda6;">def render_novel_image</span> /渲染新图片</p><p>|<span style="background:#daf5e9;">def validate_mesh</span> /当mode为'validate_mesh'时执行</p><div class="line-block"><span style="background:#f9eda6;">def interpolate_view(self, img_idx_0, img_idx_1)</span> /当mode以'interpolate'开头时执行</div></li><li><p><strong>if __name__ == '__main__'</strong></p><p>|print开始提示信息</p><p>|log输出格式</p><p>|创建arg = parser.parse_args</p><div class="line-block">  |conf</div><div class="line-block">  |mode</div><div class="line-block">  |mcube_threshold</div><div class="line-block">  |is_continue</div><div class="line-block">  |gpu</div><div class="line-block">  |case: 样本集名称，默认值为空，需要由运行指令传入</div><p>|创建runner = Runner(args.xxx,...)，传入arg参数</p><p>|用if循环判断args.mode值以执行不同runner类的函数</p></li></ul><h2 id="配置文件夹confs">配置文件夹confs</h2><p>配置文件：主要是 general、dataset、train 和 model 四部分内容</p><ul><li><p>|<span style="background:#daf5e9;">general</span></p><p>|<span style="background:#daf5e9;">dataset</span></p><p>|<span style="background:#daf5e9;">train</span></p><p>|<span style="background:#daf5e9;">model</span></p><div class="line-block">  |nerf</div><div class="line-block">  |sdf_network</div><div class="line-block">  |variance_network</div><div class="line-block">  |rendering_network</div><div class="line-block">  |neus_renderer</div></li></ul><h2 id="models-模型文件夹">models 模型文件夹</h2><h3 id="fields.py">fields.py</h3><p>fields.py 中存放了从IDR（https://github.com/lioryariv/idr）中借鉴的两个网络 <span style="background:#daf5e9;">SDFNetwork</span> 和 <span style="background:#daf5e9;">RenderingNetwork</span> ，从nerf-pytorch（https://github.com/yenchenlin/nerf-pytorch）中借鉴的网络 <span style="background:#f9eda6;">NeRF</span> 和最后一个网络 <span style="background:#dad5e9;">SingleVarianceNetwork</span> 。</p><h3 id="renderer.py">renderer.py</h3><ul><li><p>|def extract_fields</p><p>|def extract_geometry</p><p>|def sample_pdf</p><p>|class NeuSRenderer</p></li></ul><h3 id="dataset.py">dataset.py</h3><ul><li><p>|def load_K_Rt_from_P</p><p>|class Dataset /用于加载处理数据data</p><div class="line-block">  |def __init__</div><div class="line-block">  |def gen_rays_at /Generate rays at world space from one camera</div><div class="line-block">  |def gen_random_rays_at /Generate random rays at world space from one camera</div><div class="line-block">  |def gen_rays_between /Interpolate pose between two cameras.</div><div class="line-block">  |def near_far_from_sphere</div><div class="line-block">  |def image_at</div></li></ul><h1 id="bib-citation">Bib Citation</h1><figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">@article&#123;wang2021neus,</span><br><span class="line">      title=&#123;NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction&#125;, </span><br><span class="line">      author=&#123;Peng Wang and Lingjie Liu and Yuan Liu and Christian Theobalt and Taku Komura and Wenping Wang&#125;,</span><br><span class="line">  journal=&#123;NeurIPS&#125;,</span><br><span class="line">      year=&#123;2021&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="参考文献">参考文献</h1><p><strong>[1] Wang P, Liu L, Liu Y, et al. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction[J]. arixv preprint arixv:2106.10689, 2021.</strong></p><h2 id="参考网址">参考网址：</h2><ul><li><p><a href="https://blog.csdn.net/pylittlebrat/article/details/127503069">Neus学习笔记-CSDN博客</a></p></li><li><p><a href="https://longtimenohack.com/posts/paper_reading/2021_wang_neus/">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction - Jianfei Guo (longtimenohack.com)</a></p></li><li><p><a href="https://blog.csdn.net/flow_specter/article/details/126222914">论文笔记：NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction_neus 论文解读-CSDN博客</a></p></li><li><p>[<a href="https://zhuanlan.zhihu.com/p/496752239">论文笔记]NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction - 知乎 (zhihu.com)</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NIPS2021 </tag>
            
            <tag> volume rendering </tag>
            
            <tag> neural surface reconstruction </tag>
            
            <tag> SDF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Diffusion Model（一）</title>
      <link href="/2024/01/23/Diffusion%20Model%20%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2024/01/23/Diffusion%20Model%20%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://zhuanlan.zhihu.com/p/576475987">扩散模型 (Diffusion Model) 简要介绍与源码分析 - 知乎 (zhihu.com)</a></p><p><a href="https://www.zhihu.com/question/545764550">(99+ 封私信 / 81 条消息) 怎么理解今年 CV 比较火的扩散模型（DDPM）？ - 知乎 (zhihu.com)</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>DreamAvatar 论文笔记</title>
      <link href="/2024/01/23/DreamAvatar/"/>
      <url>/2024/01/23/DreamAvatar/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.bilibili.com/video/BV1SN411i7d8/?spm_id_from=333.337.search-card.all.click&amp;vd_source=124ec79ebd3e16b0f454a3994a468f98">2023082【跨模态学习驱动的三维理解与生成】韩锴：Text-and-Shape Guided 3D Human Avatar Generation……_哔哩哔哩_bilibili</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> diffusion论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积神经网络</title>
      <link href="/2024/01/23/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/"/>
      <url>/2024/01/23/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/</url>
      
        <content type="html"><![CDATA[<blockquote><p>原文链接：<a href="https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/index.html">6. 卷积神经网络 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p></blockquote><h1 id="从全连接层到卷积">从全连接层到卷积</h1><blockquote><p>本节主要讲述卷积层的原理。卷积的本质是<strong>有效提取相邻像素间的相关特征</strong>。</p><p>之前我们将softmax回归模型和多层感知机模型应用于Fashion-MNIST数据集中的服装图片。 为了能够应用softmax回归和多层感知机，我们首先将每个大小为28×28的图像展平为一个784维的固定长度的一维向量，然后用全连接层对其进行处理。 而现在，若我们掌握了卷积层的处理方法，我们就<strong>可以在图像中保留空间结构</strong>。 同时，用卷积层代替全连接层的另一个好处是：<strong>模型更简洁、所需的参数更少</strong>。</p></blockquote><p>多层感知机十分适合处理表格数据，其中行对应样本，列对应特征。 对于表格数据，我们寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构。 此时，多层感知机可能是最好的选择，然而对于高维感知数据（如图片数据），这种缺少结构的网络可能会变得不实用。</p><p>图像中本就拥有丰富的结构，而这些结构可以被人类和机器学习模型使用。 <strong>卷积神经网络（convolutional neural networks，CNN）是机器学习利用自然图像中一些已知结构的创造性方法</strong>。</p><h2 id="不变性">不变性</h2><p>假设我们想从一张图片中找到某个物体。 合理的假设是：<strong>无论哪种方法找到这个物体，都应该和物体的位置无关</strong>。 不妨假设我们要在人群中寻找小明的位置，由于小明的外表不取决于他所处的位置，因此我们可以使用一个“小明检测器“扫描图像。该检测器将图像分割为多个区域，并为每个区域包含小明的可能性打分。卷积神经网络正是将<strong>空间不变性（spatial invariance）</strong>的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。</p><p>现在，我们将上述想法总结一下，从而帮助我们设计适合于计算机视觉的神经网络架构。</p><ol type="1"><li><p>平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</p></li><li><p>局部性（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</p></li></ol><h2 id="多层感知机的限制">多层感知机的限制</h2><h3 id="平移不变性">平移不变性</h3><h3 id="局部性">局部性</h3><h2 id="卷积">卷积</h2><p>在数学中，两个函数（如<span class="math inline">\(f,g:R^d\rightarrow R\)</span>）之间的“卷积”被定义为 <span class="math display">\[\left( f*g \right) \left( \mathbf{x} \right) =\int{f\left( \mathbf{z} \right)}g\left( \mathbf{x}-\mathbf{z} \right) d\mathbf{z}.\]</span> 也就是说，卷积是当把一个函数“翻转”并移位<span class="math inline">\(\mathbf{x}\)</span>时，测量<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>之间的重叠。 当为离散对象时，积分就变成求和。对于二维张量，则为<span class="math inline">\(f\)</span>的索引(<span class="math inline">\(a\)</span>,<span class="math inline">\(b\)</span>)和<span class="math inline">\(g\)</span>的索引(<span class="math inline">\(i-a\)</span>,<span class="math inline">\(j-b\)</span>)上的对应加和： <span class="math display">\[\left( f*g \right) \left( i,j \right) =\sum_a{\sum_b{f\left( a,b \right) g\left( i-a,j-b \right)}}.\]</span></p><h2 id="通道">通道</h2><p>回到上面的找小明游戏，让我们看看它到底是什么样子。卷积层根据<strong>滤波器</strong><span class="math inline">\(V\)</span> <span style="background:#daf5e9;">（filter, 即卷积核 convolution kernel, 亦或简单地称之为该卷积层的<em>权重</em>，通常该权重是可学习的参数）</span>选取给定大小的窗口，并加权处理图片。我们的目标是学习一个模型，以便探测出“小明”最可能出现的地方。</p><p>然而这种方法有一个问题：我们忽略了图像一般包含三个通道/三种原色（红色、绿色和蓝色）。 实际上，<strong>图像不是二维张量，而是一个由高度、宽度和颜色组成的三维张量</strong>，比如包含1024×1024×3个像素。 前两个轴与像素的空间位置有关，而第三个轴可以看作每个像素的多维表示。因此，我们将输入<span class="math inline">\(X\)</span>索引为<span class="math inline">\([X]_{i,j,k}\)</span>。由此卷积相应地调整为<span class="math inline">\([V]_{a,b,c}\)</span>，而不是<span class="math inline">\([V]_{a,b}\)</span>。</p><p>此外，由于输入图像是三维的，我们的隐藏表示<span class="math inline">\(H\)</span>也最好采用三维张量。 换句话说，对于每一个空间位置，我们想要采用一组而不是一个隐藏表示。这样一组隐藏表示可以想象成一些互相堆叠的二维网格。 因此，我们可以把隐藏表示想象为一系列具有二维张量的<strong>通道（channel）</strong>。 这些通道有时也被称为<strong>特征映射（feature maps）</strong>，因为每个通道都向后续层提供一组空间化的学习特征。 直观上可以想象在靠近输入的底层，一些通道专门识别边缘，而一些通道专门识别纹理。</p><p>为了支持输入<span class="math inline">\(X\)</span>和隐藏表示<span class="math inline">\(H\)</span>中的多个通道，我们可以在<span class="math inline">\(V\)</span>中添加第四个坐标，即<span class="math inline">\([V]_{a,b,c,d}\)</span>。综上所述， <span class="math display">\[[\mathsf{H} ]_{i,j,d}=\sum_{a=-\Delta}^{\Delta}{\sum_{b=-\Delta}^{\Delta}{\sum_c{[}}}\mathsf{V} ]_{a,b,c,d}[\mathsf{X} ]_{i+a,j+b,c},\]</span> 其中隐藏表示<span class="math inline">\(H\)</span>中的索引<span class="math inline">\(d\)</span>表示输出通道，而随后的输出将继续以三维张量<span class="math inline">\(H\)</span>作为输入进入下一个卷积层。 所以，上式可以定义具有多个通道的卷积层，而其中<span class="math inline">\(V\)</span>是该卷积层的权重。</p><h2 id="从全连接层到卷积总结">从全连接层到卷积总结</h2><ul><li>图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。</li><li>局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。</li><li>在图像处理中，卷积层通常比全连接层需要更少的参数，但依旧获得高效用的模型。</li><li>卷积神经网络（CNN）是一类特殊的神经网络，它可以包含多个卷积层。</li><li>多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。</li></ul><h1 id="图像卷积">图像卷积</h1><blockquote><p>由于卷积神经网络的设计是用于探索图像数据，本节以图像为例讲述卷积的实际应用。</p></blockquote><h2 id="互相关运算">互相关运算</h2><p>严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是<em>互相关运算</em>（cross-correlation），而不是卷积运算。 根据<u>第一节</u>中的描述，在卷积层中，输入张量和核张量通过互相关运算产生输出张量。</p><p>首先，我们暂时忽略通道（第三维）这一情况，看看如何处理二维图像数据和隐藏表示。在下图中，输入是高度为3、宽度为3的二维张量（即形状为3×3）。卷积核的高度和宽度都是2，而卷积核窗口（或卷积窗口）的形状由内核的高度和宽度决定（即2×2）。</p><figure><img src="https://zh-v2.d2l.ai/_images/correlation.svg" alt="zh-v2.d2l.ai/_images/correlation.svg" /><figcaption aria-hidden="true">zh-v2.d2l.ai/_images/correlation.svg</figcaption></figure><p>二维互相关运算。阴影部分是第一个输出元素，以及用于计算输出的输入张量元素和核张量元素： <span class="math display">\[0\times0+1\times1+3\times2+4\times3=19\]</span> 在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。 在如上例子中，输出张量的四个元素由二维互相关运算得到，输出高度为2、宽度为2。</p><blockquote><p>注意，输出大小略小于输入大小。这是因为卷积核的宽度和高度大于1， 而卷积核只与图像中每个大小完全适合的位置进行互相关运算。 所以，输出大小等于输入大小<span class="math inline">\(n_h×n_w\)</span>减去卷积核大小<span class="math inline">\(k_h×k_w\)</span>，即： <span class="math display">\[(n_h-k_h+1) \times (n_w-k_w+1).\]</span> 这是因为我们需要足够的空间在图像上“移动”卷积核。稍后，我们将看到如何通过在图像边界周围填充零来保证有足够的空间移动卷积核，从而保持输出大小不变。</p></blockquote><p>接下来，我们在<code>corr2d</code>函数中实现如上过程，该函数接受输入张量<code>X</code>和卷积核张量<code>K</code>，并返回输出张量<code>Y</code>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">X, K</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算二维互相关运算&quot;&quot;&quot;</span></span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i:i + h, j:j + w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;验证上述二维互相关运算的输出&#x27;&#x27;&#x27;</span></span><br><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">K = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">corr2d(X, K)</span><br></pre></td></tr></table></figure><p>tensor([[19., 25.], [37., 43.]])</p><h2 id="卷积层">卷积层</h2><blockquote><p>高度和宽度分别为<span class="math inline">\(ℎ\)</span>和<span class="math inline">\(w\)</span>的卷积核可以被称为<span class="math inline">\(h×w\)</span>卷积或<span class="math inline">\(h×w\)</span>卷积核。 我们也将带有<span class="math inline">\(h×w\)</span>卷积核的卷积层称为<span class="math inline">\(h×w\)</span>卷积层。</p></blockquote><p>卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。 所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。 就像我们之前随机初始化全连接层一样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。</p><p>基于上面定义的<code>corr2d</code>函数实现二维卷积层。在<code>__init__</code>构造函数中，将<code>weight</code>和<code>bias</code>声明为两个模型参数<code>Parameter</code>。前向传播函数调用<code>corr2d</code>函数并添加偏置。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, kernel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#赋初值，nn.Parameter为可更新的参数（requires_grad=True）</span></span><br><span class="line">        self.weight = nn.Parameter(torch.rand(kernel_size))  <span class="comment">#随机初始化</span></span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure><h2 id="图像中目标的边缘检测">图像中目标的边缘检测</h2><p>如下是卷积层的一个简单应用：通过找到像素变化的位置，来检测图像中不同颜色的边缘。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.首先，我们构造一个6×8像素的黑白图像。中间四列为黑色（0），其余像素为白色（1）。</span></span><br><span class="line">X = torch.ones((<span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">X</span></span><br><span class="line"><span class="string">tensor([[1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.接下来，我们构造一个高度为1、宽度为2的卷积核K。当进行互相关运算时，如果水平相邻的两元素相同，则输出为零，否则输出为非零。</span></span><br><span class="line">K = torch.tensor([[<span class="number">1.0</span>, -<span class="number">1.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.现在，我们对参数X（输入）和K（卷积核）执行互相关运算。 如下所示，输出Y中的1代表从白色到黑色的边缘，-1代表从黑色到白色的边缘，其他情况的输出为0。</span></span><br><span class="line">Y = corr2d(X, K)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Y</span></span><br><span class="line"><span class="string">tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span></span><br><span class="line"><span class="string">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.最后，我们将输入的二维图像转置，再进行如上的互相关运算。 其输出如下，之前检测到的垂直边缘消失了。 不出所料，这个卷积核K只可以检测垂直边缘，无法检测水平边缘。</span></span><br><span class="line">corr2d(X.t(), K)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0., 0.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="学习卷积核">学习卷积核</h2><p>如果我们只需寻找黑白边缘，那么以上<code>[1, -1]</code>的边缘检测器足以。然而，当有了更复杂数值的卷积核，或者连续的卷积层时，我们不可能手动设计滤波器。因此考虑是否可以通过<code>仅查看“输入-输出”对</code>来学习由<code>X</code>生成<code>Y</code>的卷积核？</p><p>我们先构造一个卷积层，并将其卷积核初始化为随机张量。接下来，在每次迭代中，我们比较<code>Y</code>与卷积层输出的平方误差，然后计算梯度来更新卷积核。为了简单起见，我们在此使用内置的二维卷积层<code>nn.Conv2d</code>，并忽略偏置。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造一个二维卷积层，它具有1个输入通道、1个输出通道和形状为（1，2）的卷积核</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），</span></span><br><span class="line"><span class="comment"># 其中批量大小和通道数都为1</span></span><br><span class="line">X = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">Y = Y.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>))</span><br><span class="line">lr = <span class="number">3e-2</span>  <span class="comment"># 学习率0.03</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    loss = (Y_hat - Y) ** <span class="number">2</span></span><br><span class="line">    conv2d.zero_grad()</span><br><span class="line">    loss.<span class="built_in">sum</span>().backward()</span><br><span class="line">    <span class="comment"># 迭代卷积核</span></span><br><span class="line">    conv2d.weight.data[:] -= lr * conv2d.weight.grad</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:<span class="comment">#偶数行输出</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l.<span class="built_in">sum</span>():<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>epoch 2, loss 6.422 epoch 4, loss 1.225 epoch 6, loss 0.266 epoch 8, loss 0.070 epoch 10, loss 0.022</p><p>在10次迭代之后，误差已经降到足够低。现在我们来看看我们所学的卷积核的权重张量。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv2d.weight.data.reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>tensor([[ 1.0010, -0.9739]])</p><p>发现我们学习到的卷积核权重非常接近我们之前定义的卷积核<code>K</code>。</p><h2 id="互相关和卷积">互相关和卷积</h2><blockquote><p>为了与深度学习文献中的标准术语保持一致，我们将继续把“互相关运算”称为卷积运算，尽管严格地说，它们略有不同。 此外，对于卷积核张量上的权重，我们称其为<em>元素</em>。</p></blockquote><h2 id="特征映射和感受野">特征映射和感受野</h2><p>如前文所述，输出卷积层有时被称为<strong>特征映射</strong>（feature map），因为它可以被视为一个输入映射到下一层的空间维度的转换器。 在卷积神经网络中，对于某一层的任意元素<span class="math inline">\(x\)</span>，其<strong>感受野</strong>（receptive field）是指在前向传播期间可能影响<span class="math inline">\(x\)</span>计算的所有元素（来自所有先前层）。</p><blockquote><p>注意，感受野可能大于输入的实际大小。</p><p>因此，当一个特征图中的任意元素需要检测更广区域的输入特征时，我们可以构建一个更深的网络。</p></blockquote><h2 id="图像卷积总结">图像卷积总结</h2><ul><li>二维卷积层的核心计算是二维互相关运算。最简单的形式是，对二维输入数据和卷积核执行互相关操作，然后添加一个偏置。</li><li>我们可以设计一个卷积核来检测图像的边缘。</li><li>我们可以从数据中学习卷积核的参数。</li><li>学习卷积核时，无论用严格卷积运算或互相关运算，卷积层的输出不会受太大影响。</li><li>当需要检测输入特征中更广区域时，我们可以构建一个更深的卷积网络。</li></ul><h1 id="填充padding-和步幅stride">填充padding 和步幅stride</h1><p>正如我们在第二节中所概括的那样，假设输入形状为<span class="math inline">\(n_h×n_w\)</span>，卷积核形状为<span class="math inline">\(k_h×k_w\)</span>，那么输出形状将是<span class="math inline">\((n_h-k_h+1)×(n_w-k_w+1)\)</span>。可见，<font color=#4eb434>卷积的输出形状取决于输入形状和卷积核的形状。</font></p><p>还有什么因素会影响输出的大小呢？本节我们将介绍<font color=#985fff><strong>填充</strong>（padding）</font>和<font color=#df8400><strong>步幅</strong>（stride）</font>。假设以下情景： <font color=#985fff>有时，在应用了连续的卷积之后，我们最终得到的输出远小于输入大小。这是由于卷积核的宽度和高度通常大于1所导致的。比如，一个240×240像素的图像，经过10层5×5的卷积后，将减少到200×200像素。如此一来，原始图像的边界丢失了许多有用信息。而<em>填充</em>是解决此问题最有效的方法</font>； <font color=#df8400>有时，我们可能希望大幅降低图像的宽度和高度。例如，如果我们发现原始的输入分辨率十分冗余。<em>步幅</em>则可以在这类情况下提供帮助。</font></p><h2 id="填充">填充</h2><p><strong>填充</strong>（padding）：在输入图像的边界填充元素（通常填充元素是0）。</p><p>例如，在下图中，我们将3×3输入填充到5×5，那么它的输出就增加为4×4。阴影部分是第一个输出元素以及用于输出计算的输入和核张量元素： 0×0+0×1+0×2+0×3=0。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/conv-pad.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">带填充的二维互相关</div></center><p>通常，如果我们添加<span class="math inline">\(p_h\)</span>行填充（大约一半在顶部，一半在底部）和<span class="math inline">\(p_w\)</span>列填充（大约左侧一半，右侧一半），则输出形状将为 <span class="math display">\[(n_h-k_h+1+p_h)\times(n_w-k_w+1+p_w)。\]</span> 这意味着输出的高度和宽度将分别增加<span class="math inline">\(p_h\)</span>和<span class="math inline">\(p_w\)</span>。</p><p><strong>在许多情况下，我们需要设置<span class="math inline">\(p_h=k_h-1\)</span>和<span class="math inline">\(p_w=k_w-1\)</span>，使输入和输出具有相同的高度和宽度。</strong> 这样可以在构建网络时更容易地预测每个图层的输出形状。假设<span class="math inline">\(k_h\)</span>是奇数，我们将在高度的两侧填充<span class="math inline">\(p_h/2\)</span>行。 如果<span class="math inline">\(k_h\)</span>是偶数，则一种可能性是在输入顶部填充<span class="math inline">\(\lceil p_h/2\rceil\)</span>行，在底部填充<span class="math inline">\(\lfloor p_h/2\rfloor\)</span>行。同理，我们填充宽度的两侧。</p><p><span style="background:#daf5e9;">卷积神经网络中卷积核的高度和宽度通常为奇数，例如1、3、5或7。 </span>选择奇数的好处是，保持空间维度的同时，我们可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。</p><p>此外，使用奇数的核大小和填充大小也提供了书写上的便利。对于任何二维张量<code>X</code>，当满足： 1. 卷积核的大小是奇数； 2. 所有边的填充行数和列数相同； 3. 输出与输入具有相同高度和宽度 则可以得出：输出<code>Y[i, j]</code>是通过以输入<code>X[i, j]</code>为中心，与卷积核进行互相关计算得到的。</p><p>在下面的例子中，我们创建一个高度和宽度为3的二维卷积层，并在所有侧边填充1个像素。给定高度和宽度为8的输入，则输出的高度和宽度也是8；<span style="background:#daf5e9;">当卷积核的高度和宽度不同时，我们可以填充不同的高度和宽度，使输出和输入具有相同的高度和宽度。</span>在如下示例中，我们使用高度为5，宽度为3的卷积核，高度和宽度两边的填充分别为2和1。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了方便起见，我们定义了一个计算卷积层的函数。</span></span><br><span class="line"><span class="comment"># 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span><br><span class="line">    <span class="comment"># 这里的（1，1）表示批量大小和通道数都是1</span></span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>) + X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="comment"># 省略前两个维度：批量大小和通道</span></span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([8, 8])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([8, 8])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="步幅">步幅</h2><p>在计算互相关时，卷积窗口从输入张量的左上角开始，向下、向右滑动。 在前面的例子中，我们默认每次滑动一个元素。 但是，有时候<span style="background:#daf5e9;">为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素</span>。</p><p>我们将每次滑动元素的数量称为<strong>步幅（stride）</strong>。到目前为止，我们只使用过高度或宽度为1的步幅，那么如何使用较大的步幅呢？下图是垂直步幅为3，水平步幅为2的二维互相关运算。 着色部分是输出元素以及用于输出计算的输入和内核张量元素：0×0+0×1+1×2+2×3=8、0×0+6×1+0×2+0×3=6。</p><p>可以看到，为了计算输出中第一列的第二个元素和第一行的第二个元素，卷积窗口分别向下滑动三行和向右滑动两列。但是，当卷积窗口继续向右滑动两列时，没有输出，因为输入元素无法填充窗口（除非我们添加另一列填充）。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/conv-stride.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">垂直步幅为3，水平步幅为2的二维互相关运算</div></center><p>通常，当垂直步幅为<span class="math inline">\(s_h\)</span>、水平步幅为<span class="math inline">\(s_w\)</span>时，输出形状为 <span class="math display">\[\lfloor(n_h-k_h+s_h+p_h)/s_h\rfloor \times \lfloor(n_w-k_w+s_w+p_w)/s_w\rfloor.\]</span> <strong>如果我们设置了<span class="math inline">\(p_h=k_h-1\)</span>和<span class="math inline">\(p_w=k_w-1\)</span>，则输出形状将简化为<span class="math inline">\(\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor\)</span>。 更进一步，如果输入的高度和宽度可以被垂直和水平步幅整除，则输出形状将为<span class="math inline">\((n_h/s_h) \times (n_w/s_w)\)</span>。</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们将高度和宽度的步幅设置为2，从而将输入的高度和宽度减半。</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([4, 4])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 看一个稍微复杂的例子</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), stride=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([2, 2])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>为了简洁起见，当输入高度和宽度两侧的填充数量分别为<span class="math inline">\(p_h\)</span>和<span class="math inline">\(p_w\)</span>时，我们称之为填充<span class="math inline">\((p_h, p_w)\)</span>。当<span class="math inline">\(p_h = p_w = p\)</span>时，填充是<span class="math inline">\(p\)</span>。同理，当高度和宽度上的步幅分别为<span class="math inline">\(s_h\)</span>和<span class="math inline">\(s_w\)</span>时，我们称之为步幅<span class="math inline">\((s_h, s_w)\)</span>。特别地，当<span class="math inline">\(s_h = s_w = s\)</span>时，我们称步幅为<span class="math inline">\(s\)</span>。默认情况下，填充为0，步幅为1。在实践中，我们很少使用不一致的步幅或填充，也就是说，我们通常有<span class="math inline">\(p_h = p_w\)</span>和<span class="math inline">\(s_h = s_w\)</span>。</p><h2 id="填充和步幅总结">填充和步幅总结</h2><ul><li>填充可以增加输出的高度和宽度。在许多情况下，我们需要设置<span class="math inline">\(p_h=k_h-1\)</span>和<span class="math inline">\(p_w=k_w-1\)</span>，用来使输出与输入具有相同的高和宽。</li><li>步幅可以减小输出的高和宽，例如在设置了填充的情况下，若输入的高度和宽度可以被步幅s整除，输出的高和宽仅为输入的高和宽的1/s（步幅s是一个大于1的整数）。</li><li>填充和步幅可用于有效地调整输出数据的维度。</li></ul><h1 id="多输入多输出通道">多输入多输出通道</h1><p>虽然我们在第一节中描述了构成每个图像的多个通道和多层卷积层。例如彩色图像具有标准的RGB通道来代表红、绿和蓝。 但是到目前为止，我们仅展示了单个输入和单个输出通道的简化例子。 这使得我们可以将输入、卷积核和输出看作二维张量。</p><p>当我们添加通道时，我们的输入和隐藏的表示都变成了三维张量。例如，每个RGB输入图像具有<span class="math inline">\(3×ℎ×w\)</span>的形状。我们将这个大小为3的轴称为<strong>通道（channel）</strong>维度。本节将更深入地研究具有多输入和多输出通道的卷积核。</p><h2 id="多输入通道">多输入通道</h2><p><span style="background:#daf5e9;">当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便与输入数据进行互相关运算。</span>若输入和卷积核都有<span class="math inline">\(c_i\)</span>个通道，我们可以对每个通道输入的二维张量和卷积核的二维张量进行互相关运算，再对通道求和（将<span class="math inline">\(c_i\)</span>的结果相加）得到二维张量。这是多通道输入和多输入通道卷积核之间进行二维互相关运算的结果。</p><p>在下图中，我们演示了一个具有两个输入通道的二维互相关运算的示例。阴影部分是第一个输出元素以及用于计算这个输出的输入和核张量元素：(1×1+2×2+4×3+5×4)+(0×0+1×1+3×2+4×3)=56。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/conv-multi-in.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">两个输入通道的互相关计算</div></center><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 再来看下具体实现，我们实现一下多输入通道互相关运算。 简而言之，我们所做的就是对每个通道执行互相关操作，然后将结果相加。</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 先遍历“X”和“K”的第0个维度（通道维度），再把它们加在一起</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(d2l.corr2d(x, k) <span class="keyword">for</span> x, k <span class="keyword">in</span> <span class="built_in">zip</span>(X, K))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们可以构造与上图中的值相对应的输入张量X和核张量K，以验证互相关运算的输出。</span></span><br><span class="line">X = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]],</span><br><span class="line">               [[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]]])</span><br><span class="line">K = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]], [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]]])</span><br><span class="line"></span><br><span class="line">corr2d_multi_in(X, K)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 56.,  72.],</span></span><br><span class="line"><span class="string">        [104., 120.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>zip()</strong> 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的zip对象： &gt;&gt;&gt; a = [1,2,3] &gt;&gt;&gt; b = [4,5,6] &gt;&gt;&gt; zipped = zip(a,b) # 返回一个对象 &gt;&gt;&gt; zipped &lt;zip object at 0x103abc288&gt; &gt;&gt;&gt; list(zipped) # list() 转换为列表 [(1, 4), (2, 5), (3, 6)]</p><p>对于zip(args)这个函数，Python还提供了一种逆操作： &gt;&gt;&gt;origin = zip(<em>result) #前面加</em>号，事实上*号也是一个特殊的运算符，叫解包运算符。</p></blockquote><h2 id="多输出通道">多输出通道</h2><p>到目前为止，不论有多少输入通道，我们还只有一个输出通道。然而，正如我们在第一节中所讨论的，每一层有多个输出通道是至关重要的。在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。直观地说，我们可以将每个通道看作对不同特征的响应。而现实可能更为复杂一些，因为<u>每个通道不是独立学习的，而是为了共同使用而优化的</u>。因此，<u>多输出通道并不仅是学习多个单通道的检测器</u>。</p><p>用<span class="math inline">\(c_i\)</span>和<span class="math inline">\(c_o\)</span>分别表示输入和输出通道的数目，并让<span class="math inline">\(k_h\)</span>和<span class="math inline">\(k_w\)</span>为卷积核的高度和宽度。为了获得多个通道的输出，我们可以<span style="background:#daf5e9;">为每个输出通道创建一个形状为<span class="math inline">\(c_i\times k_h\times k_w\)</span>的卷积核张量</span>，这样卷积核的形状是<span class="math inline">\(c_o\times c_i\times k_h\times k_w\)</span>。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如下，我们实现一个计算多个通道的输出的互相关函数。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。</span></span><br><span class="line">    <span class="comment"># 最后将所有结果都叠加在一起</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过将核张量K与K+1（K中每个元素加1）和K+2连接起来，构造了一个具有3个输出通道的卷积核。</span></span><br><span class="line">K = torch.stack((K, K + <span class="number">1</span>, K + <span class="number">2</span>), <span class="number">0</span>)</span><br><span class="line">K.shape</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([3, 2, 2, 2])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面，我们对输入张量X与卷积核张量K执行互相关运算。现在的输出包含3个通道，第一个通道的结果与先前输入张量X和多输入单输出通道的结果一致。</span></span><br><span class="line">corr2d_multi_in_out(X, K)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[ 56.,  72.],</span></span><br><span class="line"><span class="string">         [104., 120.]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 76., 100.],</span></span><br><span class="line"><span class="string">         [148., 172.]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 96., 128.],</span></span><br><span class="line"><span class="string">         [192., 224.]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>输入张量X与具有3个输出通道的卷积核K执行互相关运算，得到输出的过程如下图：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nnn.jpg" alt="nnn" /><figcaption aria-hidden="true">nnn</figcaption></figure></blockquote><h2 id="卷积层-1">1×1卷积层</h2><p>1×1卷积，即<span class="math inline">\(k_h = k_w = 1\)</span>，看起来似乎没有多大意义。 毕竟，<span style="background:#daf5e9;">卷积的本质是有效提取相邻像素间的相关特征</span>，而1×1卷积显然没有此作用。 尽管如此，1×1仍然十分流行，经常包含在复杂深层网络的设计中。下面，让我们详细地解读一下它的实际作用。</p><p>因为使用了最小窗口，1×1卷积失去了卷积层的特有能力——在高度和宽度维度上，识别相邻元素间相互作用的能力。 其实1×1卷积的唯一计算发生在<strong>通道</strong>上。</p><p>下图展示了使用1×1卷积核与3个输入通道和2个输出通道的互相关计算。 这里输入和输出具有相同的高度和宽度，输出中的每个元素都是从输入图像中同一位置的元素的线性组合。 我们可以将1×1卷积层看作在每个像素位置应用的全连接层，以<span class="math inline">\(c_i\)</span>个输入值转换为<span class="math inline">\(c_o\)</span>个输出值。 因为这仍然是一个卷积层，所以跨像素的权重是一致的。 同时，1×1卷积层需要的权重维度为<span class="math inline">\(c_o\times c_i\)</span>，再额外加上一个偏置。</p><blockquote><p>这里的“权重”理解为“卷积核”</p></blockquote><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/conv-1x1.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">互相关计算使用了具有3个输入通道和2个输出通道的1×1卷积核。其中，输入和输出具有相同的高度3和宽度3</div></center><p>下面，我们使用全连接层实现1×1卷积。 请注意，我们需要对输入和输出的数据形状进行调整。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out_1x1</span>(<span class="params">X, K</span>):</span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]</span><br><span class="line">    X = X.reshape((c_i, h * w))</span><br><span class="line">    K = K.reshape((c_o, c_i))</span><br><span class="line">    <span class="comment"># 全连接层中的矩阵乘法</span></span><br><span class="line">    Y = torch.matmul(K, X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape((c_o, h, w))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当执行1×1卷积运算时，上述函数相当于先前实现的互相关函数corr2d_multi_in_out。让我们用一些样本数据来验证这一点。</span></span><br><span class="line">X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">K = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">Y1 = corr2d_multi_in_out_1x1(X, K)</span><br><span class="line">Y2 = corr2d_multi_in_out(X, K)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">float</span>(torch.<span class="built_in">abs</span>(Y1 - Y2).<span class="built_in">sum</span>()) &lt; <span class="number">1e-6</span></span><br></pre></td></tr></table></figure><h2 id="多输入多输出通道总结">多输入多输出通道总结</h2><ul><li>多输入多输出通道可以用来扩展卷积层的模型。</li><li>当以每像素为基础应用时，1×1卷积层相当于全连接层。</li><li>1×1卷积层通常用于调整网络层的通道数量和控制模型复杂性。</li></ul><h1 id="池化层-pooling">池化层 pooling</h1><p>本节将介绍<em>池化</em>（pooling）层，它具有双重目的：<strong>1.降低对空间降采样表示的敏感性</strong>，<font color=#ef042a><strong>2.降低卷积层对位置的敏感性</strong></font>。</p><ol type="1"><li><p>我们的机器学习任务通常会跟<u>全局图像的问题</u>有关（例如，“图像是否包含一只猫呢？”），所以我们<u>最后一层的神经元应该对整个输入的全局敏感</u>。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。因此，当我们处理图像时，我们希望<u>逐渐降低隐藏表示的空间分辨率、聚集信息</u>，这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野（输入）就越大。</p></li><li><p>此外，当检测较底层的特征时（例如第二节中所讨论的边缘），我们通常<u>希望这些特征保持某种程度上的平移不变性</u>。例如，如果我们拍摄黑白之间轮廓清晰的图像<code>X</code>，并将整个图像向右移动一个像素，即<code>Z[i, j] = X[i, j + 1]</code>，则新图像<code>Z</code>的输出可能大不相同。而<u>在现实中，随着拍摄角度的移动，任何物体几乎不可能发生在同一像素上</u>。即使用三脚架拍摄一个静止的物体，由于快门的移动而引起的相机振动，可能会使所有物体左右移动一个像素（除了高端相机配备了特殊功能来解决这个问题）。</p></li></ol><h2 id="最大池化层和平均池化层">最大池化层和平均池化层</h2><p>与卷积层类似，池化层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口（有时称为<strong>池化窗口</strong>）遍历的每个位置计算一个输出。 然而，<span style="background:#daf5e9;">不同于卷积层中的输入与卷积核之间的互相关计算，池化层不包含参数</span>。 相反，池运算是确定性的，我们通常计算池化窗口中所有元素的最大值或平均值。这些操作分别称为<strong>最大池化层</strong>（maximum pooling）和<strong>平均池化层</strong>（average pooling）。</p><p>在这两种情况下，与互相关运算符一样，池化窗口从输入张量的左上角开始，从左往右、从上往下的在输入张量内滑动。在池化窗口到达的每个位置，它计算该窗口中输入子张量的最大值或平均值。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/pooling.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">池化窗口形状为2×2的最大池化层。着色部分是第一个输出元素，以及用于计算这个输出的输入元素:max(0,1,3,4)=4.</div></center><p>池化窗口形状为<span class="math inline">\(p \times q\)</span>的池化层称为<u><span class="math inline">\(p \times q\)</span>池化层</u>，池化操作称为<u><span class="math inline">\(p \times q\)</span>池化</u>。</p><p><span style="background:#daf5e9;">回到本节开头提到的对象边缘检测示例，现在我们将使用卷积层的输出作为2×2最大池化的输入。 设置卷积层输入为<code>X</code>，池化层输出为<code>Y</code>。 无论<code>X[i, j]</code>和<code>X[i, j + 1]</code>的值相同与否，池化层始终输出<code>Y[i, j] = 1</code>。 也就是说，<u>使用2×2最大池化层，即使在高度或宽度上移动一个元素，卷积层仍然可以识别到模式</u>。</span></p><p>在下面的代码中的<code>pool2d</code>函数，我们实现池化层的前向传播。 这类似于第二节中的<code>corr2d</code>函数。 然而，这里我们没有卷积核中的类似参数，输出为输入中每个区域的最大值或平均值。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们可以构建上图中的输入张量X，验证二维最大池化层的输出。</span></span><br><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[4., 5.],</span></span><br><span class="line"><span class="string">        [7., 8.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们还可以验证平均池化层。</span></span><br><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>), <span class="string">&#x27;avg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[2., 3.],</span></span><br><span class="line"><span class="string">        [5., 6.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="填充和步幅">填充和步幅</h2><p>与卷积层一样，池化层也可以改变输出形状。和以前一样，我们可以通过填充和步幅以获得所需的输出形状。 下面，我们用深度学习框架中内置的二维最大池化层，来演示池化层中填充和步幅的使用。 我们首先构造了一个输入张量<code>X</code>，它有四个维度，其中样本数和通道数都是1。</p><blockquote><p>注意！默认情况下，深度学习框架中的步幅与池化窗口的大小相同</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">X</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 0.,  1.,  2.,  3.],</span></span><br><span class="line"><span class="string">          [ 4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="string">          [ 8.,  9., 10., 11.],</span></span><br><span class="line"><span class="string">          [12., 13., 14., 15.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认情况下，深度学习框架中的步幅与池化窗口的大小相同。 因此，如果我们使用形状为(3, 3)的池化窗口，那么默认情况下，我们得到的步幅形状为(3, 3)。</span></span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[10.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 填充和步幅可以手动设定。</span></span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 5.,  7.],</span></span><br><span class="line"><span class="string">          [13., 15.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当然，我们可以设定一个任意大小的矩形池化窗口，并分别设定填充和步幅的高度和宽度。</span></span><br><span class="line">pool2d = nn.MaxPool2d((<span class="number">2</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 5.,  7.],</span></span><br><span class="line"><span class="string">          [13., 15.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="多个通道">多个通道</h2><p>在处理多通道输入数据时，池化层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总。 这意味着<span style="background:#daf5e9;">池化层的输出通道数与输入通道数相同</span>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们在通道维度上连结张量X和X + 1，以构建具有2个通道的输入。</span></span><br><span class="line">X = torch.cat((X, X + <span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">X</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 0.,  1.,  2.,  3.],</span></span><br><span class="line"><span class="string">          [ 4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="string">          [ 8.,  9., 10., 11.],</span></span><br><span class="line"><span class="string">          [12., 13., 14., 15.]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[ 1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">          [ 5.,  6.,  7.,  8.],</span></span><br><span class="line"><span class="string">          [ 9., 10., 11., 12.],</span></span><br><span class="line"><span class="string">          [13., 14., 15., 16.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如下，池化后输出通道的数量仍然是2。</span></span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[[ 5.,  7.],</span></span><br><span class="line"><span class="string">          [13., 15.]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[ 6.,  8.],</span></span><br><span class="line"><span class="string">          [14., 16.]]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="池化层总结">池化层总结</h2><ul><li>对于给定输入元素，最大池化层会输出该窗口内的最大值，平均池化层会输出该窗口内的平均值。</li><li>池化层的主要优点之一是<strong>减轻卷积层对位置的过度敏感</strong>。</li><li>使用最大池化层以及大于1的步幅，可减少空间维度（如高度和宽度）。</li><li>我们可以指定池化层的填充和步幅。</li><li>池化层的输出通道数与输入通道数相同。</li></ul><h1 id="卷积神经网络-lenet">卷积神经网络 LeNet</h1><p>本节将介绍LeNet，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。 这个模型是由AT&amp;T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），目的是识别图像中的手写数字。 当时，Yann LeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果。</p><p>当时，LeNet取得了与支持向量机（support vector machines）性能相媲美的成果，成为监督学习的主流方法。 LeNet被广泛用于自动取款机（ATM）机中，帮助识别处理支票的数字。 时至今日，一些自动取款机仍在运行Yann LeCun和他的同事Leon Bottou在上世纪90年代写的代码呢！</p><h2 id="lenet">LeNet</h2><p>总体来看，LeNet（LeNet-5）由两个部分组成：</p><ul><li>卷积编码器：由两个卷积层组成;</li><li>全连接层密集块：由三个全连接层组成。</li></ul><p>该架构如下所示：</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/lenet.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">LeNet中的数据流。输入是手写数字，输出为10种可能结果的概率</div></center><p>每个卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均池化层。请注意，虽然ReLU和最大池化层更有效，但它们在20世纪90年代还没有出现。每个卷积层使用5×5卷积核和一个sigmoid激活函数。这些层将输入映射到多个二维特征输出，通常同时增加通道的数量。第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个2×2池操作（步幅2）通过空间下采样将维数即特征图大小减少4倍。卷积的输出形状由批量大小、通道数、高度、宽度决定。</p><p>为了将卷积块的输出传递给全连接层密集块即稠密块，我们必须在小批量中展平每个样本。<span style="background:#daf5e9;">换言之，我们将这个四维输入转换成全连接层所期望的二维输入。</span>这里的二维表示的<span style="background:#FFCC99;">第一个维度索引小批量中的样本，第二个维度给出每个样本的平面向量表示</span>。LeNet的稠密块有三个全连接层，分别有120、84和10个输出。因为我们在执行分类任务，所以输出层的10维对应于最后输出结果的数量。</p><p>通过下面的LeNet代码，可以看出用深度学习框架实现此类模型非常简单。我们只需要实例化一个<code>Sequential</code>块并将需要的层连接在一起。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>我们对原始模型做了一点小改动，去掉了最后一层的高斯激活。除此之外，这个网络与最初的LeNet-5一致。</p><p>下面，我们将一个大小为28×28的单通道（黑白）图像通过LeNet。通过在每一层打印输出的形状，我们可以检查模型，以确保其操作与我们期望的数据流图一致。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://zh-v2.d2l.ai/_images/lenet-vert.svg"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">LeNet 的简化版</div></center><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape: \t&#x27;</span>,X.shape)</span><br><span class="line">    </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Conv2d output shape:         torch.Size([1, 6, 28, 28])</span></span><br><span class="line"><span class="string">Sigmoid output shape:        torch.Size([1, 6, 28, 28])</span></span><br><span class="line"><span class="string">AvgPool2d output shape:      torch.Size([1, 6, 14, 14])</span></span><br><span class="line"><span class="string">Conv2d output shape:         torch.Size([1, 16, 10, 10])</span></span><br><span class="line"><span class="string">Sigmoid output shape:        torch.Size([1, 16, 10, 10])</span></span><br><span class="line"><span class="string">AvgPool2d output shape:      torch.Size([1, 16, 5, 5])</span></span><br><span class="line"><span class="string">Flatten output shape:        torch.Size([1, 400])</span></span><br><span class="line"><span class="string">Linear output shape:         torch.Size([1, 120])</span></span><br><span class="line"><span class="string">Sigmoid output shape:        torch.Size([1, 120])</span></span><br><span class="line"><span class="string">Linear output shape:         torch.Size([1, 84])</span></span><br><span class="line"><span class="string">Sigmoid output shape:        torch.Size([1, 84])</span></span><br><span class="line"><span class="string">Linear output shape:         torch.Size([1, 10])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>注意，在整个卷积块中，与上一层相比，每一层特征的高度和宽度都减小了。 第一个卷积层使用2个像素的填充（上下/左右两边就是4个像素），来补偿5×5卷积核导致的特征减少。 相反，第二个卷积层没有填充，因此高度和宽度都减少了4个像素。 随着层叠的上升，通道的数量从输入时的1个，增加到第一个卷积层之后的6个，再到第二个卷积层之后的16个。 同时，每个池化层的高度和宽度都减半。最后，每个全连接层减少维数，最终输出一个维数与结果分类数相匹配的输出。</p><h2 id="模型训练">模型训练</h2><p>现在我们已经实现了LeNet，让我们看看LeNet在Fashion-MNIST数据集上的表现。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br></pre></td></tr></table></figure><p>虽然卷积神经网络的参数较少，但与深度的多层感知机相比，它们的计算成本仍然很高，因为每个参数都参与更多的乘法。 通过使用GPU，可以用它加快训练。</p><p>为了进行评估，我们需要对之前描述的<code>evaluate_accuracy</code>函数进行轻微的修改。 由于完整的数据集位于内存中，因此在模型使用GPU计算数据集之前，我们需要将其复制到显存中。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 设置为评估模式</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    <span class="comment"># 正确预测的数量，总预测的数量</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">                <span class="comment"># BERT微调所需的（之后将介绍）</span></span><br><span class="line">                X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                X = X.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            metric.add(d2l.accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>为了使用GPU，我们还需要一点小改动。 与之前定义的<code>train_epoch_ch3</code>不同，在进行正向和反向传播之前，我们需要将每一小批量数据移动到我们指定的设备（例如GPU）上。</p><p>如下所示，训练函数<code>train_ch6</code>也类似于之前定义的<code>train_ch3</code>。 由于我们将实现多层神经网络，因此我们将主要使用高级API。 以下训练函数假定从高级API创建的模型作为输入，并进行相应的优化。 我们使用在之前介绍的Xavier随机初始化模型参数。 与全连接层一样，我们使用交叉熵损失函数和小批量随机梯度下降。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch6</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用GPU训练模型(在第六章定义)&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)<span class="comment">#小批量随机梯度下降</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()<span class="comment">#交叉熵损失函数</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                            legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># 训练损失之和，训练准确率之和，样本数</span></span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()</span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>现在，我们训练和评估LeNet-5模型。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">loss 0.469, train acc 0.823, test acc 0.779</span></span><br><span class="line"><span class="string">55296.6 examples/sec on cuda:0</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><figure><img src="https://zh-v2.d2l.ai/_images/output_lenet_4a2e9e_67_1.svg" alt="zh-v2.d2l.ai/_images/output_lenet_4a2e9e_67_1.svg" /><figcaption aria-hidden="true">zh-v2.d2l.ai/_images/output_lenet_4a2e9e_67_1.svg</figcaption></figure><h2 id="卷积神经网络-lenet总结">卷积神经网络 LeNet总结</h2><ul><li>卷积神经网络（CNN）是一类使用卷积层的网络。</li><li>在卷积神经网络中，我们组合使用卷积层、非线性激活函数和池化层。</li><li>为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。</li><li>在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。</li><li>LeNet是最早发布的卷积神经网络之一。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CBAM注意力机制</title>
      <link href="/2024/01/23/CBAM%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
      <url>/2024/01/23/CBAM%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文：《CBAM：Convolutional Block Attention Module》</p><p>论文参考样式：Woo S, Park J, Lee J Y, et al. Cbam: Convolutional block attention module[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 3-19.</p><p>论文链接：<a href="https://arxiv.org/pdf/1807.06521.pdf">CBAM：Convolutional Block Attention Module</a></p><p>demo: <a href="https://github.com/Jongchan/attention-module">GitHub - Jongchan/attention-module: Official PyTorch code for "BAM: Bottleneck Attention Module (BMVC2018)" and "CBAM: Convolutional Block Attention Module (ECCV2018)"</a></p></blockquote><blockquote><p>参考网址：</p><p><a href="https://zhuanlan.zhihu.com/p/101590167">CBAM：卷积注意力机制模块 - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/Roaddd/article/details/114646354">【注意力机制】CBAM详解（文末附代码）_cbam注意力-CSDN博客</a></p><p><a href="https://blog.csdn.net/m0_45447650/article/details/123983483">CBAM——即插即用的注意力模块（附代码）_cbam模块-CSDN博客</a></p></blockquote><h1 id="摘要">摘要</h1><p>本文（2018，ECCV）提出了卷积注意力模块——CBAM，这是一种用于前馈卷积神经网络的轻量级的注意力模块。 给定一个中间特征图，CBAM模块会沿着两个独立的维度（通道和空间）依次推断注意力图，然后将注意力图与输入特征图相乘以进行自适应特征优化。 由于CBAM是轻量级的通用模块，因此可以忽略的该模块的开销而将其无缝集成到任何CNN架构中，并且可以与基础CNN一起进行端到端训练。</p><p>论文在 ResNet 和 MobileNet 等经典结构上添加了 CBAM 模块并进行对比分析实验，同时也进行了CAM可视化，发现 CBAM 更关注识别目标物体，这也使得 CBAM 具有更好的解释性。本文验证所用的数据集有 ImageNet-1K，MS COCO检测和VOC 2007检测数据集。 实验表明，使用该模块在各种模型上，并在分类和检测性能方面的持续改进，证明了CBAM的广泛适用性。</p><blockquote><p>关于CAM可视化：<a href="https://aistudio.baidu.com/projectdetail/1655497">一文搞懂卷积网络之五（注意力可视化Grad-CAM） - 飞桨AI Studio星河社区 (baidu.com)</a></p></blockquote><h1 id="模型">模型</h1><p>CBAM模型结构如下所示：</p><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240109153927099.png" alt="image-20240109153927099" /><figcaption aria-hidden="true">image-20240109153927099</figcaption></figure><p>Convolutional Block Attention Module (CBAM) 表示卷积模块的注意力机制模块，可以看到 CBAM 包含2个独立的子模块：<font color=#4eb434>通道注意力模块（CAM，Channel Attention Module)</font> 和<font color=#985fff>空间注意力模块（SAM，Spartial Attention Module)</font> ，分别进行通道和空间的Attention。这样不只能够节约参数和计算力，并且保证了其能够做为即插即用的模块集成到现有的网络架构中去。相比于SENet 只关注<font color=#4eb434>通道（channel）</font>的注意力机制可以取得更好的效果。</p><blockquote><p>通道上的 Attention 机制在 2017 年的 SENet 就被提出，SENet可以参考<a href="https://blog.csdn.net/Roaddd/article/details/111357490">这篇文章</a>。事实上，CAM 与 SENet 相比，只是多了一个并行的 Max Pooling 层，提取到的高层特征更全面，更丰富。至于为何如此更改，论文也给出了解释和实验数据支持。</p></blockquote><p>由上图，CBAM由四个模块组成，分别是：输入特征、通道注意力模块、空间注意力模块和输出的精制特征图。整体流程大致描述如下</p><ol type="1"><li>输入特征<span class="math inline">\(F\in R^{C\times H\times W}\)</span>会先通过一个通道注意力模块，进行一维卷积<span class="math inline">\(M_c\in R^{C\times 1\times 1}\)</span>，将卷积结果乘原特征图<span class="math inline">\(F\)</span>得到加权结果<span class="math inline">\(F&#39;\)</span>。</li><li><span class="math inline">\(F&#39;\)</span>会再经过一个空间注意力模块，进行二维卷积<span class="math inline">\(M_s\in R^{1\times H\times W}\)</span>，再次将卷积结果与<span class="math inline">\(F&#39;\)</span>相乘，最终得到加权结果<span class="math inline">\(F&#39;&#39;\)</span>。</li></ol><p>用公式表示为 <span class="math display">\[F&#39;=M_c\left( F \right) \otimes F,\]</span> <span class="math display">\[F&#39;&#39;=M_s\left( F&#39; \right) \otimes F&#39;,\]</span></p><h2 id="channel-attention-modulecam">Channel Attention Module（CAM）</h2><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240109155441087.png" alt="image-20240109155441087" /><figcaption aria-hidden="true">image-20240109155441087</figcaption></figure><blockquote><p><strong>通道注意力模块</strong>：<strong>通道维度不变，压缩空间维度</strong>。该模块关注输入图片中<strong>有意义的信息</strong>(分类任务就关注因为什么分成了不同类别)。</p></blockquote><p>通道注意力模块CAM如上图所示。</p><ol type="1"><li>将输入的特征图feature map <span style="background:#dad5e9;">F</span>，分别经过并行的基于width和height的global max pooling 和global average pooling，将特征图从C×H×W变为C×1×1的大小，然后分别经过MLP。</li><li>在MLP中，它先将通道数压缩为原来的1/r（Reduction，减少率）倍，再扩张到原通道数，经过ReLU激活函数得到两个激活后的结果。</li><li>将MLP输出的两个结果进行基于element-wise的加和操作（即逐元素相加），再经过sigmoid激活操作，生成最终的channel attention feature-map <span style="background:#dad5e9;">M_c</span>。</li><li>将该channel attention feature-map和原来的input feature-map做element-wise乘法操作（乘完后变回C×H×W的大小），生成Spatial attention模块需要的输入特征 <span style="background:#dad5e9;">F'</span>。</li></ol><p>以上是通道注意力机制的步骤。</p><p>换一个角度考虑，通道注意力机制（Channel Attention Module）是将特征图在空间维度上进行压缩，得到一个一维矢量后再进行操作。在空间维度上进行压缩时，不仅考虑到了平均值池化（Average Pooling）还考虑了最大值池化（Max Pooling）。平均池化和最大池化可用来聚合特征映射的空间信息，送到一个共享网络，压缩输入特征图的空间维数，逐元素求和合并，以产生通道注意力图。单就一张图来说，通道注意力，关注的是这张图上哪些内容是有重要作用的。<u>平均值池化对特征图上的每一个像素点都有反馈，而最大值池化在进行梯度反向传播计算时，只有特征图中响应最大的地方有梯度的反馈。</u>通道注意力机制可以表达为： <span class="math display">\[\begin{aligned}    M_c\left( F \right) &amp;=\sigma \left( MLP\left( AvgPool\left( F \right) \right) +MLP\left( MaxPool\left( F \right) \right) \right)\\    &amp;=\sigma \left( W_1\left( W_0\left( F_{avg}^{c} \right) \right) +W_1\left( W_0\left( F_{\max}^{c} \right) \right) \right)\\\end{aligned}\]</span></p><blockquote><p>在channel attention中，表1对于pooling的使用进行了实验对比，发现avg &amp; max的并行池化的效果要更好。这里也有可能是池化丢失的信息太多，avg&amp;max的并行连接方式比单一的池化丢失的信息更少，所以效果会更好一点。</p></blockquote><h2 id="spatial-attention-modulesam">Spatial Attention Module（SAM）</h2><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240109163244599.png" alt="image-20240109163244599" /><figcaption aria-hidden="true">image-20240109163244599</figcaption></figure><blockquote><p><strong>空间注意力模块</strong>：<strong>空间维度不变，压缩通道维度</strong>。该模块关注的是<strong>目标的位置信息</strong>。</p></blockquote><p>空间注意力模块如上图所示。将Channel attention模块输出的特征图 <span style="background:#dad5e9;">F‘</span> 作为本模块的输入特征图。首先做一个基于channel的global max pooling 和global average pooling，得到两个1×H×W 的特征图，然后将这2个特征图基于channel 做concat操作（通道拼接）。然后经过一个7×7卷积操作（7×7比3×3效果要好），降维为1个channel的特征图，即1×H×W。再经过sigmoid生成spatial attention feature <span style="background:#dad5e9;">M_s</span>。最后将该feature和该模块的输入feature做乘法（乘之后变回C×H×W的大小），得到最终生成的特征 <span style="background:#dad5e9;">F’‘</span>。</p><p>同样，空间注意力机制（Spatial Attention Module）是对通道进行压缩，在通道维度分别进行了平均值池化和最大值池化。MaxPool的操作就是在通道上提取最大值，提取的次数是高乘以宽；AvgPool的操作就是在通道上提取平均值，提取的次数也是是高乘以宽；接着将前面所提取到的特征图（通道数都为1）合并得到一个2通道的特征图。空间注意力机制可以表达为： <span class="math display">\[\begin{aligned}    M_s\left( F \right) &amp;=\sigma \left( f^{7\times 7}\left( \left[ AvgPool\left( F \right) ;MaxPool\left( F \right) \right] \right) \right)\\    &amp;=\sigma \left( f^{7\times 7}\left( \left[ F_{avg}^{s};F_{\max}^{s} \right] \right) \right)\\\end{aligned}\]</span> 其中，σ 为sigmoid操作，7×7表示卷积核的大小，7×7的卷积核比3×3的卷积核效果更好。</p><h1 id="实验">实验</h1><p>本文中，进行了较多的对比实验，旨在验证注意力模块的积极作用。</p><p>首先，对比了通道、空间以及通道&amp;空间，不同注意力机制的效果。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240109164403936.png"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">仅使用Channel Attention，对比的是使用AvgPool、MaxPool以及都使用时的性能</div></center><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240109170246533.png"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">仅使用Spatial Attention，对比的是不同Avg，Max以及kernel_size的性能差异</div></center><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240109170551461.png"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">CBAM模块，对比的是不同顺序的性能差异</div></center><p>关于图3，通道注意力和空间注意力这两个模块能够以并行或者串行顺序的方式组合在一块儿，关于通道和空间上的串行顺序和并行作者进行了实验对比，可以看到的是，先使用Channel（AvgPool&amp;MaxPool），再使用Spatial（avg&amp;max，k=7）——即先通道后空间的性能是最优的。</p><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240109170744084.png" alt="image-20240109170744084" /><figcaption aria-hidden="true">image-20240109170744084</figcaption></figure><p>上图给出了ImageNet-1K数据集上，训练误差的曲线。同样的证明了，CBAM模块在训练集合验证集上，相比于Baseline和SE注意力机制，都有一定的提升。</p><p>最后，是使用Grad-CAM进行了可视化，以来证明CBAM是真正地提取出了积极有效的特征：利用 Grad-CAM 对不一样的网络进行可视化后，能够发现，引入 CBAM 后，特征覆盖到了待识别物体的更多部位，而且最终判别物体的几率也更高，这代表注意力机制的确让网络学会了关注重点信息。</p><center><img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240109171005799.png"> <br><div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">CBAM，SE，Baseline等最后一层卷积层输出使用 grad-CAM 进行可视化的对比图</div></center><h1 id="总结">总结</h1><ol type="1"><li><p>论文提出了一种基于注意力机制的轻量型结构 CBAM ，基本上可以添加到所有常规的卷积层中。</p></li><li><p>文中验证了 Channel Attention Module 中 avg 与 max 并行的方式最好，接下来通过实验验证了 Channel Attention Module 和 Spatial Attention Module 的最佳先后顺序是先通道后空间，还对比了 CBAM 与 SENet 的性能。</p></li><li><p>文章还在实验中应用grad-CAM可视化了 CBAM 的关注区域（在图像分类任务中可以观察feature map的特征，解释了为什么模型将原图分类到某一类的结果），使得 CBAM 具有更好的解释性。</p></li><li><p>加入CBAM模块不一定会给网络带来性能上的提升，受自身网络还有数据等其他因素影响，甚至会下降。如果网络模型的泛化能力已经很强，而你的数据集不是benchmarks而是自己采集的数据集的话，不建议加入CBAM模块。CBAM性能虽然改进的比SE高了不少，但绝不是无脑加入到网络里就能有提升的。也要根据自己的数据、网络等因素综合考量。</p></li></ol><h1 id="代码">代码</h1>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初步认识 torch.nn</title>
      <link href="/2024/01/23/%E8%AE%A4%E8%AF%86%20torch.nn/"/>
      <url>/2024/01/23/%E8%AE%A4%E8%AF%86%20torch.nn/</url>
      
        <content type="html"><![CDATA[<blockquote><p>搬运链接：[<a href="https://blog.csdn.net/HiWangWenBing/article/details/120614234">Pytorch系列-30]：神经网络基础 - torch.nn库五大基本功能：nn.Parameter、nn.Linear、nn.functioinal、nn.Module、nn.Sequentia-CSDN博客</a></p><p>官方链接1：<a href="https://pytorch.org/docs/1.2.0/">PyTorch documentation — PyTorch master documentation</a></p><p>官方链接2：<a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#torchnn">torch.nn - PyTorch中文文档 (pytorch-cn.readthedocs.io)</a></p></blockquote><blockquote><p>nn是Neural Network的简称，帮助程序员方便执行如下的与神经网络相关的行为：创建、训练、保存和恢复神经网络。</p></blockquote><h1 id="一torch.nn简介">一、torch.nn简介</h1><h2 id="相关库的导入">相关库的导入</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#环境准备</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np              <span class="comment"># numpy数组库</span></span><br><span class="line"><span class="keyword">import</span> math                     <span class="comment"># 数学运算库</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 画图库</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> torch             <span class="comment"># torch基础库</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn    <span class="comment"># torch神经网络库</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><h1 id="二nn.linear类全连接层">二、nn.Linear类（全连接层）</h1><blockquote><p>nn.Linear本身并不包含激活函数（激活函数在Functional中)</p></blockquote><h2 id="函数说明">函数说明</h2><figure><img src="https://img-blog.csdnimg.cn/20191102164419608.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyMDc5Njg5,size_16,color_FFFFFF,t_70#pic_center" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><ul><li><strong>in_features</strong>: 输入的二维张量的大小。Y = WX + b, X的维度就是in_features，X的维度决定的W的维度， 总的参数个数 = in_features + 1。</li><li><strong>out_features：</strong>输出的二维张量的大小。</li></ul><p>从输入输出的张量的shape角度来理解，相当于一个输入为[batch_size, in_features]的张量变换成了[batch_size, out_features]的输出张量。</p><h2 id="使用nn.linear类创建全连接层">使用nn.Linear类创建全连接层</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nn.Linear</span></span><br><span class="line"><span class="comment"># 建立单层的多输入、多输出全连接层</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># in_features由输入张量的形状决定，out_features则决定了输出张量的形状</span></span><br><span class="line">full_connect_layer = nn.Linear(in_features=<span class="number">64</span> * <span class="number">64</span> * <span class="number">3</span>, out_features=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;full_connect_layer:&quot;</span>, full_connect_layer)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;parameters        :&quot;</span>, full_connect_layer.parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假定输入的图像形状为[64,64,3]</span></span><br><span class="line">x_input = torch.randn(<span class="number">1</span>, <span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>) <span class="comment">#或 x_input = x_input.view(1, -1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将四维张量转换为二维张量之后，才能作为全连接层的输入</span></span><br><span class="line">x_input = x_input.view(<span class="number">1</span>, <span class="number">64</span> * <span class="number">64</span> * <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input.shape:&quot;</span>, x_input.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用全连接层</span></span><br><span class="line">y_output = full_connect_layer(x_input)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_output.shape:&quot;</span>, y_output.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_output:&quot;</span>, y_output)</span><br></pre></td></tr></table></figure><p>full_connect_layer: Linear(in_features=12288, out_features=2, bias=True) parameters : &lt;bound method Module.parameters of Linear(in_features=12288, out_features=2, bias=True)&gt; x_input.shape: torch.Size([1, 12288]) y_output.shape: torch.Size([1, 2]) y_output: tensor([[-0.5883, -0.4527]], grad_fn=<AddmmBackward>)</p><h1 id="三nn.functional常见函数">三、nn.functional（常见函数）</h1><h2 id="nn.functional-概述">nn.functional 概述</h2><p>nn.functional（通常按惯例导入到 F 命名空间中）定义了创建神经网络所需要的一些常见的处理函数。如<strong>激活函数</strong>、<strong>损失函数</strong>、<strong>正则化函数</strong>和<strong>非状态（non-stateful）版本的层（如卷积层和线性层）</strong>等。</p><figure><img src="https://img-blog.csdnimg.cn/20211005202351328.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_14,color_FFFFFF,t_70,g_se,x_16" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h1 id="四nn.xxx和nn.functional.xxx比较">四、nn.Xxx和nn.functional.xxx比较</h1><h2 id="相同点">相同点</h2><p><code>nn.Xxx</code>和<code>nn.functional.xxx</code>的实际功能是相同的，即<code>nn.Conv2d</code>和<code>nn.functional.conv2d</code> 都是进行卷积，<code>nn.Dropout</code> 和<code>nn.functional.dropout</code>都是进行dropout，。。。。。；</p><ul><li>运行效率也是近乎相同。</li></ul><h2 id="不同点">不同点</h2><ul><li>形式看：<code>nn.functional.xxx</code>是<strong>小写</strong>字母开头，nn.Xxx中的函数是<strong>大写</strong>字母开头。</li><li><code>nn.functional.xxx</code>是API函数接口，而<code>nn.Xxx</code>是对原始API函数<code>nn.functional.xxx</code>的<strong>类封装</strong>。</li><li>所有<strong><code>nn.Xxx</code>都继承于于共同祖先<code>nn.Module</code>。</strong>这一点导致<code>nn.Xxx</code>除了具有<code>nn.functional.xxx</code>功能之外，内部附带了<code>nn.Module</code>相关的属性和方法，例如<code>train(), eval(),load_state_dict, state_dict</code>等。</li><li><code>nn.Xxx</code>继承于<code>nn.Module</code>， 能够很好的与<code>nn.Sequential</code>结合使用， 而<code>nn.functional.xxx</code>无法与<code>nn.Sequential</code>结合使用。</li><li><code>nn.Xxx</code> 需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据。<code>nn.functional.xxx</code>同时传入输入数据和weight, bias等其他参数 。</li><li><code>nn.Xxx</code>不需要你自己定义和管理weight；而<code>nn.functional.xxx</code>需要你自己定义weight，每次调用的时候都需要手动传入weight, 不利于代码复用。</li></ul><h1 id="五nn.parameter类">五、nn.Parameter类</h1><h2 id="nn.parameter概述">nn.Parameter概述</h2><p><code>Parameter</code>也是<code>Tensor</code>（或者说Tensor的包装器wrapper），也就是说是一个多维矩阵，是Variable类中的一个特殊子类。它告诉 <code>Module</code> 它具有在反向传播期间需要更新的权重。 只更新具有 <code>requires_grad</code> 属性的 <code>tensor</code>。</p><p>当我们创建一个Module时，nn会自动创建相应的参数parameter，并会自动累加到模型的Parameter成员列表中。</p><h4 id="语法">语法</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.nn.parameter.Parameter(data=<span class="literal">None</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li>data (Tensor) – parameter tensor. —— 输入得是一个<code>tensor</code></li><li>requires_grad (bool, optional) – if the parameter requires gradient. See Locally disabling gradient computation for more details. <strong>Default: True</strong> —— 这个不用解释，<strong>需要注意的是<code>nn.Parameter()</code>默认有梯度</strong>。</li></ul><p><code>torch.nn.Parameter()</code>将一个不可训练的tensor转换成可以训练的类型parameter，并将这个parameter绑定到这个module里面。即在定义网络时这个tensor就是一个可以训练的参数了。使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。</p><figure><img src="https://img-blog.csdnimg.cn/20211005202810135.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_13,color_FFFFFF,t_70,g_se,x_16" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="单个全连接层中参数的个数">单个全连接层中参数的个数</h2><blockquote><p><strong>in_features的数量，决定的参数的个数 Y = WX + b, X的维度就是in_features，X的维度决定的W的维度， 总的参数个数 = in_features + 1</strong></p><p><strong>out_features的数量，决定了全连接层中神经元的个数，因为每个神经元只有一个输出。</strong></p><p>多少个输出，就需要多少个神经元 (一个 WX + b 就是一个神经元)！</p></blockquote><p>总的W参数的个数= <strong>in_features * out_features</strong></p><p>总的b参数的个数= <strong>1 * out_features</strong></p><p>总的参数（W和B）的个数= (<strong>in_features + 1) * out_features</strong></p><h2 id="使用参数创建全连接层代码例子">使用参数创建全连接层代码例子</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nn.functional.linear( )</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x_input = torch.Tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input.shape:&quot;</span>, x_input.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input      :&quot;</span>, x_input)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">Weights1 = nn.Parameter(torch.rand(<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights.shape:&quot;</span>, Weights1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights      :&quot;</span>, Weights1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">Bias1 = nn.Parameter(torch.rand(<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bias.shape:&quot;</span>, Bias1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bias      :&quot;</span>, Bias1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">Weights2 = nn.Parameter(torch.Tensor(<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights.shape:&quot;</span>, Weights2.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights      :&quot;</span>, Weights2)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nfull_connect_layer&quot;</span>)</span><br><span class="line">full_connect_layer = nn.functional.linear(x_input, Weights1)</span><br><span class="line"><span class="built_in">print</span>(full_connect_layer)</span><br></pre></td></tr></table></figure><p>x_input.shape: torch.Size([3]) x_input : tensor([1., 1., 1.])</p><p>Weights.shape: torch.Size([3]) Weights : Parameter containing: tensor([0.8948, 0.0114, 0.3688], requires_grad=True)</p><p>Bias.shape: torch.Size([1]) Bias : Parameter containing: tensor([0.8087], requires_grad=True)</p><p>Weights.shape: torch.Size([3]) Weights : Parameter containing: tensor([1.4013e-45, 0.0000e+00, 0.0000e+00], requires_grad=True)</p><p>full_connect_layer tensor(1.2750, grad_fn=<DotBackward>)</p><h1 id="六nn.mudule类">六、nn.Mudule类</h1><p>创建一个可调用的对象，其行为类似于一个函数，但也可以包含状态（例如神经网络层权重）。 它知道它包含哪些参数，并且可以将所有梯度归零，循环遍历它们更新权重等。</p><figure><img src="https://img-blog.csdnimg.cn/2021100520265815.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_14,color_FFFFFF,t_70,g_se,x_16" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h1 id="七利用nn.sequential类创建神经网络继承于nn.module类">七、利用nn.Sequential类创建神经网络（继承于nn.Module类）</h1><p>nn.Sequential是一个有序的容器，该类将按照传入构造器的顺序，依次创建相应的函数，并记录在Sequential类对象的数据结构中。同时，以神经网络模块为元素的有序字典也可以作为传入参数。</p><p>因此，Sequential可以看成是有多个函数运算对象，串联成的神经网络，其返回的是Module类型的神经网络对象。</p><h2 id="以列表的形式串联函数运算构建串行执行的神经网络">以列表的形式，串联函数运算，构建串行执行的神经网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;利用系统提供的神经网络模型类：Sequential,以参数列表的方式来实例化神经网络模型对象&quot;</span>)</span><br><span class="line"><span class="comment"># A sequential container. Modules will be added to it in the order they are passed in the constructor.</span></span><br><span class="line"><span class="comment"># Example of using Sequential</span></span><br><span class="line">model_c = nn.Sequential(nn.Linear(<span class="number">28</span> * <span class="number">28</span>, <span class="number">32</span>), </span><br><span class="line">                        nn.ReLU(), </span><br><span class="line">                        nn.Linear(<span class="number">32</span>, <span class="number">10</span>), </span><br><span class="line">                        nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">                       )</span><br><span class="line"><span class="built_in">print</span>(model_c)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n显示网络模型参数&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_c.parameters)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n定义神经网络样本输入&quot;</span>)</span><br><span class="line">x_input = torch.randn(<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line">x_input = x_input.view(x_input.size()[<span class="number">0</span>], -<span class="number">1</span>)  <span class="comment">#将输入转为二维张量</span></span><br><span class="line"><span class="built_in">print</span>(x_input.size())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用神经网络进行预测&quot;</span>)</span><br><span class="line">y_pred = model_c.forward(x_input)</span><br><span class="line"><span class="built_in">print</span>(y_pred)  <span class="comment"># torch.Size([2, 10])</span></span><br></pre></td></tr></table></figure><p>利用系统提供的神经网络模型类：Sequential,以参数列表的方式来实例化神经网络模型对象 Sequential( (0): Linear(in_features=784, out_features=32, bias=True) (1): ReLU() (2): Linear(in_features=32, out_features=10, bias=True) (3): Softmax(dim=1) )</p><p>显示网络模型参数 &lt;bound method Module.parameters of Sequential( (0): Linear(in_features=784, out_features=32, bias=True) (1): ReLU() (2): Linear(in_features=32, out_features=10, bias=True) (3): Softmax(dim=1) )&gt;</p><p>定义神经网络样本输入 torch.Size([2, 28, 28, 1]) torch.Size([2, 784])</p><p>使用神经网络进行预测 tensor([[0.0773, 0.0843, 0.1366, 0.0933, 0.1107, 0.1086, 0.0721, 0.1129, 0.1267, 0.0777], [0.0875, 0.0570, 0.1179, 0.0981, 0.1261, 0.1177, 0.1174, 0.0735, 0.0932, 0.1116]], grad_fn=<SoftmaxBackward>)</p><h2 id="以字典的形式串联函数运算构建串行执行的神经网络">以字典的形式，串联函数运算，构建串行执行的神经网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;利用系统提供的神经网络模型类：Sequential,以字典的方式来实例化神经网络模型对象&quot;</span>)</span><br><span class="line">model = nn.Sequential(OrderedDict([(<span class="string">&#x27;h1&#x27;</span>, nn.Linear(<span class="number">28</span> * <span class="number">28</span>, <span class="number">32</span>)),</span><br><span class="line">                                   (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">                                   (<span class="string">&#x27;out&#x27;</span>, nn.Linear(<span class="number">32</span>, <span class="number">10</span>)),</span><br><span class="line">                                   (<span class="string">&#x27;softmax&#x27;</span>, nn.Softmax(dim=<span class="number">1</span>))])</span><br><span class="line">                     )</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n显示网络模型参数&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model.parameters)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n定义神经网络样本输入&quot;</span>)</span><br><span class="line">x_input = torch.randn(<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用神经网络进行预测&quot;</span>)</span><br><span class="line">y_pred = model.forward(x_input.view(x_input.size()[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(y_pred)   <span class="comment"># torch.Size([2, 10])</span></span><br></pre></td></tr></table></figure><p>利用系统提供的神经网络模型类：Sequential,以字典的方式来实例化神经网络模型对象 Sequential( (h1): Linear(in_features=784, out_features=32, bias=True) (relu1): ReLU() (out): Linear(in_features=32, out_features=10, bias=True) (softmax): Softmax(dim=1) )</p><p>显示网络模型参数 &lt;bound method Module.parameters of Sequential( (h1): Linear(in_features=784, out_features=32, bias=True) (relu1): ReLU() (out): Linear(in_features=32, out_features=10, bias=True) (softmax): Softmax(dim=1) )&gt;</p><p>定义神经网络样本输入 torch.Size([2, 28, 28, 1])</p><p>使用神经网络进行预测 tensor([[0.0967, 0.1006, 0.0714, 0.0752, 0.1209, 0.1088, 0.1215, 0.1156, 0.0879, 0.1015], [0.0912, 0.1031, 0.0914, 0.0962, 0.1124, 0.0999, 0.0885, 0.1082, 0.0953, 0.1137]], grad_fn=<SoftmaxBackward>)</p><h1 id="八-自定义神经网络模型类继承于module类">八、 自定义神经网络模型类（继承于Module类）</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义网络模型：带relu的两层全连接神经网络</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;自定义新的神经网络模型的类&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">#原始写法</span></span><br><span class="line"><span class="string">class NetC(torch.nn.Module):</span></span><br><span class="line"><span class="string">    # 定义神经网络</span></span><br><span class="line"><span class="string">    def __init__(self, n_feature, n_hidden, n_output):</span></span><br><span class="line"><span class="string">        super(NetC, self).__init__()</span></span><br><span class="line"><span class="string">        self.h1 = nn.Linear(n_feature, n_hidden)  #subModule: Linear</span></span><br><span class="line"><span class="string">        self.relu1 = nn.ReLU()</span></span><br><span class="line"><span class="string">        self.out = nn.Linear(n_hidden, n_output)</span></span><br><span class="line"><span class="string">        self.softmax = nn.Softmax(dim=1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # 定义前向运算</span></span><br><span class="line"><span class="string">    def forward(self, x_input):</span></span><br><span class="line"><span class="string">        h1 = self.h1(x_input)</span></span><br><span class="line"><span class="string">        a1 = self.relu1(h1)</span></span><br><span class="line"><span class="string">        out = self.out(a1)</span></span><br><span class="line"><span class="string">        a_out = self.softmax(out)</span></span><br><span class="line"><span class="string">        return a_out</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#用torch.nn.functional引入激活函数的写法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NetC</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 定义神经网络</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">        <span class="built_in">super</span>(NetC, self).__init__()</span><br><span class="line">        self.h1 = nn.Linear(n_feature, n_hidden)  <span class="comment">#subModule: Linear</span></span><br><span class="line">        self.out = nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向运算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_input</span>):</span><br><span class="line">        h1 = F.relu(self.h1(x_input))</span><br><span class="line">        out = F.softmax(self.out(h1),dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n实例化神经网络模型对象&quot;</span>)</span><br><span class="line">model = NetC(<span class="number">28</span> * <span class="number">28</span>, <span class="number">32</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n显示网络模型参数&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model.parameters)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n定义神经网络样本输入&quot;</span>)</span><br><span class="line">x_input = torch.randn(<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line"><span class="comment"># 得到的数据格式torch.Size([64, 1, 28, 28])需要转变为（64,784）</span></span><br><span class="line">x_input = x_input.view(x_input.size()[<span class="number">0</span>], -<span class="number">1</span>)  <span class="comment"># -1表示自动匹配</span></span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用神经网络进行预测&quot;</span>)</span><br><span class="line">y_pred = model.forward(x_input)</span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br></pre></td></tr></table></figure><p>自定义新的神经网络模型的类</p><p>实例化神经网络模型对象 NetC( (h1): Linear(in_features=784, out_features=32, bias=True) (out): Linear(in_features=32, out_features=10, bias=True) )</p><p>显示网络模型参数 &lt;bound method Module.parameters of NetC( (h1): Linear(in_features=784, out_features=32, bias=True) (out): Linear(in_features=32, out_features=10, bias=True) )&gt;</p><p>定义神经网络样本输入 torch.Size([2, 28, 28, 1]) torch.Size([2, 784])</p><p>使用神经网络进行预测 tensor([[0.0884, 0.0733, 0.0890, 0.1088, 0.1589, 0.0944, 0.0861, 0.1191, 0.0876, 0.0944], [0.1134, 0.0963, 0.0595, 0.1051, 0.0881, 0.1059, 0.0627, 0.1023, 0.1605, 0.1063]], grad_fn=<SoftmaxBackward>)</p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Typora 搜狗自定义短语设置</title>
      <link href="/2024/01/23/Typora%20%E6%90%9C%E7%8B%97%E5%BF%AB%E6%8D%B7%E9%94%AE%E8%AE%BE%E7%BD%AE/"/>
      <url>/2024/01/23/Typora%20%E6%90%9C%E7%8B%97%E5%BF%AB%E6%8D%B7%E9%94%AE%E8%AE%BE%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考网址：</p><p><a href="https://blog.csdn.net/kllo__/article/details/122494151">Typora：改变字体的背景颜色_typora文字背景色_kllo__的博客-CSDN博客</a></p><p><a href="https://www.cnblogs.com/USTHzhanglu/p/16073072.html">MD两种折叠方法对比（Typora/Jupyter设置代码折叠） - USTHzhanglu - 博客园 (cnblogs.com)</a></p><p><a href="https://nickxu.me/2022/02/20/Hexo-Butterfly-建站指南（五）日常写作/">Hexo + Butterfly 建站指南（五）日常写作 | NX の 博客 (nickxu.me)</a></p></blockquote><blockquote><p>在Typora中没有快捷设置字体颜色的方式，因此我们想到在搜狗输入法的自定义短语中定义快捷输入html代码来设置字体颜色。本文给出了一些常用的自定义短语。</p></blockquote><p>[TOC]</p><h2 id="设置方法">设置方法</h2><p>打开<code>搜狗-属性设置-高级-候选扩展-自定义短语</code>，点击<code>添加新定义</code>，即可定义新的自定义短语。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231206093545264.png" alt="image-20231206093545264" /><figcaption aria-hidden="true">image-20231206093545264</figcaption></figure><h2 id="设置字体颜色">设置字体颜色</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#常见颜色</span><br><span class="line">&lt;font color=red&gt;红色字体&lt;/font&gt;  #缩写red</span><br><span class="line">&lt;font color=blue&gt;蓝色字体&lt;/font&gt;  #缩写blu</span><br><span class="line">&lt;font color=green&gt;绿色字体&lt;/font&gt;  #缩写gree</span><br><span class="line">&lt;font color=orange&gt;橙色字体&lt;/font&gt;#缩写oran</span><br><span class="line">&lt;font color=purple&gt;紫色字体&lt;/font&gt;#缩写purp</span><br><span class="line">&lt;font color=yellow&gt;黄色字体&lt;/font&gt;#缩写yel</span><br><span class="line"></span><br><span class="line">&lt;font color=#1ec4d3&gt;藻蓝色字体&lt;/font&gt;</span><br></pre></td></tr></table></figure><h4 id="样式预览">样式预览：</h4><p><font color=red>红色字体</font><br /><font color=blue>蓝色字体</font><br /><font color=green>绿色字体</font><br /><font color=orange>橙色字体</font><br /><font color=purple>紫色字体</font><br /><font color=yellow>黄色字体</font></p><p><font color=#1ec4d3>藻蓝色字体</font></p><p>另外附上我从mubu上扒下来的字体颜色设置：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;font color=#ef042a&gt;红色字体&lt;/font&gt;  #缩写red</span><br><span class="line">&lt;font color=#0091ff&gt;蓝色字体&lt;/font&gt;  #缩写blu</span><br><span class="line">&lt;font color=#4eb434&gt;绿色字体&lt;/font&gt;  #缩写gree</span><br><span class="line">&lt;font color=#df8400&gt;橙色字体&lt;/font&gt;#缩写oran</span><br><span class="line">&lt;font color=#985fff&gt;紫色字体&lt;/font&gt;#缩写purp</span><br></pre></td></tr></table></figure><h4 id="样式预览-1">样式预览：</h4><p><font color=#df8400>这是橙色</font> <font color=#ef042a>这是红色</font> <font color=#4eb434>这是绿色</font> <font color=#0091ff>这是蓝色</font> <font color=#985fff>这是紫色</font></p><h2 id="给字体加底色">给字体加底色</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;span style=&quot;background:#eef0f4;&quot;&gt;浅灰色背景&lt;/span&gt;#缩写bggrey</span><br><span class="line">&lt;span style=&quot;background:#fbd4d0;&quot;&gt;浅红色背景&lt;/span&gt;#缩写bgred</span><br><span class="line">&lt;span style=&quot;background:#d4e9d5;&quot;&gt;浅绿色背景&lt;/span&gt;#缩写bggre</span><br><span class="line">&lt;span style=&quot;background:#dad5e9;&quot;&gt;浅紫色背景&lt;/span&gt;#缩写bgpur</span><br><span class="line">&lt;span style=&quot;background:#f9eda6;&quot;&gt;浅黄色背景&lt;/span&gt;#缩写bgyel</span><br><span class="line">&lt;span style=&quot;background:#FFCC99;&quot;&gt;浅橘色背景&lt;/span&gt;#缩写bgora</span><br><span class="line"></span><br><span class="line">&lt;span style=&quot;background:#daf5e9;&quot;&gt;亮绿色背景&lt;/span&gt;</span><br><span class="line">&lt;span style=&quot;background:#fff1c9;&quot;&gt;亮黄色背景&lt;/span&gt;  </span><br></pre></td></tr></table></figure><h4 id="样式预览-2">样式预览</h4><p><span style="background:#eef0f4;">浅灰色背景</span> <span style="background:#fbd4d0;">浅红色背景</span> <span style="background:#d4e9d5;">浅绿色背景</span> <span style="background:#dad5e9;">浅紫色背景</span> <span style="background:#f9eda6;">浅黄色背景</span> <span style="background:#FFCC99;">浅橘色背景</span></p><p><span style="background:#daf5e9;">亮绿色背景</span> <span style="background:#fff1c9;">亮黄色背景</span></p><blockquote><p>配色参考网站：</p><p><a href="https://likexia.gitee.io/tools/peise/index.html">网页设计常用色彩搭配表 - 配色表 (gitee.io)</a></p><p><a href="https://encycolorpedia.cn/">十六进制颜色代码表，图表及调色板 - Encycolorpedia</a></p></blockquote><h2 id="给字体加底色且改颜色">给字体加底色且改颜色</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;span style=&quot;background:#f9eda6;color:red&quot;&gt;浅黄色背景红字&lt;/span&gt;</span><br></pre></td></tr></table></figure><p>如：<span style="background:#f9eda6;color:red">浅黄色背景</span></p><h2 id="设置图片题注tuzh">设置图片题注tuzh</h2><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">center</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">img</span> <span class="attr">style</span>=<span class="string">&quot;border-radius: 0.3125em;</span></span></span><br><span class="line"><span class="string"><span class="tag">    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);&quot;</span> </span></span><br><span class="line"><span class="tag">    <span class="attr">src</span>=<span class="string">&quot;这里输入图片地址&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">&quot;color:orange; border-bottom: 1px solid #d9d9d9;</span></span></span><br><span class="line"><span class="string"><span class="tag">    display: inline-block;</span></span></span><br><span class="line"><span class="string"><span class="tag">    color: #999;</span></span></span><br><span class="line"><span class="string"><span class="tag">    padding: 2px;&quot;</span>&gt;</span>这里输入题注<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">center</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="设置内容折叠">设置内容折叠</h2><h3 id="普通内容折叠zhed">普通内容折叠zhed</h3><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">details</span>&gt;</span> <span class="tag">&lt;<span class="name">summary</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">summary</span>&gt;</span></span><br><span class="line">contents ...</span><br><span class="line"><span class="tag">&lt;/<span class="name">details</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在<code>content</code>中可以嵌套使用 Markdown 语法和 HTML 语法。</p><h4 id="效果如下">效果如下：</h4><details><summary>Title</summary>contents ...</details><h3 id="代码内容折叠czhed">代码内容折叠czhed</h3><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">details</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">summary</span>&gt;</span>点击时的区域标题：点击查看详细内容<span class="tag">&lt;/<span class="name">summary</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">pre</span>&gt;</span><span class="tag">&lt;<span class="name">code</span>&gt;</span>#define A B</span><br><span class="line">#endif</span><br><span class="line">void init(void)<span class="tag">&lt;/<span class="name">code</span>&gt;</span><span class="tag">&lt;/<span class="name">pre</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">details</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在Typora和网站上均可以折叠代码，但是代码不能高亮。<code>&lt;pre&gt;&lt;/pre&gt;</code>用于显示多行代码，若去除则渲染为单行代码。</p><h4 id="效果如下-1">效果如下：</h4><details><summary>点击时的区域标题：点击查看详细内容</summary><pre><code>#define A B#endifvoid init(void)</code></pre></details><h2 id="butterfly-标签外挂">Butterfly 标签外挂</h2><blockquote><p>官方文档：<a href="https://butterfly.js.org/posts/4aa8abbe/#標籤外掛（Tag-Plugins）">Butterfly 安裝文檔(三) 主題配置-1 | Butterfly</a></p></blockquote><h3 id="折叠栏-toggle类似内容折叠">折叠栏 Toggle（类似内容折叠）</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">&#123;% hideToggle 点击以打开 %&#125;</span><br><span class="line"></span><br><span class="line">内容</span><br><span class="line"></span><br><span class="line">&#123;% endhideToggle %&#125;</span><br></pre></td></tr></table></figure><h4 id="效果如下-2">效果如下：</h4><details class="toggle" ><summary class="toggle-button" style="">点击以打开</summary><div class="toggle-content"><p>内容</p></div></details><h3 id="选项卡-tabs">选项卡 Tabs</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">&#123;% tabs 样例 %&#125;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab 代码  --&gt;</span><br><span class="line"></span><br><span class="line">这里是代码</span><br><span class="line"></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab 预览  --&gt;</span><br><span class="line"></span><br><span class="line">这里是预览</span><br><span class="line"></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&#123;% endtabs %&#125;</span><br></pre></td></tr></table></figure><h4 id="效果如下-3">效果如下：</h4><div class="tabs" id="样例"><ul class="nav-tabs"><button type="button" class="tab  active" data-href="样例-1">代码</button><button type="button" class="tab " data-href="样例-2">预览</button></ul><div class="tab-contents"><div class="tab-item-content active" id="样例-1"><p>这里是代码</p></div><div class="tab-item-content" id="样例-2"><p>这里是预览</p></div></div><div class="tab-to-top"><button type="button" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div><h3 id="时间轴-timeline">时间轴 timeline</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">&#123;% timeline 2024 %&#125;</span><br><span class="line">&lt;!-- timeline 01-02 --&gt;</span><br><span class="line">这是测试文本</span><br><span class="line">&lt;!-- endtimeline --&gt;</span><br><span class="line">&#123;% endtimeline %&#125;</span><br></pre></td></tr></table></figure><h4 id="效果如下-4">效果如下：</h4><div class="timeline undefined"><div class='timeline-item headline'><div class='timeline-item-title'><div class='item-circle'><p>2024</p></div></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>01-02</p></div></div><div class='timeline-item-content'><p>这是测试页面</p></div></div></div>]]></content>
      
      
      <categories>
          
          <category> blog搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对象和类 学习笔记</title>
      <link href="/2024/01/08/%E7%B1%BB%E5%92%8C%E5%AF%B9%E8%B1%A1%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2024/01/08/%E7%B1%BB%E5%92%8C%E5%AF%B9%E8%B1%A1%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<blockquote><p>部分内容参考网址：<a href="https://www.cnblogs.com/jialexu/articles/14188861.html">【Python学习】对象和类 - xujiale - 博客园 (cnblogs.com)</a></p></blockquote><h1 id="什么是面向对象编程">什么是面向对象编程</h1><h2 id="背景知识">背景知识</h2><p>20世纪70年代中期，人们开始撰写文章来介绍这种编程方法的优点。大概在同一时间， 编程语言SmallTalk（施乐研究中心）和CLU（麻省理工学院）为这种思想提供了语言上 的支持。</p><p>1967年5月20日，挪威奥斯陆计算中心的科学家Ole-Johan Dahl和Kristen Nygaard正式 发布了Simula 67语言。它被认为是最早的 面向对象程序设计 语言，首次引入类、对象、 继承、动态绑定等重要概念。因此，1983年共同获得图灵奖。</p><p>SmallTalk语言起源于施乐帕罗奥多研究中心（Xerox PARC）的一项研究项目。它被公认 为历史上第二个面向对象的程序设计语言和第一个真正的集成开发环境 (IDE) 。</p><p>但C++和Java的出现才真正使这种思想成为现实。</p><h2 id="具体概念">具体概念</h2><blockquote><p>传统的程序设计，即<strong>面向过程的编程</strong>主张将程序看作一系列函数的集合，或者直接就是一系列对计算机下达的指令。</p></blockquote><p><strong>面向对象程序设计（Object Oriented Programming, OOP）</strong>可以看作一种<strong>在程序中包含各种独立而又互相调用的对象</strong>的思想。面向对象编程的一个重要特点就是<span style="background:#f9eda6;color:red"><strong>数据封装</strong></span>。</p><p><strong>对象（Object）</strong>则指的是<strong>类（Class）</strong>的实例。 在面向对象程序编程里，计算机程序将<strong>对象</strong>作为程序的基本单元，将程序和数据封装其中，以提高软件的重用性、灵活性和扩展性。</p><p>面向对象程序设计中的每一个对象都应该能够接受数据、处理数据并将数据传达给其它对象，因此它们都可以被看作一个小型的“机器”，即<strong>对象</strong>。<u>Python里面所有的东西都是对象(<em>objects</em>)，连同一个整数也是一种对象</u>，该语法设计可以巧妙的隐藏诸多细节。面向对象程序设计推广了程序的灵活性和可维护性，并且<strong>在大型项目设计中广为应用</strong>。</p><p>面向对象不仅指一种程序设计方法。它更多意义上是一种程序开发方式。许多流行的编程语言是面向对象的: <span style="background:#fbd4d0;">Python、C++、Objective-C、Java、Swift、C#、Perl、Ruby 与 PHP</span>等。</p><p><strong>Python支持面向过程、面向对象等编程方式。</strong>Python不强制使用任何一种编程方式，可以使用面向过程方式编写任何程序，但在中大型项目中，面向对象会带来很多优势。</p><h3 id="一个栗子">一个栗子</h3><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231218105617559.png" alt="image-20231218105617559" /><figcaption aria-hidden="true">image-20231218105617559</figcaption></figure><h1 id="什么是对象和类">什么是对象和类</h1><h2 id="对象的概念">对象的概念</h2><p><strong>对象是封装的数据抽象：对象是一个数据和操作的封装体。</strong>封装的目的就是<u>阻止非法的访问</u>，因此<strong>对象实现了信息的隐藏</strong>，外部只能通过操作接口访 问对象数据。</p><p>对象包含<strong>属性（attribute）</strong>和<strong>方法（method）</strong>。当你要创建一个新的object时，就必须先定义一个新的类，用它来清楚规范其类别可以创造出来的对象有什么样的属性与方法。</p><p><font color=orange>对象</font>像名词，<font color=blue>方法</font>就像个动词。<font color=orange>对象</font>代表一个独立的事物，<font color=blue>方法</font>用来定义它如何与其他<font color=orange>对象</font>相互作用。与模块不同的是，你可以同时创建多个同类别的<font color=orange>对象</font>，他们之间的属性值可能各有不同。</p><blockquote><p><strong><u>抽象数据类型</u></strong>是一个由对象以及对象上的操作组成的集合，对象和操作被捆绑为一个整体，可以从程序的一个部分传递到另一个部分。</p><p>这些操作的规范定义了抽象数据类型和程序其他部分之间的<strong><u>接口</u></strong>。</p><p><u>接口定义了操作的行为，即它们做什么，但没有说明如何去做。</u>于是，接口建立了一个<strong><u>抽象边界</u></strong>，将程序的其他部分与实现类型抽象的数据结构、算法和代码隔离开来。</p></blockquote><h2 id="类的概念">类的概念</h2><p>类和变量之间存在着一定的联系，类型是模板，而变量则是具有这种模板的一个实体。<strong>类（class）是对客观世界中事物的抽象，对象是类的实例(Instance)。</strong></p><p>从本质上说，对象是一组<u>数据</u>以及<u>操作这些数据的函数</u>。之前介绍的数字、字符串、列表、 字典和函数都是Python提供的内置对象。<strong>要创建新型对象，必须先创建类</strong>。类就类似于内置数据类型，可用于创建特定类型的对象。 <u>类指定了对象将包含哪些数据和函数，还指定了对象与其他类的关系</u>。</p><blockquote><p>一个重要的OOP功能是<strong>继承</strong>：创建新类时，可让其继承父类的数据和函数。使用好继承可避免重新编写代码，还可让程序更容易理解。对象和</p></blockquote><h1 id="对象和类">对象和类</h1><h2 id="定义">定义</h2><h3 id="定义类">定义类</h3><p>Python使用<font color=red>class关键字</font>定义一个类，类名首字符一般要大写。类定义中存在一个函数定义时，被定义的函数称为<strong>方法</strong>，并与这个类相关联。这些方法有时称为类的<strong>方法属性</strong>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Leiming</span>(<span class="title class_ inherited__">继承的父类名</span>)：</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,xx</span>):</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Xinfangfa</span>(<span class="params">xx</span>):</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">XXinfangfa</span>(<span class="params">self,xx</span>):</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><h3 id="在类中定义对象">在类中定义对象</h3><p>创建对象的过程称为<strong>实例化</strong>。</p><p>当一个对象被创建之后，包含3方面的特性：对象的<strong>标识</strong>、<strong>属性</strong>和<strong>方法</strong>。对象的标识用于区分不同的对象，当对象被创建之后，该对象会获取一块存储空间，<strong>存储空间的地址</strong>即为对象的标识。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自定义一个Person()</span></span><br><span class="line"><span class="comment"># __init__为定义属性部分</span></span><br><span class="line"><span class="comment"># self为object自己</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Person</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, email</span>): <span class="comment">#类的构造函数，用来初始化对象</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.email = email</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">say</span>(<span class="params">self, tem</span>):  <span class="comment">#类中定义的函数，也成为方法</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;I am &#x27;</span> + self.name + tem</span><br><span class="line"></span><br><span class="line">hunter = Person(<span class="string">&#x27;Elmer Fudd&#x27;</span>, <span class="string">&quot;QQ@WW.tw&quot;</span>)   <span class="comment">#实例化</span></span><br><span class="line"></span><br><span class="line">Husky = Person(<span class="string">&#x27;Hsuky&#x27;</span>, <span class="string">&quot;XDD@WW.tw&quot;</span>)</span><br><span class="line">Husky.ff = “oo”  <span class="comment"># 定义新对象后可自定义不在类中的新属性（属性的动态添加）</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(Husky.ff)</span><br><span class="line"><span class="comment"># print(hunter.ff)  报错&#x27;Person&#x27; object has no attribute &#x27;ff&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(hunter.name)</span><br><span class="line"><span class="built_in">print</span>(hunter.email)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(hunter.say(<span class="string">&#x27;!!!&#x27;</span>))  <span class="comment">#方法引用：通过点标记法，访问与类关联的方法</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(Husky.name)</span><br><span class="line"><span class="built_in">print</span>(Husky.email)</span><br></pre></td></tr></table></figure><p>oo Elmer Fudd QQ@WW.tw I am Elmer Fudd!!! Hsuky XDD@WW.tw</p><p>值得注意的是，当在类中定义函数时，若函数的第一个参数不是self，则该函数方法只能被类所使用。<span style="background:#fbd4d0;">因此在类中定义一般对象使用的方法时（<strong>除了静态方法外</strong>），第一个参数必须写<strong>self</strong>！</span>下面是个栗子</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">test</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sayhi</span>():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Hi&quot;</span>)</span><br><span class="line"></span><br><span class="line">gg = test()</span><br><span class="line"><span class="comment">#gg.sayhi() #报错 test.sayhi() takes 0 positional arguments but 1 was given</span></span><br><span class="line">test.sayhi() <span class="comment">#Hi</span></span><br></pre></td></tr></table></figure><h2 id="类的属性">类的属性</h2><p>Python并没有真正的私有化支持，但可用下划线得到伪私有。</p><h3 id="protected类型">protected类型</h3><p><font color=#ef042a>"单下划线 "</font> 开始的成员变量叫做保护变量，意思是只有<font color=#df8400><strong>类实例</strong></font>和<font color=#df8400><strong>子类实例</strong></font>能访问到这些变量， 需通过类提供的接口进行访问；不能用'from module import *'导入</p><h3 id="private类型">private类型</h3><p>在Python中，若<u>不希望类中的属性在类外被直接访问</u>，可以将实例的变量名改为<font color=#ef042a>以__（双下划线）开头</font>，就变成了一个私有类型（<font color=#ef042a>private</font>），只有内部可以访问，外部不能访问。</p><p>双下划线开头的类型是不是一定不能从外部访问呢？其实也不是。不能直接访问__xxx是因为Python解释器对外把__xxx变量/方法改成了_Class__xxx，所以，仍然可以通过<font color=#ef042a><strong>instance._Class__xxx</strong></font>来访问__xxx变量/方法。Python提供了直接访问私有属性的方式，可用于程序的测试和调试。当不知道类名时，只有<font color=#df8400><strong>类对象自己内部</strong></font>能访问，连子类对象也不能访问到这个数据。</p><h4 id="使用属性对特性进行访问和设置">使用属性对特性进行访问和设置</h4><p>在其他语言中，可以设置getter 和 setter來确保私有属性的读写。但是在python一切都是公开的，可以通过property()来达到python风格的写法，即可將属性值藏起來，不用通过调用每个getter()和setter()来达到改变私有变量。</p><p>若沒有给定setter函数，则无法通过property()来改变属性值，当然前提是在別人不知道实际存储变量的属性名称是什么。</p><blockquote><p>关于property()的用法请看这里：<a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017502538658208">使用@property - 廖雪峰的官方网站 (liaoxuefeng.com)</a>，里面说得很详细啦！</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Duck</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        self.hidden_name = input_name</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#取的 name 的函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_name</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用get函数---&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.hidden_name + <span class="string">&#x27;!!&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#设定 name 的函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_name</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用set函数---&#x27;</span>)</span><br><span class="line">        self.hidden_name = input_name + <span class="string">&#x27;??&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#使用property(get,set)来包裝，让使用上更方便</span></span><br><span class="line">    name = <span class="built_in">property</span>(get_name, set_name)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义Object为Duck类，并给定name，从头到尾都沒有直接抄作hidden_name來改变属性值</span></span><br><span class="line">fowl = Duck(<span class="string">&#x27;Howard&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;提取名称时，则调用get函数&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n设定名称时，则调用set函数&#x27;</span>)</span><br><span class="line">fowl.name = <span class="string">&#x27;Daffy&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;nname被改成Daffy&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n当然也可以通过原始的set_name()与get_name()进行修改私有属性&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.get_name())</span><br><span class="line">fowl.set_name(<span class="string">&#x27;Daffyyyy&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.get_name())</span><br></pre></td></tr></table></figure><p>提取名称时，则调用get函数 ---使用get函数--- Howard!!</p><p>设定名称时，则调用set函数 ---使用set函数--- nname被改成Daffy ---使用get函数--- Daffy??!!</p><p>当然也可以通过原始的set_name()与get_name()进行修改私有属性 ---使用get函数--- Daffy??!! ---使用set函数--- ---使用get函数--- Daffyyyy??!!</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#当然可以通过装饰器decorator，来写得更漂亮!!!</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Duck</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        self.hidden_name = input_name</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">name</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用get函数---&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.hidden_name</span><br><span class="line"></span><br><span class="line"><span class="meta">    @name.setter</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">name</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用set函数---&#x27;</span>)</span><br><span class="line">        self.hidden_name = input_name</span><br><span class="line">        </span><br><span class="line"><span class="comment">#定义Object为Duck类，并给定name</span></span><br><span class="line">fowl = Duck(<span class="string">&#x27;Howard&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;提取名称时，则调用get函数&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n设定名称时，则调用set函数&#x27;</span>)</span><br><span class="line">fowl.name = <span class="string">&#x27;Daffy&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;nname被改成Daffy&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br></pre></td></tr></table></figure><p>提取名称时，则调用get函数 ---使用get函数--- Howard</p><p>设定名称时，则调用set函数 ---使用set函数--- nname被改成Daffy ---使用get函数--- Daffy</p><h4 id="使用名称重整保持私有性推荐">使用名称重整保持私有性（推荐）</h4><p>前面的用法如果被知道实际储存属性的名称为什么，也是可以对其修改 所以可以通过名称重整来把实际储存的名称改写</p><p>在属性名称前面加上( __ )来重整名称，虽然不能完全的防止修改私有属性，但可以通过有效的方法降低有意或无意的修改</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Duck</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        self.__name = input_name</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">name</span>(<span class="params">self</span>):  <span class="comment">#该函数名不一定为name，也可以改为names等其他名字</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用get函數---&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.__name</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @name.setter  </span><span class="comment">#这里相应的也要改改为names等其他名字，同理访问时也是</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">name</span>(<span class="params">self, input_name</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---使用set函數---&#x27;</span>)</span><br><span class="line">        self.__name = input_name</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">fowl = Duck(<span class="string">&#x27;Howard&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br><span class="line">fowl.name = <span class="string">&#x27;Donald&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(fowl.name)</span><br><span class="line"></span><br><span class="line"><span class="comment">#fowl.__name        #直接访问或修改会错误</span></span><br><span class="line"><span class="comment">#fowl._Duck__name   #重整完的名称</span></span><br><span class="line"><span class="comment">#print(fowl.__name) # 报错</span></span><br><span class="line"><span class="comment">#print(fowl._Duck__name) # OK</span></span><br><span class="line"></span><br><span class="line">fowl.__name=<span class="string">&quot;gg&quot;</span>  <span class="comment">#相当于属性的动态添加，即添加了一个名为__name的新属性</span></span><br><span class="line"><span class="built_in">print</span>(fowl.__name)      <span class="comment">#gg</span></span><br><span class="line"><span class="built_in">print</span>(fowl._Duck__name) <span class="comment">#Donald</span></span><br><span class="line"><span class="built_in">print</span>(fowl.name)        <span class="comment">#仍然为Donald，说明使用双下划线后无法轻易地改变类内的属性</span></span><br><span class="line"></span><br><span class="line">fowl._Duck__name=<span class="string">&quot;GGBond&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(fowl.name)  <span class="comment">#只有知道类名，才能改变该属性</span></span><br></pre></td></tr></table></figure><p>---使用get函數--- Howard ---使用set函數--- ---使用get函數--- Donald gg Donald ---使用get函數--- Donald</p><p>---使用get函數--- GGBond</p><h3 id="public类型">public类型</h3><p>C++有定义属性的关键字（public、private、protect），而Python没有这类关键字，默认情况下所有的属性都是“公有的”，对公有属性的访问没有任何限制，且都会被子类继承，也能从子类中进行访问。</p><h3 id="特殊类型">特殊类型</h3><h4 id="特殊变量">特殊变量</h4><p>在Python中，变量名类似<font color=#4eb434>__xxx__</font>的，也就是<font color=#4eb434>以双下划线开头，并且以双下划线结尾</font>的，是<font color=#4eb434>特殊变量</font>，特殊变量是可以直接访问的，不是private变量，如 init（）代表类的构造函数。</p><h4 id="特殊方法">特殊方法</h4><p>在python中，存在一些特殊方法( special method )或者称为( magic method )。我们可以通过<strong>在类中重写特殊方法</strong>，使编程更加便捷！ 这些方法为<span style="background:#fbd4d0;">双下划线( __ )开头与结尾</span>的用法。前面介绍过的( <strong>init</strong> )就是一个特殊方法，<u>只要一个类被实例化，就会调用该类中定义的__init__方法</u>。<u>__init__方法的第一个参数永远都是self</u>，表示创建实例本身，在__init__方法内部，可以把各种属性绑定到self，因为形参self指向创建的实例实参本身。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#---------------采用一般方法写法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Word</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, text</span>):</span><br><span class="line">        self.text = text</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">equals</span>(<span class="params">self, word2</span>):</span><br><span class="line">        <span class="keyword">return</span> self.text.lower() == word2.text.lower()</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建3个字符Object</span></span><br><span class="line">first = Word(<span class="string">&#x27;ha&#x27;</span>)</span><br><span class="line">second = Word(<span class="string">&#x27;HA&#x27;</span>)</span><br><span class="line">third = Word(<span class="string">&#x27;eh&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#进行比较</span></span><br><span class="line"><span class="built_in">print</span>(first.equals(second))  <span class="comment">#True</span></span><br><span class="line"><span class="built_in">print</span>(first.equals(third))   <span class="comment">#False</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#---------------采用特殊方法写法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Word</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, text</span>):</span><br><span class="line">        self.text = text</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__eq__</span>(<span class="params">self, word2</span>):   <span class="comment">#在Word类中重写__eq__方法</span></span><br><span class="line">        <span class="keyword">return</span> self.text.lower() == word2.text.lower()</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建3个字符对象</span></span><br><span class="line">first = Word(<span class="string">&#x27;ha&#x27;</span>)</span><br><span class="line">second = Word(<span class="string">&#x27;HA&#x27;</span>)</span><br><span class="line">third = Word(<span class="string">&#x27;eh&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#进行比较</span></span><br><span class="line"><span class="built_in">print</span>(first == second)  <span class="comment">#True</span></span><br><span class="line"><span class="built_in">print</span>(first == third)   <span class="comment">#False</span></span><br></pre></td></tr></table></figure><p>由上可见，特殊方法将使输出更加漂亮!</p><p>我们再来看一些常见的特殊方法与普通方法对比：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Element</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,name,symbol,number</span>):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.symbol = symbol</span><br><span class="line">        self.number = number</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dump</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;name=&#123;&#125;,\nsymbol=&#123;&#125;,\nnumber=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(self.name,self.symbol,self.number))</span><br><span class="line">        <span class="comment"># 若将 print 改为 return，输出为：&#x27;name=GGBond,\nsymbol=G,\nnumber=666&#x27;</span></span><br><span class="line">    </span><br><span class="line">hydrogen = Element(<span class="string">&#x27;GGBond&#x27;</span>,<span class="string">&#x27;G&#x27;</span>,<span class="number">666</span>)</span><br><span class="line">hydrogen.dump()</span><br></pre></td></tr></table></figure><p>name=GGBond, symbol=G, number=666</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Element</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,name,symbol,number</span>):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.symbol = symbol</span><br><span class="line">        self.number = number</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):  <span class="comment">#相当于重写__str__方法</span></span><br><span class="line">        <span class="keyword">return</span>(<span class="string">&#x27;name=&#123;&#125;,\nsymbol=&#123;&#125;,\nnumber=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(self.name,self.symbol,self.number))</span><br><span class="line">        <span class="comment"># 若将 return 改为 print, 报错：TypeError: __str__ returned non-string (type NoneType)</span></span><br><span class="line">        </span><br><span class="line">hydrogen = Element(<span class="string">&#x27;GGBond&#x27;</span>,<span class="string">&#x27;G&#x27;</span>,<span class="number">888</span>)</span><br><span class="line"><span class="built_in">print</span>(hydrogen)</span><br></pre></td></tr></table></figure><p>name=GGBond, symbol=G, number=888</p><blockquote><p>关于__str__和__repr__的区别：<a href="https://www.cnblogs.com/huhuxixi/p/10619289.html">__rept__和<strong>str</strong> - 呼呼嘻嘻 - 博客园 (cnblogs.com)</a></p><p>我们在用print输出任何东西的时候，都会有一个渲染步骤，而且默认的就是用str进行渲染，因为任何一样东西都可以看做一个对象，那么它必有一个类型，如果它的类里面没有定义str和repr也没关系，object里面定义了str和repr，object是一切类的父类，所以输出的对象一定会是渲染过的。这个类里面自己写了str和repr，它覆盖了object里面的str和repr，相当于print的重定向。</p><p>调用 print(i1) （#等同与print(str(i1))）的时候，解释器第一个寻找的就是i1这个类的方法里面有没有<u>重新定义str</u>，如果没有，那么它第二步会去寻找这个类里面有没有<u>重新定义repr</u>，如果有则会用类方法的重新定义的repr，如果还没有，那么解释器会找这个类的上一层父类，按同样的规则进行寻找。</p><p>调用print（repr（i1））的时候就不一样了，repr只会调用repr方法，当自定义的类中没有重写repr方法的时候，它会直接找上一级的父类中有没有repr方法，而不会考虑调用str方法。</p></blockquote><p>常用的特殊方法整理如下：</p><p><strong>非常常用</strong></p><table><thead><tr class="header"><th>方法名</th><th>使用</th></tr></thead><tbody><tr class="odd"><td>__init__(self)</td><td>定义新对象时</td></tr><tr class="even"><td>__str__(self)</td><td>print(对象名) 或 print(str(对象名))</td></tr><tr class="odd"><td>__repr__(self)</td><td>print(对象名) 或 print(repr(对象名))</td></tr><tr class="even"><td>__len__(self)</td><td>len(对象名)</td></tr><tr class="odd"><td>__type__(self)</td><td>type(对象名) == 类名</td></tr></tbody></table><p><strong>比较用</strong></p><table><thead><tr class="header"><th>方法名称</th><th>使用</th></tr></thead><tbody><tr class="odd"><td>__eq__(self,other)</td><td>对象名 == other</td></tr><tr class="even"><td>__ne__(self,other)</td><td>对象名 != other</td></tr><tr class="odd"><td>__lt__(self,other)</td><td>对象名 &lt; other</td></tr><tr class="even"><td>__gt__(self,other)</td><td>对象名 &gt; other</td></tr><tr class="odd"><td>__le__(self, other)</td><td>对象名 &lt;= other</td></tr><tr class="even"><td>__ge__(self, other)</td><td>对象名 &gt;= other</td></tr></tbody></table><p><strong>数学用</strong></p><table><thead><tr class="header"><th>方法名</th><th>使用</th></tr></thead><tbody><tr class="odd"><td>__add__(self, other)</td><td>对象名 + other</td></tr><tr class="even"><td>__sub__(self, other)</td><td>对象名 - other</td></tr><tr class="odd"><td>__mul__(self, other)</td><td>对象名 * other</td></tr><tr class="even"><td>__floordiv__(self, other)</td><td>对象名 // other</td></tr><tr class="odd"><td>__truediv__(self, other)</td><td>对象名 / other</td></tr><tr class="even"><td>__mod__(self, other)</td><td>对象名 % other</td></tr><tr class="odd"><td>__pow__(self, other)</td><td>对象名**other</td></tr></tbody></table><blockquote><p>完整清单见官方清单：<a href="https://docs.python.org/3/reference/datamodel.html#special-method-names">3. Data model — Python 3.12.1 documentation</a></p></blockquote><h3 id="其他类型">其他类型</h3><h4 id="类变量静态变量"><font color=#985fff>类变量/静态变量</font></h4><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231218165934920.png" alt="image-20231218165934920" /><figcaption aria-hidden="true">image-20231218165934920</figcaption></figure><p>C++中有一类特殊的属性称为静态变量。静态变量可以被类直接调用，而不被实例化对象调用。当创建新的实例化对象后，静态变量并不会获取新的内存空间，而是使用类创建的内存空间。因此，静态变量能够被多个实例化对象共享。<font color=#ef042a>在Python中静态变量称为类变量，类变量可以在该类的所有实例中被共享</font>。</p><blockquote><p><strong>有实例属性的变量：</strong>实例属性是以<font color=#ef042a>self</font>为前缀的属性，没有该前缀的属性是普通的局部变量。</p><p>如上图中的普通局部变量zone只有在__init__()方法中能被访问到，而有实例属性的变量color则能在外部被实例调用访问。</p></blockquote><h4 id="静态方法和类方法"><font color=#985fff>静态方法和类方法</font></h4><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231218165535528.png" alt="image-20231218165535528" /><figcaption aria-hidden="true">image-20231218165535528</figcaption></figure><blockquote><p>类的方法也分为公有方法和私有方法。私有方法不能被模块外的类或方法调用，私有方法也不能被外部的类或函数调用。</p></blockquote><h5 id="静态方法"><font color=#4eb434>静态方法</font></h5><p>Python使用<font color=#ef042a>函数staticmethod()</font>或 <font color=#ef042a>@ staticmethod修饰器</font>将普通的函数转换为静态方法。在开发中，我们常常需要定义一些方法，这些方法跟类有关，但在实现时并不需要引用类或者实例，例如，设置环境变量，修改另一个类的变量，等。这个时候，我们可以使用静态方法。</p><blockquote><p>静态方法一定要加函数staticmethod()或@ staticmethod修饰器，否则会被识别为没有self参数的普通方法，进而只能被类调用！</p></blockquote><ol type="1"><li>静态方法可以使用<font color=#df8400><strong>类调用</strong></font>也可以使<font color=#df8400><strong>用对象调用</strong></font>。</li><li>一般不需要传参数self。</li><li>Python的静态方法并没有和类的实例进行名称绑定，只是名义上归类管理，<u>实际上在静态方法里面访问不了类或者实例的任何属性</u>。</li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231218165603714.png" alt="image-20231218165603714" /><figcaption aria-hidden="true">image-20231218165603714</figcaption></figure><h5 id="类方法"><font color=#4eb434>类方法</font></h5><p>使用<font color=#ef042a>函数classmethod()</font>或 <font color=#ef042a>@ classmethod修饰器</font>，没有被调用的类中其余参数不会加载进内存中。</p><ol type="1"><li><font color=#df8400><strong>只能访问类变量</strong></font>，不能访问实例变量。需要有参数。类方法能被<u>类本身</u>和<u>类的实例</u>调用。</li><li>类方法的第一个参数长什么样不重要，它指向的都是类本身。</li><li>在类方法中，可以调用类里面的类属性和普通方法/静态方法。</li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231218165618888.png" alt="image-20231218165618888" /><figcaption aria-hidden="true">image-20231218165618888</figcaption></figure><h5 id="静态方法和类方法的栗子"><font color=#4eb434>静态方法和类方法的栗子</font></h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>():</span><br><span class="line">    count = <span class="number">0</span>           <span class="comment">#类属性</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,name</span>):</span><br><span class="line">        A.count += <span class="number">1</span>    <span class="comment">#修改类属性,修改时必须用A.调用</span></span><br><span class="line">        self.name = name</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">exclaim</span>():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;I&#x27;m an A!&quot;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod        </span><span class="comment">#类方法(methond)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kids</span>(<span class="params">cls</span>):</span><br><span class="line">        cls.exclaim()  <span class="comment">#类方法能调用普通方法/静态方法，但在调用实例方法时会报错</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;A has&quot;</span>, cls.count, <span class="string">&quot;little objects.&quot;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="meta">    @classmethod        </span><span class="comment">#类方法(methond)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kids2</span>(<span class="params">pp</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;A has&quot;</span>, A.count, <span class="string">&quot;little objects.&quot;</span>)</span><br><span class="line"></span><br><span class="line">easy_a = A(<span class="string">&quot;easy&quot;</span>)</span><br><span class="line">breezy_a = A(<span class="string">&quot;breezy&quot;</span>)</span><br><span class="line">wheezy_a = A(<span class="string">&quot;wheezy&quot;</span>)</span><br><span class="line">A.kids()</span><br><span class="line">A.kids2()</span><br><span class="line">easy_a.kids()  <span class="comment">#类方法也能被类的实例所调用 </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CoyoteWeapon</span>():</span><br><span class="line">    yyo = <span class="number">1</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">commercial</span>():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;This CoyoteWeapon has been brought to you by Acme&#x27;</span>)</span><br><span class="line">        <span class="comment"># print(yyo) 报错，在静态方法中访问不了类的任何实例和变量</span></span><br><span class="line">        </span><br><span class="line">CoyoteWeapon.commercial()</span><br><span class="line">ppp = CoyoteWeapon()</span><br><span class="line">ppp.commercial()</span><br></pre></td></tr></table></figure><p>I'm an A! A has 3 little objects. A has 3 little objects. I'm an A! A has 3 little objects. This CoyoteWeapon has been brought to you by Acme This CoyoteWeapon has been brought to you by Acme</p><h2 id="属性和方法的动态添加">属性和方法的动态添加</h2><blockquote><p>动态语言目前非常具有活力。例如JavaScript， PHP 、 Ruby 、 Python 等。而 C 、 C++ 等语言则不属于动态语言。</p></blockquote><h3 id="属性的动态添加">属性的动态添加</h3><ol type="1"><li>运行过程中可以给<strong>对象/类</strong>添加属性</li><li>语法格式：xx.new_attribute = attri_value</li></ol><h3 id="方法的动态添加">方法的动态添加</h3><ol type="1"><li>语法格式：class_name.method_name=function_name</li><li>可以动态添加/更改类的方法，将某个已经定义的函数添加到类中。当method_name表示已经存在的方法名，function_name表示1个已经存在的函数，该赋值表达式表示将函数的内容更新到方法。</li></ol><h2 id="继承与多态">继承与多态</h2><blockquote><p>这一章有个例子，感兴趣的可以搜一下 Undercut游戏~</p></blockquote><h3 id="继承">继承</h3><p>不同的类型中有许多通用的属性。例如，list类型和str类型都具有len函数，意义也完全 一样，这是因为它们继承了同一个父类。<strong>“继承”</strong>是面向对象编程 (OOP) 语言的一个主要功能。</p><p>在编写类时，如果发现已经有前人开发过，那就可以不用整段赋值，<strong>可以采用<font color=#985fff>继承</font>的方法取得他的属性与方法</strong>。 <strong>并且补充自己会用的功能</strong>，一方面可以减少去改已有的类的辛苦，也可以省去复制粘贴的功夫。<font color=#985fff>原始的类称为父类或超类（Base class、Super class），新类称为子类（Subclass）</font>。Python在类名后使用一对括号表示继承关系，括号中即为父类。</p><blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219152748146.png" alt="image-20231219152748146" /><figcaption aria-hidden="true">image-20231219152748146</figcaption></figure><p>如果子类中除了pass没有其他的内容，则pass必写；否则，报错下一行的缩进问题</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">math</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;add:&quot;</span>, a + b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">mean</span>(<span class="title class_ inherited__">math</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">ab = mean()</span><br><span class="line">ab.add(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">ac = math()</span><br><span class="line">ac.add(<span class="number">1</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>add: 4 add: 6</p><p>继承的过程，就是从一般到特殊的过程。object类在最顶层。因为在Python中，存在于运行时的一切都是对象。</p><h4 id="构造函数的继承">构造函数的继承</h4><p>python中<u>如果子类有自己的构造函数</u>，<u>不会自动调用父类的构造函数</u>，<font color=#ef042a>如果需要用到父类的构造函数，则需要在子类的构造函数中显式地调用，且调用代码应位于第一行</font>。</p><p>如果子类需要扩展父类的行为，可以添加__init__方法的参数。一种方法如图，即<code>Fruit.__init__(self,color)</code>; 另一种也可以用<code>super().__init__(color)</code>。</p><p>如果子类没有自己的构造函数，则会直接从父类继承构造函数。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219151850198.png" alt="image-20231219151850198" /><figcaption aria-hidden="true">image-20231219151850198</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219152043676.png" alt="image-20231219152043676" /><figcaption aria-hidden="true">image-20231219152043676</figcaption></figure><blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219152242748.png" alt="image-20231219152242748" /><figcaption aria-hidden="true">image-20231219152242748</figcaption></figure><p>为什么P4&lt;P1能够执行，而P1&lt;P4不行？</p><ul><li>P4&lt;P1相当于调用P4.__lt__(P1)，相当于使用与P4对象相关的Person的方法，根据人的姓名做比较，因此可正常执行；</li><li>P1&lt;P4相当于调用P1.__ lt __(P4)，相当于使用与P1对象相关的MITPeron的方法，根据人的IDNum做比较，而P4是一个Person，没有IDNum，因此无法比较。</li></ul></blockquote><blockquote><p>说明:函数isinstance是内置在Python中的一个常用函数,其中第一个参数可以是任何对象,但第二个参数必须是一个type类型的对象。函数当且仅当第一个参数是第二个 参数的一个实例时,才返回True。</p><p>例如,isinstance([1,2],list)的值是True。</p><p>常见用法： def isStudent(self): ​ return isinstance(self, Student)</p><p>请注意，isinstance(p6, Student)与type(p6) == Student在意义上是截然不同的。与p6绑定的对象类型是UnderGraduate，不是Student， 但因为UnderGraduate是Student的子类，所以p6绑定的对象被认为是Student类的一个实例（也是MITPerson类和Person类的实例）。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">math</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;add:&quot;</span>, a + b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">mean</span>(<span class="title class_ inherited__">math</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;add:&quot;</span>, a + b)</span><br><span class="line">    </span><br><span class="line">ab = mean()</span><br><span class="line">ac = math()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(ab)==mean) <span class="comment">#True</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(ab)==math) <span class="comment">#False</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">isinstance</span>(ab,mean)) <span class="comment">#True</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">isinstance</span>(ab,math)) <span class="comment">#True</span></span><br></pre></td></tr></table></figure></blockquote><h4 id="覆盖">覆盖</h4><p>覆盖——也就是替换——超类中的方法。</p><p><font color=#0091ff>如果一个方法被覆盖，那么调用这个方法时使用的版本就要根据调用这个方法的对象来确定。</font>如果这个对象的类型是子类，那么就使用定义在子类中的方法版本；如果对象的 类型是超类，那么就使用超类中的版本。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">math</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;add:&quot;</span>, a + b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">mean</span>(<span class="title class_ inherited__">math</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;add:&quot;</span>, a + b + b)</span><br><span class="line">    </span><br><span class="line">ab = mean()</span><br><span class="line">ab.add(<span class="number">1</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>add: 7</p><h4 id="添加新的属性或方法">添加新的属性或方法</h4><p>添加新的属性。例如，子类MITPerson中新增了类变量nextIdNum、实例变量idNum 和方法getIdNum。我们也可以在新的类中加入新的方法：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class math():</span><br><span class="line">    def add(self, a, b):</span><br><span class="line">        print(&quot;add:&quot;, a + b)</span><br><span class="line"></span><br><span class="line">class mean(math):</span><br><span class="line">    def less(self, a, b):</span><br><span class="line">        print(&quot;add:&quot;, a - b)</span><br><span class="line">    </span><br><span class="line">ab = mean()</span><br><span class="line">ab.add(1, 3)</span><br><span class="line">ab.less(1, 3)</span><br><span class="line"></span><br><span class="line">ac = math()</span><br><span class="line">ac.add(1, 5)</span><br></pre></td></tr></table></figure><p>add: 4 add: -2 add: 6</p><h3 id="多态">多态</h3><p>继承机制说明子类具有父类的公有属性和方法，而且子类可以扩展自身的功能，添加新的属性和方法。因此，子类可以替代父类对象，这种特性称为<strong>多态性</strong>。</p><p><strong>多态（polymorphism）</strong>是指父类的同一个方法在不同的子类对象中具有不同的表现和 行为。 （<u>事实上，即使是同一个类的对象，也可以有不同的属性</u>）</p><p>子类继承了父类的属性和方法之后，还会增加某些特定的属性和方法，同时还会对继承来的某些方法进行改变，都是多态的表现形式。</p><p>Python大多数运算符可以作用于不同类型的操作数，且对不不同类型的操作数往往有不 同的表现，这本身就是多态，是通过重写特殊方法与运算符重载实现的。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219160503820.png" alt="image-20231219160503820" /><figcaption aria-hidden="true">image-20231219160503820</figcaption></figure><p>多态的好处就是，当我们需要传入Dog、Cat、Tortoise……时，我们只需要接收Animal类型就可以了，因为Dog、Cat、Tortoise……都是Animal类型，然后，按照Animal类型进行操作即可。由于Animal类型有run()方法，因此，传入的任意类型，只要是Animal类或者子 类，就会自动调用实际类型的run()方法，这就是多态的意思。</p><p>对于一个变量，我们只需要知道它是Animal类型，无需确切地知道它的子类型，就可以放心地调用run()方法，而具体调用的run()方法是作用在Animal、Dog、Cat还是Tortoise对象上，由运行时该对象的确切类型决定，这就是多态真正的威力：</p><p><font color=#985fff>调用方只管调用，不管细节，而当我们新增一种Person的子类时，只要确保新方法编写正确，而不用管原来的代码。这就是著名的<strong>“开闭”原则</strong>：</font></p><p><font color=#985fff><strong>对扩展开放（Open for extension）：允许子类重写方法函数</strong></font> <font color=#985fff><strong>对修改封闭（Closed for modification）：不重写，直接继承父类方法函数</strong></font></p><h3 id="多重继承">多重继承</h3><p>Python支持多重继承，即一个类可以继承多个父类。多重继承的语法格式： <code>class_name(parent_class1, parent_class2…)</code>。其中class_name是类名，parent_class1和parent_class2是父类名。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219160947542.png" alt="image-20231219160947542" /><figcaption aria-hidden="true">image-20231219160947542</figcaption></figure><h4 id="多重继承关系中的构造函数">多重继承关系中的构造函数</h4><p>子类从多个父类派生，而子类又没有自己的构造函数时， （1）按顺序继承，哪个父类在最前面且它又有自己的构造函数，就继承它的构造函数； （2）如果最前面第一个父类没有构造函数，则继承第2个的构造函数，第2个没有的话， 再往后找，以此类推。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231219161134386.png" alt="image-20231219161134386" /><figcaption aria-hidden="true">image-20231219161134386</figcaption></figure><h2 id="抽象基类">抽象基类</h2><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/kkk" alt="image-20231218165633785" /><figcaption aria-hidden="true">image-20231218165633785</figcaption></figure><ol type="1"><li>抽象基类是对一类事物的特征行为的抽象，由抽象方法组成。在Python3中可以使用<font color=#985fff>abc模块</font>，该模块中有一个<font color=#985fff>元类ABCMeta</font>和<font color=#985fff>修饰器 @ abstractmethod</font>。抽象基类不能被直接实例化。</li><li><strong>继承抽象类的子类必须重写抽象函数</strong>。</li></ol><h2 id="组合">组合</h2><p>如果要新建的类有相似的类可以继承的話就可以采用继承来取得父类的所有， 但若两个类差异太大，或是沒有关系，我们就可以采用组合來合并这些类</p><p>例如，鸭子是鸟的一种，所以可以继承鸟的类， 但是嘴巴和尾巴不是鸟的一种，而是鸭子的组成。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Bill</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, description</span>):</span><br><span class="line">        self.description = description</span><br><span class="line">        </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tail</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, length</span>):</span><br><span class="line">        self.length = length</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Duck</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, bill, tail</span>):</span><br><span class="line">        self.bill = bill</span><br><span class="line">        self.tail = tail</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">about</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;这只鸭子有一个&#x27;</span>, bill.description, <span class="string">&#x27;嘴巴，然后有&#x27;</span>, tail.length, <span class="string">&#x27;长的尾巴&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">bill = Bill(<span class="string">&#x27;红色的&#x27;</span>)</span><br><span class="line">tail = Tail(<span class="string">&#x27;白色，15cm&#x27;</span>)</span><br><span class="line"></span><br><span class="line">duck = Duck(bill, tail)</span><br><span class="line">duck.about()</span><br></pre></td></tr></table></figure><p>这只鸭子有一个 红色的 嘴巴，然后有 白色，15cm 长的尾巴</p><h2 id="何时使用类和对象而不是模块">何时使用类和对象而不是模块</h2><p>有一些方法可以帮助你決定是把你的代码封裝到类里还是模块里。</p><ul><li>当你需要许多具有相似行为（方法），但不同状态（特性）的实例时，使用对象是最好的选择。</li><li>类支持继承，但模块不支持。</li><li>如果你想要保证实例的唯一性，使用模块是最好的选择。不管模块在程序中被引用多少次，始终只有一个实例被加载。</li><li>如果你有一系列包含多个值的变量，并且他们能作为参数传入不同的函数，那么最好將它们封裝到类里面。举个例子，你可能会使用以大小和颜色为键的字典代表一张 彩色图片。你可以在程序中为每张图片创建不同的字典，并把它们作为参数传递给像规模（）或者变换（）之类的函数。但这么做的话，一旦你想要添加其他的键或者函数会变得非常麻烦。为了保证统一性，应该定义一个图片类，把大小和颜色作为特性，把规模（）和变换（）定义为方法。这么一来，关于一张图片的所有数据和可执行的操作都存储在了统一的位置。</li><li>用最简单的方式解決问题。使用字典，列表和元組往往要比使用模块更加简单，简介且快速。而使用类则更为复杂。</li></ul><h3 id="命名tuplenamed-tuple">命名Tuple(named tuple)</h3><p>可以用来创造可以用名称访问的Tuple子类</p><p>跟Tuple一样，不可被改变，但是可以透过替换來产生新的命名Tuple</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple <span class="comment">#引入函数库</span></span><br><span class="line"></span><br><span class="line">Duck = namedtuple(<span class="string">&#x27;Duck&#x27;</span>, <span class="string">&#x27;bill tail&#x27;</span>) <span class="comment">#定义为命名Tuple，並且有bill和tail两种名称</span></span><br><span class="line">duck = Duck(<span class="string">&#x27;wide orange&#x27;</span>, <span class="string">&#x27;long&#x27;</span>)     <span class="comment">#赋值</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(duck)</span><br><span class="line"><span class="built_in">print</span>(duck.bill)</span><br><span class="line"><span class="built_in">print</span>(duck.tail)</span><br><span class="line"></span><br><span class="line">parts = &#123;<span class="string">&#x27;bill&#x27;</span>: <span class="string">&#x27;wide orange&#x27;</span>, <span class="string">&#x27;tail&#x27;</span>: <span class="string">&#x27;long&#x27;</span>&#125;  <span class="comment">#使用dictionary赋值</span></span><br><span class="line">duck2 = Duck(**parts)  <span class="comment">#键名相同，因此可以用**提取字典内容导入</span></span><br><span class="line"><span class="built_in">print</span>(duck2)</span><br><span class="line"></span><br><span class="line">duck3 = duck2._replace(tail=<span class="string">&#x27;magnificent&#x27;</span>, bill=<span class="string">&#x27;crushing&#x27;</span>)  <span class="comment">#替换內容</span></span><br><span class="line"><span class="built_in">print</span>(duck3)</span><br></pre></td></tr></table></figure><p>Duck(bill='wide orange', tail='long') wide orange long Duck(bill='wide orange', tail='long') Duck(bill='crushing', tail='magnificent')</p><blockquote><p>学习资料链接：</p><p>链接：https://pan.baidu.com/s/1fg10feuxUbm4bdk5faX9MA?pwd=9k78 提取码：9k78 --来自百度网盘超级会员V1的分享</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> course </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2023/12/31/DVGO%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/12/31/DVGO%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p><strong>这篇文章的主要贡献点在于两点：</strong></p><ol type="1"><li><p><strong>在体素密度直接优化中，采用了两个先验算法来避免几何陷入局部最优解：</strong>直接优化密度体素网格会导致超快收敛，但容易出现次优解，所提出方法在自由空间分配"云"，并试图将光度损失与云拟合，而不是搜索具有更好多视图一致性的几何。对这个问题的解决方案简单而有效。<u>首先</u>，初始化密度体素网格，以产生非常接近于零的不透明度，以避免几何解决方案偏向于相机的附近平面。<u>其次</u>，给较少视图可见的体素一个较低的学习率，可以避免仅为解释少量视图的观察而分配的冗余体素。所提出的解决方案可以成功地避免次优几何，并在五个数据集上表现良好。</p></li><li><p><strong>提出了先插值后激活的体素网格插值，它可以在较低的网格分辨率下实现清晰的边界建模：</strong>之前的工作要么对激活的不透明度进行体素网格插值，要么使用最近邻插值，从而在每个网格单元中产生光滑的表面。从数学和经验上证明，所提出的后激活可以在单个网格单元内建模(超越)尖锐的线性表面。因此，可以使用更少的体素来实现更好的质量——具有160^3个密集体素的方法在大多数情况下已经优于NeRF。</p></li></ol><p><strong>框架：</strong></p><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240101105729136.png" alt="image-20240101105729136" /><figcaption aria-hidden="true">image-20240101105729136</figcaption></figure><p>softplus激活函数：</p><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240101125119035.png" alt="image-20240101125119035" /><figcaption aria-hidden="true">image-20240101125119035</figcaption></figure><p><strong>inward-facing</strong></p><blockquote><p><a href="https://immortalqx.github.io/2022/08/22/nerf-notes-3/#!">(ฅ&gt;ω&lt;*ฅ) 噫？又好了~ (immortalqx.github.io)</a></p></blockquote><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240101144152110.png" alt="image-20240101144152110" /><figcaption aria-hidden="true">image-20240101144152110</figcaption></figure><p>文献34</p><figure><img src="C:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20240101112426521.png" alt="image-20240101112426521" /><figcaption aria-hidden="true">image-20240101112426521</figcaption></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>复现代码指南</title>
      <link href="/2023/12/27/%E5%A4%8D%E7%8E%B0%E4%BB%A3%E7%A0%81%E6%8C%87%E5%8D%97/"/>
      <url>/2023/12/27/%E5%A4%8D%E7%8E%B0%E4%BB%A3%E7%A0%81%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<h1 id="复现代码指南">复现代码指南</h1><p>安装环境时需要注意<strong>Python版本</strong>、<strong>Pytorch版本</strong>和<strong>CUDA版本</strong>是否兼容</p><h2 id="python修改pip源">Python修改pip源</h2><h3 id="一步到位版">一步到位版</h3><p>参考网址：<a href="https://www.jianshu.com/p/b2412f7fc93f">pip换源一行命令直接搞定 - 简书 (jianshu.com)</a></p><p>打开cmd,输入：<code>pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</code></p><p>或者：<code>pip install 安装包名 -i https://pypi.tuna.tsinghua.edu.cn/simple/</code></p><p>如果临时使用的话，可以使用：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install +库名 -i +源</span><br><span class="line">eg:    pip install numpy -i http://mirrors.aliyun.com/pypi/simple/</span><br></pre></td></tr></table></figure><h3 id="几个国内源">几个国内源</h3><p>阿里云 <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fmirrors.aliyun.com%2Fpypi%2Fsimple%2F">http://mirrors.aliyun.com/pypi/simple/</a> 中国科技大学 <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fpypi.mirrors.ustc.edu.cn%2Fsimple%2F">https://pypi.mirrors.ustc.edu.cn/simple/</a> 豆瓣(douban) <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fpypi.douban.com%2Fsimple%2F">http://pypi.douban.com/simple/</a> 清华大学 <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fpypi.tuna.tsinghua.edu.cn%2Fsimple%2F">https://pypi.tuna.tsinghua.edu.cn/simple/</a> 中国科学技术大学 <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fpypi.mirrors.ustc.edu.cn%2Fsimple%2F">http://pypi.mirrors.ustc.edu.cn/simple/</a></p><h2 id="离线安装pytorch">离线安装Pytorch</h2><p>参考网址：<a href="https://blog.csdn.net/weixin_47142735/article/details/113684365">离线安装Pytorch 最简单 高效的方法_pytorch离线安装_正在学习的浅语的博客-CSDN博客</a></p><blockquote><p>在线安装的一些指令：</p><ul><li>安装torch1.10.1: pip --default-timeout=1000 install torch==1.10.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html</li><li>安装 torch_scatter：pip install torch_scatter==2.0.9 --extra-index-url https://download.pytorch.org/whl/cu111</li><li>同时安装多个包：pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116</li></ul></blockquote><p>在线下载安装包，特别是比较大的安装包，很容易因为网络原因失败:</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231202203943.png" alt="image-20231202203938666" /><figcaption aria-hidden="true">image-20231202203938666</figcaption></figure><p>所以我们考虑先将需要的包下载后，再离线安装。下载地址：<span style="background:#fbd4d0;"><a href="http://download.pytorch.org/whl/torch_stable.html">download.pytorch.org/whl/torch_stable.html</a></span></p><blockquote><p>附上pytorch其他相关包的下载地址：</p><ul><li><p><a href="https://pytorch-geometric.com/whl/">pytorch-geometric.com/whl/</a> （下载 torch_scatter）</p></li><li><p><a href="https://mmcv.readthedocs.io/zh-cn/latest/get_started/installation.html">安装 MMCV — mmcv 2.1.0 文档</a></p></li></ul></blockquote><h3 id="下载离线安装包">下载离线安装包</h3><p>点进去后有pytorch安装包、torchaudio安装包和torchvision安装包等，可以通过<span style="background:#eef0f4;">Ctrl + F</span>寻找需要下载的包。主要有两种pytorch安装包（<span style="background:#f9eda6;">一般只使用gpu版本的安装包</span>）：</p><ol type="1"><li><p>cpu版本pytorch,<font color=red>开头为cpu</font>;</p></li><li><p>gpu版本pytorch,<font color=red>开头为cu</font>，如cu111表示gpu版本pytorch，且该pytorch的cuda版本为11.1;</p></li></ol><p>cp表示python版本，linux/window 表示系统版本。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231202204318.png" alt="cpu" /><figcaption aria-hidden="true">cpu</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231202204355.png" alt="gpu" /><figcaption aria-hidden="true">gpu</figcaption></figure><p><strong>注意，我们要根据自己cuda的版本和系统版本来下载安装包，且一定要使Python版本、Pytorch版本和CUDA版本三者兼容！！！</strong>下面附一张版本对应关系表（也可以去问chatgpt）： <img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231202212206.png" alt="cuda_pytorch" /></p><blockquote><ol type="1"><li>torchvision 与 pytorch 版本对应关系（如果直接安装torchvision，可能会自动安装最新的版本，同时也把torch升级到最新）</li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240118150625277.png" alt="image-20240118150625277" /><figcaption aria-hidden="true">image-20240118150625277</figcaption></figure><ol start="2" type="1"><li>torch_scatter 与 pytorch版本对应关系：见<a href="https://pytorch-geometric.com/whl/">pytorch-geometric.com/whl/</a></li></ol></blockquote><h3 id="离线安装以pycharm安装为例">离线安装（以Pycharm安装为例）</h3><p>打开Pycharm中的终端，切换到需要安装包的环境，先使用<span style="background:#dad5e9;">cd指令</span>跳转到下载文件夹；然后<span style="background:#d4e9d5;">pip install 安装包名.后缀</span>进行安装；最后可使用<span style="background:#f9eda6;">conda list 或 python - import torch - torch.cuda.is_available()（需要完成cuda+cudnn+torch配套安装）</span>进行查看。<span style="background:#fbd4d0;">大功告成！</span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231202212635162.png" alt="image-20231203191239667" /><figcaption aria-hidden="true">image-20231203191239667</figcaption></figure><p><strong>值得注意的是，一定要使用纯英文的路径，并且安装包的名字不饿能有任何变动，这些问题都会导致安装的失败。</strong>如果在终端中直接安装，别忘了先用<span style="background:#eef0f4;">conda activate</span>先激活环境。</p><h2 id="安装cuda">安装cuda</h2><h3 id="安装过程">安装过程</h3><p>cuda官网下载地址：<span style="background:#fbd4d0;"><a href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA Toolkit Archive | NVIDIA Developer</a></span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20231203184824.png" alt="image-20231203184822148" /><figcaption aria-hidden="true">image-20231203184822148</figcaption></figure><p>按照图中白色位置选择下载包，然后选择自定义安装。</p><p>在文件资源管理器中创建一个名为cuxx的文件夹，用来存放cuda；然后设置安装路径</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203190515923.png" alt="image-20231203190515923" /><figcaption aria-hidden="true">image-20231203190515923</figcaption></figure><p>检查完系统兼容性后，点击<span style="background:#d4e9d5;">同意并继续</span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203190837475.png" alt="image-20231202212635162" /><figcaption aria-hidden="true">image-20231202212635162</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203190858663.png" alt="image-20231203190858663" /><figcaption aria-hidden="true">image-20231203190858663</figcaption></figure><p>勾选<span style="background:#d4e9d5;">自定义</span>，点击<span style="background:#d4e9d5;">下一步</span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203191209483.png" alt="image-20231203191209483" /><figcaption aria-hidden="true">image-20231203191209483</figcaption></figure><p>第一次安装需要把组件都勾选上</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203191239667.png" alt="image-20231203190837475" /><figcaption aria-hidden="true">image-20231203190837475</figcaption></figure><p><span style="background:#fbd4d0;"><strong>选择安装位置，并记住安装路径：</strong></span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203191504491.png" alt="image-20231203192352716" /><figcaption aria-hidden="true">image-20231203192352716</figcaption></figure><p>等待安装完成</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203191624776.png" alt="image-20231203193341199" /><figcaption aria-hidden="true">image-20231203193341199</figcaption></figure><h3 id="验证是否安装成功">验证是否安装成功</h3><p>打开<span style="background:#dad5e9;">设置 - 系统 - 系统信息 - 高级系统设置 - 高级 - 环境变量</span>，出现橙色框中的两个环境变量（也可在cmd中输入<code>set cuda</code>查看cuda设置的环境变量）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203192352716.png" alt="image-20231203191624776" /><figcaption aria-hidden="true">image-20231203191624776</figcaption></figure><p><span style="background:#f9eda6;">重启电脑！！（否则nvcc -V会找不到指令）</span>进入cmd界面，输入 <code>nvcc -V / nvcc --version</code> 查看版本号，出现如下界面，说明 cuda 安装成功啦！</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203193341199.png" alt="image-20231203191504491" /><figcaption aria-hidden="true">image-20231203191504491</figcaption></figure><h2 id="安装cudnn">安装cudnn</h2><p>注：cudnn是用于配置深度学习使用，相当于cuda的一个专为深度学习运算进行优化的补丁</p><h3 id="安装过程-1">安装过程</h3><p>cudnn 官方下载地址：<span style="background:#d4e9d5;"><a href="https://developer.nvidia.com/rdp/cudnn-download">cuDNN Download | NVIDIA Developer</a></span></p><p>进入cudnn下载时需要注册或登录账号，然后选择需要对应版本下载安装包。这里下载的是压缩后的安装包。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203193624549.png" alt="image-20231203193813587" /><figcaption aria-hidden="true">image-20231203193813587</figcaption></figure><p>解压后出现三个文件夹和License</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203193813587.png" alt="image-20231203193624549" /><figcaption aria-hidden="true">image-20231203193624549</figcaption></figure><p><span style="background:#fbd4d0;">找到cuda的安装路径，如：<strong>C:FilesGPU Computing Toolkit1.1</strong></span>。将cudnn三个文件夹里的内容分别替换到对应的文件夹里。</p><p>打开<span style="background:#dad5e9;">设置 - 系统 - 系统信息 - 高级系统设置 - 高级 - 环境变量</span>，再找到<span style="background:#d4e9d5;">系统变量 - path</span>，将以下三个变量添加进去，完成安装(<font color=red>注意修改对应变量名喔</font>)</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\bin</span><br><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\include</span><br><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\lib</span><br><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\libnvvp</span><br></pre></td></tr></table></figure><h3 id="验证是否安装成功-1">验证是否安装成功</h3><p>在cmd中使用<span style="background:#d4e9d5;">cd命令</span>进入cuda的安装路径找到测试工具，如：<span style="background:#f9eda6;">C:FilesGPU Computing Toolkit1.1_suite</span>，执行如下两个.exe文件</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231203195334390.png" alt="image-20231203195334390" /><figcaption aria-hidden="true">image-20231203195334390</figcaption></figure><p>执行：<code>deviceQuery.exe</code>查询本机的GPU设备和<code>bandwidthTest.exe</code>测试带宽，如果结果都为PASS，说明运行正常</p><h1 id="问题汇总">问题汇总：</h1><h2 id="cuda-driver-version-is-insufficient-for-cuda-runtime-version"><strong>CUDA driver version is insufficient for CUDA runtime version</strong></h2><p>cuda驱动程序版本和cuda运行时版本不匹配：问题可大可小，可能是程序装错了，也可能你的电脑根本配不了cuda环境（如amd显卡）</p><h2 id="failed-to-initialize-nvml-unknown-error"><strong>Failed to initialize NVML: Unknown Error</strong></h2><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216143546376.png" alt="image-20231216143546376" /><figcaption aria-hidden="true">image-20231216143546376</figcaption></figure><p>在复现代码时，配置完cuda后，想验证<code>torch.cuda.is_available()</code>，结果过了很久也没有输出；然后又输入<code>nvidia-smi</code>查看显卡配置，报错！</p><p>在网上查找后发现了几个方法：</p><h3 id="win11下运行nvidia-smi报错failed-to-initialize-nvml-unknown-error-csdn博客"><strong><a href="https://blog.csdn.net/qq_39499680/article/details/134855395">WIN11下运行nvidia-smi报错Failed to initialize NVML: Unknown Error-CSDN博客</a></strong></h3><p>使用Everything(或电脑自带的文件搜索)去查找nvidia-smi的位置，然后进入文件目录（如：<span style="background:#d4e9d5;">C:_dispig.inf_amd64_49aadc39d4f73881</span>），点击运行 <span style="background:#f9eda6;color:red">setup.exe</span> 重新安装即可。</p><p>尝试方法一，在点击 setup.exe 后报错：<span style="background:#fb6172d0;color:white">所需文件丢失</span></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216144502672.png" alt="image-20231216144502672" /><figcaption aria-hidden="true">image-20231216144502672</figcaption></figure><h3 id="考虑重新安装nvidia驱动"><strong>考虑重新安装nvidia驱动</strong></h3><h4 id="卸载驱动">2.1 卸载驱动</h4><p>打开<font color=purple>设备管理器</font>，点击<span style="background:#dad5e9;">显卡适配器-驱动程序-卸载设备</span>，然后重启电脑</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216151900323.png" alt="image-20231216151900323" /><figcaption aria-hidden="true">image-20231216151900323</figcaption></figure><p>发现这时显卡已经变为<font color=purple>Microsoft基本显示适配器</font>：</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216152946150.png" alt="image-20231216152946150" /><figcaption aria-hidden="true">image-20231216152946150</figcaption></figure><h4 id="重新安装驱动">2.2 重新安装驱动</h4><p>打开nvidia驱动下载地址：<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn#">官方驱动 | NVIDIA</a>，按自己的电脑型号选择合适的驱动（笔记本产品系列是<font color=red>notebooks</font>），开始下载</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216153411364.png" alt="image-20231216153411364" /><figcaption aria-hidden="true">image-20231216153411364</figcaption></figure><p>完成下载后双击，在解压提示框中点击OK，等待进度完成（此处默认安装路径是：<font color=orange>C:\546.33_Win10-DCH_64</font>）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154244543.png" alt="image-20231216154244543" /><figcaption aria-hidden="true">image-20231216154244543</figcaption></figure><p>等待检查系统兼容性完成，后续工作见图</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154626870.png" alt="image-20231216154626870" /><figcaption aria-hidden="true">image-20231216154626870</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154713103.png" alt="image-20231216154713103" /><figcaption aria-hidden="true">image-20231216154713103</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154731465.png" alt="image-20231216154731465" /><figcaption aria-hidden="true">image-20231216154731465</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154826092.png" alt="image-20231216154826092" /><figcaption aria-hidden="true">image-20231216154826092</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216154918798.png" alt="image-20231216154918798" /><figcaption aria-hidden="true">image-20231216154918798</figcaption></figure><blockquote><p>安装快完成时会提示<strong>是否立即重启计算机完成安装</strong>，点击确定前记得保存未保存的文件</p></blockquote><p>耐心等待安装完成后，再次打开<strong>设备管理器</strong>，即可发现安装成功！</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216155635451.png" alt="image-20231216155635451" /><figcaption aria-hidden="true">image-20231216155635451</figcaption></figure><p>我们打开终端，输入<code>nvidia-smi</code>,成功输出显卡驱动信息</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216155757435.png" alt="image-20231216155757435" /><figcaption aria-hidden="true">image-20231216155757435</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 环境搭建 </tag>
            
            <tag> cuda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计学习及监督学习概论第一章 读书笔记</title>
      <link href="/2023/12/25/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/12/25/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="统计学习">统计学习</h1><p><strong>统计学习(statistical learning)</strong>，也称统计机器学习(statistical machine learning)，是关于计算机基于<u>数据</u>构建<u>概率统计模型</u>并运<u>用模型对数据进行预测与分析</u>的一门学科。现在,当人们提及机器学习时,往往是指统计机器学习。所以可以认为本书介绍的是机器学习方法。</p><p>统计学习研究的对象是<strong>数据(data)</strong>。它从数据出发,提取数据的特征,抽象出数 据的模型,发现数据中的知识,又回到对数据的分析与预测中去。统计学习关于数据的基本假设是<u>同类数据具有一定的统计规律性</u>,这是统计学习的前提。比如,<em>可以用随机变量描述数据中的特征,用概率分布描述数据的统计规律</em>。在统计学习中,以变量或变量组表示数据。数据分为由<u>连续变量</u><u>和表示的类型</u>。本书以讨论离散变量的方法为主。另外,本书只涉及利用数据构建模型及利用模型对数据进行分析与预测,对数据的观测和收集等问题不作讨论。</p><p>统计学习的方法是<u>基于数据构建概率统计模型从而对数据进行预测与分析</u>。统计学习由<strong>监督学习(supervised learning)</strong>、<strong>无监督学习(unsupervised learning)</strong>和<strong>强化学习(reinforcement learning)</strong>等组成。其中，监督学习、无监督学习方法是最主要的统计学习方法。</p><p>统计学习方法可以概括如下:从<u>给定的、有限的、用于学习</u>的训练数据(training data)集合出发,假设数据是独立同分布产生的;并且假设要学习的模型属于某个函数的集合,称为<font color=#df8400>假设空间(hypothesis space)</font>;应用某个<font color=#0091ff>评价准则(evaluation criterion)</font>,从假设空间中选取一个最优模型,使它对已知的训练数据及未知的测试数据(test data)在给定的评价准则下有最优的预测; 最优模型的选取由<font color=#4eb434>算法</font>实现。这样,统计学习方法包括<font color=#df8400>模型的假设空间</font>、<font color=#0091ff>模型选择的准则</font>以及<font color=#4eb434>模型学习的算法</font>。称其为统计学习方法的三要素,简称为<font color=#df8400><strong>模型(model)</strong></font>、<font color=#0091ff><strong>策略(strategy)</strong></font>和<font color=#4eb434><strong>算法(algorithm)</strong></font>。</p><p><strong>实现统计学习方法的步骤如下:</strong> (1) 得到一个有限的<u>训练数据集合</u>; (2)确定包含所有可能的<u>模型的假设空间</u>,即学习模型的集合; (3)确定模型选择的准则,即<u>学习的策略/选择最优模型的评价准则</u>; (4)实现求解最优模型的算法,即<u>学习的算法/选取最优模型的方法</u>; (5)通过学习方法选择最优模型; (6)利用学习的最优模型对新数据进行预测或分析。</p><p>本书第1篇介绍监督学习方法,主要包括用于<u>分类、标注与回归问题的方法</u>。这些方法在<u>自然语言处理、信息检索、文本数据挖掘等领域</u>中有着极其广泛的应用。统计学习研究一般包括统计学习方法、统计学习理论及统计学习应用三个方面。统计学习是计算机科学发展的一个重要组成部分。可以认为计算机科学由三维组成: <u>系统、计算、信息</u>。统计学习主要属于<u>信息</u>这一维,并在其中起着核心作用。</p><h2 id="统计学习的分类">统计学习的分类</h2><h3 id="基本分类">基本分类</h3><p>统计学习或机器学习一般包括<u>监督学习</u>、<u>无监督学习</u>、<u>强化学习</u>。有时还包括半 监督学习、主动学习。</p><h4 id="监督学习">监督学习</h4><p>监督学习(supervised learning)的本质是学习输入到输出的映射的统计规律。</p><hr /><p>几个概念：</p><ul><li><strong>输入空间(input space)</strong>：输入所有可能取值的集合</li><li><strong>输出空间(output space)</strong>：输出所有可能取值的集合</li><li><strong>实例(instance)</strong>：每个具体的输入是一个实例，通常由特征向量(feature vector)表示。</li><li><strong>特征空间(feature space)</strong>：所有特征向量存在的空间称为特征空间。特征空间的每一维对应于一个特征。模型实际上都是定义在特征空间上的。</li></ul><hr /><p>在监督学习中,将输入与输出看作是定义在输入(特征)空间与输出空间上的随机变量的取值。输入输出变量用大写字母表示,习惯上输入变量写作X,输出变量写作Y。输入输出变量的取值用小写字母表示,输入变量的取值写作x,输出变量的取值写作y。变量可以是标量或向量,都用相同类型字母表示。除特别声明外,本书中向量均为列向量。</p><p>输入变量X和输出变量Y有不同的类型,可以是连续的,也可以是离散的。人们根据输入输出变量的不同类型,对预测任务给予不同的名称：</p><ul><li><strong>回归问题</strong>：输入变量与输出变量均为连续变量的预测问题;</li><li><strong>分类问题</strong>：输出变量为有限个离散变量的预测问题;</li><li><strong>标注问题</strong>：输入变量与输出变量均为变量序列的预测问题。</li></ul><h4 id="无监督学习">无监督学习</h4><p>无监督学习(unsupervised learning)是指从无标注数据中学习预测模型的机器学习问题。无标注数据是自然得到的数据,预测模型表示数据的类别、转换或概率。无监督学习的本质是学习数据中的统计规律或潜在结构。</p><h4 id="强化学习">强化学习</h4><p>强化学习(reinforcement learning)是指智能系统在与环境的连续互动中学习 最优行为策略的机器学习问题。假设智能系统与环境的互动基于马尔可夫决策过 程(Markov decision process),智能系统能观测到的是与环境互动得到的数据序列。 强化学习的本质是学习最优的序贯决策。</p><h4 id="半监督学习">半监督学习</h4><p>半监督学习(semi-supervised learning)是指利用标注数据和未标注数据学习预 测模型的机器学习问题。通常有少量标注数据、大量未标注数据,因为标注数据的构 建往往需要人工,成本较高,未标注数据的收集不需太多成本。半监督学习旨在利用 未标注数据中的信息,辅助标注数据,进行监督学习,以较低的成本达到较好的学习 效果。</p><h4 id="主动学习">主动学习</h4><p>主动学习(active learning)是指机器不断主动给出实例让教师进行标注,然后利 用标注数据学习预测模型的机器学习问题。通常的监督学习使用给定的标注数据,往 往是随机得到的,可以看作是“被动学习”,主动学习的目标是找出对学习最有帮助的 实例让教师标注,以较小的标注代价,达到较好的学习效果。</p><blockquote><p>半监督学习和主动学习更接近监督学习。</p></blockquote><h3 id="按模型分类">按模型分类</h3><h4 id="概率模型与非概率模型">概率模型与非概率模型</h4><p>统计学习的模型可以分为概率模型(probabilistic model)和非概率模型(nonprobabilistic model)或者确定性模型(deterministic model)。</p><h4 id="线性模型与非线性模型">线性模型与非线性模型</h4><p>统计学习模型,特别是非概率模型,可以分为线性模型(linear model)和非线性模型(non-linear model)。如果函数y=f(x)或z=g(x)是线性函数,则称模型是线 性模型,否则称模型是非线性模型。</p><h4 id="参数化模型与非参数化模型">参数化模型与非参数化模型</h4><p>统计学习模型又可以分为参数化模型(parametric model)和非参数化模型(nonparametric model)。参数化模型假设模型参数的维度固定,模型可以由有限维参数完全刻画;非参数化模型假设模型参数的维度不固定或者说无穷大,随着训练数据量的增加而不断增大。</p><p>参数化模型适合问题简单的情况,现实中问题往往比较复杂,非参数化模型更加有效。</p><h3 id="按算法分类">按算法分类</h3><p>统计学习根据算法,可以分为在线学习(online learning)与批量学习(batch learning)。在线学习是指每次接受一个样本,进行预测,之后学习模型,并不断重复该操作的机器学习。与之对应,批量学习一次接受所有数据,学习模型,之后进行预测。有些实际应用的场景要求学习必须是在线的。比如,数据依次达到无法存储,系统需要及时做出处理;数据规模很大,不可能一次处理所有数据;数据的模式随时间动态变化,需要算法快速适应新的模式(不满足独立同分布假设)。</p><p>在线学习通常比批量学习更难,很难学到预测准确率更高的模型,因为每次模型更新中,可利用的数据有限。</p><h3 id="按技巧分类">按技巧分类</h3><h4 id="贝叶斯学习">贝叶斯学习</h4><p>贝叶斯学习(Bayesian learning), 又称为贝叶斯推理(Bayesian inference), 是 统计学、机器学习中重要的方法。其主要想法是, <u>在概率模型的学习和推理中, 利用贝</u> <u>叶斯定理, 计算在给定数据条件下模型的条件概率,即后验概率, 并应用这个原理进</u><u>行模型的估计, 以及对数据的预测</u>。将模型、未观测要素及其参数用变量表示, 使用模型的先验分布是贝叶斯学习的特点。贝叶斯学习中也使用基本概率公式(图1.4)。</p><p>本书介绍的<u>朴素贝叶斯、潜在狄利克雷分配的学习</u>属于贝叶斯学习。</p>]]></content>
      
      
      <categories>
          
          <category> 原理笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3D Gaussian Splatting for Real-Time Radiance Field Rendering 论文笔记</title>
      <link href="/2023/12/20/3D%20Gaussian%20Splatting%20for%20Real-Time%20Radiance%20Field%20Rendering%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/12/20/3D%20Gaussian%20Splatting%20for%20Real-Time%20Radiance%20Field%20Rendering%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p><strong>数据集</strong>：Mip-NeRF360 数据集、Deep Blending 数据集[Hedman et al. 2018]</p><h1 id="abstract">Abstract</h1><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231220144503261.png" alt="image-20231220144503261" /><figcaption aria-hidden="true">image-20231220144503261</figcaption></figure><p>优化时间与<strong>InstantNGP</strong>相当 优化质量与<strong>光学体素</strong>相当 训练时间达到51分钟时，我们的质量达到最好，甚至比Mip-NeRF更胜一筹</p><p>mip_splatting</p><p>我们引入了三个关键元素，使我们能够在保持竞技训练时间的同时实现最先进的视觉质量，重要的是允许在1080p分辨率下实现高质量的实时(≥30 fps)新视图合成。<strong>首先</strong>，从相机标定过程中产生的稀疏点云出发，用三维高斯模型表示场景，该模型保留了场景优化所需的连续体亮度场特性，同时避免了在空空间中进行不必要的计算; <strong>其次</strong>，我们对三维高斯模型进行了交错优化/密度控制，特别是优化了各向异性协方差，以实现对场景的准确表示; <strong>最后</strong>，我们开发了一个快速的可视性感知的渲染算法，支持各向异性喷溅，既加速训练过程，又允许实时渲染。我们在几个已建立的数据集上展现出了最先进的视觉质量和实时渲染。</p><p>其他关键词和短语: <strong>新视图合成(novel view synthesis)，亮度场(radiance fields)，三维高斯(3D gaussians)，实时渲染(real-time rendering)</strong></p><h1 id="introduction">Introduction</h1><p>网格和点是最常见的3D场景表示，因为它们是显式的，并且非常适合基于GPU/ cuda的快速栅格化。相比之下，最近的神经辐射场(NeRF)方法建立在连续场景表示的基础上，通常使用体积射线行进优化多层感知器(MLP)，以获得捕获场景的新视图合成。类似地，迄今为止最有效的亮度场解决方案建立在连续表示的基础上，通过插值存储在体素[fridovic - keil和Yu等人，2022]或哈希[Müller等人，2022]网格或点[Xu等人，2022]中的值。虽然这些方法的连续性质有助于优化，但渲染所需的随机采样代价高昂，并可能导致噪声。我们引入了一种新方法，它结合了两个方面的优点: 我们的3D高斯表示允许优化最先进的(SOTA)视觉质量和有竞争性的训练时间，而我们的基于瓦片的喷溅解决方案确保在几个之前发布的数据集上以1080p分辨率的SOTA质量进行实时渲染[Barron等，2022; Hedman等人2018年;Knapitsch等人，2017](见图1)。</p><p>我们的目标是允许对多张照片捕获的场景进行实时渲染，并在优化时间内创建典型真实场景的表示，速度与之前最有效的方法一样快。最近的方法实现了快速训练[fridovic - keil和Yu等人，2022;Müller等人，2022]，但很难达到目前SOTA NeRF方法，即Mip-NeRF360 [Barron等人，2022]获得的视觉质量，这需要多达48小时的训练时间。快速但质量较低的亮度场方法可以根据场景实现<u>交互渲染</u>时间(10-15帧/秒)，但在高分辨率的实时渲染上达不到要求。</p><blockquote><p>fridovic - keil和Yu等人，2022;Müller等人，2022</p><p>InstantNGP <strong>光学体素</strong></p></blockquote><p>我们的解决方案构建在三个主要组件上。<font color=#985fff>我们首先介绍三维高斯模型作为一种灵活和富有表现力的场景表示。</font>我们从与之前类似nerf的方法相同的输入开始，即使用结构从运动(SfM)校准的相机[Snavely等人，2006]，并使用SfM过程中免费生成的稀疏点云初始化3D高斯模型。与大多数需要多视图立体视觉(MVS)数据的基于点的解决方案相比[Aliev等人，2020;Kopanas等人2021年;Rückert et al. 2022]，我们仅以SfM点作为输入就获得了高质量的结果。注意，<u>对于nerf合成数据集，我们的方法即使在随机初始化的情况下也能实现高质量</u>。我们表明，3D Gaussians是一个非常好的选择，因为它们是可微分的体积表示，但它们也可以非常有效地栅格化，通过将它们投影到2D，并应用标准的𝛼-blending，使用等效的图像生成模型作为NeRF。</p><p><font color=#df8400>我们方法的第二部分</font>是优化<u>三维高斯函数的性质</u>—<span style="background:#fbd4d0;">三维位置，不透明度𝛼，各向异性协方差，和 球面调和(SH)系数</span>—与<u>自适应密度控</u>制步骤交错，在优化过程中我们添加和偶尔删除三维高斯函数。优化过程生成了一个相当紧凑的、非结构化的、精确的场景表示(测试的所有场景的1-5百万高斯值)。</p><p><font color=#4eb434>我们方法的第三个也是最后一个部分是我们的实时渲染解决方案</font>，它使用了快速的GPU排序算法，这是受到基于贴图的栅格化的启发，遵循最近的工作[Lassner和Zollhofer 2021]。然而，由于我们的3D高斯表示，我们可以执行各向异性溅射，它遵循可见性排序——这要归功于排序和𝛼blending——并通过跟踪所需的多个排序splats (splats是排序好的) 的遍历来实现快速和准确的向后遍历。</p><blockquote><p>可以不提供真实相机参数, 运行colmap，可以估计图像的相机参数</p><p>3DGS只有两个数据加载器一个是Blender另一个是colmap数据加载器; Blender数据集用的是随机点云，不是从SFM生成的, blender数据集用的是box内随机生成点云</p><p>抛雪球算法：splatting</p></blockquote><p><strong>我们的贡献主要如下:</strong></p><ul><li><p>引入各向异性三维高斯函数作为高质量、非结构化的亮度场表示。</p></li><li><p>3D高斯属性的优化方法，与自适应密度控制交织，为捕获的场景创建高质量的表示。</p></li><li><p>GPU的一种快速、可微分的渲染方法，它是可见感知的，允许各向异性喷溅和快速反向传播，以实现高质量的新视图合成。</p></li></ul><p>我们在之前发布的数据集上的结果表明，我们可以从多视图捕获的图像中优化3D高斯，并获得与之前的隐式亮度场方法相同或更好的质量。我们还可以达到与最快的方法相似的训练速度和质量，重要的是，为新视图合成提供第一次高质量的实时渲染。</p><p>我们首先简要概述了传统的重建，然后讨论了基于点的渲染和亮度场工作，讨论了它们的相似性;亮度场是一个很大的区域，所以我们只关注直接相关的工作。有关该领域的完整报道，请参阅最近的优秀调查[Tewari等人。2022;谢等。2022]。</p><h2 id="traditional-scene-reconstruction-and-rendering">2.1 Traditional Scene Reconstruction and Rendering</h2><p>第一个新视觉合成方法是基于光场，第一个密集采样[Gortler等人，1996;Levoy和Hanrahan 1996]然后允许非结构化捕获[Buehler等人2001]。结构从运动(SfM)的出现[Snavely等人，2006]使一个全新的领域，一组照片可以用来合成新的视图。SfM在相机标定过程中估计了一个稀疏的点云，最初用于简单的三维空间可视化。随后的多视图立体视觉(MVS)在过去几年里产生了令人印象深刻的完整的三维重建算法[Goesele等人，2007]，使几个视图合成算法的发展成为可能[Chaurasia等人，2013;Eisemann等人2008年;Hedman等人2018年;Kopanas等人，2021]。所有这些方法都将输入图像重新投影和混合到新的视图相机中，并使用几何图形来引导这种重新投影。这些方法在许多情况下都产生了很好的效果，但是当MVS生成不存在的几何时，通常无法从未重建区域或“过度重建”区域完全恢复。最近的神经渲染算法[Tewari等人，2022]大大减少了这些工件，并避免了在GPU上存储所有输入图像的巨大成本，在大多数方面都优于这些方法。</p><blockquote><p>工件 (artifacts, 理解为人为在软件运行过程中造出的阶段性产物，可以是软件代码、文档、图纸、数据等非天然存在的资产，<a href="https://www.zhihu.com/question/455298119">(99+ 封私信 / 80 条消息) artifact 一词在计算机编程里面表示什么意思？ - 知乎 (zhihu.com)</a>)</p></blockquote><h2 id="neural-rendering-and-radiance-fields">2.2 Neural Rendering and Radiance Fields</h2><p>深度学习技术很早就被用于新视角合成[Flynn等人，2016;周等2016;cnn被用于估计混合权重[Hedman等人2018]，或用于纹理空间解决方案[Riegler和Koltun 2020;Thies等。2019]。使用基于MVS的几何是这些方法的一个主要缺点; 此外，使用CNNs进行最终渲染经常会导致时间闪烁(temporal flickering)。</p><p>新颖视图合成的体积表示由Soft3D发起[Penner和Zhang 2017]; 随后提出了深度学习技术和体积射线推进技术[Henzler等人，2019; Sitzmann等人。2019]基于连续可微密度场来表示几何。由于查询体积需要大量的样本，使用体积射线行进进行渲染的成本非常高。神经辐射场(NeRFs) [Mildenhall等人，2020]引入了重要性采样和位置编码来提高质量，但使用了大型多层感知器，对速度产生了负面影响。NeRF的成功导致了后续方法的爆炸式增长，这些后续方法通常通过引入正则化策略来解决质量和速度问题; 当前最先进的新型视图合成图像质量是Mip-NeRF360 [Barron等人，2022]。虽然渲染质量卓越，训练和渲染时间仍然非常高; 在提供快速训练和实时渲染的同时，我们能够达到甚至在某些情况下超过这个质量。</p><p>最近的方法主要通过三种设计选择来实现更快的训练和/或渲染: <font color=#4eb434>使用空间数据结构来存储(神经)特征，这些特征随后在体积射线行进过程、不同的编码和MLP容量中被插值，</font>。这些方法包括空间离散化的不同变体[Chen et al. 2022b,a; fridovic - keil和Yu等人，2022年;Garbin等人2021年; Hedman等人，2021年; Reiser等人，2021年; Takikawa等人，2021年; Wu等人，2022; Yu等人，2021年]，码本[Takikawa等人，2022年]，<font color=#ef042a>编码如哈希表</font>[Müller等人，2022年]，<font color=#0091ff>完全允许使用较小的MLP或先前提到的神经网络</font>[fridovic - keil和Yu等人，2022年; Sun等，2022]。</p><blockquote><p>Plenoxels [fridovic - keil和Yu等人，2022] 即 光学体素</p><p>360场景合成——处理图片并生成带背景的场景</p><p>3dgs用到了两个cuda扩展包，主体还是pytorch</p></blockquote><p>这些方法中最值得注意的是 <strong>InstantNGP</strong> [Müller等人。2022]，它使用哈希网格和占用网格来加速计算，并使用较小的 MLP 来表示密度和外观; 和 <strong>Plenoxels</strong> [fridovic - keil和Yu等人，2022]使用稀疏体素网格插值连续密度场，并能够完全放弃神经网络。<u>两者都依赖于球面谐波: 前者直接表示方向效果，后者将其输入编码到颜色网络</u>。虽然这两种方法都提供了出色的结果，但这些方法仍然难以有效地表示空白空间，这部分取决于场景/捕获类型。此外，图像质量在很大程度上受到用于加速的结构化网格的选择的限制，而渲染速度则因需要为给定的光线推进步骤查询许多样本而受到阻碍。我们使用的非结构化的、显式gpu友好的3D高斯函数，在没有神经组件的情况下，实现了更快的渲染速度和更好的质量。</p><h2 id="point-based-rendering-and-radiance-fields">2.3 Point-Based Rendering and Radiance Fields</h2><p>基于点的方法可以有效地渲染不连接的和非结构化的几何样本(即点云)[Gross和Pfister 2011]。在最简单的形式中，点采样渲染[Grossman和Dally 1998]栅格化一个固定大小的非结构化点集，它可以利用图形API中自然支持的点类型[Sainz和Pajarola 2004]或GPU上的并行软件栅格化[Laine和Karras 2011;Schütz et al. 2022]。虽然对底层数据来说是真实的，但点采样呈现会出现漏洞，导致混叠，并且严格来说是不连续的。基于点的高质量渲染的开创性工作通过“喷溅”大于一个像素的点基元来解决这些问题，例如圆形或椭圆形圆盘、椭球或面元surfels [Botsch et al. 2005;Pfister等人2000年;Ren等人，2002年;Zwicker等人。2001b]。</p><p>最近人们对基于可微点的渲染技术产生了兴趣[Wiles等人，2020;Yifan等，2019。用神经特征增强了点，并使用CNN渲染[Aliev等人，2020;Rückert等人，2022]导致快速甚至实时的视图合成; 然而，他们仍然依赖于MVS得到初始几何，并继承其工件，最明显的过度或重构不足的情况下，如无特征/闪亮的区域或薄结构。</p><p>基于点的𝛼-blending和NeRF风格的体绘制在本质上共享相同的图像生成模型。具体来说，颜色𝐶是由沿着光线的体积渲染给出的: $$</p><p>$$</p><blockquote><p>该公式与volsdf中的公式26下面一个公式类似</p><p>splatting的另外相关论文包括，mip-splatting</p><p>instantngp比mipNeRF跑得好，但是比mipNeRF360差</p></blockquote><h1 id="overview">Overview</h1><p>我们的方法的输入是一组静态场景的图像，以及由SfM [Schönberger和Frahm 2016]校准的相应摄像机，这将产生一个稀疏的点云作为副作用。从这些点出发，我们创建了一组3D高斯(第4节)，由位置(均值)、协方差矩阵和不透明度𝛼定义，这允许一个非常灵活的优化机制。这就产生了一个合理紧凑的3D场景表示，部分原因是高度各向异性的体块可以用来紧凑地表示精细结构。亮度场的方向外观分量(颜色)通过球面谐波(SH)表示，遵循标准实践[fridovic - keil和Yu等人。2022;Müller et al. 2022]。我们的算法通过三维高斯参数的一系列优化步骤，即位置、协方差、𝛼和SH系数与高斯密度的自适应控制操作交织，来创建亮度场表示(第5节)。我们的方法效率的关键是我们的基于tile的光栅化(第6节)，它允许𝛼-blending的各向异性碎片，由于快速排序而尊重可见性顺序。我们的快速光栅化还包括一个快速的向后通过跟踪累积的𝛼值，没有限制高斯的数量，可以接收梯度。我们的方法概述如图2所示。</p><h1 id="differentiable-3d-gaussian-splatting">DIFFERENTIABLE 3D GAUSSIAN SPLATTING</h1><p>我们的目标是优化一个允许高质量的新视图合成的场景表示，从一个没有法线的稀疏集(SfM)点开始。为了做到这一点，我们需要一个原语，它继承了可微体积表示的属性，同时是非结构化和显式的，以允许非常快速的渲染。我们选择3D高斯，这是可微分的，可以很容易地投影到2D splats允许快速𝛼-blending渲染。我们的表示方法与之前使用2D点的方法相似[Kopanas等人，2021;并假设每个点都是一个带【数】法线的平面小圆。由于SfM点的极度稀疏性，很难估计法线。类似地，</p>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维重建 </tag>
            
            <tag> NeRF </tag>
            
            <tag> splatting </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>制作电脑纹理壁纸</title>
      <link href="/2023/12/16/%E8%87%AA%E5%88%B6%E7%94%B5%E8%84%91%E7%BA%B9%E7%90%86%E5%A3%81%E7%BA%B8/"/>
      <url>/2023/12/16/%E8%87%AA%E5%88%B6%E7%94%B5%E8%84%91%E7%BA%B9%E7%90%86%E5%A3%81%E7%BA%B8/</url>
      
        <content type="html"><![CDATA[<h1 id="查看电脑壁纸尺寸">查看电脑壁纸尺寸</h1><p>在桌面右键，选择<font color=orange>显示设置</font></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216200845959.png" alt="image-20231216200845959" /><figcaption aria-hidden="true">image-20231216200845959</figcaption></figure><p>便可查看到目前电脑的分辨率是：<font color=orange>1920×1080</font></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216201136929.png" alt="image-20231216201136929" /><figcaption aria-hidden="true">image-20231216201136929</figcaption></figure><h1 id="准备纹理图片">准备纹理图片</h1><blockquote><p>补充于2023/12/17</p><p>如果选择的纹理图片和电脑显示器尺寸一样，可以跳过这一步哦~</p></blockquote><blockquote><p>纹理图片可参考网站：</p><ol type="1"><li><a href="https://texturelabs.org/?pg=1">Free Textures, Tutorials, and More (texturelabs.org)</a></li><li><a href="https://www.transparenttextures.com/">Transparent Textures</a></li><li><a href="https://www.cgbookcase.com/textures">Textures | cgbookcase.com</a></li><li><a href="https://www.toptal.com/designers/subtlepatterns/">Subtle Patterns | Free textures for your next web project (toptal.com)</a></li><li><a href="https://polyhaven.com/">Poly Haven</a></li><li><a href="https://www.poliigon.com/search?credit=0">Search - Poliigon</a></li></ol></blockquote><p>考虑到纹理图片和电脑显示器图片可能不一致，由此制作壁纸的最终效果可能稍有不完美，所以我们可以<strong>设法先将尺寸不匹配的纹理图片铺满整个屏幕</strong>，再导出作为制作壁纸使用的纹理图片。</p><ol type="1"><li><p>在PS软件中点击<font color=purple>文件-打开</font>，或者<font color=purple>Ctrl+O</font>打开纹理图片</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231217150953342.png" alt="image-20231217150953342" /><figcaption aria-hidden="true">image-20231217150953342</figcaption></figure></li><li><p>点击<font color=blue>编辑-定义图案</font>，给纹理图案起一个好记的名字，点击<font color=blue>确定</font></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231217151110446.png" alt="image-20231217151110446" /><figcaption aria-hidden="true">image-20231217151110446</figcaption></figure></li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/wwwwww.png" alt="wwwwww" /><figcaption aria-hidden="true">wwwwww</figcaption></figure><ol start="3" type="1"><li><p>新建项目，将尺寸设置为电脑显示器大小</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/屏幕截图%202023-12-17%20151328.png" alt="屏幕截图 2023-12-17 151328" /><figcaption aria-hidden="true">屏幕截图 2023-12-17 151328</figcaption></figure></li><li><p>选择<font color=red>油漆桶工具</font>，将填充物设置为<font color=red>图案</font>，<font color=red><strong>在图案里找到我们定义的纹理图案</strong></font>，进行填充</p></li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231217151756597.png" alt="image-20231217151756597" /><figcaption aria-hidden="true">image-20231217151756597</figcaption></figure><ol start="4" type="1"><li>最后导出即可</li></ol><h1 id="制作壁纸">制作壁纸</h1><blockquote><p>这里使用的图案编辑工具是在线PS工具（<a href="https://www.photopea.com/">Photopea | Online Photo Editor</a>），也可以直接在PS软件中编辑</p></blockquote><blockquote><p>颜色设置可参考配色网站：<a href="http://zhongguose.com/">zhongguose － 传统颜色</a></p></blockquote><ol type="1"><li><p>新建项目，设置分辨率为<strong>电脑显示器分辨率</strong>，设置<strong>背景色</strong>为你喜欢的颜色（或者直接打开一张你喜欢的背景图片）</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216202126809.png" alt="image-20231216202126809" /><figcaption aria-hidden="true">image-20231216202126809</figcaption></figure></li><li><p>点击<strong><font color=green>文件-打开</font></strong>或者<font color=blue>Ctrl+O</font>打开下载好的纹理图片</p><blockquote><p>这一步用的纹理图案尺寸比电脑显示器尺寸小一些，最后的纹理效果有点放大失真。如想避免这一点可以跳回去看第二步：准备纹理图片</p></blockquote></li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216210041490.png" alt="image-20231216210041490" /><figcaption aria-hidden="true">image-20231216210041490</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216214805133.png" alt="image-20231216214805133" /><figcaption aria-hidden="true">image-20231216214805133</figcaption></figure><ol start="3" type="1"><li><p>在纹理图案界面，点击<font color=red>编辑-定义新的图案</font>，定义成功后回到壁纸界面</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216215020114.png" alt="image-20231216215020114" /><figcaption aria-hidden="true">image-20231216215020114</figcaption></figure></li><li><p>点击图层，右键点击<font color=purple>混合模式</font></p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216215322738.png" alt="image-20231216215322738" /><figcaption aria-hidden="true">image-20231216215322738</figcaption></figure></li></ol><p>在出现的界面选择<font color=orange>纹理</font>，然后<strong>选择我们定义的图案</strong>，点击确定</p><blockquote><p>纹理界面的深度为正，代表纹理效果是凸出的；深度为负，表示纹理效果是凹陷的</p></blockquote><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216215555128.png" alt="image-20231216215555128" /><figcaption aria-hidden="true">image-20231216215555128</figcaption></figure><ol start="5" type="1"><li><p>最后依次点击<font color=green>文件-导出为-PNG</font>，将图片导出即可</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20231216215902983.png" alt="image-20231216215902983" /><figcaption aria-hidden="true">image-20231216215902983</figcaption></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> blog搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Butterfly主题优化 </tag>
            
            <tag> PS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Butterfly主题 网页标题崩溃欺骗特效</title>
      <link href="/2023/12/05/Butterfly%E4%B8%BB%E9%A2%98%20%E7%BD%91%E9%A1%B5%E6%A0%87%E9%A2%98%E5%B4%A9%E6%BA%83%E6%AC%BA%E9%AA%97%E7%89%B9%E6%95%88/"/>
      <url>/2023/12/05/Butterfly%E4%B8%BB%E9%A2%98%20%E7%BD%91%E9%A1%B5%E6%A0%87%E9%A2%98%E5%B4%A9%E6%BA%83%E6%AC%BA%E9%AA%97%E7%89%B9%E6%95%88/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考网址：<a href="https://asdfv1929.github.io/2018/01/25/crash-cheat/">Hexo NexT主题中添加网页标题崩溃欺骗搞怪特效 | asdfv1929 's Home</a></p></blockquote><p>在butterfly主题中给网页标题增加一些搞怪特效</p><h4 id="创建js文件-crash_cheat.js">创建js文件 crash_cheat.js</h4><p>打开theme文件夹下的<code>\butterfly\source\js</code>，创建文件<span style="background:#f9eda6;color:red">crash_cheat.js</span>，添加代码：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;!--崩溃欺骗--&gt;</span><br><span class="line"> var OriginTitle = document.title;</span><br><span class="line"> var titleTime;</span><br><span class="line"> document.addEventListener(&#x27;visibilitychange&#x27;, function () &#123;</span><br><span class="line">     if (document.hidden) &#123;</span><br><span class="line">         $(&#x27;[rel=&quot;icon&quot;]&#x27;).attr(&#x27;href&#x27;, &quot;/img/favicon.png&quot;);</span><br><span class="line">         document.title = &#x27;╭(°A°`)╮ 页面崩溃啦 ~&#x27;;</span><br><span class="line">         clearTimeout(titleTime);</span><br><span class="line">     &#125;</span><br><span class="line">     else &#123;</span><br><span class="line">         $(&#x27;[rel=&quot;icon&quot;]&#x27;).attr(&#x27;href&#x27;, &quot;/favicon.png&quot;);</span><br><span class="line">         document.title = &#x27;(ฅ&gt;ω&lt;*ฅ) 噫又好了~&#x27; + OriginTitle;</span><br><span class="line">         titleTime = setTimeout(function () &#123;</span><br><span class="line">             document.title = OriginTitle;</span><br><span class="line">         &#125;, 2000);</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;)</span><br></pre></td></tr></table></figure><h4 id="inject-引用">inject 引用</h4><p>在主题配置文件<code>_config.butterfly.yml</code>的<span style="background:#f9eda6;color:red">inject-bottom</span>下直接引入js文件：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Inject</span><br><span class="line"># Insert the code to head (before &#x27;&lt;/head&gt;&#x27; tag) and the bottom (before &#x27;&lt;/body&gt;&#x27; tag)</span><br><span class="line"># 插入代码到头部 &lt;/head&gt; 之前 和 底部 &lt;/body&gt; 之前</span><br><span class="line">inject:</span><br><span class="line">  head:</span><br><span class="line">    # - &lt;link rel=&quot;stylesheet&quot; href=&quot;/xxx.css&quot;&gt;</span><br><span class="line"></span><br><span class="line">  bottom:</span><br><span class="line">    - &lt;script src=&quot;/js/crash_cheat.js&quot;&gt;&lt;/script&gt;     #页面崩溃欺骗特效</span><br><span class="line">    # - &lt;script src=&quot;xxxx&quot;&gt;&lt;/script&gt;     主题/source/js文件夹中的.js文件</span><br></pre></td></tr></table></figure><h4 id="解决引入js文件时出现inject不生效的问题">解决引入js文件时出现inject不生效的问题</h4><p>在上面一通操作后，再hexo三连，发现页面标题并未出现变化。经过多方查找解决方案，得到如下方法：</p><ol type="1"><li><p>在网站按<code>F12</code>检查，发现控制台报错：<span style="background:#fbd4d0;">Uncaught ReferenceError: $ is not defined</span></p></li><li><p>再次查找，发现原因是没有引用jquery库的jquery.min.js文件</p></li><li><p>引入jquery库：一种是从本地项目路径引用；另一种是通过网页链接引入（<span style="background:#f9eda6;color:red">无论哪种引用库的方式，都要把jquery库的引用放到第一个&lt;s<!--断开script关键字-->cript&gt;引用的前面，这样才能使后面的js文件顺序执行时被成功识别</span>）</p><p>如果我们的项目是https安全域名，那么引入代码为：</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://libs.baidu.com/jquery/2.1.4/jquery.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p>反之：</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;http://libs.baidu.com/jquery/2.1.4/jquery.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> blog搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Butterfly主题优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>学术英语写作与沟通 笔记</title>
      <link href="/2023/11/25/%E5%AD%A6%E5%A0%82%E5%9C%A8%E7%BA%BF%20%E5%AD%A6%E6%9C%AF%E8%8B%B1%E8%AF%AD%E5%86%99%E4%BD%9C%E4%B8%8E%E6%B2%9F%E9%80%9A/"/>
      <url>/2023/11/25/%E5%AD%A6%E5%A0%82%E5%9C%A8%E7%BA%BF%20%E5%AD%A6%E6%9C%AF%E8%8B%B1%E8%AF%AD%E5%86%99%E4%BD%9C%E4%B8%8E%E6%B2%9F%E9%80%9A/</url>
      
        <content type="html"><![CDATA[<p>课程链接：https://www.xuetangx.com/learn/hfut05021002478/hfut05021002478/16907237/video/36265667</p><p><strong>本文只作学习交流使用！</strong></p><h1 id="ch1-title-author-affiliation-论文题目-作者姓名-单位">Ch1 Title + Author + Affiliation 论文题目 + 作者姓名 + 单位</h1><h2 id="how-to-write-the-title-of-academic-paper">1.1 How to Write the Title of Academic Paper</h2><p>Titles are succinct descriptive labels of texts and are meant to fulfifil different purposes, such as to individualize a publication, summarize its content and appeal to its audience among others. They are ideally relevant to present the content of a study and, in general, they are self-explanatory to their readers. This topic will cover 5 sections; they are function, basic requirements, classification, syntax and tips.</p><h2 id="ex-1.1">ex 1.1</h2><p>1.Which type does this title belong to?</p><p>Effect of non-pharmaceutical interventions to contain COVID-19 in China</p><p><font color=red>Noun phrase title </font></p><p>2.Analyze the grammatical construction of the two following titles from top academic journals?</p><p>An investigation of transmission control measures during the first 50 days of the covid-19 epidemic in China ------ From <em>Science</em></p><p><font color=red>Nominal group construction</font></p><h2 id="author-affiliation">1.2 Author + Affiliation</h2><p>Author and affiliation provide important information of a published paper so we need to write them correctly in English. This section will talk about how to write author and affiliation for an academic paper from four aspects, namely definition, function, layout and writing tips.</p><h2 id="ex-1.2">ex 1.2</h2><p>1.Affiliations lie just above authors’ names in published papers, which usually contain such information as authors’ institutions or addresses.</p><p><font color=red>false</font></p><p>2.Corresponding author has the authority to act on behalf of all authors and is the contact person for the research paper.</p><p><font color=red>true</font></p><p>3.In terms of author’s affiliated information, we tend to put bigger unit first then followed by smaller one.</p><p><font color=red>false</font></p><p>4.We usually use the word “and” or the sign “&amp;” to connect the last two authors’ names.</p><p><font color=red>true</font></p><h1 id="ch2-outline-大纲">Ch2 Outline 大纲</h1><h2 id="ex-2">ex 2</h2><p>1.Which of the following is not true about the reverse outlining?</p><p><font color=red>It is useful only when your paper focuses on complex issues in detail.</font></p><p>2.Analyze the sample outline, and try to tell the type of the sample outline.</p><p><strong>Thesis:</strong> Explorers who went to conquer Mt. Everest have achieved much in many ways, but their expeditions also have exerted impacts, negative as well as positive, on Mt. Everest and the local community.</p><ol type="I"><li><pre><code>Background Information</code></pre></li></ol><p>​ A. Location of Mt. Everest</p><p>​ B. Geography of the Surrounding Area</p><p>​ C. Facts about Mt. Everest</p><p>​ 1. Height of the Mountain</p><p>​ 2. How the Mountain Was Named</p><p>​ a. Peak XV</p><p>​ b. Jomolungma (Tibetan name)</p><p>​ c. Sagarmatha (Nepalese name)</p><p>​ 3. The Number of People Who Have Climbed Everest to Date</p><ol start="2" type="I"><li><pre><code>Major Explorers Covered in this Paper</code></pre></li></ol><p>​ A. Sir Edmund Hillary</p><p>​ 1. First to Reach the Summit (1953)</p><p>​ 2. Leader of a Team of Experienced Mountain Climbers Who Worked Together</p><p>​ B. Tenzing Norgay and the Sherpas</p><p>​ 1. Norgay the Experienced Climber and Guide Who Accompanied Hillary</p><p>​ 2. Sherpas Still Used to Guide Expeditions</p><p>​ C. Rob Hall</p><p>​ 1. Leader of the Failed 1996 Expedition</p><p>​ 2. Leading Group of Tourists with Little Mountain Climbing Experience</p><ol start="3" type="I"><li><pre><code>The Impact Expeditions have had on Mt. Everest and Local Community</code></pre></li></ol><p>​ A. Ecological Effects</p><p>​ 1. Loss of Trees Due to High Demand for Wood for Cooking and Heating for Tourists.</p><p>​ 2. Piles of Trash Left by Climbing Expeditions</p><p>​ B. Economic Effects</p><p>​ 1. Expedition Fees Providing Income for the Country</p><p>​ 2. Expeditions Providing Work for the Sherpas, Contributing to the Local Economy.</p><p>​ C. Cultural Effects</p><p>​ 1. Introduction of Motor Vehicles</p><p>​ 2. Introduction of Electricity</p><p><font color=red>The topic outline</font></p><h2 id="importantce-of-an-outline-in-reading-and-writing">Importantce of an outline in reading and writing</h2><p><em>Outline can help the paper readers sort out the important and unimportant details, so that they can read more effectively.</em></p><p><em>When writing, the authors tend to use brainstorming, a useful tool for generating ideas in a free thinking way. Then outlining can help organize all the generated ideas, and help the authors prioritize the important information and eliminate the trivial details.</em></p><p><em>Therefore, although outline is not a part of your paper, it is an important tool to facilitate your academic career.</em></p><h2 id="outline-巩固题">outline 巩固题</h2><p>1.A thesis statement should be _____.</p><p><font color=red>a sentence</font></p><p>2.What is the role of thesis statement in an outline?</p><p><font color=red>the writer’s position about the topic</font></p><h1 id="ch3-abstract-摘要">Ch3 Abstract 摘要</h1><h2 id="ex-3.1">ex 3.1</h2><p>1.An abstract is a ____ summary of your published or unpublished research paper, usually about a paragraph long.</p><p><font color=red>short</font></p><p>2.An abstract lets readers get the gist or essence of your paper or article quickly, in order todecide whether to read the ____ paper.</p><p><font color=red>full</font></p><p>3.An abstract is a short statement about a paper designed to give a complete yet concise understanding of its research and finding.</p><p><font color=red>true</font></p><p>4.An abstract should include a ____ but ____ statement of the problem or issue, a description of the research method and design, the major findings and their significance, and the conclusion.</p><p><font color=red>brief</font> <font color=red>precise</font></p><p>5.The function of an abstract is to ___ .</p><p><font color=red>let the reader understand the main content of the paper </font></p><p><font color=red>provide convenience for the construction and maintenance of scientific and technological literature retrieval database</font></p><p>6.The features of an abstract is ___ .</p><p><font color=red>concise、objective、consistent、complete</font></p><h2 id="ex-3.2">ex 3.2</h2><p>1.An abstract prepares readers to follow the ____ information, analyses, and arguments in your full paper.</p><p><font color=red>detailed</font></p><p>2.An abstract helps readers remember ____ from your paper.</p><p><font color=red>key points</font></p><p>3.An abstract is often composed of____.</p><p><font color=red>Background or purpose、Methods、Results、Conclusion</font></p><p>4.The premises, objectives and tasks of the research work, and the scope of the topics covered will be introduced in the abstract.</p><p><font color=red>true</font></p><p>5.Methods are going to define how do you get answers to your research question, so ____, conditions, ____, means, ____, procedures employed in the paper will be illustrated define what material, what method, and what procedures are to be used.</p><p><font color=red>theories、materials、equipment</font></p><p>6.You may choose ____ as keywords.</p><p><font color=red>3-5 highly relevant terms</font></p><h2 id="samples-of-abstract">Samples of Abstract</h2><h3 id="descriptive-abstract"><strong>Descriptive abstract</strong></h3><p>Translation is not only a linguistic transference, but also an intercultural communication. For quite a long time, translation studies have been concentrated on the prescription of translation methods, with scant attention paid to the description of macro-cultural factors involved in the translating. In this paper, the writer contends that the study of the macro-cultural factors will surely enlarge the scope and enrich the content of the translation studies. The paper is largely a rudimentary step, both in theory and in practice, to expose some of the factors influencing Mr. Fu’s translation of <em>Gone with the Wind</em>, with a historic and descriptive approach employed.</p><h3 id="informative-abstract"><strong>Informative abstract</strong></h3><p>This study investigated the role of "signaling" in helping good readers comprehend expository text. As the existing literature on signaling, reviewed in the last issue of the Journal, pointed to deficiencies in previous studies' methodologies, one goal of this study was to refine prose research methods. Two passages were designed in one of eight signaled versions each. The design was constructed to assess the individual and combined effect of headings, previews, and logical connectives. The study also assessed the effect of passage length, familiarity and difficulty. The results showed that signals do improve a reader's comprehension, particularly comprehension two weeks after the reading of a passage and comprehension of subordinate and superordinate inferential information. This study supports the hypothesis that signals can influence retention of text-based information, particularly with long, unfamiliar, or difficult passages.</p><h1 id="ch4-data-collection-and-evaluation-数据收集与评价">Ch4 Data Collection and Evaluation 数据收集与评价</h1><h2 id="ex-4">ex 4</h2><p>1.There are ____ formats of data collection and evaluation.</p><p><font color=red>3</font></p><p>2.When introducing background information, if the content of a sentence is a general fact that is not affected by time, apply_____.</p><p><font color=red>the present tense</font></p><p>3.Data evaluation refers to the process of ____.</p><p><font color=red>critically reading and evaluating the sources</font></p><p>4.The titles of tables or diagrams are usually expressed in ____.</p><p><font color=red>phrases</font></p><h2 id="tips-for-data-collection-and-evaluation">Tips for Data Collection and Evaluation</h2><h3 id="i.-data-collection"><strong>I</strong>. <strong>Data Collection</strong></h3><p><strong>Collecting primary sources</strong></p><p><strong>Texts</strong>: Once the main arguments are in mind, the text should be re-read while highlighting, and underlining, scribbling in the margins, or using sticky notes to pick out what is needed.</p><p><strong>Interviews</strong>: Locate someone through friends and family networks.</p><p><strong>Collecting secondary sources</strong></p><p>Searching the Web for government documents. Government records may be helpful but in most cases secondary-source research begins at the library.</p><p>The library is the main source of data collection, where readers can access the relevant literature, books, periodicals, audio and video recording, radio, television and the Internet.</p><p><strong>As to the types of information</strong>, in the library there are data in print and electronic resources, including encyclopedias, almanacs (年鉴)，indexes (索引), abstracts, dictionaries, reference literature, compilations, bibliographic catalogs, online databases, web sites, online communities, search engines and so on.</p><p>Researchers often follow the following four steps in collecting data from the library:</p><p>l Search encyclopedias.</p><p>l Search biographical references, yearbooks, atlases（地图册），gazettes（公报，报纸）and professional dictionaries relevant to the topic.</p><p>l Search library catalogs to find appropriate books. Search by author, title or subject.</p><p>l Retrieve journal articles citation from journals or newspapers index with the relevant terms found in encyclopedias, dictionaries and other reference directories (目录).</p><h3 id="ii.-data-evaluation"><strong>II. Data Evaluation</strong></h3><p>Data evaluation refers to the process of critically reading and evaluating the sources. The gist of being critical is not just to criticize, but to question and, not take anything read at face value.</p><p>Structure, purpose, audience and author are four important dimensions of the text to pay close attention to in critical thinking and reading.</p><p><strong>Structure</strong></p><p>If starting with a book, look at the table of contents. See the shape of what is to come and identify places where the thesis or question might be most directly addressed. Notice the subsections. Is there anything very obviously missing?</p><p>Glance at any appendices, diagrams, tables, or figures and see what kinds of things make it into the Endnotes section if there is one.</p><p>Look at the topics listed in the Index at the back. Which of the entries has the most page numbers listed next to it? This will give you an indication of the subjects that contribute to the real scope of the book.</p><p><strong>Purpose</strong></p><p>Examine the title and first few paragraphs. What is the author trying to do? What is his or her bias? Any assumptions to be challenged?</p><p><strong>Audience</strong></p><p>Who is the intended audience? How narrow or broad is it? To answer this, look at stylistic choices such as diction and tone. Who is the target audience?</p><p><strong>Author</strong></p><p>Who is the author? Is it someone a professor has mentioned or one that you come across in the course of other reading? Has the person been mentioned in other texts or bibliographies of other texts? Is the person a teacher or researcher from a reputable academic institution?</p><p>Does the person have considerable knowledge of what he or she is talking about? Is the author respected and well-received（深受好评的）?</p><p><strong>Evaluating Web Pages</strong></p><p>Authorship</p><p>Who wrote this?</p><p>The publishing body</p><p>Is the name of any organization given on the document? Are there headers (页眉) , footers, or a distinctive watermark that show the document to be part of an official academic or scholarly web site?</p><p>Point of view or bias</p><p>Referral to or display of knowledge of the literature</p><p>Accuracy or verifiability of details</p><p>Currency: the date of publication</p><p>All information needs to be evaluated by readers for authority, appropriateness and other personal criteria for value. Never use information that cannot be verified. Establishing and learning criteria to filter information found on the Internet is a good beginning for becoming a critical consumer of information in all forms.</p><p>Learn to be skeptical and then learn to trust your instincts.</p><h2 id="useful-expressions-and-sentence-patterns">Useful Expressions and Sentence Patterns</h2><ol type="1"><li><h3 id="useful-expressions">Useful Expressions</h3></li></ol><ol type="1"><li><p>开头 图表类型：table（表格）、chart（图表）、graph（多指曲线图）、diagram（图标）、column chart（柱状图）、pie graph（饼图）、tree diagram（树形图） 描述：show, describe, illustrate, can be seen from, clear, apparent, reveal, represent 内容：figure, statistic, number, percentage, proportion</p></li><li><p>表示数据变化的单词或者词组 rapid/rapidly 迅速的/地，飞快的/地，险峻的/地 dramatic/dramatically 戏剧性的/地</p></li></ol><p>significant/significantly 有意义的/地，重大的/地 sharp/sharply 锐利的/地，急剧的/地 steep/steeply 急剧升降的/地</p><p>gradual/gradually 渐进的/地，逐渐的/地 slow/slowly 缓慢的/地 slight/slightly 稍微的/略微地</p><p>stable/stably 稳定的/地</p><p>steady/steadily 稳固的/地，坚定不移的/地</p><ol start="3" type="1"><li>其它在描述中的常用到的词</li></ol><p>grow 增长 distribute 分布 unequally 不相等地</p><p>measure n. 方法，措施 v. 估量，调节 forecast n.先见，预见 v. 猜测</p><p>significant changes 图中一些较大变化 noticeable trend 明显趋势 during the same period 在同一时期 in the case of 在……的情况下 in terms of/in respect of/regarding 在……方面 in contrast 相反，大不相同</p><ol start="2" type="1"><li><h3 id="useful-sentence-patterns">Useful Sentence Patterns</h3></li></ol><ol type="1"><li><p>The table shows the changes in the number of…over the period from…to…. 该表格描述了在……年至……年间……数量的变化。</p></li><li><p>The bar chart illustrates that…. 该柱状图展示了……</p></li><li><p>The graph provides some interesting data regarding…. 该图为我们提供了有关……有趣数据。</p></li><li><p>The diagram shows（that）…. 该图向我们展示了……</p></li><li><p>The pie graph depicts （that）…. 该圆形图揭示了……</p></li><li><p>This is a cure graph which describes the trend of…. 这个曲线图描述了……的趋势。</p></li><li><p>The figures/statistics show （that）…. 数据（字）表明……</p></li><li><p>The tree diagram reveals how…. 该树型图向我们揭示了如何……</p></li><li><p>The data/statistics show （that）…. 该数据（字）可以这样理解……</p></li><li><p>The data/statistics/figures lead us to the conclusion that….</p></li><li><p>From the table/chart/diagram/figure，we can see clearly that……or it is clear/apparent from the chart that…. 从图表我们可以很清楚（明显）看到……</p></li><li><p>This is a graph which illustrates…. 这个图表向我们展示了……</p></li><li><p>This table shows the changing proportion of a and b from……to…. 该表格描述了……年到……年间a与b的比例关系。</p></li><li><p>The graph，presented in a pie chart，shows the general trend in…. 该图以圆形图形式描述了……总的趋势。</p></li><li><p>This is a column chart showing…. 这是一个柱型图，描述了……</p></li><li><p>As can be seen from the graph，the two curves show the fluctuation of…. 如图所示，两条曲线描述了……的波动情况。</p></li></ol><h1 id="ch5-summary-概要">Ch5 Summary 概要</h1><h2 id="ex-5">ex 5</h2><p>1.A good summary is an accurate reflection of the author’s viewpoint.</p><p><font color=red>true</font></p><p>2.When you write a summary, you need to________.</p><p><font color=red>Write in your own words</font></p><p>3.How would you avoid plagiarism when you want to cite source materials?</p><p><font color=red>Summarize、Quote、Paraphrase</font></p><p>4.Most summaries start with a sentence containing two elements:____ and ____.</p><p><font color=red>the source、the main idea</font></p><h1 id="ch6-literature-review-文献综述">Ch6 Literature Review 文献综述</h1><h2 id="ex-6.1">ex 6.1</h2><p>1.Is it plagiarism if I use big parts of someone’s literature review for the introduction section of my paper?</p><p><font color=red>Yes</font></p><p>2.The most important reason for you to write literature review in an article is to ____.</p><p><font color=red>find a gap or something contradictory in the previous studies to justify your research</font></p><p>3.Which of the following title looks like a literature review?</p><p><font color=red>Background subtraction techniques: a review.</font></p><h2 id="summarize-vs-synthesize">Summarize VS Synthesize</h2><p><strong>Summarizing and synthesizing information from multiple sources is an indispensable step for writing a literature review.</strong> <strong>But are you really clear about their differences?</strong></p><ul><li><p><strong>A summary reiterates what the study is about.</strong></p><p><strong>Summary is important in a literature review because some research may not be familiar to the reader. It helps the reader develop an understanding of the subject matter. But a literature review goes beyond retelling what the data points out.</strong></p></li><li><p><strong>Synthesis pulls several sources together and explains through the writer’s words what his interpretation of the data means to the writer in his own voice.</strong></p><p><strong>For example, you have five research studies and they all point to the same conclusion.</strong></p></li></ul><h2 id="ex-6.2">ex 6.2</h2><p>1.We can use past tense when describing an action in a research beginning in the past and continuing to the present.</p><p><font color=red>false</font></p><p>2.The literature review is a synthesis and analysis of research on your topic in your own words. Most ideas can be and should be paraphrased. Paraphrase is a preferred choice over direct quotations on most occasions.</p><p><font color=red>true</font></p><h1 id="ch7-proposal-开题报告">Ch7 Proposal 开题报告</h1><h2 id="ex-7.1">ex 7.1</h2><p>1.The research proposal can __________.</p><ul><li><font color=red> clearly and systematically present the research problem and objective</font></li><li><font color=red>indicate the significance and introduce the specific methodology and research procedures synthesize current knowledge, seek gaps and formulate a plan to address the problem</font></li><li><font color=red>synthesize current knowledge, seek gaps and formulate a plan to address the problem</font></li><li><font color=red>provide a timetable for the study and a budget of the investigation or experiments</font></li></ul><p>2.A research proposal is intended to_________.</p><ul><li><font color=red>convince the readers that you are ready to do a research</font></li><li><font color=red>demonstrate that you have the knowledge, full understanding and the expertise to complete the project</font></li><li><font color=red>show your competency in a particular area of study</font></li><li><font color=red>serve as a planning tool</font></li></ul><p>3.The main elements of a research proposal include __________.</p><p><font color=red>what、why、how、expected result</font></p><p>4.Examine carefully the following to determine to what extent the topic chosen meets the criteria for a proposal:</p><ul><li><font color=red>It must be interesting to you.</font></li><li><font color=red>It must be within your competence.</font></li><li><font color=red> It must be feasible.</font></li><li><font color=red>It must be sufficiently delimited.</font></li><li><font color=red>It must have the potential to make a contribution to knowledge or practice in the appropriate area.</font></li></ul><p>5.<strong>The potential supervisors use research proposals to assess</strong></p><ul><li><font color=red>The quality and originality of your ideas</font></li><li><font color=red>Your skills in critical thinking</font></li><li><font color=red>The feasibility of the research project</font></li></ul><h2 id="ex-7.2">ex 7.2</h2><p>1.The budget of a research proposal consists of __________.</p><p><font color=red>a section details the amount of cost of equipment and service</font></p><p><font color=red>a section provides the justification for the funding requested for reviewers to check and see if it is reasonable</font></p><p>2.A thesis statement is a simple sentence that formulates both your topic and _________ toward it.</p><p><font color=red>point of view</font></p><p>3.In exploring data for a research proposal, sources can be divided into ____________.</p><p><font color=red>primary source and secondary source</font></p><p>4.Sometimes the literature review section is incorporated into ( ) section.</p><p><font color=red> Introduction</font></p><p>5.Most students’ literature reviews suffer from the following problems:</p><ul><li><font color=red>Lacking organization and structure</font></li><li><font color=red>Lacking focus, unity and coherence</font></li><li><font color=red>Failing to cite influential papers</font></li><li><font color=red>Failing to critically evaluate cited papers</font></li><li><font color=red>Depending too much on secondary sources</font></li></ul><h2 id="ex-7.3">ex 7.3</h2><p>1.In a successful writing of a research proposal, we should __________.</p><ul><li><font color=red>use accepted scientific terms</font></li><li><font color=red> try to use full forms instead of abbreviations and avoid contractions</font></li><li><font color=red>use formal word and phrases instead of nonstandard or informal expressions</font></li><li><font color=red>use impersonal expressions and passive voice in a proper manner</font></li></ul><p>2.In proposal, the discussion section will include:</p><ul><li><font color=red>An analysis of sources of error in the data</font></li><li><font color=red>Integration with what was previously known</font></li><li><font color=red>Implications for future study</font></li></ul><h1 id="ch8-how-to-write-a-research-paper-introduction">Ch8 How to Write a Research Paper Introduction</h1><h2 id="ex-8">ex 8</h2><p>1.Generally speaking, what will be covered in Introduction Part?</p><ul><li><font color=red>Significance and necessity of the study</font></li><li><font color=red>Background, scope of the issue being study </font></li><li><font color=red>Clear definitions of key term involved in the study</font></li><li><font color=red>Theoretical foundations for the study</font></li><li><font color=red>Objectives of the present study</font></li><li><font color=red>Brief literature review and comment on the previous studies</font></li></ul><h1 id="ch9-material-and-methods-材料与方法">Ch9 Material and Methods 材料与方法</h1><h2 id="ex-9">ex 9</h2><p>1.Among the following research methods, which one is cheaper and easier?</p><p><font color=red>Opinion-based research method.</font></p><p>2.Compared with qualitative research method, quantitative research method_________.</p><p><font color=red>focuses on specific and narrow area.</font></p><p>3.In the materials and methods section, the ____ tense is more natural since you are describing work that is already completed at the time of writing, and the____ voice is preferable since this section focuses more on research than on researcher.</p><p><font color=red>past、passive</font></p><h1 id="ch10-resultsfindings-figures-tables-结果图表">Ch10 Results/Findings + Figures &amp; Tables 结果+图表</h1><p>The Results/Findings describes the statistical results/ findings of a research, which directly answers the research questions raised previously. It is important as it is the section where authors present new information and make new knowledge claims. The results/ findings can be given in the form of numerical data, verbal description or the combination of the above two.</p><h2 id="ex-10.1">ex 10.1</h2><p>1.In which form can the Results/ Findings section be given? ( )</p><p><font color=red>numerical data、verbal description、the combination of B and C</font></p><p>2.What are the three main functions of Results/ Findings section? ( )</p><p><font color=red>locating results、reporting results、explaining results</font></p><p>3.What are the three writing principles for the Results/ Findings section?</p><p><font color=red>faithfulness、innovation、generalization</font></p><p>4.How can the main findings be presented in the Results/ Findings section?</p><ul><li><font color=red>in a certain logical order</font></li><li><font color=red>in chronological order</font></li><li><font color=red>in order of importance</font></li></ul><h2 id="tips-for-writing-the-results-findings">Tips for Writing the Results/ Findings</h2><h3 id="structure-of-the-resultsfindings-section"><strong>1) structure of the Results/Findings section</strong></h3><p><strong>The first subdivision</strong> in this section is to provide preparatory information for the presentation of the results and it functions as a reminder and connector between the Methods section and the Results section. Authors often introduce source of data such as the type of data, the size of data, and the data collection method to prepare for the presentation of the significant results.</p><p><strong>The second subdivision</strong> in this section is to present results. Authors present the results of the study with relevant evidence such as statistics and examples. To report results is obligatory while to locate results and to explain results are optional.</p><h3 id="writing-principles-for-the-results-findings"><strong>2) writing principles for the Results/ Findings</strong></h3><p>There are some writing principles to follow here. They are faithfulness, innovation and generalization, or FIG.</p><p><strong>Principle of faithfulness.</strong> Whether research results can hold water depends on whether they can be tested repeatedly. Never add or delete the research results subjectively. Conflicting results sometimes lead to more meaningful further hypotheses and even more scientific conclusions. So be faithful in presenting your results.</p><p><strong>Principle of innovation.</strong> It is not necessary to cite too much of others' research results here. Therefore, when this section is written, the content of authors’ own original findings should be highlighted.</p><p><strong>Principle of generalization.</strong> Authors need to generalize essential facts and summarize key results in this section. No need to report all specific raw data here. State the main findings in a certain logical order, say, in chronological order or in order of importance.</p><h2 id="ex-10.2">ex 10.2</h2><p><em>Tables and figures are very important in academic writing and communication. They provide visual ways of presenting data and each type has its own advantages and disadvantages.</em></p><p>1.Tables and figures provide visual ways of presenting data and each type has its own advantages and disadvantages.</p><p><font color=red>true</font></p><p>2.Figures can display exact data or statistical information.</p><p><font color=red>false</font></p><p>3.There is no need to classify, process or select data in tables or figures.</p><p><font color=red>false</font></p><p>4.Which one is a flow chart?</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/ba2ccc22-193a-4164-8a21-f41d0087b960.png" alt="chart 4.png" /><figcaption aria-hidden="true">chart 4.png</figcaption></figure><h2 id="tips-for-various-tables-and-figures">Tips for Various Tables and Figures</h2><p>A table is an arrangement of data in rows and columns, or possibly in a more complex structure. Tables are widely used in communication, research and data analysis. They can display exact data or statistical information.</p><p>Figures have many various types. Here we just focus on pie chart, bar chart, line chart and flow chart.</p><ol type="1"><li>A pie chart is a circular statistical graphic which is divided into slices to illustrate numerical proportions. But sometimes it is not easy to compare different sections of a given pie chart or compare data across different pie charts.</li><li>A bar chart is a chart to present grouped data with rectangular bars whose lengths are proportional to the values they present. It can show comparisons among various categories.</li><li>A line chart displays information as a series of data points connected by straight line segments. It is often used to visualize a trend in data changing with the time or condition.</li><li>A flow chart is a diagram to show the order of operations or sequence of tasks for solving a problem or managing a complex project.</li></ol><h1 id="ch11-discussion-and-conclusion">Ch11 Discussion and Conclusion</h1><h2 id="ex-11.1">ex 11.1</h2><p>1.The results show that those who reported driving unsupervised 1-12 times were 80% more likely to be involved in crash than those who reported never having done this.</p><p><font color=red>Reporting results</font></p><p>2.The research has some potential limitations.</p><p><font color=red>Indicating limitations</font></p><p>3.We bring everything together in this part by discussing the ______ of our findings and its relationship to previous research in the area.</p><p><font color=red>significance</font></p><p>4.Each result reported should be followed by a proper ______.</p><p><font color=red>discussion</font></p><h2 id="tips-for-discussion">Tips for Discussion</h2><ol type="1"><li>summarized the main results.</li><li>interpreted (not described) the results.</li><li>discussed the significance of the results.</li><li>explained whether the results prove or disprove the hypothesis.</li><li>discussed the results in the light of previous research.</li><li>explained the wider implications of the work.</li><li>discussed any problems with or limitations of the study.</li><li>made suggestions for improvements.</li><li>suggested directions for future research.</li></ol><h2 id="ex-11.2">ex 11.2</h2><p>1.Given that a significant portion of learner license holders report driving unsupervised and those that violate this condition the most are more likely to crash, improving compliance with learner license supervised driving conditions varrants increased attention.</p><p><font color=red>Interpreting results</font></p><p>2.Evaluations of the initiatives designed to increase compliance supervised driving condition should be a research priority.</p><p><font color=red>Recommending further research</font></p><p>3.Though components of a conclusion may vary, a conclusion generally contains a restatement of the thesis in the ________, a summary of key points in the body, and a broad statement.</p><p><font color=red>introduction </font></p><p>4.This exploratory case study ________the stated beliefs and actual instructional practices of two experienced teachers of English language in a primary school in Singapore.</p><p><font color=red>investigated </font></p><h2 id="tips-for-conclusion">Tips for Conclusion</h2><p>The structure of a conclusion generally follows a pattern, moving from specific to general.</p><ol type="1"><li>Restatement of the main premise.</li><li>Summary of key points in the essay.</li><li>Broad statement.</li></ol><h1 id="ch12-how-to-write-acknowledgements-致谢">Ch12 How to Write Acknowledgements 致谢</h1><h2 id="ex-12">ex 12</h2><p>1.请翻译：国家自然科学基金</p><p><font color=red>National Natural Science Foundation of China</font></p><p>2.Whom will we thank in Acknowledgement Part?</p><ul><li><font color=red>Those who have helped the author's scientific research</font></li><li><font color=red>Reviewers and editors </font></li><li><font color=red>Funding / program</font></li></ul><h1 id="ch13-references-参考文献">Ch13 References 参考文献</h1><h2 id="ex-13">ex 13</h2><p>1.The functions of the references section include the following points except:</p><p><font color=red>It concludes the whole researc</font></p><p>2.The following two samples are documented in ____ style.</p><p>Wysocki, Anne Frances, et al. <em>Writing New Media: Theory and Applications for Expanding the Teaching of Composition</em>. Logan, UT: Utah State UP, 2004. Print.</p><p>Foucault, Michel. <em>Madness and Civilization: A History of Insanity in the Age of Reason</em>. Trans. Richard Howard. New York: Vintage-Random House, 1988. Print.</p><p><font color=red>MLA</font></p><p>3.Which of the following is a documentation tool?</p><p><font color=red>Latex、Endnote、Bibtex（Trados 是翻译软件）</font></p><h2 id="samples-of-apa-and-mla-style">Samples of APA and MLA style</h2><p>Anderson, A. K., Christoff, K., Panitz, D., De Rosa, E., &amp; Gabrieli, J. D. E. (2003). Neural correlates of the automatic processing of threat facial signals. <em>Journal of Neuroscience, 23,5627-5633.</em></p><p>Chow T. W., &amp; Cummings, J. L. (2000). The amygdala and Alzheimer<em>’</em>s disease. In J. P. Aggleton (Ed.), <em>The amygdala: A functional analysis</em> (pp. 656–680). Oxford,England: Oxford University Press.</p><p>Shipley, W. C. (1986). <em>Shipley Institute of Living Scale</em>. Los Angeles, CA: Western Psychological Services.</p><p>Wheeler, D.P., &amp; Bragin, M. (2007). Bringing it all back home: Social work and the challenge of returning veterans. Health and Social Wor, 32, 297-300. Retrieved from <a href="http://www/">http://www</a>. naswpressonline. org</p><p>Samples of MLA style</p><p>Wysocki, Anne Frances, et al. <em>Writing New Media: Theory and Applications for Expanding the Teaching of Composition</em>. Logan, UT: Utah State UP, 2004. Print.</p><p>Foucault, Michel. <em>Madness and Civilization: A History of Insanity in the Age of Reason</em>. Trans. Richard Howard. New York: Vintage-Random House, 1988. Print.</p><p>Crowley, Sharon, and Debra Hawhee. <em>Ancient Rhetorics for Contemporary Students</em>. 3rd ed. New York: Pearson/Longman, 2004. Print.</p><p><strong>The New Jerusalem Bible</strong>. Ed. Susan Jones. New York: Doubleday, 1985. Print.</p><p>Bishop, Karen Lynn. <em>Documenting Institutional Identity: Strategic Writing in the IUPUI Comprehensive Campaign</em>. Diss. Purdue University, 2002. Ann Arbor: UMI, 2004. Print.</p><p>Poniewozik, James. "TV Makes a Too-Close Call." <em>Time</em> 20 Nov. 2000: 70-71. Print.</p><p>Buchman, Dana. "A Special Education." <em>Good Housekeeping</em> Mar. 2006: 143-48. Print.</p><p>"Of Mines and Men." Editorial. <em>Wall Street Journal</em> east. ed. 24 Oct. 2003: A14. Print.</p><p>Hamer, John. Letter. <em>American Journalism Review</em> Dec. 2006/Jan. 2007: 7. Print.</p><p>Bagchi, Alaknanda. "Conflicting Nationalisms: The Voice of the Subaltern in Mahasweta Devi's Bashai Tudu." <em>Tulsa</em> <em>Studies in Women's Literature</em> 15.1 (1996): 41-50. Print.</p><p>Aristotle. <em>Poetics</em>. Trans. S. H. Butcher. <em>The Internet Classics Archive</em>. Web Atomic and Massachusetts Institute of Technology, 13 Sept. 2007. Web. 4 Nov. 2008. <a href="http://classics.mit.edu/Aristotle.html" class="uri">http://classics.mit.edu/Aristotle.html</a>.</p><h2 id="references-作业">references 作业</h2><p>1.If writers do not give credit for borrowed ideas or words, they make a serious error, i.e. ____.</p><p><font color=red>plagiarism</font></p><p>2.When you use a direct quotation in APA style, you must state the following elements except _____.</p><p><font color=red>the edition of the journal</font></p><h2 id="文献引用的三种格式">文献引用的三种格式</h2><p>文献引用一般有三种形式。（参考链接：<a href="https://blog.csdn.net/programmer_jiang/article/details/118557929#:~:text=Author%20Prominent%20Citation**：一般过去时**%20关注完成研究的人。%20一般以作者姓放在句首做主语，%20其后括号标注引注年份%20，汇报动词（reporting%20verb）做谓语动词。,higher%20in%20order%20to%20yield%20acceptable%20sliceability.%204">文献英语期末_hedging模糊语有哪些-CSDN博客</a>）</p><ul><li>Information Prominent Citation<strong>：一般现在时</strong></li></ul><p>关注前人研究的内容。一般陈述研究内容，引注将作者和年份放在句末括号里。 如：Scientific paper writing skill is usually adopted with learning by doing and formal training (Auvinen, 2015).</p><ul><li>Author Prominent Citation<strong>：一般过去时</strong></li></ul><p>关注完成研究的人。一般以作者姓放在句首做主语，<strong>其后括号标注引注年份</strong>，汇报动词（reporting verb）做谓语动词。 如：Theno et al. (1978) concluded that salt content had to be 2% or higher in order to yield acceptable sliceability. 4</p><ul><li>Weak Author Prominent Citation<strong>：现在完成时</strong></li></ul><p>关注一系列研究的相似结果。一般以 many researchers，many scholars 等一群人作为句子主语，谓语动词用完成时，引注将诸多作者和年份排列在句末括号里。 如： Researchers have noted that resolving this debate hinges on understanding the relationship between yield and biodiversity, the likelihood of land being spared, and external consequences of practices raising yield, such as agrochemical runoff (Grau et al., 2013; Green et al., 2005; Phalan et al., 2011).</p><h1 id="ch14-academic-poster-学术海报">Ch14 Academic Poster 学术海报</h1><h2 id="ex-14.1">ex 14.1</h2><p>1.Important information should be ____ from about 10 feet away.</p><p><font color=red>readable</font></p><p>2.Title is ____ and draws interest.</p><p><font color=red>short</font></p><p>3.Academic posters could be an ____way of communicating concisely, visually and attractively.</p><p><font color=red>effective</font></p><p>4.Use ____ language to present your work. Avoid ____ unless you're really positive that yours will be a specialist-only audience.</p><p><font color=red>plain、jargon and acronyms</font></p><p>5.An effective poster lets ____ tell the story, uses ____ sparingly, and keeps the sequence well-ordered and obvious.</p><p><font color=red>graphs and images、text</font></p><p>6.Academic posters summarize information or research concisely and attractively, to help publicize information or research and generate discussion.</p><p><font color=red>true</font></p><p>7.What roles does an academic poster play in academic communication？</p><ul><li><font color=red>Posters are widely used in the academic community.</font></li><li><font color=red>Most conferences include poster presentations in their program. </font></li><li><font color=red>Academic posters may be displayed at national or international conferences.</font></li><li><font color=red>They may also be published online as part of conference proceedings.</font></li><li><font color=red>Academic Posters can be used as assessment at university.</font></li></ul><h2 id="ex-14.2">ex 14.2</h2><p>1.An effective poster can make a ____impact, so it's worth developing your poster planning skills.</p><p><font color=red>strong</font></p><p>2.In some courses, ____ and ____ may be weighted at 60%, with ____ and ____weighted at 40%.</p><p><font color=red>content、structure、visual organization、presentation</font></p><p>3.If you are reporting on a piece of research with an academic poster , you can turn to the some language signals to ____, ____, ____, and ____.</p><p><font color=red>introduce the poster、locate a point on the poster、answer directly、handle complex questions</font></p><p>4.Academic posters need to show evidence of reading and research, so you must always include Literature cited.</p><p><font color=red>true</font></p><p>5.Acknowledgments section is to thank individuals for specific contributions to the project Also include in this section explicit disclosures for any conflicts of interest and conflicts of commitment.</p><p><font color=red>true</font></p><h2 id="poster">poster</h2><p>• An effective poster is a visual communications tool;</p><p>• An effective poster will get your main point(s) across to as many people as possible;</p><p>• An effective poster is focused on a single message; lets graphs and images tell the story; uses text sparingly; keeps the sequence well-ordered and obvious.</p><h1 id="ch15-linguistic-features-of-academic-english-学术英语的语言特点">Ch15 Linguistic Features of Academic English 学术英语的语言特点</h1><h2 id="ex-15.1">ex 15.1</h2><p>1.In writing academic paper, we should avoid expressing____ arising out of intuition, feeling, prejudice or your own experience.</p><p><font color=red>personal opinions</font></p><p>2.Another common feature of academic writing is nominalization, whereby verbs become _____.</p><p><font color=red>nouns</font></p><p>3.We can make tentative statements by applying ____ into our academic writing.</p><p><font color=red>models、verbs、adverbs、adjectives</font></p><h2 id="tips-for-academic-writing">Tips for Academic Writing</h2><p><strong>Why do we need a formal language style?</strong></p><p>To fulfil the expectations of academic readers.</p><p>It is important to follow the specific genre requirements in academic writing. This is a defining feature of academic writing.</p><p>Term papers and research reports are generally formal.</p><p><strong>How to achieve formality?</strong></p><p>Advanced and academic vocabulary</p><p>Long and complex sentences</p><p>The minimized use of 1st - and 2nd –person pronouns</p><p><strong>The use of advanced and academic vocabulary</strong></p><p>Avoid using simple and colloquial words</p><p><strong>Simple words</strong> <strong>Advanced words</strong></p><p>eat consume</p><p>try attempt/endeavor</p><p>keep remain/maintain</p><p>good (for) beneficial</p><p>bad(for) harmful/detrimental</p><p><strong>Avoid using phrasal verbs</strong></p><p>It would be better to use a single verb instead of a phrasal verb.</p><p><strong>Phrasal verbs</strong> <strong>One-word verbs</strong></p><p>look into investigate</p><p>find out discover</p><p>cut down reduce</p><p>go up increase</p><p>get rid of eliminate</p><p><strong>Avoid using shortened forms of words/contractions</strong></p><p><strong>Contractions Full forms</strong></p><p>won’t will not</p><p>didn’t did not</p><p>can’t cannot</p><p>it’s it is</p><p>you’re you are</p><p>​</p><p><strong>Academic Word List (AWL)</strong></p><p>Developed by Averil Coxhead, a scholar in New Zealand</p><p>570 word families</p><p>Formal, advanced and academic</p><p><strong>The use of long and complex sentences</strong></p><p>Subordinate clauses: add extra information to the main clause</p><p>Subordinate conjunctions 从属连词</p><p>Some youngsters admit getting restless if their phones is not nearby.</p><p>Relative pronouns 关系代词</p><p>It is advisable to write an outline, which will help you organize the essay in a more logical way.</p><p><strong>Subordinate conjunctions</strong> <strong>Relative pronouns</strong></p><p>Once after until that who whose</p><p>Provided that although when</p><p>Rather than as whenever which whoever whosever</p><p>Since because where</p><p>So that before whereas whichever whom whomever than even if wherever whether if</p><p>Though while unless why</p><p>l Do not always use too long and complex sentences in your essay, because readers will find them difficult to understand.</p><p>l The best way is to use a combination of simple and complex sentences.</p><p><strong>The minimized use of 1st - and 2nd –person pronouns</strong></p><p>First-person pronouns: I, we, us, my, our</p><p>second-person pronouns: you and your</p><p>E.g. Recently, we have discussed COVID-19 and China-US relations a lot.</p><p>→ Recently, COVID-19 and China-US relations have been much discussed.</p><p>Tip: Use third-person and passive voice.</p><h2 id="useful-expressions-in-results-and-tables-figures">Useful Expressions in Results and Tables &amp; Figures</h2><h3 id="useful-expressions-in-results"><strong>1.</strong> <strong>Useful Expressions in Results</strong></h3><p><strong>1.1 Active voice VS Passive voice</strong></p><p>Table 1 presents…and Table 2 presents…</p><p>Our hypothesis predicted….</p><p>Information…was obtained….</p><p>…this information was collected when</p><p><strong>1.2 Suitable reporting verbs</strong> show, indicate, reveal, report,</p><p>describe, explain, display, present…</p><p><strong>1.2.1 Locating the data</strong></p><p>As can be seen from Table 1…</p><p>… are shown/given/provided/ summarized in Table 1.</p><p>Table 1 demonstrates/</p><p>indicates/suggests….</p><p><strong>1.2.2 Highlighting the data</strong></p><p>…is exactly/approximately/almost the same as</p><p>… is completely/entirely/quite different from…</p><p>The main difference between…and… is that…</p><p><strong>1.2.3 Discussing the data</strong></p><p>The data clarify the relationship</p><p>between… and…</p><p>There is some evidence in the data to support our hypothesis, which proposed that…</p><p>This particular result may be</p><p>attributed to the influence of …</p><p><strong>1.3 Phrases of generality</strong></p><p>Overall</p><p>In general</p><p>On the whole</p><p>In the main</p><p>With …exception(s)</p><p>Overall, the results indicate that students performed above the 12th-grade level.</p><p>The overall results indicate…</p><p>The results indicate, overall, that…</p><p>In general, the experimental samples resisted…</p><p>With one exception, the experimental samples resisted…</p><h3 id="useful-expressions-in-tables-and-figures"><strong>2.</strong> <strong>Useful Expressions in</strong> <strong>Tables and Figures</strong></h3><p><strong>2.1 Very frequent and appropriate verbs</strong></p><p>reported, show, characterized, suggests, used, intended, contradict, suggest, prevail, focused, enables, speculate, maintain, compared, focused, claimed, shows, tend, represent</p><p><strong>2.2 Tense of verbs</strong></p><p>The tables show that those who reported driving unsupervised 1-12 times were 80% more likely to be involved in crash than those who reported never having done this.</p><h2 id="ex-15.2">ex 15.2</h2><p>1.Regardless of the placement, each figure must be _____ consecutively and complete with caption.</p><p><font color=red>numbered</font></p><p>2.What kind of tense do we usually use if we start a sentence with words like “Table 1 or Table n”?</p><p><font color=red>The present tense</font></p><p>3.We can place figures and tables within ____, or we can include them at____.</p><p><font color=red>the text of the results、the end of the report</font></p><h2 id="additional-material-for-describing-tables-and-figures-in-academic-language">Additional material for describing tables and figures in academic language</h2><h3 id="图形种类及概述法">1、 图形种类及概述法：</h3><p>泛指一份数据图表： a data graph(曲线图)/chart/diagram/illustration/table 饼图：pie chart 直方图或柱形图：bar chart/histogram 趋势曲线图：line chart/curve diagram 表格图：table 流程图或过程图：flow chart/sequence diagram 程序图：processing/procedures diagram</p><h3 id="常用的描述用法">2、常用的描述用法</h3><p>The table/chart diagram/graph shows （that） According to the table/chart diagram/graph As （is） shown in the table/chart diagram/graph As can be seen from the table/chart/diagram/graph/figures， figures/statistics shows （that）…… It can be seen from the figures/statistics We can see from the figures/statistics It is clear from the figures/statistics It is apparent from the figures/statistics table/chart/diagram/graph figures （that） …… table/chart/diagram/graph shows/describes/illustrates</p><h3 id="图表中的数据data具体表达法">3、图表中的数据（Data）具体表达法</h3><p>数据（Data）在某一个时间段固定不变：fixed in time 在一系列的时间段中转变：changes over time 持续变化的data在不同情况下： 增加：increase/raise/rise/go up …… 减少：decrease/grow down/drop/fall …… 波动：fluctuate/rebound/undulate/wave …… 稳定：remain stable/stabilize/level off ……</p><h3 id="二相关常用词组">二、相关常用词组</h3><h4 id="主章开头">1、主章开头</h4><p>图表类型：table（表格）、chart（图表）、diagram（图标）、graph（多指曲线图）、column chart（柱状图）、pie graph（饼图）、tree diagram（树形图） 描述：show、describe、illustrate、can be seen from、clear、apparent、reveal、represent 内容：figure、statistic、number、percentage、proportion</p><h4 id="表示数据变化的单词或者词组">2、表示数据变化的单词或者词组</h4><p>rapid/rapidly 迅速的，飞快的，险峻的 dramatic/dramatically 戏剧性的，生动的 significant/significantly 有意义的，重大的，重要的 sharp/sharply 锐利的，明显的，急剧的 steep/steeply 急剧升降的 steady/steadily 稳固的，坚定不移的 gradual/gradually 渐进的，逐渐的 slow/slowly 缓慢的，不活跃的 slight/slightly 稍微的、略微地 stable/stably 稳定的</p><h4 id="其它在描述中的常用到的词">3、其它在描述中的常用到的词</h4><p>significant changes 图中一些较大变化 noticeable trend 明显趋势 during the same period 在同一时期 grow/grew 增长 distribute 分布 unequally 不相等地 in the case of 在……的情况下 in terms of/in respect of/regarding 在……方面 in contrast 相反，大不相同 government policy 政府政策 market forces <a href="https://www.baidu.com/s?wd=市场力量&amp;tn=SE_PcZhidaonwhc_ngpagmjz&amp;rsv_dl=gh_pc_zhidao">市场力量</a> measure n.尺寸，方法，措施 v.估量，调节 forecast n.先见，预见 v.猜测</p><h3 id="三图表描述套句精选">三、图表描述套句精选</h3><p>1.The table shows the changes in the number of…over the period from…to…. 该表格描述了在……年之……年间……数量的变化。 2.The bar chart illustrates that…. 该柱状图展示了…… 3.The graph provides some interesting data regarding…. 该图为我们提供了有关……有趣数据。 4.The diagram shows （that）…. 该图向我们展示了…… 5.The pie graph depicts （that）…. 该圆形图揭示了…… 6.This is a cure graph which describes the trend of…. 这个曲线图描述了……的趋势。 7.The figures/statistics show （that）…. 数据（字）表明……</p><p>8.The tree diagram reveals how….</p><p>该树型图向我们揭示了如何……</p><p>9.The data/statistics show （that）….</p><p>该数据（字）可以这样理解……</p><p>10.The data/statistics/figures lead us to the conclusion that….</p><p>这些数据资料令我们得出结论…… 11.As is shown/demonstrated/exhibited in the diagram/graph/chart/table…. 如图所示…… 12.According to the chart/figures…. 根据这些表（数字）…… 13.As is shown in the table…. 如表格所示…… 14.As can be seen from the diagram，great changes have taken place in…. 从图中可以看出，……发生了巨大变化。 15.From the table/chart/diagram/figure，we can see clearly that……or it is clear/apparent from the chart that…. 从图表我们可以很清楚（明显）看到…… 16.This is a graph which illustrates…. 这个图表向我们展示了…… 17.This table shows the changing proportion of a &amp; b from……to…. 该表格描述了……年到……年间a与b的比例关系。 18.The graph，presented in a pie chart，shows the general trend in…. 该图以圆形图形式描述了……总的趋势。 19.This is a column chart showing…. 这是一个柱型图，描述了…… 20.As can be seen from the graph，the two curves show the fluctuation of…. 如图所示，两条曲线描述了……的波动情况。 21.Over the period from…to…, the…remained level. 在……至……期间，……基本不变。 22.In the year between…and…. 在……年到……期间…… 23.In the 3 years spanning from 1995 through 1998…. 1995年至1998三年里…… 24.From then on/from this time onwards…. 从那时起…… 25.The number of…remained steady/stable from （month/year） to （month/year）. ……月（年）至……月（年）……的数量基本不变。 26.The number sharply went up to…. 数字急剧上升至…… 27.The percentage of…stayed the same between…and…. ……至……期间……的比率维持不变。 28.The figures peaked at…in（month/year）. ……的数目在……月（年）达到顶点，为…… 29.The percentage remained steady at…. 比率维持在…… 30.The percentage of…is slightly larger/smaller than that of…. ……的比例比……的比例略高（低）。 31.There is not a great deal of difference between…and… ……与……的区别不大。 32.The graphs show a threefold increase in the number of…. 该图表表明……的数目增长了三倍。 33…decreased year by year while…increased steadily. ……逐年减少，而……逐步上升。 34.The situation reached a peak（a high point at）of …%. ……的情况（局势）到达顶（高）点，为……百分点。 35.The figures/situation bottomed out in…. 数字（情况）在……达到底部。</p><p>36.The figures reached the bottom/a low point/hit a trough.</p><p>数字（情况）达到底部（低谷）。</p><p>37.A is …times as much/many as b.</p><p>a是b的……倍。 38.A increased by…. a增长了…… 39.A increased to…. a增长到……</p><p>40.high/low/great/small/ percentage.</p><p>比率高（低） 41.There is an upward trend in the number of…. ……数字呈上升趋势。 42.Aconsiderable increase/decrease occurred from…to…. ……到……发生急剧上升。 43.From…to…the rate of decrease slow down. 从……到……，下降速率减慢。 44.from this year on，there was a gradual decline/ reduction in the…，reaching a figure of…. 从这年起，……逐渐下降至…… 45.be similar to… 与……相似 46.be the same as… 与……相同 47.There are a lot similarities/differences between…and…. ……与……之间有许多相似（不同）之处 48.A has something in common with b. a与b有共同之处。 49.The difference between a and b lies in…. a与b之间的差别在于…… 50.…（year）witnessed/saw a sharp rise//in….</p><p>……年……急剧上升。</p><h2 id="ex-15.3">ex 15.3</h2><p>1.We analyzed ____ variety of tissue samples.</p><p><font color=red>a</font></p><p>2.____colors affect our perception of reality.</p><p><font color=red>x</font></p><p>3.Those interested in donating the kidney should notify the hospital’s donation committee.</p><p><font color=red>false</font></p><p>4.The cheetah is the quickest of the land animals.</p><p><font color=red>true</font></p><h2 id="samples-for-articles">Samples for Articles</h2><ol type="1"><li><p>Becoming an expert takes a lot of ✗ experience.</p></li><li><p>The Cheetah is the quickest of the land animals.</p></li></ol><h2 id="culture-and-ethics-练习">culture and ethics 练习</h2><p>1.Ethics in academic writing exclude the following action(s):</p><p><font color=red>academic theft</font></p><p>2.Scientific writing is objective, impersonal and detached, so cultural difference cannot be detected in the academic field.</p><p><font color=red>false</font></p><h2 id="ex-15.4">ex 15.4</h2><p>1.In the paper discussed in this lecture, the word “Creator” should not be used because it ____.</p><p><font color=red>makes references to Creationism</font></p><p>2.Academic Integrity and ethics includes:</p><ul><li><font color=red>honesty</font></li><li><font color=red>no fabrication, falsification, or plagiarism</font></li><li><font color=red>honoring property rights</font></li></ul><h1 id="ch16-academic-correspondence-with-the-editors-与编辑的学术联系">Ch16 Academic Correspondence with the Editors 与编辑的学术联系</h1><h2 id="ex-16.1">ex 16.1</h2><p>1.You write a cover letter to______.</p><ul><li><font color=red>Introduce your paper to the editor</font></li><li><font color=red>Recommend reviewers to the editor</font></li><li><font color=red>Oppose reviewers to the editor</font></li></ul><p>2.What is a cover letter?</p><p><font color=red>A cover letter also called submission letter is a letter of transmittal to the editor of the journal for possible publication.</font></p><p>3.In order to choose your target journal, you need to find a journal without any peer review.</p><p><font color=red>false</font></p><h2 id="ex-16.2">ex 16.2</h2><p>1.If you want to know whether your paper is accepted or not, you need to write______.</p><p><font color=red>An inquiry letter</font></p><p>2.Peer review will provide you with comments and suggestions from ____and editors.</p><p><font color=red>reviewers</font></p><p>3.You can write a rebuttal letter to accuse the reviewers of bias or incompetence.</p><p><font color=red>false</font></p><h1 id="ch17-international-academic-conference-presentation-skills-国际学术会议宣讲技巧">Ch17 International Academic Conference Presentation Skills 国际学术会议宣讲技巧</h1><h2 id="ex-17">ex 17</h2><p>1.A presentation is the act of effective _____ communication with an audience.</p><p><font color=red>oral</font></p><p>2.Presentation skills consist of three major parts: ____,____ , and____ .</p><p><font color=red>audience analysis、delivery、managing stage fright</font></p><p>3.Which of the following should NOT be done in international academic conference presentation?</p><p><font color=red> speak towards the screen</font></p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> course </tag>
            
            <tag> 论文写作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to write an outline 笔记</title>
      <link href="/2023/11/09/How%20to%20write%20an%20outline%20%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/11/09/How%20to%20write%20an%20outline%20%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="how-to-write-an-outline-笔记">How to write an outline 笔记</h1><h2 id="outline-定义">outline 定义</h2><p>Outline是你的写作地图。写作者使用outline来以一种有逻辑的、清晰的顺序展示思想，并有目的地组织话题及支撑的细节。</p><p>它展示了每一段或每一节将包含的信息，以及它们之间的顺序。outline可以被用来辨别并减少文章中可能存在的缺点或缺少的重点。</p><h2 id="outline-的功能">outline 的功能</h2><p>撰写outline将帮助你集中于眼前的任务，并避免不必要的切题、逻辑谬误和段落的不完善。</p><p>在进行大型研究项目时，很容易混淆资源来源，或者在研究过程中忘记阅读到的内容。为了让你对阅读的内容保持清晰，阅读时可以在页边空白处或另一张纸上记录reverse outline。这个reverse outline可以让你轻松地浏览你的资源，突出显示那些可能有助于你研究的信息。</p><h2 id="outline-的类型和结构">outline 的类型和结构</h2><p><strong>Topic outline</strong> 由简短的短语组成。当您处理的问题可能会以各种不同的方式出现在您的论文中时，这种方法非常有用。</p><p><strong>Sentence outline</strong> 为完整句子。当您的论文侧重于复杂问题的细节时，这种方法非常有用。</p><p>两种outline都遵循严格的格式，使用罗马数字和阿拉伯数字以及字母表中的大写和小写字母。</p><figure><img src="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/654cdb69c458853aefce1aba.png" alt="image-20231109124822463" /><figcaption aria-hidden="true">image-20231109124822463</figcaption></figure><h2 id="outlining-步骤">outlining 步骤</h2><ol type="1"><li><strong>初步工作</strong>：写下主题，然后开始头脑风暴。</li><li><strong>确定研究问题</strong>：研究问题是outline其余部分的重点。试着用一句话或短语来概括你的论文要点。它也是决定论文标题的关键。</li><li><strong>确定主要类别</strong>：你将分析哪些要点？导言（Introduction）描述了你所有的要点；你论文的其余部分可以用来阐述这些要点。</li><li><strong>创建第一个类别</strong>：您要介绍的第一点是什么？如果文章围绕一个复杂的术语展开，那么可以从定义开始。对于一篇涉及特定理论的应用和测验的论文来说，提供该理论的一般背景是一个很好的切入点。</li><li><strong>创建子类别</strong> ：完成这些步骤后，在其下方叙述支撑主要论点的材料。使用的类别数量取决于尝试涵盖的信息量。</li></ol><h2 id="tips">Tips</h2><ul><li><p>在开始outlining之前，需要一个清晰的论文陈述或明确的目的或论点，因为outline中的其他内容都将用于支撑主题。</p></li><li><p>论文陈述应该是一个完整的肯定陈述句，而不是疑问句、短语或从属分句。</p></li><li><p>避免混合类型。与 topic outline 或 sentence outline 保持一致。请勿混合使用两种类型。</p></li><li><p>使用平行。每个标题和副标题都应保持与其他标题平行的结构。最明显的是 "topic" 和 "sentence" 的 outline 格式；平行性也指词性和时态。</p></li><li><p>使信息相互关联协调。第一个主标题所提供的信息应该与第二个主标题所提供的信息同等重要。副标题也是如此。</p></li><li><p>学会划分。每个主标题应该被划分为两个或多个部分。换言之，每个主标题至少应有两个副标题。</p></li><li><p>大写问题。在写outline、主标题和副标题时，几乎总是按照正确的句子大写规则书写。</p></li></ul><h2 id="附在读写中-outline-的重要性">附：在读写中 outline 的重要性</h2><p>Outline can help the paper readers sort out the important and unimportant details, so that they can read more effectively.</p><p>When writing, the authors tend to use brainstorming, a useful tool for generating ideas in a free thinking way. Then outlining can help organize all the generated ideas, and help the authors prioritize the important information and eliminate the trivial details.</p><p>Therefore, although outline is not a part of your paper, it is an important tool to facilitate your academic career.</p><p>参考网址：https://www.xuetangx.com/learn/hfut05021002478/hfut05021002478/16907237/video/36265680</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> course </tag>
            
            <tag> 论文写作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/11/06/hello-world/"/>
      <url>/2023/11/06/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
