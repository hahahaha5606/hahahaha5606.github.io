<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Diffusion Model（二） | 藕片侠大战论文怪</title><meta name="author" content="胖胖大藕片"><meta name="copyright" content="胖胖大藕片"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="引入
 模型网站：
 DALL·E2： DALL·E 2 (openai.com)
 DALL·E3：DALL·E 3 (openai.com)
 Sora: Sora (openai.com)
 发展简介
 
 李沐论文精读系列——由DALL·E 2看图像生成模型 - 知乎 (zhihu.com)
 AI艺术的背后：详解文本生成图像模型 | 集智俱乐部 (swarma.org)
 
 近几年图像">
<meta property="og:type" content="article">
<meta property="og:title" content="Diffusion Model（二）">
<meta property="og:url" content="http://example.com/2024/04/14/Diffusion%20Model%20%EF%BC%88%E4%BA%8C%EF%BC%89/index.html">
<meta property="og:site_name" content="藕片侠大战论文怪">
<meta property="og:description" content="引入
 模型网站：
 DALL·E2： DALL·E 2 (openai.com)
 DALL·E3：DALL·E 3 (openai.com)
 Sora: Sora (openai.com)
 发展简介
 
 李沐论文精读系列——由DALL·E 2看图像生成模型 - 知乎 (zhihu.com)
 AI艺术的背后：详解文本生成图像模型 | 集智俱乐部 (swarma.org)
 
 近几年图像">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/post_img/20231125235032.jpg">
<meta property="article:published_time" content="2024-04-14T01:05:36.341Z">
<meta property="article:modified_time" content="2024-04-14T05:42:42.040Z">
<meta property="article:author" content="胖胖大藕片">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/post_img/20231125235032.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/04/14/Diffusion%20Model%20%EF%BC%88%E4%BA%8C%EF%BC%89/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Diffusion Model（二）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-14 13:42:42'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/transpancy.css"><link rel="stylesheet" href="/css/showAppreciation.css"><link rel="stylesheet" href="/css/ancientPoetry.css"><link rel="stylesheet" href="/css/mycss.css"><style>#article-container.post-content h1:before, h2:before, h3:before, h4:before, h5:before, h6:before { -webkit-animation: avatar_turn_around 1s linear infinite; -moz-animation: avatar_turn_around 1s linear infinite; -o-animation: avatar_turn_around 1s linear infinite; -ms-animation: avatar_turn_around 1s linear infinite; animation: avatar_turn_around 1s linear infinite; }</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/macblack.css"><link rel="stylesheet" href="/css/datu.css"><link rel="stylesheet" href="/css/aplayer.css"><link rel="stylesheet" href="/css/myvaline.css"></head><body><svg aria-hidden="true" style="position:absolute; overflow:hidden; width:0; height:0"><symbol id="icon-sun" viewBox="0 0 1024 1024"><path d="M960 512l-128 128v192h-192l-128 128-128-128H192v-192l-128-128 128-128V192h192l128-128 128 128h192v192z" fill="#FFD878" p-id="8420"></path><path d="M736 512a224 224 0 1 0-448 0 224 224 0 1 0 448 0z" fill="#FFE4A9" p-id="8421"></path><path d="M512 109.248L626.752 224H800v173.248L914.752 512 800 626.752V800h-173.248L512 914.752 397.248 800H224v-173.248L109.248 512 224 397.248V224h173.248L512 109.248M512 64l-128 128H192v192l-128 128 128 128v192h192l128 128 128-128h192v-192l128-128-128-128V192h-192l-128-128z" fill="#4D5152" p-id="8422"></path><path d="M512 320c105.888 0 192 86.112 192 192s-86.112 192-192 192-192-86.112-192-192 86.112-192 192-192m0-32a224 224 0 1 0 0 448 224 224 0 0 0 0-448z" fill="#4D5152" p-id="8423"></path></symbol><symbol id="icon-moon" viewBox="0 0 1024 1024"><path d="M611.370667 167.082667a445.013333 445.013333 0 0 1-38.4 161.834666 477.824 477.824 0 0 1-244.736 244.394667 445.141333 445.141333 0 0 1-161.109334 38.058667 85.077333 85.077333 0 0 0-65.066666 135.722666A462.08 462.08 0 1 0 747.093333 102.058667a85.077333 85.077333 0 0 0-135.722666 65.024z" fill="#FFB531" p-id="11345"></path><path d="M329.728 274.133333l35.157333-35.157333a21.333333 21.333333 0 1 0-30.165333-30.165333l-35.157333 35.157333-35.114667-35.157333a21.333333 21.333333 0 0 0-30.165333 30.165333l35.114666 35.157333-35.114666 35.157334a21.333333 21.333333 0 1 0 30.165333 30.165333l35.114667-35.157333 35.157333 35.157333a21.333333 21.333333 0 1 0 30.165333-30.165333z" fill="#030835" p-id="11346"></path></symbol></svg><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css">
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"><!--#loading-box//.loading-left-bg
//.loading-right-bg
//.spinner-box
  //.configure-border-1
    //.configure-core
  //.configure-border-2
    //.configure-core
  //.loading-word= _p('loading')
--><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="wizard-scene"><div class="wizard-objects"><div class="wizard-square"></div><div class="wizard-circle"></div><div class="wizard-triangle"></div></div><div class="wizard"><div class="wizard-body"></div><div class="wizard-right-arm"><div class="wizard-right-hand"></div></div><div class="wizard-left-arm"><div class="wizard-left-hand"></div></div><div class="wizard-head"><div class="wizard-beard"></div><div class="wizard-face"><div class="wizard-adds"></div></div><div class="wizard-hat"><div class="wizard-hat-of-the-hat"></div><div class="wizard-four-point-star --first"></div><div class="wizard-four-point-star --second"></div><div class="wizard-four-point-star --third"></div></div></div></div></div></div><script async="async">(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = 'auto'
      //$loadingBox.classList.add('loaded')
      setTimeout(function(){
      document.getElementById('loading-box').classList.add("loaded")
    }, 1500);
      document.getElementById('loading-box').style.transition = 'opacity 1s ease 0.3s' //000
      document.getElementById('loading-box').style.opacity = '0'  // 000
    },
    initLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.remove('loaded')
      document.getElementById('loading-box').style.transition = '';  //000
      document.getElementById('loading-box').style.opacity = '1'  //000
    }
  }

  preloader.initLoading()
  // window.addEventListener('load',() => { preloader.endLoading() })
  setTimeout(function(){preloader.endLoading();}, 1500);
  document.getElementById('loading-box').addEventListener('click',()=> {preloader.endLoading()})

  if (true) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }

})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/yazi.gif" data-original="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">37</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">39</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 议事堂</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 藏书阁</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清风亭</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments"></i><span> 分享</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-space-shuttle"></i><span> 忘忧涧</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-cubes"></i><span> 2048</span></a></li><li><a class="site-page child" href="/catch_cat/"><i class="fa-fw fa fa-cat"></i><span> 抓猫猫</span></a></li><li><a class="site-page child" href="/flower/"><i class="fa-fw fa fa-eye"></i><span> 花花世界迷人眼</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 传送门</span></a></div><div class="menus_item"><a class="site-page" href="/Message/"><i class="fa-fw fa fa-paper-plane"></i><span> 打卡处</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/post_img/20231125235032.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="藕片侠大战论文怪"><span class="site-name">藕片侠大战论文怪</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 议事堂</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 藏书阁</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清风亭</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments"></i><span> 分享</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-space-shuttle"></i><span> 忘忧涧</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-cubes"></i><span> 2048</span></a></li><li><a class="site-page child" href="/catch_cat/"><i class="fa-fw fa fa-cat"></i><span> 抓猫猫</span></a></li><li><a class="site-page child" href="/flower/"><i class="fa-fw fa fa-eye"></i><span> 花花世界迷人眼</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 传送门</span></a></div><div class="menus_item"><a class="site-page" href="/Message/"><i class="fa-fw fa fa-paper-plane"></i><span> 打卡处</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Diffusion Model（二）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-14T01:05:36.341Z" title="发表于 2024-04-14 09:05:36">2024-04-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-14T05:42:42.040Z" title="更新于 2024-04-14 13:42:42">2024-04-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%8E%9F%E7%90%86%E7%AC%94%E8%AE%B0/">原理笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>32分钟</span></span><span class="post-meta-separator">|</span><span class="leancloud_visitors" id="/2024/04/14/Diffusion%20Model%20%EF%BC%88%E4%BA%8C%EF%BC%89/" data-flag-title="Diffusion Model（二）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span class="leancloud-visitors-count"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2024/04/14/Diffusion%20Model%20%EF%BC%88%E4%BA%8C%EF%BC%89/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2024/04/14/Diffusion%20Model%20%EF%BC%88%E4%BA%8C%EF%BC%89/" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><div class="top-img" style="background-image: url('https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/post_img/20231125235032.jpg');"></div><article class="post-content" id="article-container"><h1 id="引入">引入</h1>
<p><strong>模型网站：</strong></p>
<p>DALL·E2： <a target="_blank" rel="noopener" href="https://openai.com/dall-e-2">DALL·E 2 (openai.com)</a></p>
<p>DALL·E3：<a target="_blank" rel="noopener" href="https://openai.com/dall-e-3">DALL·E 3 (openai.com)</a></p>
<p>Sora: <a target="_blank" rel="noopener" href="https://openai.com/sora?ref=aihub.cn">Sora (openai.com)</a></p>
<h2 id="发展简介">发展简介</h2>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/593896912">李沐论文精读系列——由DALL·E 2看图像生成模型 - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://swarma.org/?p=37227">AI艺术的背后：详解文本生成图像模型 | 集智俱乐部 (swarma.org)</a></p>
</blockquote>
<p>近几年图像生成模型的工作：</p>
<figure>
<img src="/img/yazi.gif" data-original="https://pic1.zhimg.com/80/v2-6350528949be8a46b618e7fb84c28e04_720w.webp" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="基于vq-vae">基于VQ-VAE</h2>
<p>顺序：<code>AE</code>(20世纪80年代)——<code>VAE</code>（Kingma et al, 2014<sup>[2]</sup>）——<code>VQ-VAE</code>(Van et al, 2017<sup>[3]</sup>)——<code>DALL·E</code>（Ramesh et al, 2021[4]）</p>
<h3 id="ae">AE</h3>
<p>自编码器（Auto Encoder）主要由编码器 encoder 和解码器 decoder 组成，目标是重建出输入图像。首先，输入图像 x 通过编码器被压缩，提取出高维特征 latent feature，然后这些高维特征再经过解码器重建出图像 x' 。AE 希望重建出的图像和输入图像越接近越好，因此损失函数为 <span class="math inline">\(loss=||x-x'||^2\)</span> 。通常 latent feature 的维度比输入、输出的维度小，因此称之为 bottleneck。</p>
<figure>
<img src="/img/yazi.gif" data-original="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-d690623495c81ce7adb97e3eab397475.png" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>由于 AE 是一个自重建的过程，因此被称为自编码器，实际常用于降维，去噪，异常检测或者神经风格迁移中。</p>
<h3 id="vae">VAE</h3>
<p>VAE（Variational Auto Encoder）仍然由一个编码器和一个解码器构成，并且目标仍然是重建原始输入。与 AE 不同的是，VAE 的中间表征是通过采样高斯分布得到的 <span class="math inline">\(z\)</span> 。首先，VAE 将原图 <span class="math inline">\(x\)</span> 通过编码器网络映射为高斯分布 $( _x,_x ) $ 。然后通过重参数技巧在该分布中采样中间表征 $z=_x+_x$ ，其中 <span class="math inline">\(\epsilon\thicksim \mathcal{N}\left( 0,\mathbf{I} \right)\)</span> 。最后，解码器解码 <span class="math inline">\(z\)</span>​ 重建出图像。其损失函数定义为： <span class="math display">\[
loss=||x-\hat{x}||^2+D_{KL}\left[ \mathcal{N}\left( \mu _x,\sigma _x \right) ,\mathcal{N}\left( 0,\mathbf{I} \right) \right]
\]</span> 在 VAE 的损失函数中，第二项是之所以让编码器输出的分布尽可能接近标准正态分布，是因为这样在生成时可以直接从正态分布中采样，然后通过解码器重建图像（怪怪的）。</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/574959469">事实上，编码器拟合的是后验概率 <span class="math inline">\(p(z|x)\)</span> ，解码器拟合的是似然函数 <span class="math inline">\(p(x|z)\)</span>。</a>似然函数理解为，在模型参数 <span class="math inline">\(z\)</span> 下，使得模型输出为 <span class="math inline">\(x\)</span> 的概率最大化的函数。</p>
</blockquote>
<figure>
<img src="/img/yazi.gif" data-original="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-249c281b4571a5b27ac531512e7c1cc0.png" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>VAE 提高了生成结果的多样性（引入了符合高斯分布的随机变量）。对于同一个输入，由于 latent vector 不再是固定的，而是采样得到，我们可以得到不同的但类似的任意多个输出。</p>
<h3 id="vq-vae">VQ-VAE</h3>
<p>VAE 具有一个最大的问题就是使用了固定的先验（正态分布），其次是使用了连续的中间表征，这样会导致图片生成的多样性并不是很好以及模型的可控性差。为了解决这个问题，VQ-VAE（ Vector Quantized Variational Auto Encoder） 选择使用<strong>离散的中间表征</strong>，同时，通常会使用一个自回归模型来学习先验（例如 PixelCNN 或者 Transformer），在训练完成后，直接用来采样得到 <span class="math inline">\(z_q\)</span>​ ，然后通过匹配 Codebook, 使用解码器进行图片生成。</p>
<blockquote>
<p>VAE的目的是训练完成后, 丢掉 encoder, 在 prior 上直接采样, 加上 decoder 就能生成. 如果我们现在独立地采 <span class="math inline">\(H×W\)</span> 个 <span class="math inline">\(z\)</span> 组成 <span class="math inline">\(z_e(x)\)</span> , 然后查表得到维度为 <span class="math inline">\(H×W×D\)</span> 的 <span class="math inline">\(z_q(x)\)</span>，那么生成的图片在空间上的每块区域之间几乎就是独立的。<strong>因此我们需要让各个 <span class="math inline">\(z\)</span> 之间有关系</strong>。用 PixelCNN, 对这些 <span class="math inline">\(z\)</span> 建立一个自回归模型： <span class="math inline">\(p(z_1,z_2,z_3,...)=p(z_1)p(z_2|z_1)p(z_3|z_1,z_2)...\)</span> 这样就可以进行 <a target="_blank" rel="noopener" href="https://hyper.ai/wiki/2986">ancestral sampling</a> 生成 <span class="math inline">\(x\)</span>, 得到一个互相之间有关联的 <span class="math inline">\(H×W\)</span> 的整数矩阵。 <span class="math inline">\(p(z_1,z_2,z_3,...)\)</span> 这个联合概率即为我们想要的 prior。——<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/91434658">Elijha：VQ-VAE解读</a></p>
<p>The prior distribution over the discrete latents <span class="math inline">\(p(z)\)</span> is a categorical distribution, and can be made autoregressive by depending on other <span class="math inline">\(z\)</span> in the feature map. Whilst training the VQ-VAE, the prior is kept constant and uniform. After training, we fit an autoregressive distribution over <span class="math inline">\(z\)</span>, <span class="math inline">\(p(z)\)</span>, so that we can generate <span class="math inline">\(x\)</span> via ancestral sampling. We use a PixelCNN over the discrete latents for images, and a WaveNet for raw audio. Training the prior and the VQ-VAE jointly, which could strengthen our results, is left as future research. ——来自<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.00937.pdf">原论文</a></p>
</blockquote>
<figure>
<img src="/img/yazi.gif" data-original="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-b761643dc19adceb1734df82210eda76.png" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>VQ-VAE 的算法流程为：</strong></p>
<ol type="1">
<li><span style="background:#dad5e9;">（Embedding Space）</span> 首先设置 <span class="math inline">\(K\)</span> 个 <span class="math inline">\(D\)</span> 维向量 <span class="math inline">\(e_1,e_2...e_k\)</span> 作为可查询的 Codebook。</li>
<li><span style="background:#daf5e9;">（绿方块）</span> 输入图片通过编码器 CNN 来得到中间表征 <span class="math inline">\(z_e(x)\)</span>，<span class="math inline">\(z_e(x)\)</span> 是 <span class="math inline">\(H×W\)</span> 个 <span class="math inline">\(D\)</span> 维向量。</li>
<li><span style="background:#dcffff;">（蓝方块）</span> 通过最邻近算法，在 Codebook 中查询与 <span class="math inline">\(z_e(x)\)</span> 中每个 <span class="math inline">\(D\)</span> 维向量最相似的向量 <span class="math inline">\(e_i^{(1)},e_i^{(2)}...e_i^{(H×W)}\)</span> ，用 index 表示，就得到了 <span class="math inline">\(q(z|x)\)</span> 。</li>
<li><span style="background:#dad5e9;">（紫方块）</span>根据 <span class="math inline">\(q(z|x)\)</span> 将 Codebook 中查询的相似向量放到对应 <span class="math inline">\(z_e(x)\)</span> 的位置上，得到 <span class="math inline">\(z_q(x)\)</span>​ 。</li>
<li>解码器通过得到的中间表征 <span class="math inline">\(z_q(x)\)</span> 重建图片。</li>
</ol>
<p>一般 <span class="math inline">\(K=8192\)</span>，<span class="math inline">\(D=512\)</span> or <span class="math inline">\(768\)</span>。</p>
<p>VQ-VAE 最核心的部分就是 <strong>Codebook 查询操作</strong>，通过使用具有高度一致性的 Codebook 来代替混乱的中间表征，可以有效的提高图像生成的可控性。初代的DALL·E 模型就是基于 VQ-VAE 的架构实现的。</p>
<h3 id="delle">DELL·E</h3>
<blockquote>
<p>Project page: <a target="_blank" rel="noopener" href="https://openai.com/research/dall-e">DALL·E: Creating images from text (openai.com)</a></p>
</blockquote>
<p>文本生成图像模型 DALL·E 由 OpenAI 开发，其第一代版本使用的是在 VQ-VAE 自回归生成的基础上加上文本条件，实现了 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/624793654">zero-shot</a> 的 text-to-image 生成。</p>
<p>在 DELL·E 中使用了 Transformer 来作为自回归模型。在生成过程中，输入文本通过 Transformer 预测中间表征 <span class="math inline">\(z\)</span> ,然后匹配 <span class="math inline">\(K=8192\)</span> 的 Codebook得到 <span class="math inline">\(z_q\)</span> ，最后通过 Decoder 模块将 <span class="math inline">\(z_q\)</span>​ 解码为图像。 具体过程见<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/683882116">DALL-E 系列 (1-3) - 知乎 (zhihu.com)</a>。在 DALL-E 的论文中，作者还提出了很多技术上的细节，例如，在最后挑选图片的时候，可以使用 CLIP 模型来选择与文本相似度最高的模型，以及分布式训练，混合精度训练等，具体细节可以查看<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.12092.pdf">原论文</a>。</p>
<blockquote>
<p>CLIP (Contrastive Language-Image Pre-training，对比图文预训练) 是一种 zero-shot 的视觉分类模型，它是和 DALL·E 一起被发布的。</p>
<p>CLIP 是一种神经网络，为输入的图像返回最佳的标题。它所做的事情与 DALL-E 所做的相反 —— 它是将图像转换为文本，而 DALL-E 是将文本转换为图像。引入 CLIP 的目的是为了学习物体的视觉和文字表示之间的联系。</p>
</blockquote>
<figure>
<img src="/img/yazi.gif" data-original="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-5bb80123f487ae06090f48943cfa8322.png" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="基于-gan">基于 GAN</h2>
<p>顺序： <code>GAN</code>（Creswell et al, 2014<sup>[5]</sup>）——<code>VQGAN</code>（Esser et al, 2021<sup>[6]</sup>）——<code>VQGAN-CLIP</code>（Crowson et al, 2022<sup>[7]</sup>）——<code>DELL·E Mini</code>——<code>Parti</code>（Google, 2022 在 Imagen 之后一个月推出, AI 绘图）——<code>NUWA-Infinity</code>（微软亚洲研究院， 2022， 无限创作）</p>
<h3 id="gan">GAN</h3>
<p><strong>生成对抗网络</strong>（<strong>GAN</strong>，Generative Adversarial Networks）由两个主要的模块构成：生成器和判别器。生成器负责生成一张图片，而判别器则负责判断这张图片质量，也就是判断是真实样本还是生成的虚假样本，通过逐步的迭代，左右互博，最终生成器可以生成越来越逼真的图像，而判别器则可以更加精准的判断图片的真假。GAN 的最大优势是其<strong>不依赖于先验假设，而是通过迭代的方式逐渐学到数据的分布</strong> [6]。</p>
<p>其训练流程如下：</p>
<ol type="1">
<li>初始化一个生成器 <span class="math inline">\(G\)</span> 和一个判别器 <span class="math inline">\(D\)</span> ;</li>
<li>固定生成器 <span class="math inline">\(G\)</span> 的参数， 只更新判别器 <span class="math inline">\(D\)</span> 的参数。具体过程为：选择一部分真实样本，以及从生成器 <span class="math inline">\(G\)</span> 得到一些生成的样本，送入到判别器 <span class="math inline">\(D\)</span> 中，判别器需要判断哪些样本为真实的，哪些样本为生成的，通过与真实结果的误差来优化判别器;</li>
<li>固定判别器 <span class="math inline">\(D\)</span> 的参数, 只更新生成器 <span class="math inline">\(G\)</span> 的参数。具体过程为：使用生成器生成一部分样本， 将生成的样本喂入到判别器 <span class="math inline">\(D\)</span> 中，判别器会对其进行判断，优化生成器 <span class="math inline">\(G\)</span> 的参数，使得判别器将其判断为更加偏向于真实样本。</li>
</ol>
<h3 id="vqgan">VQGAN</h3>
<p>VQGAN （ Vector Quantized Generative Adversarial Networks ） 是一种 GAN 的变种，一种视觉生成模型,来自德国海德堡大学IWR研究团队受到 VQ-VAE 的启发，<strong>使用了 Codebook 来进行离散表征</strong>。</p>
<p>具体来说，预先定义 <span class="math inline">\(K\)</span> 个向量作为离散的特征查询表 Codebook。当一张图片被送入到 CNN Encoder 中后，会得到 <span class="math inline">\(h×w×n_z\)</span> 维的中间表征 <span class="math inline">\(\hat{z}\)</span> ,之后与 VQ-VAE 类似地查询 Codebook 得到 <span class="math inline">\(z_q\)</span> 。最后，CNN Decoder 也就是生成器 <span class="math inline">\(G\)</span> 根据得到的表征 <span class="math inline">\(z_q\)</span> 重建出图像。其中，通过 <span class="math inline">\(\hat{z}\)</span> 得到 <span class="math inline">\(z_q\)</span> 的公式为： <span class="math display">\[
z_q=q(\hat{z}):=\left( \underset{z_k\in \mathcal{Z}}{arg\ \min}||\hat{z}_{ij}-z_k|| \right) \in \mathbb{R}^{h×w×n_z}
\]</span> 同样地，这里的对于生成器部分的优化，需要使用与 VQ-VAE 一样的方法去进行：</p>
<p>在训练好 VQGAN 之后，在生成的时候，可以直接初始化一个 <span class="math inline">\(z_q\)</span> 去生成图像，然而，为了能够得到稳定的 <span class="math inline">\(z_q\)</span> ，需要使用一个模型对先验进行学习。这里使用了 Transformer 模型来学习 <span class="math inline">\(z_q\)</span> 中离散表征的序列，可以简单的将其建模为自回归模型 <span class="math inline">\(p(s)=\prod_i{p(s_i|s_{&lt;i})}\)</span> ，这样，我们只需要给定一个初始的随机向量，就可以通过 Transfomer 模型生成完整的 <span class="math inline">\(z_q\)</span> ，从而可以通过 CNN Decoder 模块生成最终的图像。</p>
<figure>
<img src="/img/yazi.gif" data-original="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-dfb34aff6b13b8f38a0ed05e4c0b0de1.png" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<blockquote>
<p>这里的判别器不是对每张整体图片进行判断，而是对图片的每一小块进行判断。</p>
</blockquote>
<h2 id="基于-diffusion">基于 Diffusion</h2>
<p>顺序: Diffusion（Sohl-Dickstein et al, 2015<sup>[12]</sup>）——Diffusion DDPM（ Ho et al, 2020<sup>[1]</sup>）——GLIDE（Nichol et al, 2021.12<sup>[8]</sup>）——DALL·E2（Ramesh, 2022.4<sup>[9]</sup>）——Imagen（Saharia, 2022.5<sup>[10]</sup>）——Stable Diffusion(Rombach et al, 2022<sup>[11]</sup>)——ChatGPT（2022.11）——ChatGPT-4（2023.3）——DALL·E3（2023.10）——Sora（2024.2）</p>
<h3 id="diffusion">Diffusion</h3>
<p>回忆上文提到的 VQ-VAE 以及 VQ-GAN，都是先通过编码器将图像映射到中间潜变量，然后解码器在通过中间潜变量进行还原。实际上，扩散模型做的事情本质上是一样的，不同的是，扩散模型完全使用了全新的思路来实现这个目标。</p>
<blockquote>
<p><strong>为什么叫“扩散”模型呢？</strong> 扩散这个名词来自于热力学，某个区域物质密度很高，那么它就会向低密度的区域扩散，最终达到平衡；就好比香水的味道会扩散到整个房间。这里的平衡指的是标准高斯分布的噪声。图片由原来的有序，逐渐变为无序的噪声，可以认为是“无序度”的扩散。</p>
</blockquote>
<p>在传统的扩散模型中，主要有两个过程组成，<strong>前向扩散过程，反向去噪过程</strong>，前向扩散过程主要是在一张图片上随机添加高斯噪声，而逆向去噪过程则是将一张随机噪音的图片还原为一张完整的图片。</p>
<p>最初的 Diffusion 在2015 年甚至更早就被提出了，当时有人基于非平衡热力学提出了一个纯数学的生成模型（Sohl-Dickstein et al, 2015<sup>[12]</sup>）。这种生成模型通过迭代正向扩散破坏数据分布结构，并通过学习反向扩散过程恢复数据结构。最初的模型并没有用代码实现。同时由于 forward 和 backward 训练采样和推理都需要很长时间，所以扩散模型一直不如 GAN 受关注，直到2020年 DDPM 的出现。DDPM 主要有两个贡献：</p>
<ol type="1">
<li>不需要推测 <span class="math inline">\(x_t\)</span> ，只需要推测 <span class="math inline">\(\epsilon_\theta\)</span> 就可以了。这样大大降低了推理的难度。这里计算梯度的网络是 U-net。</li>
<li>由于每步都是高斯噪声，只需要预测它的均值和方差即可。并且作者提出，固定方差 <span class="math inline">\(\sigma\)</span>​ ，仅仅预测噪声的均值就可以达到很好的效果。</li>
</ol>
<p>DDPM 中 Diffusion 的损失函数如下（具体推导见下文）： <span class="math display">\[
L=\lVert \boldsymbol{\epsilon }_t-\boldsymbol{\epsilon }_{\theta}\left( \sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon }_t,t \right) \rVert ^2
\]</span> 其算法流程为：</p>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240320200738823.png" alt="image-20240320200738823"><figcaption aria-hidden="true">image-20240320200738823</figcaption>
</figure>
<p>在完成训练之后，只需要通过重参数化技巧，进行采样操作即可，具体流程如上边右图所示，通过不断的「减去」模型预测的噪音，可以逐渐生成一张完整的图片。</p>
<p><strong>Diffusion 的初步改进： classifier guided diffusion</strong></p>
<p>DDPM的成功引起了大家的兴趣。2020年底， <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2102.09672.pdf">improved DDPM</a> 就出炉了。它在DDPM的基础上，不仅仅预测均值，而且预测方差；并且在更大模型上尝试了DDPM，发现大模型会带来很大的效果提升。于是紧接着有一篇 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.05233.pdf">Diffusion Models Beat GANs on Image Synthesis</a>（2021.5, OpenAI），进一步把模型做大、做复杂，取得了很好的效果；并且，作者在这篇论文里使用了 <code>classifier guidance</code> 的方法，引导模型做采样和生成。这不仅让生成的图像更逼真，而且加速了反向采样的速度，只需要25次采样就能从噪声还原出高质量的图片。</p>
<p>简单来说，<code>classifier guided diffusion</code>使用一个额外训练好的图像分类器 classifier ，每次判别 <span class="math inline">\(x_t\)</span> 的类别，得到交叉熵损失函数，进一步可以得到梯度 gradient ，用这个梯度去引导 <span class="math inline">\(f_\theta\)</span> （这里的 <span class="math inline">\(f_\theta\)</span>​ 是如上文的 U-net）做预测。这里的梯度大概暗含了当前生成的图像有没有某个物体，以及生成的物体真不真实。通过引导，扩散模型的逼真度提升了很多，击败了一些GAN模型。</p>
<figure>
<img src="/img/yazi.gif" data-original="https://pic3.zhimg.com/80/v2-9d9e7d2039c35f9f82056b3c180cd4c2_720w.webp" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>这和 conditional GAN 的思路差不多，提供更多的 condition，也就是引导，辅助它完成任务。这里的引导不一定非得是一个分类器，可以是文本，也可以是CLIP模型。</p>
<p><strong>Diffusion 的再次改进： Classifier-Free Guidance Diffusion</strong></p>
<p>后续又有一些改进操作，这些改进操作使得扩散模型被广泛的应用于文本生成图像任务中。其中，最常用的改进版本为 Classifier-Free Guidance Diffusion。OpenAI 后续的 <strong>GLIDE</strong> 模型和 <strong>DALL·E 2</strong> ，以及谷歌的 <strong>Imagen</strong>，在推理过程中抛弃了分类器的引导——Classifier free guidance。</p>
<p>这种扩散模型在每个时间步 <span class="math inline">\(t\)</span>， 除了原有的无引导情况下的输出 <span class="math inline">\(\epsilon_\theta (x_t)\)</span> 外，还增加了在有引导情况下的输出 <span class="math inline">\(\epsilon_\theta (x_t,y)\)</span> ，由此 Classifier-Free Guidance Diffusion 结合了条件和无条件噪声估计模型，定义为： <span class="math display">\[
\hat{\epsilon}_\theta(x_t|y)=\epsilon_\theta(x_t)+s·(\epsilon_\theta(x_t,y)-\epsilon_\theta(x_t))
\]</span> 通过有引导时的输出与无引导时的输出作差，就得到了一个方向，告诉网络如何从无引导的输出到达有引导的输出。这样在训练完成后，就能达到抛开引导的目的。这种改进优点是<strong>训练过程非常稳定，且摆脱了分类器的限制</strong>（实际上等价于学习了一个隐含的分类器），缺点是，训练成本比较高，相当于每次要生成两个输出，尽管如此，后面的大部份知名文本生成图像模型，都是基于这个方法进行的。</p>
<p>在使用了很多技巧之后，基于扩散模型的 <strong>GLIDE</strong> 用了35亿参数，效果就直逼基于VQ-VAE 的、用了120亿参数的<strong>DALL·E</strong> 模型。OpenAI 看到扩散模型确实靠谱，所以顺着 GLIDE 的思路，孕育出了现在的 <strong>DALL·E 2</strong>。</p>
<h3 id="glide">GLIDE</h3>
<blockquote>
<p>代码已开源</p>
</blockquote>
<p>GLIDE 使用了文本作为 condition 来引导扩散模型，主要用了两种策略: Classifier-Free Guidance Diffusion 和 CLIP 来作为条件监督。GLIDE 的扩散模型中的 <span class="math inline">\(\hat{\epsilon}_\theta(x_t|y)\)</span> 中的 <span class="math inline">\(y\)</span> 是一段文本描述。</p>
<p>GLIDE的生成效果。下图是GLIDE基于不同的文本提示生成的16个图像集，例如“使用计算器的刺猬”、“戴着红色领带和紫色帽子的柯基”等等，如图所示，生成的图像基本符合文本描述。</p>
<figure>
<img src="/img/yazi.gif" data-original="https://img-blog.csdnimg.cn/img_convert/d2e902a630e984757eb922d0b5662bdc.png" alt="d2e902a630e984757eb922d0b5662bdc.png"><figcaption aria-hidden="true">d2e902a630e984757eb922d0b5662bdc.png</figcaption>
</figure>
<p>除了图文转换，该论文还包括一个交互式系统的原型，支持通过 <strong>选取区域+文本Prompt</strong> 来对图像进行编辑操作，用于逐步细化图像的选定部分。使用过程中，只需要将遮蔽区域进行 mask，以及剩下的图片一起送入到网络中，即可产生补全之后的图片。</p>
<figure>
<img src="/img/yazi.gif" data-original="https://img-blog.csdnimg.cn/img_convert/da853db5c9f392a658cf03922989c6c4.png" alt="da853db5c9f392a658cf03922989c6c4.png"><figcaption aria-hidden="true">da853db5c9f392a658cf03922989c6c4.png</figcaption>
</figure>
<p>此外，<strong>GLIDE 的语义理解能力并不是很强，在一些少见的文本描述下（如八条腿的猫），很难产生合乎逻辑的图像，而 DALL-E2 在这方面的能力上，要远超 GLIDE</strong></p>
<figure>
<img src="/img/yazi.gif" data-original="https://img-blog.csdnimg.cn/img_convert/65b3e3049c10e65525e3e5e92f57b60b.png" alt="65b3e3049c10e65525e3e5e92f57b60b.png"><figcaption aria-hidden="true">65b3e3049c10e65525e3e5e92f57b60b.png</figcaption>
</figure>
<h3 id="dalle2">DALL·E2</h3>
<p>DALL-E2 是 OpenAI 2022年4月推出的 AI 生成图像模型，其最大的特色是<strong>模型具有惊人的理解力和创造力</strong>，它可以根据给定的概念、特性以及风格来生成原创性的图片。除此之外，DALL·E 2 还能根据描述，对已有的图片进行二次编辑，比如移除或添加某个物体，并且把阴影、反射、纹理考虑在内。还有，就算不给定语言描述，DALL·E 2 也能根据已有的图片，生成一系列风格相似的新的图片。 其参数大约 3.5B , 相对于上一代版本，DALL-E2 可以生成4倍分倍率的图片，且非常贴合语义信息。作者使用了人工评测方法，让志愿者看1000张图，71.7% 的人认为其更加匹配文本描述 ，88.8% 认为画的图相对于上一代版本更加好看。</p>
<figure>
<img src="/img/yazi.gif" data-original="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-0544da18e16e0b011f1d4d85d5b1cf07.png" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>DALL-E2 由三个模块组成</strong>（相当于两阶段生成，第一阶段用 prior 模型从文本生成图像特征；第二阶段用 decoder 生成图像）：</p>
<ul>
<li>CLIP模型，对齐图片文本表征</li>
<li>先验模型，接收文本信息，将其转换成 CLIP 图像表征</li>
<li>扩散模型，接受图像表征，解码来生成完整图像</li>
</ul>
<figure>
<img src="/img/yazi.gif" data-original="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-0fce8bc8d73882ce798bb7e4b18763df.png" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>DALL-E2 的训练过程</strong>为：</p>
<ul>
<li>训练一个 CLIP 模型，使其能够对齐文本和图片特征。</li>
<li>训练一个先验模型，由自回归模型或者一个扩散先验模型（实验证明，扩散先验模型表现更好。这里的扩散模型是用 sequence (text feature) 预测 sequence (image feature)，没有要求前后尺寸不变。作者这里没有用 U-net，而是用了 Transformer——将CLIP的文本编码、加入噪声的CLIP的图像编码、扩散时间步的time embedding等输入，去预测未加噪声的CLIP图像编码。），其功能是将文本表征映射为图片表征。</li>
<li>训练一个扩散解码模型，其目标是根据图片表征，还原原始图片。</li>
</ul>
<p>在训练完成之后，推理过程就比较直接了，首先使用CLIP 文本编码器，获得文本编码，之后使用先验模型将文本编码映射为图片编码，最后使用扩散解码器用图片编码生成完整图片。注意这里扩散解码模型使用的是经过修改的 GLIDE 扩散模型，其生成的图像尺寸为 64×64，然后使用两个上采样扩散模型将其上采样至 256×256，以及 1024×1024。</p>
<p>DALL·E2 原论文中也提到了其许多不足，例如容易将物体和属性混淆，无法精确的将文本放置到图像中，以及社会伦理道德等方面的问题。</p>
<p>谷歌的 Imagen 模型是继 DALL·E2 的后续工作，它没有用两阶段的生成，直接用了一个 U-net 搞定，更简单，效果也好。另外谷歌2022年6月份最新的基于 GAN 的 <a target="_blank" rel="noopener" href="https://sites.research.google/parti/">Pathways Autoregressive Text-to-Image Model</a>（Parti）模型，用 200亿参数的 Pathways 模型做自回归的图像生成，效果直接超越了 DALL·E 2 和 Imagen。</p>
<h3 id="imagen">Imagen</h3>
<p>在 DALL-E2 提出没多久，Google 就提出了一个新的文本生成图像模型 Imagen，论文中提到，<strong>其生成的图片相对于 DALL-E2 真实感和语言理解能力都更加强大</strong>（使用一种新的评测方法 DrawBench）。</p>
<figure>
<img src="/img/yazi.gif" data-original="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-67fa2ff5f289e376304e905a8c87eff7.png" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>Imagen 的模型结构与 DALL·E2 十分类似，首先将文本进行编码表征，然后使用扩散模型将表征解码为完整图像，最后使用两个 text-conditional super-resolution 的扩散模型将其上采样至 256×256，以及 1024×1024。不同的是，<strong>Imagen 使用了 T5-XXL 模型直接编码文本信息，然后使用条件扩散模型，直接用文本编码生成图像。因此，在 Imagen 中，无需学习先验模型。</strong></p>
<figure>
<img src="/img/yazi.gif" data-original="https://pic2.zhimg.com/80/v2-be42b33c6d55d5fece5b8ae9866e42a9_720w.webp" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>由于直接使用 T5-XXL 模型，其语义知识相对于 CLIP 要丰富很多（图文匹配数据集数量要远远少于纯文本数据集数量），因此 Imagen 相对于 DALL-E2 在语义保真度上做的更好。同时，作者也发现，增大语言模型，可以有效的提高样本的语义保真度。</p>
<h3 id="stable-diffusion">Stable Diffusion</h3>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/593896912">李沐论文精读系列——由DALL·E 2看图像生成模型 - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/560645985">图像生成：VQGAN，CLIP，DALLE，DIFFUSION - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://swarma.org/?p=37227">AI艺术的背后：详解文本生成图像模型 | 集智俱乐部 (swarma.org)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/640545463">一文读懂Stable Diffusion 论文原理+代码超详细解读 - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/imwaters/article/details/127269368">【论文简介】Stable Diffusion的基础论文:2112.High-Resolution Image Synthesis with Latent Diffusion Models_stable diffusion论文-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_31711799/article/details/130874306">【AI绘图】一、stable diffusion的发展史_stable diffusion研究现状-CSDN博客</a></p>
</blockquote>
<p>DALL·E3</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/660733999#:~:text=DALL-E%203%20将于%2010,月初向%20ChatGPT%20Plus%20和企业客户推出。">OpenAI发布DALL-E 3 | 原理简介 - 知乎 (zhihu.com)</a></p>
</blockquote>
<h3 id="sora">Sora</h3>
<p>Sora 是由 OpenAI 于 2024.2.15 发布的文生视频大模型。</p>
<h2 id="各生成模型对比">各生成模型对比</h2>
<p>翻译自 Lil' Log:</p>
<p>GAN、VAE 和流动模型等生成模型在生成高质量 samples 方面取得了巨大成功，但它们都有其自身的局限性。GAN模型因对抗性训练性质而具有潜在的不稳定训练和较少的生成多样性（需要精心选择的超参数和正则化器）。VAE依赖于替代损失 surrogate loss 。流动模型必须使用专门的架构来构建可逆转换。</p>
<p>扩散模型的灵感来自非平衡热力学。他们定义了一个扩散步骤的马尔可夫链，以缓慢地将随机噪声添加到数据中，然后学习反向扩散过程以从噪声中构建所需的数据样本。与VAE或流动模型不同，扩散模型是通过固定程序学习的，并且潜在变量 latent code（z）具有高维数（与原图同尺寸大小）。</p>
<figure>
<img src="/img/yazi.gif" data-original="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png" alt="Edge Image Viewer (lilianweng.github.io)"><figcaption aria-hidden="true">Edge Image Viewer (lilianweng.github.io)</figcaption>
</figure>
<blockquote>
<p>各种生成模型的对比图，来自<a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models? | Lil'Log (lilianweng.github.io)</a></p>
</blockquote>
<h1 id="ddpm">DDPM</h1>
<details>
<summary>
参考链接
</summary>
<p>
<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/525106459">由浅入深了解Diffusion Model - 知乎 (zhihu.com)</a>
</p>
<p>
<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/576475987">扩散模型 (Diffusion Model) 简要介绍与源码分析 - 知乎 (zhihu.com)</a>
</p>
<p>
<a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models? | Lil'Log (lilianweng.github.io)</a>
</p>
<p>
<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/601641045">Diffusion Model学习笔记(1)——DDPM - 知乎 (zhihu.com)</a>
</p>
</details>
<p>本文主要基于 <strong>DDPM：Denoising Diffusion Probabilistic Models</strong><sup>[1]</sup> [Ho et al. 2020] 展开 Diffusion 模型的原理推导。</p>
<h1 id="forward-与-backward-过程">forward 与 backward 过程</h1>
<p>如下，存在一系列高斯噪声（ <span class="math inline">\(T\)</span> 轮），将输入图片 <span class="math inline">\(x_0\)</span> 变为纯高斯噪声 <span class="math inline">\(x_T\)</span> 。而我们的模型则负责将 <span class="math inline">\(x_T\)</span> 复原回图片 <span class="math inline">\(x_0\)</span> 。这样一来其实 diffusion model 和 GAN 很像，都是给定噪声 <span class="math inline">\(x_t\)</span> 生成图片 <span class="math inline">\(x_0\)</span> ，但是要强调的是，这里噪声 <span class="math inline">\(x_T\)</span> 与图片 <span class="math inline">\(x_0\)</span> 是<strong>同维度</strong>的。</p>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240315_145856_0.jpg" alt="20240315_145856_0"><figcaption aria-hidden="true">20240315_145856_0</figcaption>
</figure>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240315_145951_1.jpg" alt="20240315_145951_1"><figcaption aria-hidden="true">20240315_145951_1</figcaption>
</figure>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/diff222.jpg" alt="20240315_150015_2"><figcaption aria-hidden="true">20240315_150015_2</figcaption>
</figure>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240315_150043_3.jpg" alt="20240315_150043_3"><figcaption aria-hidden="true">20240315_150043_3</figcaption>
</figure>
<h1 id="模型训练">模型训练</h1>
<h2 id="预备知识">预备知识</h2>
<blockquote>
<p>关于变分下界与KL散度参考： <a target="_blank" rel="noopener" href="https://bluefisher.github.io/2020/02/06/理解-Variational-Lower-Bound/">理解 Variational Lower Bound | Fisher's Blog (bluefisher.github.io)</a></p>
</blockquote>
<p><strong>变分贝叶斯</strong>：变分贝叶斯（Variational Bayesian, VB）是一类非常受欢迎的统计类机器学习方法。VB 非常有用的一个特性是推断优化的二元性：<u>我们可以将统计推断问题（从一个随机变量的值推断出另一种随机变量的值）作为优化问题（找到参变量的值来最小化某些目标函数）</u>。另外，<strong>variational lower bound</strong> ，也被称作 <strong>evidence lower bound</strong> (ELBO)，在 VB 的推导中起了非常重要的作用。</p>
<p><strong>变分下界</strong>：变分下界（variational lower bound 或 evidence lower bound, ELBO）是一种用于估计概率分布参数的方法。它通过最大化似然函数的下界来逼近真实的后验概率分布。这种方法可以用于在无法直接计算后验概率分布的情况下，近似地推断参数的取值。</p>
<p>已知变分贝叶斯（VB）将统计推断问题，即从一种随机变量推断出另一种随机变量的值，作为优化问题。</p>
<p>这里，将问题设置成从观测值（observed variable） <span class="math inline">\(X\)</span> 推断隐变量（hidden/latent variable） <span class="math inline">\(Z\)</span>​ ,由于两者之间存在关系：</p>
<blockquote>
<p>隐变量 <span class="math inline">\(Z\)</span> 可能包含参数 <span class="math inline">\(\theta\)</span> 。</p>
</blockquote>
<figure>
<img src="/img/yazi.gif" data-original="https://bluefisher.github.io/images/2020-02-06-%E7%90%86%E8%A7%A3-Variational-Lower-Bound/image-20200206195028885.png" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>因此，<span class="math inline">\(X \rightarrow Z\)</span> 可视为求隐变量的后验概率： <span class="math display">\[
P\left( Z|X \right) =\frac{P\left( X|Z \right) P\left( Z \right)}{P\left( X \right)}=\frac{P\left( X|Z \right) P\left( Z \right)}{\int_Z{p\left( X,Z \right)}}
\]</span> 其中，大写的 <span class="math inline">\(P(X)\)</span> 表示某个变量的概率分布，小写的 <span class="math inline">\(p(X)\)</span> 表示 <span class="math inline">\(X\)</span> 分布的概率密度函数。</p>
<h3 id="推导变分下界-l">推导变分下界 <span class="math inline">\(L\)</span></h3>
<p>从观测值 <span class="math inline">\(X\)</span> 的边缘分布出发，设 <span class="math inline">\(q(Z)\)</span> 为VB中后验概率 <span class="math inline">\(p(Z|X)\)</span> 的估计概率，则： <span class="math display">\[
\begin{align}
\log P(X) &amp;=\log \int_{Z} p(X, Z) \tag{1}\\
&amp;=\log \int_{Z} p(X, Z) \frac{q(Z)}{q(Z)} \tag{2}\\
&amp;=\log \left(\mathbb{E}_{q}\left[\frac{p(X, Z)}{q(Z)}\right]\right) \tag{3}\\
&amp; \geq \mathbb{E}_{q}\left[\log \frac{p(X, Z)}{q(Z)}\right] \tag{4}\\
&amp;=\mathbb{E}_{q}[\log p(X, Z)]+H[Z] \tag{5}\\
&amp;=L\ \ --\ variational\ lower\ bound
\end{align}
\]</span> 从公式（2）到公式（3）运用到期望函数的定义： <span class="math inline">\(E\left( f\left( x \right) \right) =\int{xf\left( x \right) dx}\)</span> 。公式（4）对凸函数 log 运用了<strong>琴生不等式</strong>： <span class="math inline">\(f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)]\)</span> 。公式（5）中 <span class="math inline">\(H[Z]=-\mathbb{E}_{q}[\log q(Z)]\)</span> 是香农熵。</p>
<p>我们做标记： <span class="math display">\[
L=\mathbb{E}_{q}[\log p(X, Z)]+H[Z]
\]</span> 很明显 <span class="math inline">\(L\)</span> 就是观测变量的 log 概率的一个 lower bound。<strong>也就是说，如果我们想要去最大化边缘分布，我们可以转而最大化它的 variational lower bound <span class="math inline">\(L\)</span></strong> 。</p>
<h3 id="推导-kl-散度相对熵">推导 KL 散度/相对熵</h3>
<p>在很多情况下，后验概率 <span class="math inline">\(P(Z|X)\)</span> 的计算是十分困难的，比如我们可能需要对所有的隐变量做积分（求和）来计算分母。</p>
<p>变分方法的主要思想就是找一个估计的概率分布 <span class="math inline">\(q(Z)\)</span> 来尽可能地接近后验概率 <span class="math inline">\(p(Z|X)\)</span> 。这些估计的概率分布可以有它们独有的<em>变分参数（variational parameters）</em>：<span class="math inline">\(q(Z|\theta)\)</span> ，所以我们想要去寻找这些参数来使 <span class="math inline">\(q(Z)\)</span> 尽可能接近后验概率。当然 <span class="math inline">\(q(Z)\)</span> 的分布肯定要在推断中相对来说更加简单好求一些。</p>
<p>为了衡量两个概率分布 <span class="math inline">\(q(Z)\)</span> 和 <span class="math inline">\(p(Z|X)\)</span> 的相似程度，一个常用的标准就是就是 <strong>Kullback-Leibler (KL) 散度</strong>。其计算如下：</p>
<blockquote>
<p>来自百度百科：https://baike.baidu.com/item/%E7%9B%B8%E5%AF%B9%E7%86%B5/4233536</p>
<p><strong>相对熵（relative entropy）</strong>，又被称为KL散度（Kullback-Leibler divergence）或信息散度（information divergence），是两个概率分布间差异的非对称性度量。在信息理论中，<strong>相对熵等价于两个概率分布的信息熵（Shannon entropy）的差值</strong>。</p>
<p>相对熵是一些优化算法，例如最大期望算法（Expectation-Maximization algorithm, EM）的损失函数。此时参与计算的一个概率分布为真实分布，另一个为理论（拟合）分布，相对熵表示使用理论分布拟合真实分布时产生的信息损耗。</p>
<p>设 <span class="math inline">\(P(x),Q(x)\)</span> 是随机变量 <span class="math inline">\(X\)</span> 上的两个概率分布，则在离散和连续随机变量的情形下，相对熵的定义分别为: <span class="math display">\[
KL\left( P||Q \right) =\sum{P\left( x \right) \log \frac{P\left( x \right)}{Q\left( x \right)}} \\
KL\left( P||Q \right) =\int{P\left( x \right) \log \frac{P\left( x \right)}{Q\left( x \right)}}dx
\]</span></p>
<p><strong>相对熵具有非负性</strong>，根据<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%90%89%E5%B8%83%E6%96%AF%E4%B8%8D%E7%AD%89%E5%BC%8F/22780937">吉布斯不等式</a>推导离散时的情况如下（事实上吉布斯不等式可以直接得到<span class="math inline">\(0 \geq \sum_{i=1}^{n} p_{i} \log q_{i}-\sum_{i=1}^{n} p_{i} \log p_{i}=\sum_{i=1}^{n} p_{i} \log \left(q_{i} / p_{i}\right)=-D_{\mathrm{KL}}(P \| Q)\)</span> ，百度百科这里大等于号后还用到了琴生不等式，这是由于通过琴生不等式可以证明吉布斯不等式）。由于 -log<span class="math inline">\(x\)</span> 是凸函数，因此根据相对熵的定义有： <span class="math display">\[
\mathrm{KL}(P \| Q)=\sum_{x \in X} P(x) \log \frac{P(x)}{Q(x)}=-E\left[\log \frac{Q(x)}{P(x)}\right] 
\geq-\log \left[\sum_{x \in X} P(x) \frac{Q(x)}{P(x)}\right]=-\log \left[\sum_{x \in X} Q(x)\right]=0
\]</span> 由上可知，相对熵是恒大于等于0的。当且仅当两分布相同时，相对熵等于0。</p>
</blockquote>
<p><span class="math display">\[
\begin{align} 
K L[q(Z) \| p(Z | X)] &amp;=\int_{Z} q(Z) \log \frac{q(Z)}{p(Z | X)} \tag{6}\\ 
&amp;=-\int_{Z} q(Z) \log \frac{p(Z | X)}{q(Z)} \tag{7}\\ 
&amp;=-\left(\int_{Z} q(Z) \log \frac{p(X, Z)}{q(Z)}-\int_{Z} q(Z) \log p(X)\right) \tag{8}\\ 
&amp;=-\int_{Z} q(Z) \log \frac{p(X, Z)}{q(Z)}+\log p(X) \int_{Z} q(Z) \tag{9}\\ 
&amp;=-L+\log p(X) \tag{10}
\end{align}
\]</span></p>
<p>这里 <span class="math inline">\(L\)</span> 是变分下界 variational lower bound 。公式 (10) 是归一化常量 $ _{Z} q(Z) =1$ 而推导得来。整理可得： <span class="math display">\[
L=\log p(X)-K L[q(Z) \| p(Z | X)]
\]</span> 因为 KL 散度永远是 ≥0 的，所以，再一次，我们得到了 <span class="math inline">\(L \le log\ p(X)\)</span> 是观测变量的分布的一个 lower bound。同时我们也知道了它们之间的区别就在于估计分布和真实分布之间的 KL 散度。换句话说，如果估计分布与真实后验分布完美接近，那么 lower bound <span class="math inline">\(L\)</span>​ 就等于 log 概率。</p>
<h3 id="推导交叉熵">推导交叉熵</h3>
<blockquote>
<p>参考自：<a target="_blank" rel="noopener" href="https://blog.csdn.net/tsyccnh/article/details/79163834">一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉-CSDN博客</a></p>
<p>简洁版介绍看：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/115277553">损失函数：交叉熵详解 - 知乎 (zhihu.com)</a></p>
</blockquote>
<p>在此前推导相对熵时，注解里给出了离散情况下 <span class="math inline">\(P(x)、Q(x)\)</span> 两种概率分布的相对熵计算公式： <span class="math display">\[
D_{KL}\left( p||q \right) =\sum_{i=1}^n{p\left( x_i \right) \log \frac{p\left( x_i \right)}{q\left( x_i \right)}}
\]</span> 对该式变形可得： <span class="math display">\[
\begin{aligned}
    D_{KL}\left( p|q \right) &amp;=\sum_{i=1}^n{p}\left( x_i \right) \log \left( p\left( x_i \right) \right) -\sum_{i=1}^n{p}\left( x_i \right) \log \left( q\left( x_i \right) \right)\\
    &amp;=-H\left( p\left( x \right) \right) +\left[ -\sum_{i=1}^n{p}\left( x_i \right) \log \left( q\left( x_i \right) \right) \right]\\
    &amp;=-H\left( p\left( x \right) \right) +E_p\left[ -\log \left( q\left( x_i \right) \right) \right]\\
    &amp;=-H\left( p\left( x \right) \right) +H\left( p,q \right)\\
\end{aligned}
\]</span> 即 KL 散度等式的前一部分恰巧就是 <span class="math inline">\(p\)</span> 的熵，后一部分正是交叉熵 <span class="math inline">\(H(p,q)\)</span> （<u>交叉熵 <span class="math inline">\(H(p,q)\)</span> 可理解为把来自一个分布 <span class="math inline">\(q\)</span> 的消息使用另一个分布 <span class="math inline">\(p\)</span> 的最佳代码传达的平均消息长度</u>）： <span class="math display">\[
H\left( p,q \right) =E_p\left[ -\log \left( q\left( x_i \right) \right) \right] =-\sum_{i=1}^n{p}\left( x_i \right) \log \left( q\left( x_i \right) \right)
\]</span> 在机器学习中，如果 <span class="math inline">\(P(x)\)</span> 用来表示样本的真实分布，而 <span class="math inline">\(Q(x)\)</span> 用来表示模型所预测的分布。如果我们需要评估 label 和 predicts 之间的差距，可以使用 KL 散度，即 $D_{KL}( p|q ) $ 。<strong>由于 KL 散度中的前一部分 <span class="math inline">\(-H(p)\)</span> 只与真值有关，是不变的，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做loss，评估模型</strong>。</p>
<h2 id="损失函数">损失函数</h2>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240320_194222_0.jpg" alt="20240320_194222_0"><figcaption aria-hidden="true">20240320_194222_0</figcaption>
</figure>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/diff22111.jpg" alt="20240320_194348_1"><figcaption aria-hidden="true">20240320_194348_1</figcaption>
</figure>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240320_194446_2.jpg" alt="20240320_194446_2"><figcaption aria-hidden="true">20240320_194446_2</figcaption>
</figure>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/diff22333.jpg" alt="20240320_194530_3"><figcaption aria-hidden="true">20240320_194530_3</figcaption>
</figure>
<p><strong>注3</strong>： 上述&lt;8&gt;式去掉权重系数是因为 DDPM 发现这样能使得 diffusion 模型工作得更好。</p>
<p>最后的简化形式： <span class="math display">\[
L_{simple}=L^{simple}_t+C
\]</span> 其中， <span class="math inline">\(C\)</span> 是与模型参数 <span class="math inline">\(\theta\)</span> 无关的常量。</p>
<h1 id="最终算法流程">最终算法流程</h1>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240320200738823.png" alt="image-20240320200738823"><figcaption aria-hidden="true">image-20240320200738823</figcaption>
</figure>
<blockquote>
<p>The training and sampling algorithms in DDPM (Image source: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.11239">Ho et al. 2020</a>)</p>
</blockquote>
<p><strong>训练阶段</strong>重复如下步骤:</p>
<ul>
<li>从数据集中采样 <span class="math inline">\(x_0\)</span></li>
<li>从 <span class="math inline">\(1...T\)</span> 随机采样 time step <span class="math inline">\(t\)</span> ：对该 batch 中的每个图像随机取 <span class="math inline">\(t\)</span> 进行训练</li>
<li>从标准高斯分布采样噪声 $_t( 0, ) $</li>
<li>调用模型预估 $_{}( _0+_t,t ) $：这里使用的模型主要是 U-net 模型</li>
<li>计算最小化噪声之间的 MSE Loss: $<em>t-</em>{}( _0+_t,t ) ^2 $, 并利用反向传播算法训练模型.</li>
</ul>
<p><strong>逆向阶段</strong>采用如下步骤进行采样:</p>
<ul>
<li>从高斯分布采样 <span class="math inline">\(x_T\)</span></li>
<li>按照 <span class="math inline">\(T,...,1\)</span> 的顺序进行迭代:
<ul>
<li>如果 <span class="math inline">\(t=1\)</span> , 令 <span class="math inline">\(\mathbf{z}=0\)</span> ; 如果 <span class="math inline">\(t&gt;1\)</span> , 从高斯分布中采样 $( 0, ) $ <span style="background:#FFCC99;">（最后一步不加噪音）</span></li>
<li>利用式 &lt;6&gt; 学习出均值 $_{}( _t,t ) =( <em>t-</em>{}( _t,t ) ) $ , 并计算均方差 <span class="math inline">\(\sigma _t=\sqrt{\tilde{\beta}_t}=\sqrt{\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\cdot \beta _t}\)</span></li>
<li>通过重参数技巧采样 <span class="math inline">\(x_{t-1}=\mu _{\theta}\left( x_t,t \right) +\sigma _t\mathbf{z}\)</span></li>
</ul></li>
<li>经过以上过程的迭代, 最终恢复 <span class="math inline">\(x_0\)</span> .</li>
</ul>
<h1 id="参考文献">参考文献</h1>
<p>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.11239.pdf">Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.6114">Kingma D P, Welling M. Auto-encoding variational bayes[J]. arxiv preprint arxiv:1312.6114, 2013.</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/684456268">Van Den Oord A, Vinyals O. Neural discrete representation learning[J]. Advances in neural information processing systems, 2017, 30.</a></p>
<p>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.12092.pdf">Ramesh A, Pavlov M, Goh G, et al. Zero-shot text-to-image generation[C]International conference on machine learning. Pmlr, 2021: 8821-8831.</a></p>
<p>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1406.2661.pdf">Creswell A, White T, Dumoulin V, et al. Generative adversarial networks: An overview[J]. IEEE signal processing magazine, 2018, 35(1): 53-65.</a></p>
<p>[6] <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html">Esser P, Rombach R, Ommer B. Taming transformers for high-resolution image synthesis[C]Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 12873-12883.</a></p>
<p>[7] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2204.08583">Crowson K, Biderman S, Kornis D, et al. Vqgan-clip: Open domain image generation and editing with natural language guidance[C]European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022: 88-105.</a></p>
<p>[8] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.10741.pdf">Nichol A, Dhariwal P, Ramesh A, et al. Glide: Towards photorealistic image generation and editing with text-guided diffusion models[J]. arixv preprint arixv:2112.10741, 2021.</a></p>
<p>[9] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.06125.pdf">Ramesh A, Dhariwal P, Nichol A, et al. Hierarchical text-conditional image generation with clip latents[J]. arixv preprint arixv:2204.06125, 2022, 1(2): 3.</a></p>
<p>[10] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.11487.pdf">Saharia C, Chan W, Saxena S, et al. Photorealistic text-to-image diffusion models with deep language understanding[J]. Advances in neural information processing systems, 2022, 35: 36479-36494.</a></p>
<p>[11] <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf">Rombach R, Blattmann A, Lorenz D, et al. High-resolution image synthesis with latent diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 10684-10695.</a></p>
<p>[12] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.03585">Sohl-Dickstein J, Weiss E, Maheswaranathan N, et al. Deep unsupervised learning using nonequilibrium thermodynamics[C]//International conference on machine learning. PMLR, 2015: 2256-2265.</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">胖胖大藕片</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/04/14/Diffusion%20Model%20%EF%BC%88%E4%BA%8C%EF%BC%89/">http://example.com/2024/04/14/Diffusion Model （二）/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">藕片侠大战论文怪</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/post_img/20231125235032.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer=""></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/yazi.gif" data-original="/img/wechat.jpg" alt="微信"></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/yazi.gif" data-original="/img/alipay.jpg" alt="支付宝"></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/04/19/%E7%82%B9%E4%BA%91%E4%B8%8E%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" title=""><img class="cover" src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/hexue.jpg" onerror="onerror=null;src='/img/problem.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info"></div></div></a></div><div class="next-post pull-right"><a href="/2024/04/11/%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E6%A6%82%E8%BF%B0/" title="最优化问题概述"><img class="cover" src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/post_img/20231125235146.jpg" onerror="onerror=null;src='/img/problem.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">最优化问题概述</div></div></a></div></nav><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-switch"><span class="first-comment">Valine</span><span id="switch-btn"></span><span class="second-comment">Twikoo</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/yazi.gif" data-original="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info__name">胖胖大藕片</div><div class="author-info__description">人生无完美，曲折亦风景</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">37</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">39</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/hahahaha5606"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/hahahaha5606" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:212188767@qq.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content"></div><timing></timing></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%95%E5%85%A5"><span class="toc-number">1.</span> <span class="toc-text">引入</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%91%E5%B1%95%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">发展简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8Evq-vae"><span class="toc-number">1.2.</span> <span class="toc-text">基于VQ-VAE</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ae"><span class="toc-number">1.2.1.</span> <span class="toc-text">AE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#vae"><span class="toc-number">1.2.2.</span> <span class="toc-text">VAE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#vq-vae"><span class="toc-number">1.2.3.</span> <span class="toc-text">VQ-VAE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#delle"><span class="toc-number">1.2.4.</span> <span class="toc-text">DELL·E</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E-gan"><span class="toc-number">1.3.</span> <span class="toc-text">基于 GAN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#gan"><span class="toc-number">1.3.1.</span> <span class="toc-text">GAN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#vqgan"><span class="toc-number">1.3.2.</span> <span class="toc-text">VQGAN</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E-diffusion"><span class="toc-number">1.4.</span> <span class="toc-text">基于 Diffusion</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#diffusion"><span class="toc-number">1.4.1.</span> <span class="toc-text">Diffusion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#glide"><span class="toc-number">1.4.2.</span> <span class="toc-text">GLIDE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dalle2"><span class="toc-number">1.4.3.</span> <span class="toc-text">DALL·E2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#imagen"><span class="toc-number">1.4.4.</span> <span class="toc-text">Imagen</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#stable-diffusion"><span class="toc-number">1.4.5.</span> <span class="toc-text">Stable Diffusion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sora"><span class="toc-number">1.4.6.</span> <span class="toc-text">Sora</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%84%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94"><span class="toc-number">1.5.</span> <span class="toc-text">各生成模型对比</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ddpm"><span class="toc-number">2.</span> <span class="toc-text">DDPM</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#forward-%E4%B8%8E-backward-%E8%BF%87%E7%A8%8B"><span class="toc-number">3.</span> <span class="toc-text">forward 与 backward 过程</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">4.</span> <span class="toc-text">模型训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="toc-number">4.1.</span> <span class="toc-text">预备知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E5%AF%BC%E5%8F%98%E5%88%86%E4%B8%8B%E7%95%8C-l"><span class="toc-number">4.1.1.</span> <span class="toc-text">推导变分下界 \(L\)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E5%AF%BC-kl-%E6%95%A3%E5%BA%A6%E7%9B%B8%E5%AF%B9%E7%86%B5"><span class="toc-number">4.1.2.</span> <span class="toc-text">推导 KL 散度/相对熵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E5%AF%BC%E4%BA%A4%E5%8F%89%E7%86%B5"><span class="toc-number">4.1.3.</span> <span class="toc-text">推导交叉熵</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">4.2.</span> <span class="toc-text">损失函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%80%E7%BB%88%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="toc-number">5.</span> <span class="toc-text">最终算法流程</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">6.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/04/22/%E5%88%B6%E4%BD%9C%20PPT%20%E8%BD%AC%20PDF%20%E5%B0%8F%E7%A8%8B%E5%BA%8F/" title="PPT 转 PDF 小程序"><img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/sonnnwwww.png" onerror="this.onerror=null;this.src='/img/problem.jpg'" alt="PPT 转 PDF 小程序"></a><div class="content"><a class="title" href="/2024/04/22/%E5%88%B6%E4%BD%9C%20PPT%20%E8%BD%AC%20PDF%20%E5%B0%8F%E7%A8%8B%E5%BA%8F/" title="PPT 转 PDF 小程序">PPT 转 PDF 小程序</a><time datetime="2024-04-22T03:46:36.249Z" title="发表于 2024-04-22 11:46:36">2024-04-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/04/20/3DGS%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" title="3D Gaussian Splatting for Real-Time Radiance Field Rendering 论文笔记"><img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/hexue.jpg" onerror="this.onerror=null;this.src='/img/problem.jpg'" alt="3D Gaussian Splatting for Real-Time Radiance Field Rendering 论文笔记"></a><div class="content"><a class="title" href="/2024/04/20/3DGS%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" title="3D Gaussian Splatting for Real-Time Radiance Field Rendering 论文笔记">3D Gaussian Splatting for Real-Time Radiance Field Rendering 论文笔记</a><time datetime="2024-04-20T07:44:16.966Z" title="发表于 2024-04-20 15:44:16">2024-04-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/04/20/Windows%20%E5%A4%8D%E7%8E%B0%E4%BB%A3%E7%A0%81%E6%8C%87%E5%8D%97/" title="Windows 复现代码指南"><img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/yunhai.jpg" onerror="this.onerror=null;this.src='/img/problem.jpg'" alt="Windows 复现代码指南"></a><div class="content"><a class="title" href="/2024/04/20/Windows%20%E5%A4%8D%E7%8E%B0%E4%BB%A3%E7%A0%81%E6%8C%87%E5%8D%97/" title="Windows 复现代码指南">Windows 复现代码指南</a><time datetime="2024-04-20T07:44:16.784Z" title="发表于 2024-04-20 15:44:16">2024-04-20</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">©2023 - 2024  <i id="heartbeat" class="fa fas fa-heartbeat"></i> 胖胖大藕片</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><p id="ghbdages"></p></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><a class="icon-V hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><svg width="25" height="25" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'ybVvPd5f8ymUrtyDgiPIcU3E-gzGzoHsz',
      appKey: 'P08uK63s8SFzZd9oN7vr8tmn',
      avatar: 'monsterid',
      serverURLs: 'https://ybvvpd5f.lc-cn-n1-shared.com',
      emojiMaps: "",
      master: 'cda7cc5c3b41c1ce1e80dade99393150',   //博主邮箱md5加密32位小写
      tagMeta: ["博主","小伙伴","访客"],     //标识字段名
      friends:  [],  //小伙伴邮箱Md5
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('/js/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script><script>(() => {
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://blogtwikoo-one.vercel.app',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(init,0)
    else getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(init)
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://blogtwikoo-one.vercel.app',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  if ('Valine' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else {
      loadTwikoo()
      GLOBAL_CONFIG_SITE.isPost && getCount()
    }
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><script src="https://libs.baidu.com/jquery/2.1.4/jquery.min.js"></script><script src="/js/crash_cheat.js"></script><script src="/js/heartbeat.js"></script><script src="/js/showAppreciation.js"></script><script src="/js/timing.js"></script><script src="/js/sun_moon.js" async=""></script><script defer="" src="https://rmt.dogedoge.com/fetch/~/source/jsdelivr/npm/jquery@latest/dist/jquery.min.js"></script><script defer="" data-pjax="" src="https://cdn.jsdelivr.net/gh/sirxemic/jquery.ripples/dist/jquery.ripples.js"></script><script defer="" data-pjax="" src="/js/ripples.js"></script><script async="" data-pjax="" src="/js/anzhiyu.js"></script><script async="" data-pjax="" src="/js/anzhiyufunction.js"></script><script async="" src="/js/anzhiyuOnlyOne.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><link rel="stylesheet" href="/css/Aplayer.min.css" media="print" onload="this.media='all'"><script src="/js/Aplayer.min.js"></script><script src="/js/Meting2.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  //document.querySelectorAll('script[data-pjax]').forEach(item => {
  document.querySelectorAll('script[data-pjax], .pjax-reload script').forEach(*item* *=>* {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async="" data-pjax="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div><div class="pjax-reload"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInLeft');
    arr[i].setAttribute('data-wow-duration', '600ms');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInRightBig');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer="defer" src="https://cdn.jsdelivr.net/gh/graingert/wow@1.3.0/dist/wow.min.js"></script><script defer="defer" src="/js/wow_init.js"></script></div><div id="nav-music"><div id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()">播放音乐</div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random"></meting-js></div><!-- hexo injector body_end start --><script data-pjax="">
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async="" src="/js/runtime_huibiao.js"></script><!-- hexo injector body_end end -->
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a,i=c[o];e=function(){c=c.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(n=new Image,a=t.getAttribute("data-original"),n.onload=function(){t.src=a,t.removeAttribute("data-original"),e&&e()},t.src!==a&&(n.src=a))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script></body></html>