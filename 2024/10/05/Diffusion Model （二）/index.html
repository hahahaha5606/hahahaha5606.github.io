<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Diffusion Model（二） | 藕片种植基地</title><meta name="author" content="胖胖大藕片"><meta name="copyright" content="胖胖大藕片"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="引入
 模型网站：
 DALL·E2： DALL·E 2 (openai.com)
 DALL·E3：DALL·E 3 (openai.com)
 Sora: Sora (openai.com)
 发展简介
 
 李沐论文精读系列——由DALL·E 2看图像生成模型 - 知乎 (zhihu.com)
 AI艺术的背后：详解文本生成图像模型 | 集智俱乐部 (swarma.org)
 
 近几年图像">
<meta property="og:type" content="article">
<meta property="og:title" content="Diffusion Model（二）">
<meta property="og:url" content="http://example.com/2024/10/05/Diffusion%20Model%20%EF%BC%88%E4%BA%8C%EF%BC%89/index.html">
<meta property="og:site_name" content="藕片种植基地">
<meta property="og:description" content="引入
 模型网站：
 DALL·E2： DALL·E 2 (openai.com)
 DALL·E3：DALL·E 3 (openai.com)
 Sora: Sora (openai.com)
 发展简介
 
 李沐论文精读系列——由DALL·E 2看图像生成模型 - 知乎 (zhihu.com)
 AI艺术的背后：详解文本生成图像模型 | 集智俱乐部 (swarma.org)
 
 近几年图像">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/post_img/20231125235054.jpg">
<meta property="article:published_time" content="2024-10-05T09:54:16.033Z">
<meta property="article:modified_time" content="2025-05-08T07:36:04.271Z">
<meta property="article:author" content="胖胖大藕片">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/post_img/20231125235054.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/10/05/Diffusion%20Model%20%EF%BC%88%E4%BA%8C%EF%BC%89/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Diffusion Model（二）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-05-08 15:36:04'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/transpancy.css"><link rel="stylesheet" href="/css/showAppreciation.css"><link rel="stylesheet" href="/css/ancientPoetry.css"><link rel="stylesheet" href="/css/mycss.css"><style>#article-container.post-content h1:before, h2:before, h3:before, h4:before, h5:before, h6:before { -webkit-animation: avatar_turn_around 1s linear infinite; -moz-animation: avatar_turn_around 1s linear infinite; -o-animation: avatar_turn_around 1s linear infinite; -ms-animation: avatar_turn_around 1s linear infinite; animation: avatar_turn_around 1s linear infinite; }</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/macblack.css"><link rel="stylesheet" href="/css/datu.css"><link rel="stylesheet" href="/css/aplayer.css"><link rel="stylesheet" href="/css/myvaline.css"><link rel="stylesheet" href="/css/music.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/firacode@6.2.0/distr/fira_code.css"><link rel="stylesheet" href="/css/cloud.css"><link rel="stylesheet" href="/css/maoboli_bg.css"><link rel="stylesheet" type="text/css" href="/css/perfectscrollbar.css"><link rel="stylesheet" type="text/css" href="/css/szgotop.css"></head><body><svg aria-hidden="true" style="position:absolute; overflow:hidden; width:0; height:0"><symbol id="icon-sun" viewBox="0 0 1024 1024"><path d="M960 512l-128 128v192h-192l-128 128-128-128H192v-192l-128-128 128-128V192h192l128-128 128 128h192v192z" fill="#FFD878" p-id="8420"></path><path d="M736 512a224 224 0 1 0-448 0 224 224 0 1 0 448 0z" fill="#FFE4A9" p-id="8421"></path><path d="M512 109.248L626.752 224H800v173.248L914.752 512 800 626.752V800h-173.248L512 914.752 397.248 800H224v-173.248L109.248 512 224 397.248V224h173.248L512 109.248M512 64l-128 128H192v192l-128 128 128 128v192h192l128 128 128-128h192v-192l128-128-128-128V192h-192l-128-128z" fill="#4D5152" p-id="8422"></path><path d="M512 320c105.888 0 192 86.112 192 192s-86.112 192-192 192-192-86.112-192-192 86.112-192 192-192m0-32a224 224 0 1 0 0 448 224 224 0 0 0 0-448z" fill="#4D5152" p-id="8423"></path></symbol><symbol id="icon-moon" viewBox="0 0 1024 1024"><path d="M611.370667 167.082667a445.013333 445.013333 0 0 1-38.4 161.834666 477.824 477.824 0 0 1-244.736 244.394667 445.141333 445.141333 0 0 1-161.109334 38.058667 85.077333 85.077333 0 0 0-65.066666 135.722666A462.08 462.08 0 1 0 747.093333 102.058667a85.077333 85.077333 0 0 0-135.722666 65.024z" fill="#FFB531" p-id="11345"></path><path d="M329.728 274.133333l35.157333-35.157333a21.333333 21.333333 0 1 0-30.165333-30.165333l-35.157333 35.157333-35.114667-35.157333a21.333333 21.333333 0 0 0-30.165333 30.165333l35.114666 35.157333-35.114666 35.157334a21.333333 21.333333 0 1 0 30.165333 30.165333l35.114667-35.157333 35.157333 35.157333a21.333333 21.333333 0 1 0 30.165333-30.165333z" fill="#030835" p-id="11346"></path></symbol></svg><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css">
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"><script>window.paceOptions = {
  restartOnPushState: false
}

document.addEventListener('pjax:send', () => {
  Pace.restart()
})
</script><link rel="stylesheet" href="/css/loading-bar.css"><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg"></div><div id="an_music_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/load.gif" data-original="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">80</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">47</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-space-shuttle"></i><span> 游戏</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-cubes"></i><span> 2048</span></a></li><li><a class="site-page child" href="/catch_cat/"><i class="fa-fw fa fa-cat"></i><span> 抓猫猫</span></a></li><li><a class="site-page child" href="/flower/"><i class="fa-fw fa fa-eye"></i><span> 花花世界迷人眼</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/Message/"><i class="fa-fw fa fa-paper-plane"></i><span> 打卡</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/post_img/20231125235054.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="藕片种植基地"><span class="site-name">藕片种植基地</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-space-shuttle"></i><span> 游戏</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-cubes"></i><span> 2048</span></a></li><li><a class="site-page child" href="/catch_cat/"><i class="fa-fw fa fa-cat"></i><span> 抓猫猫</span></a></li><li><a class="site-page child" href="/flower/"><i class="fa-fw fa fa-eye"></i><span> 花花世界迷人眼</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/Message/"><i class="fa-fw fa fa-paper-plane"></i><span> 打卡</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Diffusion Model（二）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-10-05T09:54:16.033Z" title="发表于 2024-10-05 17:54:16">2024-10-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-05-08T07:36:04.271Z" title="更新于 2025-05-08 15:36:04">2025-05-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%8E%9F%E7%90%86%E7%AC%94%E8%AE%B0/">原理笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">11.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>41分钟</span></span><span class="post-meta-separator">|</span><span class="leancloud_visitors" id="/2024/10/05/Diffusion%20Model%20%EF%BC%88%E4%BA%8C%EF%BC%89/" data-flag-title="Diffusion Model（二）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span class="leancloud-visitors-count"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2024/10/05/Diffusion%20Model%20%EF%BC%88%E4%BA%8C%EF%BC%89/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2024/10/05/Diffusion%20Model%20%EF%BC%88%E4%BA%8C%EF%BC%89/" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><div class="top-img" style="background-image: url('https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/post_img/20231125235054.jpg');"></div><article class="post-content" id="article-container"><h1 id="引入">引入</h1>
<p><strong>模型网站：</strong></p>
<p>DALL·E2： <a target="_blank" rel="noopener" href="https://openai.com/dall-e-2">DALL·E 2 (openai.com)</a></p>
<p>DALL·E3：<a target="_blank" rel="noopener" href="https://openai.com/dall-e-3">DALL·E 3 (openai.com)</a></p>
<p>Sora: <a target="_blank" rel="noopener" href="https://openai.com/sora?ref=aihub.cn">Sora (openai.com)</a></p>
<h2 id="发展简介">发展简介</h2>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/593896912">李沐论文精读系列——由DALL·E 2看图像生成模型 - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://swarma.org/?p=37227">AI艺术的背后：详解文本生成图像模型 | 集智俱乐部 (swarma.org)</a></p>
</blockquote>
<p>近几年图像生成模型的工作：</p>
<figure>
<img src="/img/load.gif" data-original="https://pic1.zhimg.com/80/v2-6350528949be8a46b618e7fb84c28e04_720w.webp" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="基于vq-vae">基于VQ-VAE</h2>
<p>顺序：<code>AE</code>(20世纪80年代)——<code>VAE</code>（Kingma et al, 2014<sup>[2]</sup>）——<code>VQ-VAE</code>(Van et al, 2017<sup>[3]</sup>)——<code>DALL·E</code>（Ramesh et al, 2021[4]）</p>
<h3 id="ae">AE</h3>
<p>自编码器（Auto Encoder）主要由编码器 encoder 和解码器 decoder 组成，目标是重建出输入图像。首先，输入图像 x 通过编码器被压缩，提取出高维特征 latent feature，然后这些高维特征再经过解码器重建出图像 x' 。AE 希望重建出的图像和输入图像越接近越好，因此损失函数为 <span class="math inline">\(loss=||x-x'||^2\)</span> 。通常 latent feature 的维度比输入、输出的维度小，因此称之为 bottleneck。</p>
<p><img src="/img/load.gif" data-original="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-d690623495c81ce7adb97e3eab397475.png"></p>
<p>由于 AE 是一个自重建的过程，因此被称为自编码器，实际常用于降维，去噪，异常检测或者神经风格迁移中。</p>
<h3 id="vae">VAE</h3>
<p>VAE（Variational Auto Encoder）仍然由一个编码器和一个解码器构成，并且目标仍然是重建原始输入。与 AE 不同的是，VAE 的中间表征是通过采样高斯分布得到的 <span class="math inline">\(z\)</span> 。首先，VAE 将原图 <span class="math inline">\(x\)</span> 通过编码器网络映射为高斯分布 <span class="math inline">\(\mathcal{N}\left( \mu_x,\sigma_x \right)\)</span> 。然后通过重参数技巧在该分布中采样中间表征 <span class="math inline">\(z=\mu_x+\sigma_x\cdot \epsilon\)</span> ，其中 <span class="math inline">\(\epsilon\thicksim \mathcal{N}\left( 0,\mathbf{I} \right)\)</span> 。最后，解码器解码 <span class="math inline">\(z\)</span> 重建出图像。其损失函数定义为： <span class="math display">\[
loss=||x-\hat{x}||^2+D_{KL}\left[ \mathcal{N}\left( \mu _x,\sigma _x \right) ,\mathcal{N}\left( 0,\mathbf{I} \right) \right]
\]</span> 在 VAE 的损失函数中，第二项是之所以让编码器输出的分布尽可能接近标准正态分布，是因为这样在生成时可以直接从正态分布中采样，然后通过解码器重建图像（怪怪的）。</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/574959469">事实上，编码器拟合的是后验概率 <span class="math inline">\(p(z|x)\)</span> ，解码器拟合的是似然函数 <span class="math inline">\(p(x|z)\)</span>。</a>似然函数理解为，在模型参数 <span class="math inline">\(z\)</span> 下，使得模型输出为 <span class="math inline">\(x\)</span> 的概率最大化的函数。</p>
</blockquote>
<figure>
<img src="/img/load.gif" data-original="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-249c281b4571a5b27ac531512e7c1cc0.png" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>VAE 提高了生成结果的多样性（引入了符合高斯分布的随机变量）。对于同一个输入，由于 latent vector 不再是固定的，而是采样得到，我们可以得到不同的但类似的任意多个输出。</p>
<h3 id="vq-vae">VQ-VAE</h3>
<p>VAE 具有一个最大的问题就是使用了固定的先验（正态分布），其次是使用了连续的中间表征，这样会导致图片生成的多样性并不是很好以及模型的可控性差。为了解决这个问题，VQ-VAE（ Vector Quantized Variational Auto Encoder） 选择使用<strong>离散的中间表征</strong>，同时，通常会使用一个自回归模型来学习先验（例如 PixelCNN 或者 Transformer），在训练完成后，直接用来采样得到 <span class="math inline">\(z_q\)</span>​ ，然后通过匹配 Codebook, 使用解码器进行图片生成。</p>
<blockquote>
<p>VAE的目的是训练完成后, 丢掉 encoder, 在 prior 上直接采样, 加上 decoder 就能生成. 如果我们现在独立地采 <span class="math inline">\(H×W\)</span> 个 <span class="math inline">\(z\)</span> 组成 <span class="math inline">\(z_e(x)\)</span> , 然后查表得到维度为 <span class="math inline">\(H×W×D\)</span> 的 <span class="math inline">\(z_q(x)\)</span>，那么生成的图片在空间上的每块区域之间几乎就是独立的。<strong>因此我们需要让各个 <span class="math inline">\(z\)</span> 之间有关系</strong>。用 PixelCNN, 对这些 <span class="math inline">\(z\)</span> 建立一个自回归模型： <span class="math inline">\(p(z_1,z_2,z_3,...)=p(z_1)p(z_2|z_1)p(z_3|z_1,z_2)...\)</span> 这样就可以进行 <a target="_blank" rel="noopener" href="https://hyper.ai/wiki/2986">ancestral sampling</a> 生成 <span class="math inline">\(x\)</span>, 得到一个互相之间有关联的 <span class="math inline">\(H×W\)</span> 的整数矩阵。 <span class="math inline">\(p(z_1,z_2,z_3,...)\)</span> 这个联合概率即为我们想要的 prior。——<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/91434658">Elijha：VQ-VAE解读</a></p>
<p>The prior distribution over the discrete latents <span class="math inline">\(p(z)\)</span> is a categorical distribution, and can be made autoregressive by depending on other <span class="math inline">\(z\)</span> in the feature map. Whilst training the VQ-VAE, the prior is kept constant and uniform. After training, we fit an autoregressive distribution over <span class="math inline">\(z\)</span>, <span class="math inline">\(p(z)\)</span>, so that we can generate <span class="math inline">\(x\)</span> via ancestral sampling. We use a PixelCNN over the discrete latents for images, and a WaveNet for raw audio. Training the prior and the VQ-VAE jointly, which could strengthen our results, is left as future research. ——来自<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.00937.pdf">原论文</a></p>
</blockquote>
<figure>
<img src="/img/load.gif" data-original="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-b761643dc19adceb1734df82210eda76.png" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>VQ-VAE 的算法流程为：</strong></p>
<ol type="1">
<li><span style="background:#dad5e9;">（Embedding Space）</span> 首先设置 <span class="math inline">\(K\)</span> 个 <span class="math inline">\(D\)</span> 维向量 <span class="math inline">\(e_1,e_2...e_k\)</span> 作为可查询的 Codebook。</li>
<li><span style="background:#daf5e9;">（绿方块）</span> 输入图片通过编码器 CNN 来得到中间表征 <span class="math inline">\(z_e(x)\)</span>，<span class="math inline">\(z_e(x)\)</span> 是 <span class="math inline">\(H×W\)</span> 个 <span class="math inline">\(D\)</span> 维向量。</li>
<li><span style="background:#dcffff;">（蓝方块）</span> 通过最邻近算法，在 Codebook 中查询与 <span class="math inline">\(z_e(x)\)</span> 中每个 <span class="math inline">\(D\)</span> 维向量最相似的向量 <span class="math inline">\(e_i^{(1)},e_i^{(2)}...e_i^{(H×W)}\)</span> ，用 index 表示，就得到了 <span class="math inline">\(q(z|x)\)</span> 。</li>
<li><span style="background:#dad5e9;">（紫方块）</span>根据 <span class="math inline">\(q(z|x)\)</span> 将 Codebook 中查询的相似向量放到对应 <span class="math inline">\(z_e(x)\)</span> 的位置上，得到 <span class="math inline">\(z_q(x)\)</span>​ 。</li>
<li>解码器通过得到的中间表征 <span class="math inline">\(z_q(x)\)</span> 重建图片。</li>
</ol>
<p>一般 <span class="math inline">\(K=8192\)</span>，<span class="math inline">\(D=512\)</span> or <span class="math inline">\(768\)</span>。</p>
<p>VQ-VAE 最核心的部分就是 <strong>Codebook 查询操作</strong>，通过使用具有高度一致性的 Codebook 来代替混乱的中间表征，可以有效的提高图像生成的可控性。初代的DALL·E 模型就是基于 VQ-VAE 的架构实现的。</p>
<h3 id="delle">DELL·E</h3>
<blockquote>
<p>Project page: <a target="_blank" rel="noopener" href="https://openai.com/research/dall-e">DALL·E: Creating images from text (openai.com)</a></p>
</blockquote>
<p>文本生成图像模型 DALL·E 由 OpenAI 开发，其第一代版本使用的是在 VQ-VAE 自回归生成的基础上加上文本条件，实现了 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/624793654">zero-shot</a> 的 text-to-image 生成。</p>
<p>在 DELL·E 中使用了 Transformer 来作为自回归模型。在生成过程中，输入文本通过 Transformer 预测中间表征 <span class="math inline">\(z\)</span> ,然后匹配 <span class="math inline">\(K=8192\)</span> 的 Codebook得到 <span class="math inline">\(z_q\)</span> ，最后通过 Decoder 模块将 <span class="math inline">\(z_q\)</span>​ 解码为图像。 具体过程见<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/683882116">DALL-E 系列 (1-3) - 知乎 (zhihu.com)</a>。在 DALL-E 的论文中，作者还提出了很多技术上的细节，例如，在最后挑选图片的时候，可以使用 CLIP 模型来选择与文本相似度最高的模型，以及分布式训练，混合精度训练等，具体细节可以查看<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.12092.pdf">原论文</a>。</p>
<blockquote>
<p>CLIP (Contrastive Language-Image Pre-training，对比图文预训练) 是一种 zero-shot 的视觉分类模型，它是和 DALL·E 一起被发布的。</p>
<p>CLIP 是一种神经网络，为输入的图像返回最佳的标题。它所做的事情与 DALL-E 所做的相反 —— 它是将图像转换为文本，而 DALL-E 是将文本转换为图像。引入 CLIP 的目的是为了学习物体的视觉和文字表示之间的联系。</p>
</blockquote>
<figure>
<img src="/img/load.gif" data-original="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-5bb80123f487ae06090f48943cfa8322.png" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="基于-gan">基于 GAN</h2>
<p>顺序： <code>GAN</code>（Creswell et al, 2014<sup>[5]</sup>）——<code>VQGAN</code>（Esser et al, 2021<sup>[6]</sup>）——<code>VQGAN-CLIP</code>（Crowson et al, 2022<sup>[7]</sup>）——<code>DELL·E Mini</code>——<code>Parti</code>（Google, 2022 在 Imagen 之后一个月推出, AI 绘图）——<code>NUWA-Infinity</code>（微软亚洲研究院， 2022， 无限创作）</p>
<h3 id="gan">GAN</h3>
<p><strong>生成对抗网络</strong>（<strong>GAN</strong>，Generative Adversarial Networks）由两个主要的模块构成：生成器和判别器。生成器负责生成一张图片，而判别器则负责判断这张图片质量，也就是判断是真实样本还是生成的虚假样本，通过逐步的迭代，左右互博，最终生成器可以生成越来越逼真的图像，而判别器则可以更加精准的判断图片的真假。GAN 的最大优势是其<strong>不依赖于先验假设，而是通过迭代的方式逐渐学到数据的分布</strong> [6]。</p>
<p>其训练流程如下：</p>
<ol type="1">
<li>初始化一个生成器 <span class="math inline">\(G\)</span> 和一个判别器 <span class="math inline">\(D\)</span> ;</li>
<li>固定生成器 <span class="math inline">\(G\)</span> 的参数， 只更新判别器 <span class="math inline">\(D\)</span> 的参数。具体过程为：选择一部分真实样本，以及从生成器 <span class="math inline">\(G\)</span> 得到一些生成的样本，送入到判别器 <span class="math inline">\(D\)</span> 中，判别器需要判断哪些样本为真实的，哪些样本为生成的，通过与真实结果的误差来优化判别器;</li>
<li>固定判别器 <span class="math inline">\(D\)</span> 的参数, 只更新生成器 <span class="math inline">\(G\)</span> 的参数。具体过程为：使用生成器生成一部分样本， 将生成的样本喂入到判别器 <span class="math inline">\(D\)</span> 中，判别器会对其进行判断，优化生成器 <span class="math inline">\(G\)</span> 的参数，使得判别器将其判断为更加偏向于真实样本。</li>
</ol>
<h3 id="vqgan">VQGAN</h3>
<p>VQGAN （ Vector Quantized Generative Adversarial Networks ） 是一种 GAN 的变种，一种视觉生成模型,来自德国海德堡大学IWR研究团队受到 VQ-VAE 的启发，<strong>使用了 Codebook 来进行离散表征</strong>。</p>
<p>具体来说，预先定义 <span class="math inline">\(K\)</span> 个向量作为离散的特征查询表 Codebook。当一张图片被送入到 CNN Encoder 中后，会得到 <span class="math inline">\(h×w×n_z\)</span> 维的中间表征 <span class="math inline">\(\hat{z}\)</span> ,之后与 VQ-VAE 类似地查询 Codebook 得到 <span class="math inline">\(z_q\)</span> 。最后，CNN Decoder 也就是生成器 <span class="math inline">\(G\)</span> 根据得到的表征 <span class="math inline">\(z_q\)</span> 重建出图像。其中，通过 <span class="math inline">\(\hat{z}\)</span> 得到 <span class="math inline">\(z_q\)</span> 的公式为： <span class="math display">\[
z_q=q(\hat{z}):=\left( \underset{z_k\in \mathcal{Z}}{arg\ \min}||\hat{z}_{ij}-z_k|| \right) \in \mathbb{R}^{h×w×n_z}
\]</span> 同样地，这里的对于生成器部分的优化，需要使用与 VQ-VAE 一样的方法去进行：</p>
<p>在训练好 VQGAN 之后，在生成的时候，可以直接初始化一个 <span class="math inline">\(z_q\)</span> 去生成图像，然而，为了能够得到稳定的 <span class="math inline">\(z_q\)</span> ，需要使用一个模型对先验进行学习。这里使用了 Transformer 模型来学习 <span class="math inline">\(z_q\)</span> 中离散表征的序列，可以简单的将其建模为自回归模型 <span class="math inline">\(p(s)=\prod_i{p(s_i|s_{&lt;i})}\)</span> ，这样，我们只需要给定一个初始的随机向量，就可以通过 Transfomer 模型生成完整的 <span class="math inline">\(z_q\)</span> ，从而可以通过 CNN Decoder 模块生成最终的图像。</p>
<figure>
<img src="/img/load.gif" data-original="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-dfb34aff6b13b8f38a0ed05e4c0b0de1.png" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<blockquote>
<p>这里的判别器不是对每张整体图片进行判断，而是对图片的每一小块进行判断。</p>
</blockquote>
<h2 id="基于-diffusion">基于 Diffusion</h2>
<p>顺序: Diffusion（Sohl-Dickstein et al, 2015<sup>[12]</sup>）——Diffusion DDPM（ Ho et al, 2020<sup>[1]</sup>）——GLIDE（Nichol et al, 2021.12<sup>[8]</sup>）——DALL·E2（Ramesh, 2022.4<sup>[9]</sup>）——Imagen（Saharia, 2022.5<sup>[10]</sup>）——Stable Diffusion(Rombach et al, 2022<sup>[11]</sup>)——ChatGPT（2022.11）——ChatGPT-4（2023.3）——DALL·E3（2023.10）——Sora（2024.2）</p>
<h3 id="diffusion">Diffusion</h3>
<p>回忆上文提到的 VQ-VAE 以及 VQ-GAN，都是先通过编码器将图像映射到中间潜变量，然后解码器在通过中间潜变量进行还原。实际上，扩散模型做的事情本质上是一样的，不同的是，扩散模型完全使用了全新的思路来实现这个目标。</p>
<blockquote>
<p><strong>为什么叫“扩散”模型呢？</strong> 扩散这个名词来自于热力学，某个区域物质密度很高，那么它就会向低密度的区域扩散，最终达到平衡；就好比香水的味道会扩散到整个房间。这里的平衡指的是标准高斯分布的噪声。图片由原来的有序，逐渐变为无序的噪声，可以认为是“无序度”的扩散。</p>
</blockquote>
<p>在传统的扩散模型中，主要有两个过程组成，<strong>前向扩散过程，反向去噪过程</strong>，前向扩散过程主要是在一张图片上随机添加高斯噪声，而逆向去噪过程则是将一张随机噪音的图片还原为一张完整的图片。</p>
<p>最初的 Diffusion 在2015 年甚至更早就被提出了，当时有人基于非平衡热力学提出了一个纯数学的生成模型（Sohl-Dickstein et al, 2015<sup>[12]</sup>）。这种生成模型通过迭代正向扩散破坏数据分布结构，并通过学习反向扩散过程恢复数据结构。最初的模型并没有用代码实现。同时由于 forward 和 backward 训练采样和推理都需要很长时间，所以扩散模型一直不如 GAN 受关注，直到2020年 DDPM 的出现。DDPM 主要有两个贡献：</p>
<ol type="1">
<li>不需要推测 <span class="math inline">\(x_t\)</span> ，只需要推测 <span class="math inline">\(\epsilon_\theta\)</span> 就可以了。这样大大降低了推理的难度。这里计算梯度的网络是 U-net。</li>
<li>由于每步都是高斯噪声，只需要预测它的均值和方差即可。并且作者提出，固定方差 <span class="math inline">\(\sigma\)</span>​ ，仅仅预测噪声的均值就可以达到很好的效果。</li>
</ol>
<p>DDPM 中 Diffusion 的损失函数如下（具体推导见下文）： <span class="math display">\[
L=\lVert \boldsymbol{\epsilon }_t-\boldsymbol{\epsilon }_{\theta}\left( \sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon }_t,t \right) \rVert ^2
\]</span> 其算法流程为：</p>
<figure>
<img src="/img/load.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240320200738823.png" alt="image-20240320200738823"><figcaption aria-hidden="true">image-20240320200738823</figcaption>
</figure>
<p>在完成训练之后，只需要通过重参数化技巧，进行采样操作即可，具体流程如上边右图所示，通过不断的「减去」模型预测的噪音，可以逐渐生成一张完整的图片。</p>
<p><strong>Diffusion 的初步改进： classifier guided diffusion</strong></p>
<p>DDPM的成功引起了大家的兴趣。2020年底， <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2102.09672.pdf">improved DDPM</a> 就出炉了。它在DDPM的基础上，不仅仅预测均值，而且预测方差；并且在更大模型上尝试了DDPM，发现大模型会带来很大的效果提升。于是紧接着有一篇 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.05233.pdf">Diffusion Models Beat GANs on Image Synthesis</a>（2021.5, OpenAI），进一步把模型做大、做复杂，取得了很好的效果；并且，作者在这篇论文里使用了 <code>classifier guidance</code> 的方法，引导模型做采样和生成。这不仅让生成的图像更逼真，而且加速了反向采样的速度，只需要25次采样就能从噪声还原出高质量的图片。</p>
<p>简单来说，<code>classifier guided diffusion</code>使用一个额外训练好的图像分类器 classifier ，每次判别 <span class="math inline">\(x_t\)</span> 的类别，得到交叉熵损失函数，进一步可以得到梯度 gradient ，用这个梯度去引导 <span class="math inline">\(f_\theta\)</span> （这里的 <span class="math inline">\(f_\theta\)</span>​ 是如上文的 U-net）做预测。这里的梯度大概暗含了当前生成的图像有没有某个物体，以及生成的物体真不真实。通过引导，扩散模型的逼真度提升了很多，击败了一些GAN模型。</p>
<figure>
<img src="/img/load.gif" data-original="https://pic3.zhimg.com/80/v2-9d9e7d2039c35f9f82056b3c180cd4c2_720w.webp" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>这和 conditional GAN 的思路差不多，提供更多的 condition，也就是引导，辅助它完成任务。这里的引导不一定非得是一个分类器，可以是文本，也可以是CLIP模型。</p>
<p><strong>Diffusion 的再次改进： Classifier-Free Guidance Diffusion</strong></p>
<p>后续又有一些改进操作，这些改进操作使得扩散模型被广泛的应用于文本生成图像任务中。其中，最常用的改进版本为 Classifier-Free Guidance Diffusion。OpenAI 后续的 <strong>GLIDE</strong> 模型和 <strong>DALL·E 2</strong> ，以及谷歌的 <strong>Imagen</strong>，在推理过程中抛弃了分类器的引导——Classifier free guidance。</p>
<p>这种扩散模型在每个时间步 <span class="math inline">\(t\)</span>， 除了原有的无引导情况下的输出 <span class="math inline">\(\epsilon_\theta (x_t)\)</span> 外，还增加了在有引导情况下的输出 <span class="math inline">\(\epsilon_\theta (x_t,y)\)</span> ，由此 Classifier-Free Guidance Diffusion 结合了条件和无条件噪声估计模型，定义为： <span class="math display">\[
\hat{\epsilon}_\theta(x_t|y)=\epsilon_\theta(x_t)+s·(\epsilon_\theta(x_t,y)-\epsilon_\theta(x_t))
\]</span> 通过有引导时的输出与无引导时的输出作差，就得到了一个方向，告诉网络如何从无引导的输出到达有引导的输出。这样在训练完成后，就能达到抛开引导的目的。这种改进优点是<strong>训练过程非常稳定，且摆脱了分类器的限制</strong>（实际上等价于学习了一个隐含的分类器），缺点是，训练成本比较高，相当于每次要生成两个输出，尽管如此，后面的大部份知名文本生成图像模型，都是基于这个方法进行的。</p>
<p>在使用了很多技巧之后，基于扩散模型的 <strong>GLIDE</strong> 用了35亿参数，效果就直逼基于VQ-VAE 的、用了120亿参数的<strong>DALL·E</strong> 模型。OpenAI 看到扩散模型确实靠谱，所以顺着 GLIDE 的思路，孕育出了现在的 <strong>DALL·E 2</strong>。</p>
<h3 id="glide">GLIDE</h3>
<blockquote>
<p>代码已开源</p>
</blockquote>
<p>GLIDE 使用了文本作为 condition 来引导扩散模型，主要用了两种策略: Classifier-Free Guidance Diffusion 和 CLIP 来作为条件监督。GLIDE 的扩散模型中的 <span class="math inline">\(\hat{\epsilon}_\theta(x_t|y)\)</span> 中的 <span class="math inline">\(y\)</span> 是一段文本描述。</p>
<p>GLIDE的生成效果。下图是GLIDE基于不同的文本提示生成的16个图像集，例如“使用计算器的刺猬”、“戴着红色领带和紫色帽子的柯基”等等，如图所示，生成的图像基本符合文本描述。</p>
<figure>
<img src="/img/load.gif" data-original="https://img-blog.csdnimg.cn/img_convert/d2e902a630e984757eb922d0b5662bdc.png" alt="d2e902a630e984757eb922d0b5662bdc.png"><figcaption aria-hidden="true">d2e902a630e984757eb922d0b5662bdc.png</figcaption>
</figure>
<p>除了图文转换，该论文还包括一个交互式系统的原型，支持通过 <strong>选取区域+文本Prompt</strong> 来对图像进行编辑操作，用于逐步细化图像的选定部分。使用过程中，只需要将遮蔽区域进行 mask，以及剩下的图片一起送入到网络中，即可产生补全之后的图片。</p>
<figure>
<img src="/img/load.gif" data-original="https://img-blog.csdnimg.cn/img_convert/da853db5c9f392a658cf03922989c6c4.png" alt="da853db5c9f392a658cf03922989c6c4.png"><figcaption aria-hidden="true">da853db5c9f392a658cf03922989c6c4.png</figcaption>
</figure>
<p>此外，<strong>GLIDE 的语义理解能力并不是很强，在一些少见的文本描述下（如八条腿的猫），很难产生合乎逻辑的图像，而 DALL-E2 在这方面的能力上，要远超 GLIDE</strong></p>
<figure>
<img src="/img/load.gif" data-original="https://img-blog.csdnimg.cn/img_convert/65b3e3049c10e65525e3e5e92f57b60b.png" alt="65b3e3049c10e65525e3e5e92f57b60b.png"><figcaption aria-hidden="true">65b3e3049c10e65525e3e5e92f57b60b.png</figcaption>
</figure>
<h3 id="dalle2">DALL·E2</h3>
<p>DALL-E2 是 OpenAI 2022年4月推出的 AI 生成图像模型，其最大的特色是<strong>模型具有惊人的理解力和创造力</strong>，它可以根据给定的概念、特性以及风格来生成原创性的图片。除此之外，DALL·E 2 还能根据描述，对已有的图片进行二次编辑，比如移除或添加某个物体，并且把阴影、反射、纹理考虑在内。还有，就算不给定语言描述，DALL·E 2 也能根据已有的图片，生成一系列风格相似的新的图片。 其参数大约 3.5B , 相对于上一代版本，DALL-E2 可以生成4倍分倍率的图片，且非常贴合语义信息。作者使用了人工评测方法，让志愿者看1000张图，71.7% 的人认为其更加匹配文本描述 ，88.8% 认为画的图相对于上一代版本更加好看。</p>
<figure>
<img src="/img/load.gif" data-original="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-0544da18e16e0b011f1d4d85d5b1cf07.png" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>DALL-E2 由三个模块组成</strong>（相当于两阶段生成，第一阶段用 prior 模型从文本生成图像特征；第二阶段用 decoder 生成图像）：</p>
<ul>
<li>CLIP模型，对齐图片文本表征</li>
<li>先验模型，接收文本信息，将其转换成 CLIP 图像表征</li>
<li>扩散模型，接受图像表征，解码来生成完整图像</li>
</ul>
<figure>
<img src="/img/load.gif" data-original="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-0fce8bc8d73882ce798bb7e4b18763df.png" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>DALL-E2 的训练过程</strong>为：</p>
<ul>
<li>训练一个 CLIP 模型，使其能够对齐文本和图片特征。</li>
<li>训练一个先验模型，由自回归模型或者一个扩散先验模型（实验证明，扩散先验模型表现更好。这里的扩散模型是用 sequence (text feature) 预测 sequence (image feature)，没有要求前后尺寸不变。作者这里没有用 U-net，而是用了 Transformer——将CLIP的文本编码、加入噪声的CLIP的图像编码、扩散时间步的time embedding等输入，去预测未加噪声的CLIP图像编码。），其功能是将文本表征映射为图片表征。</li>
<li>训练一个扩散解码模型，其目标是根据图片表征，还原原始图片。</li>
</ul>
<p>在训练完成之后，推理过程就比较直接了，首先使用CLIP 文本编码器，获得文本编码，之后使用先验模型将文本编码映射为图片编码，最后使用扩散解码器用图片编码生成完整图片。注意这里扩散解码模型使用的是经过修改的 GLIDE 扩散模型，其生成的图像尺寸为 64×64，然后使用两个上采样扩散模型将其上采样至 256×256，以及 1024×1024。</p>
<p>DALL·E2 原论文中也提到了其许多不足，例如容易将物体和属性混淆，无法精确的将文本放置到图像中，以及社会伦理道德等方面的问题。</p>
<p>谷歌的 Imagen 模型是继 DALL·E2 的后续工作，它没有用两阶段的生成，直接用了一个 U-net 搞定，更简单，效果也好。另外谷歌2022年6月份最新的基于 GAN 的 <a target="_blank" rel="noopener" href="https://sites.research.google/parti/">Pathways Autoregressive Text-to-Image Model</a>（Parti）模型，用 200亿参数的 Pathways 模型做自回归的图像生成，效果直接超越了 DALL·E 2 和 Imagen。</p>
<h3 id="imagen">Imagen</h3>
<p>在 DALL-E2 提出没多久，Google 就提出了一个新的文本生成图像模型 Imagen，论文中提到，<strong>其生成的图片相对于 DALL-E2 真实感和语言理解能力都更加强大</strong>（使用一种新的评测方法 DrawBench）。</p>
<figure>
<img src="/img/load.gif" data-original="https://swarma.org/wp-content/uploads/2022/09/wxsync-2022-09-67fa2ff5f289e376304e905a8c87eff7.png" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>Imagen 的模型结构与 DALL·E2 十分类似，首先将文本进行编码表征，然后使用扩散模型将表征解码为完整图像，最后使用两个 text-conditional super-resolution 的扩散模型将其上采样至 256×256，以及 1024×1024。不同的是，<strong>Imagen 使用了 T5-XXL 模型直接编码文本信息，然后使用条件扩散模型，直接用文本编码生成图像。因此，在 Imagen 中，无需学习先验模型。</strong></p>
<figure>
<img src="/img/load.gif" data-original="https://pic2.zhimg.com/80/v2-be42b33c6d55d5fece5b8ae9866e42a9_720w.webp" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>由于直接使用 T5-XXL 模型，其语义知识相对于 CLIP 要丰富很多（图文匹配数据集数量要远远少于纯文本数据集数量），因此 Imagen 相对于 DALL-E2 在语义保真度上做的更好。同时，作者也发现，增大语言模型，可以有效的提高样本的语义保真度。</p>
<h3 id="stable-diffusion">Stable Diffusion</h3>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/593896912">李沐论文精读系列——由DALL·E 2看图像生成模型 - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/560645985">图像生成：VQGAN，CLIP，DALLE，DIFFUSION - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://swarma.org/?p=37227">AI艺术的背后：详解文本生成图像模型 | 集智俱乐部 (swarma.org)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/640545463">一文读懂Stable Diffusion 论文原理+代码超详细解读 - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/imwaters/article/details/127269368">【论文简介】Stable Diffusion的基础论文:2112.High-Resolution Image Synthesis with Latent Diffusion Models_stable diffusion论文-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_31711799/article/details/130874306">【AI绘图】一、stable diffusion的发展史_stable diffusion研究现状-CSDN博客</a></p>
</blockquote>
<p>DALL·E3</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/660733999#:~:text=DALL-E%203%20将于%2010,月初向%20ChatGPT%20Plus%20和企业客户推出。">OpenAI发布DALL-E 3 | 原理简介 - 知乎 (zhihu.com)</a></p>
</blockquote>
<h3 id="sora">Sora</h3>
<p>Sora 是由 OpenAI 于 2024.2.15 发布的文生视频大模型。</p>
<h2 id="各生成模型对比">各生成模型对比</h2>
<p>翻译自 Lil' Log:</p>
<p>GAN、VAE 和流动模型等生成模型在生成高质量 samples 方面取得了巨大成功，但它们都有其自身的局限性。GAN模型因对抗性训练性质而具有潜在的不稳定训练和较少的生成多样性（需要精心选择的超参数和正则化器）。VAE依赖于替代损失 surrogate loss 。流动模型必须使用专门的架构来构建可逆转换。</p>
<p>扩散模型的灵感来自非平衡热力学。他们定义了一个扩散步骤的马尔可夫链，以缓慢地将随机噪声添加到数据中，然后学习反向扩散过程以从噪声中构建所需的数据样本。与VAE或流动模型不同，扩散模型是通过固定程序学习的，并且潜在变量 latent code（z）具有高维数（与原图同尺寸大小）。</p>
<figure>
<img src="/img/load.gif" data-original="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png" alt="Edge Image Viewer (lilianweng.github.io)"><figcaption aria-hidden="true">Edge Image Viewer (lilianweng.github.io)</figcaption>
</figure>
<blockquote>
<p>各种生成模型的对比图，来自<a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models? | Lil'Log (lilianweng.github.io)</a></p>
</blockquote>
<h1 id="ddpm">DDPM</h1>
<details>
<summary>
参考链接
</summary>
<p>
<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/525106459">由浅入深了解Diffusion Model - 知乎 (zhihu.com)</a>
</p>
<p>
<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/576475987">扩散模型 (Diffusion Model) 简要介绍与源码分析 - 知乎 (zhihu.com)</a>
</p>
<p>
<a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models? | Lil'Log (lilianweng.github.io)</a>
</p>
<p>
<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/601641045">Diffusion Model学习笔记(1)——DDPM - 知乎 (zhihu.com)</a>
</p>
</details>
<p>本文主要基于 <strong>DDPM：Denoising Diffusion Probabilistic Models</strong><sup>[1]</sup> [Ho et al. 2020] 展开 Diffusion 模型的原理推导。</p>
<h1 id="forward-与-backward-过程">forward 与 backward 过程</h1>
<p>如下，存在一系列高斯噪声（ <span class="math inline">\(T\)</span> 轮），将输入图片 <span class="math inline">\(x_0\)</span> 变为纯高斯噪声 <span class="math inline">\(x_T\)</span> 。而我们的模型则负责将 <span class="math inline">\(x_T\)</span> 复原回图片 <span class="math inline">\(x_0\)</span> 。这样一来其实 diffusion model 和 GAN 很像，都是给定噪声 <span class="math inline">\(x_t\)</span> 生成图片 <span class="math inline">\(x_0\)</span> ，但是要强调的是，这里噪声 <span class="math inline">\(x_T\)</span> 与图片 <span class="math inline">\(x_0\)</span> 是<strong>同维度</strong>的。</p>
<figure>
<img src="/img/load.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240315_145856_0.jpg" alt="20240315_145856_0"><figcaption aria-hidden="true">20240315_145856_0</figcaption>
</figure>
<figure>
<img src="/img/load.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240315_145951_1.jpg" alt="20240315_145951_1"><figcaption aria-hidden="true">20240315_145951_1</figcaption>
</figure>
<figure>
<img src="/img/load.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/diff222.jpg" alt="20240315_150015_2"><figcaption aria-hidden="true">20240315_150015_2</figcaption>
</figure>
<figure>
<img src="/img/load.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240315_150043_3.jpg" alt="20240315_150043_3"><figcaption aria-hidden="true">20240315_150043_3</figcaption>
</figure>
<h1 id="模型训练">模型训练</h1>
<h2 id="预备知识">预备知识</h2>
<blockquote>
<p>关于变分下界与KL散度参考： <a target="_blank" rel="noopener" href="https://bluefisher.github.io/2020/02/06/理解-Variational-Lower-Bound/">理解 Variational Lower Bound | Fisher's Blog (bluefisher.github.io)</a></p>
</blockquote>
<h3 id="名词概念">名词概念</h3>
<h4 id="重参数技巧">重参数技巧</h4>
<p>重参数（reparameterization trick）通过将随机性保存在一个独立的随机变量 <span class="math inline">\(\epsilon\)</span> 中，解决了从某个分布中随机采样导致的无法反传梯度问题。在 diffusion 模型中，我们有很多通过高斯噪声采样得到 <span class="math inline">\(x_t\)</span> 的过程，这些过程都用到了重参数技巧使其可微。例如，从高斯分布 <span class="math inline">\(z \sim \mathcal{N}(z;\mu_{\theta},\sigma^2_{\theta} \bold{I})\)</span> 中采样 <span class="math inline">\(z\)</span>, 可以写成： <span class="math display">\[
z = \mu_{\theta} + \sigma_{\theta}\odot \epsilon, \ \epsilon\sim\mathcal{N}(0,\bold{I})
\]</span> 通过重参数，采样得到的 <span class="math inline">\(z\)</span> 仍然具有随机性，其随机性保存在 <span class="math inline">\(\epsilon\)</span> 中。且 <span class="math inline">\(z\)</span> 满足均值为 <span class="math inline">\(\mu_{\theta}\)</span>、方差为 <span class="math inline">\(\sigma^2_{\theta}\)</span> 的高斯分布，这里的 <span class="math inline">\(\mu_{\theta}、\sigma^2_{\theta}\)</span> 可以是由参数 <span class="math inline">\(\theta\)</span> 的神经网络推断得到，整个”采样”过程依然梯度可导。</p>
<h4 id="后验分布">后验分布</h4>
<p>在贝叶斯推断中，我们通常需要计算后验分布 <span class="math inline">\(p(z| x)\)</span>，即在给定观测数据 <span class="math inline">\(x\)</span> 的情况下推断潜在变量 <span class="math inline">\(z\)</span> 的分布。<strong>贝叶斯定理中的后验分布公式为：</strong> <span class="math display">\[
\colorbox{RedOrange}{p(z | x)} = \frac{\colorbox{Apricot}{p(x| z)}\colorbox{pink}{p(z)}}{\colorbox{SpringGreen}{p(x)}}
\]</span> 其中，</p>
<p><font color="#ef042a"><strong>后验分布</strong><span class="math inline">\(p(z | x)\)</span></font>：在给定观测数据 <span class="math inline">\(x\)</span> 时，潜在变量 <span class="math inline">\(z\)</span> 的分布。</p>
<p><font color="#df8400"><strong>似然函数</strong> <span class="math inline">\(p(x| z)\)</span></font>：在给定潜在变量 <span class="math inline">\(z\)</span> 时，数据样本 <span class="math inline">\(x\)</span> 出现的概率。</p>
<p><font color="#985fff"><strong>先验分布</strong> <span class="math inline">\(p(z)\)</span></font>：在没有观测数据时，我们对潜在变量的先验知识。</p>
<p><font color="#4eb434"><strong>证据</strong> <span class="math inline">\(p(x)\)</span></font>：Evidence，也称<strong>归一化常数</strong>（<span style="background:#eef0f4;">证据 <span class="math inline">\(p(x)\)</span> 之所以被称为“归一化常数”，是因为它能确保后验分布 <span class="math inline">\(p(z | x)\)</span> 是一个有效的概率分布，使得其积分为 1 </span>），表示观测到数据 <span class="math inline">\(x\)</span> 的总概率。它是通过对所有可能的潜在变量 <span class="math inline">\(z\)</span> 进行积分得到的，即： <span class="math display">\[
\colorbox{SpringGreen}{p(x)} = \int \colorbox{Apricot}{p(x| z)} \colorbox{pink}{p(z)}dz
\]</span> 因为证据 <span class="math inline">\(p(x)\)</span> 涉及对所有潜在变量 <span class="math inline">\(z\)</span> 进行积分，且这个积分往往没有解析解（即无法通过常规的数学计算直接得到）。尤其是在<strong>复杂的高维模型</strong>中，积分操作会变得非常复杂和难以计算。在<strong>复杂的非线性模型</strong>中，似然函数 <span class="math inline">\(p(x|z)\)</span> 本身也可能非常复杂，使得积分更加难以求解。</p>
<p>因此在实际的贝叶斯推断中，我们往往使用近似推断方法（如变分推断、马尔可夫链蒙特卡洛方法）来避开直接计算证据的难题。</p>
<h4 id="什么是变分">什么是“变分”</h4>
<blockquote>
<p>详细内容可点击<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1TR4y1c7ns/?spm_id_from=333.337.search-card.all.click&amp;vd_source=124ec79ebd3e16b0f454a3994a468f98">这里</a></p>
</blockquote>
<p><strong>变分（Variational）</strong>在数学和物理中指的是研究函数变化如何影响其关联量（如积分、函数值等）的过程。变分的概念源于<strong>变分法（Calculus of Variations）</strong>，变分法是一种用于研究如何在某些约束下优化函数或泛函的数学技术。<span style="background:#daf5e9;"><strong>注意！与传统优化问题不同的是，变分法优化函数本身，而不是对单个变量进行优化。</strong></span>其主要原理如下：</p>
<p><strong>1.构造泛函</strong>：在变分法中，我们先定义一个与问题相关的<strong>泛函（Functional）</strong><span class="math inline">\(J[y]\)</span>，它是一个<u>将函数映射到实数</u>的对象。该泛函常可表示为一个积分： <span class="math display">\[
J[y] = \int^b_a F(x,y(x),y'(x)) \ dx
\]</span> 问题目标是找到一个函数 <span class="math inline">\(y(x)\)</span> ，使该泛函极大化或极小化。</p>
<p><strong>2.变分 / 引入微小变化</strong>：<strong>变分</strong>表示我们通过对函数 <span class="math inline">\(y(x)\)</span> 进行微小的变化，得到 <span class="math inline">\(Y(x)=y(x) + \epsilon \eta (x)\)</span>。其中，<span class="math inline">\(\epsilon\)</span> 是一个非常小的常数，<span class="math inline">\(\eta(x)\)</span>​ 是一个<strong>任意</strong>扰动函数（有时也把 <span class="math inline">\(\eta(x)\)</span> 称为“变分”）。</p>
<p><u>值得注意的是，一般要求 <span class="math inline">\(\eta(a) = \eta(b) = 0\)</span></u> ，<u>即要求积分曲线的初始位置和终止位置不能上下移动（定端变分 / 固定边界的变分）</u>。否则则为自由变分 / 自由边界变分。</p>
<p>我们有： <span class="math display">\[
Y' = y' + \epsilon \eta', \ \frac{dY}{d\epsilon}= \eta(x), \ \frac{dY'}{d\epsilon} = \eta'(x)
\]</span> <strong>3.计算泛函变化</strong>：将变化后的函数 <span class="math inline">\(Y(x)\)</span> 代入泛函，得到新的泛函值： <span class="math display">\[
\colorbox{Apricot}{J[Y]}=\int^b_a F(x,Y(x),Y'(x)) \ dx=J[y+\epsilon \eta]
\\J[y + \epsilon \eta] = \int^b_a F(x,\ y(x)+\epsilon \eta(x),\ y'(x)+\epsilon \eta '(x)) \ dx = \colorbox{Apricot}{J(epsilon)}
\]</span> 此时，若假设 <span class="math inline">\(y(x),\eta(x)\)</span>均取定（注意这是对 <span class="math inline">\(x\)</span> 的积分，因此 <span class="math inline">\(x\)</span> 无需处理），泛函的值随着 <span class="math inline">\(\epsilon\)</span> 的变化而变化，即此时<span style="background:#FFCC99;"> <span class="math inline">\(J\)</span> 可视为 <span class="math inline">\(\epsilon\)</span> 的函数</span>（上面橙色块）。</p>
<hr>
<p>4.<strong>极小化条件</strong></p>
<p>特别地，当 <span class="math inline">\(\epsilon=0\)</span>，有 <span class="math inline">\(Y=y\)</span> ,且： <span class="math display">\[
J'_{\epsilon}(\epsilon)|_{\epsilon =0}=0 \tag{*}
\]</span> 导数为0的原因是此时 <span class="math inline">\(J\)</span> 的取值与 <span class="math inline">\(\epsilon\)</span> 无关。由导数为0，知在点 <span class="math inline">\(\epsilon = 0\)</span> 处泛函 <span class="math inline">\(J[\epsilon]\)</span> 的变化量为0，故点 <span class="math inline">\(\epsilon = 0\)</span> 是极值点（类似导数为0）。那么我们只需利用极值情况下的（*）式得到一个不含 <span class="math inline">\(\epsilon 、\eta(x)\)</span>的方程，满足该方程的 <span class="math inline">\(y(x)\)</span> 即可使得泛函最大化或最小化。求解 <span class="math inline">\(J'_{\epsilon}(\epsilon)\)</span> 的公式如下：</p>
<figure>
<img src="/img/load.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/f811426a956e419245f70d9b74dcc99.jpg" alt="f811426a956e419245f70d9b74dcc99"><figcaption aria-hidden="true">f811426a956e419245f70d9b74dcc99</figcaption>
</figure>
<p>将 <span class="math inline">\(\epsilon=0\)</span> 和（*）代入，有 :</p>
<p><span class="math display">\[
J'(\epsilon)|_{\epsilon=0} = \int_a^b \eta(x) \cdot \left[\frac{dF}{dy}-\frac{d}{dx} [\frac{dF}{dy'}]\right]dx = 0 \tag{**}
\]</span></p>
<p>由于变分 <span class="math inline">\(\eta(x)\)</span> 可以取定为任意的扰动函数，且无论它如何变化，(**) 式始终都为0，因此： <span class="math display">\[
\frac{dF}{dy}-\frac{d}{dx} [\frac{dF}{dy'}] = 0 \tag{***}
\]</span> 最终得到的(***) 式即为<strong>欧拉-拉格朗日方程</strong>，这是变分法中的核心方程。我们找到满足该方程的 <span class="math inline">\(y(x)\)</span> 即可。</p>
<hr>
<blockquote>
<p>注：欧拉-拉格朗日方程的正确写法应该为： <span class="math display">\[
\frac{\partial F}{\partial y} - \frac{d}{dx}\left(\frac{\partial F}{\partial y'} \right) = 0
\]</span> 上面的偏分和微分符号写错了，对 <span class="math inline">\(F\)</span> 的偏导符号都应该写成“<span class="math inline">\(\partial\)</span>”。<sub>不过这个也问题不大啦，懒得改了</sub> ~</p>
</blockquote>
<h4 id="变分推断">变分推断</h4>
<p>变分推断（Variational Inference, VI）是一种用于复杂概率模型（如贝叶斯网络、变分自编码器）中近似推断的技术，尤其在后验分布 <span class="math inline">\(p(z| x)\)</span> 难以直接计算时，VI 提供了一种高效的近似方法。它通过将概率推断问题转化为一个优化问题: <span style="background:#daf5e9;">找到一个易于处理的<strong>变分分布</strong> <span class="math inline">\(q(z)\)</span>&nbsp;（如高斯分布、拉普拉斯分布）来逼近复杂的后验分布 <span class="math inline">\(p(z| x)\)</span> </span>。</p>
<p>这个“逼近”可以通过最小化两者之间的差异来实现，通常使用<strong>Kullback-Leibler 散度</strong>（KL 散度）作为损失函数来度量差异： <span class="math display">\[
KL(q(z)||p(z| x)) = \int q(z) \log \frac{q(z)}{p(z| x)} dz
\]</span> KL 散度越小，表示 <span class="math inline">\(q(z)\)</span> 越接近 <span class="math inline">\(p(z|x)\)</span> 。因此我们的目标是通过优化方法最小化 KL 散度，从而找到最优的变分分布 <span class="math inline">\(q(z)\)</span>。<strong>因为直接计算KL散度时，归一化常数（即证据 <span class="math inline">\(p(z| x)\)</span>）通常难以计算，因此我们引入一个可以优化的变分下界 （Variational Lower Bound，VLB）/ 证据下界（Evidence Lower Bound, ELBO）：</strong> <span class="math display">\[
\text{ELBO} = \mathbb{E}_{q(z)}[\log p(x,z)] - \mathbb{E}_{q(z)}[\log q(z)]
\]</span> <strong>通过最大化证据下界来间接最小化 KL 散度</strong>，最大化 ELBO 等价于最小化 KL 散度 （证明见下）。那么最后只需通过梯度下降或其他优化方法来最大化 ELBO，即可找到最优的 <span class="math inline">\(q(z)\)</span>​​ 。</p>
<blockquote>
<p><strong>变分推断的变分体现在哪里?</strong> <sub>AI的解释，没看懂</sub>~</p>
<p>1.求解优化问题的过程是通过最小化 <span class="math inline">\(q(z)\)</span> 和 <span class="math inline">\(p(z∣x)\)</span>​ 之间的 KL 散度，或等价地最大化证据下界（ELBO）。这一过程本质上是一个<strong>基于函数的优化问题</strong>，与传统的变分法非常相似。</p>
<p>2.变分推断的优化过程可以看作是应用了变分法的思想：我们引入一个变分分布（即函数的变化），并通过调整这个分布的参数来最小化某种目标函数（这里是 KL 散度）。这种通过优化近似分布的方式，体现了变分法中对函数变化的处理。</p>
</blockquote>
<p>变分推断有很多应用，如应用于拥有复杂后验分布的大型贝叶斯网络、无向图模型中的复杂概率分布（马尔科夫随机场）和变分自编码器（Variational Autoencoder, VAE）等。</p>
<h4 id="变分贝叶斯">变分贝叶斯</h4>
<p><strong>变分贝叶斯</strong>（Variational Bayesian, VB）是<strong>变分推断</strong>在贝叶斯推断中的一种具体应用。VB 非常有用的一个特性是推断优化的二元性：<u>我们可以将统计推断问题（从一个随机变量的值推断出另一种随机变量的值）作为优化问题（找到参变量的值来最小化某些目标函数）</u>。另外，<strong>变分下界 variational lower bound</strong> ，也被称作 <strong>evidence lower bound</strong> (ELBO)，在 VB 的推导中起了非常重要的作用。</p>
<h3 id="推导变分下界-l">推导变分下界 <span class="math inline">\(L\)</span></h3>
<p><strong>变分下界（variational lower bound 或 evidence lower bound, ELBO）</strong>是一种用于估计概率分布参数的方法。它通过最大化似然函数的下界来逼近真实的后验概率分布。这种方法可以用于在无法直接计算后验概率分布的情况下，近似地推断参数的取值。</p>
<p>已知变分贝叶斯（VB）将统计推断问题，即从一种随机变量推断出另一种随机变量的值，作为优化问题。</p>
<p>这里，将问题设置成从观测值（observed variable） <span class="math inline">\(X\)</span> 推断隐变量（hidden/latent variable） <span class="math inline">\(Z\)</span>​ ,由于两者之间存在关系：</p>
<blockquote>
<p>隐变量 <span class="math inline">\(Z\)</span> 可能包含参数 <span class="math inline">\(\theta\)</span> 。</p>
</blockquote>
<figure>
<img src="/img/load.gif" data-original="https://bluefisher.github.io/images/2020-02-06-%E7%90%86%E8%A7%A3-Variational-Lower-Bound/image-20200206195028885.png" alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>因此，<span class="math inline">\(X \rightarrow Z\)</span> 可视为求隐变量的后验概率： <span class="math display">\[
P\left( Z|X \right) =\frac{P\left( X|Z \right) P\left( Z \right)}{P\left( X \right)}=\frac{P\left( X|Z \right) P\left( Z \right)}{\int_Z{p\left( X,Z \right)}}
\]</span> 其中，大写的 <span class="math inline">\(P(X)\)</span> 表示某个变量的概率分布，小写的 <span class="math inline">\(p(X)\)</span> 表示 <span class="math inline">\(X\)</span> 分布的概率密度函数。</p>
<p>从观测值 <span class="math inline">\(X\)</span> 的边缘分布出发，设 <span class="math inline">\(q(Z)\)</span> 为VB中后验概率 <span class="math inline">\(p(Z|X)\)</span> 的估计概率，则： <span class="math display">\[
\begin{align}
\log P(X) &amp;=\log \int_{Z} p(X, Z) \tag{1}\\
&amp;=\log \int_{Z} p(X, Z) \frac{q(Z)}{q(Z)} \tag{2}\\
&amp;=\log \left(\mathbb{E}_{q}\left[\frac{p(X, Z)}{q(Z)}\right]\right) \tag{3}\\
&amp; \geq \mathbb{E}_{q}\left[\log \frac{p(X, Z)}{q(Z)}\right] \tag{4}\\
&amp;=\mathbb{E}_{q}[\log p(X, Z)]+H[Z] \tag{5}\\
&amp;=L\ \ --\ variational\ lower\ bound
\end{align}
\]</span> 从公式（2）到公式（3）运用到期望函数的定义： <span class="math inline">\(E\left( f\left( x \right) \right) =\int{xf\left( x \right) dx}\)</span> 。公式（4）对凸函数 log 运用了<strong>琴生不等式</strong>： <span class="math inline">\(f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)]\)</span> 。公式（5）中 <span class="math inline">\(H[Z]=-\mathbb{E}_{q}[\log q(Z)]\)</span> 是香农熵。</p>
<p>我们做标记： <span class="math display">\[
L=\mathbb{E}_{q}[\log p(X, Z)]+H[Z]
\]</span> 很明显 <span class="math inline">\(L\)</span> 就是观测变量的 log 概率的一个 lower bound。<strong>也就是说，如果我们想要去最大化边缘分布，我们可以转而最大化它的 variational lower bound <span class="math inline">\(L\)</span></strong> 。</p>
<h3 id="推导-相对熵-kl-散度">推导 相对熵 / KL 散度</h3>
<p>在很多情况下，后验概率 <span class="math inline">\(P(Z|X)\)</span> 的计算是十分困难的，比如我们可能需要对所有的隐变量做积分（求和）来计算分母。</p>
<p>变分方法的主要思想就是找一个估计的概率分布 <span class="math inline">\(q(Z)\)</span> 来尽可能地接近后验概率 <span class="math inline">\(p(Z|X)\)</span> 。这些估计的概率分布可以有它们独有的<em>变分参数（variational parameters）</em>：<span class="math inline">\(q(Z|\theta)\)</span> ，所以我们想要去寻找这些参数来使 <span class="math inline">\(q(Z)\)</span> 尽可能接近后验概率。当然 <span class="math inline">\(q(Z)\)</span> 的分布肯定要在推断中相对来说更加简单好求一些。</p>
<p>为了衡量两个概率分布 <span class="math inline">\(q(Z)\)</span> 和 <span class="math inline">\(p(Z|X)\)</span> 的相似程度，一个常用的标准就是就是 <strong>Kullback-Leibler (KL) 散度</strong>。其计算如下：</p>
<p><span class="math display">\[
\begin{align} 
K L[q(Z) \| p(Z | X)] &amp;=\int_{Z} q(Z) \log \frac{q(Z)}{p(Z | X)} \tag{6}\\ 
&amp;=-\int_{Z} q(Z) \log \frac{p(Z | X)}{q(Z)} \tag{7}\\ 
&amp;=-\left(\int_{Z} q(Z) \log \frac{p(X, Z)}{q(Z)}-\int_{Z} q(Z) \log p(X)\right) \tag{8}\\ 
&amp;=-\int_{Z} q(Z) \log \frac{p(X, Z)}{q(Z)}+\log p(X) \int_{Z} q(Z) \tag{9}\\ 
&amp;=-L+\log p(X) \tag{10}
\end{align}
\]</span></p>
<p>这里 <span class="math inline">\(L\)</span> 是变分下界 variational lower bound 。公式 (10) 是归一化常量 $ _{Z} q(Z) =1$ 而推导得来。整理可得： <span class="math display">\[
L=\log p(X)-K L[q(Z) \| p(Z | X)]
\]</span> 因为 KL 散度永远是 ≥0 的，所以，再一次，我们得到了 <span class="math inline">\(L \le log\ p(X)\)</span> 是观测变量的分布的一个 lower bound。同时我们也知道了它们之间的区别就在于估计分布和真实分布之间的 KL 散度。换句话说，如果估计分布与真实后验分布完美接近，那么 lower bound <span class="math inline">\(L\)</span>​ 就等于 log 概率。</p>
<h3 id="推导交叉熵">推导交叉熵</h3>
<blockquote>
<p>参考自：<a target="_blank" rel="noopener" href="https://blog.csdn.net/tsyccnh/article/details/79163834">一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉-CSDN博客</a></p>
<p>简洁版介绍看：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/115277553">损失函数：交叉熵详解 - 知乎 (zhihu.com)</a></p>
</blockquote>
<p>在离散情况下 <span class="math inline">\(P(x)、Q(x)\)</span> 两种概率分布的相对熵计算公式： <span class="math display">\[
D_{KL}\left( p||q \right) =\sum_{i=1}^n{p\left( x_i \right) \log \frac{p\left( x_i \right)}{q\left( x_i \right)}}
\]</span> 对该式变形可得： <span class="math display">\[
\begin{aligned}
    D_{KL}\left( p||q \right) &amp;=\sum_{i=1}^n{p}\left( x_i \right) \log \left( p\left( x_i \right) \right) -\sum_{i=1}^n{p}\left( x_i \right) \log \left( q\left( x_i \right) \right)\\
    &amp;=-H\left( p\left( x \right) \right) +\left[ -\sum_{i=1}^n{p}\left( x_i \right) \log \left( q\left( x_i \right) \right) \right]\\
    &amp;=-H\left( p\left( x \right) \right) +E_p\left[ -\log \left( q\left( x_i \right) \right) \right]\\
    &amp;=-H\left( p\left( x \right) \right) +H\left( p,q \right)\\
\end{aligned}
\]</span> 即 KL 散度等式的前一部分恰巧就是 <span class="math inline">\(p\)</span> 的熵，后一部分正是交叉熵 <span class="math inline">\(H(p,q)\)</span> （<u>交叉熵 <span class="math inline">\(H(p,q)\)</span> 可理解为</u>）： <span class="math display">\[
H\left( p,q \right) =E_p\left[ -\log \left( q\left( x_i \right) \right) \right] =-\sum_{i=1}^n{p}\left( x_i \right) \log \left( q\left( x_i \right) \right)
\]</span> 在机器学习中，如果 <span class="math inline">\(P(x)\)</span> 用来表示样本的真实分布，而 <span class="math inline">\(Q(x)\)</span> 用来表示模型所预测的分布。如果我们需要评估 label 和 predicts 之间的差距，可以使用 KL 散度，即 $D_{KL}( p||q ) $ 。<strong>由于 KL 散度中的前一部分 <span class="math inline">\(-H(p)\)</span> 只与真值有关，是不变的，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用交叉熵做 loss，评估模型</strong>。</p>
<h2 id="损失函数">损失函数</h2>
<figure>
<img src="/img/load.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240320_194222_0.jpg" alt="20240320_194222_0"><figcaption aria-hidden="true">20240320_194222_0</figcaption>
</figure>
<figure>
<img src="/img/load.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/diff22111.jpg" alt="20240320_194348_1"><figcaption aria-hidden="true">20240320_194348_1</figcaption>
</figure>
<figure>
<img src="/img/load.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/20240320_194446_2.jpg" alt="20240320_194446_2"><figcaption aria-hidden="true">20240320_194446_2</figcaption>
</figure>
<figure>
<img src="/img/load.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/diff22333.jpg" alt="20240320_194530_3"><figcaption aria-hidden="true">20240320_194530_3</figcaption>
</figure>
<p><strong>注3</strong>： 上述&lt;8&gt;式去掉权重系数是因为 DDPM 发现这样能使得 diffusion 模型工作得更好。</p>
<p>最后的简化形式： <span class="math display">\[
L_{simple}=L^{simple}_t+C
\]</span> 其中， <span class="math inline">\(C\)</span> 是与模型参数 <span class="math inline">\(\theta\)</span> 无关的常量。</p>
<h1 id="最终算法流程">最终算法流程</h1>
<p><span style="background:#dad5e9;"><strong>总结：</strong> DDPM（Denoising Diffusion Probabilistic Models）作为一种生成模型，它通过逐步添加噪声的方式将数据转化为噪声，然后通过学习逆过程来生成数据。</span></p>
<figure>
<img src="/img/load.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240320200738823.png" alt="image-20240320200738823"><figcaption aria-hidden="true">image-20240320200738823</figcaption>
</figure>
<blockquote>
<p>The training and sampling algorithms in DDPM (Image source: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.11239">Ho et al. 2020</a>)</p>
</blockquote>
<p><span style="background:#FFCC99;"><strong>训练阶段</strong>重复如下步骤:</span></p>
<ul>
<li><p>从数据集中采样 <span class="math inline">\(x_0\)</span></p></li>
<li><p>从 <span class="math inline">\(1...T\)</span> 随机采样 time step <span class="math inline">\(t\)</span> ：对该 batch 中的每个图像随机取 <span class="math inline">\(t\)</span> 进行训练</p></li>
<li><p>从标准高斯分布采样噪声 $_t( 0, ) $</p></li>
<li><p>调用模型预估 $_{}( _0+_t,t ) $：这里使用的模型主要是 U-net 模型</p></li>
<li><p>计算<strong>最小化噪声之间的 MSE Loss</strong>: $<em>t-</em>{}( _0+_t,t ) ^2 $, 并利用反向传播算法训练模型.</p></li>
</ul>
<blockquote>
<p>DDPM（Denoising Diffusion Probabilistic Models）是一种生成模型，它通过逐步添加噪声的方式将数据转化为噪声，然后通过学习逆过程来生成数据。DDPM的训练过程包括以下几个关键步骤： 1. 初始化：首先，从数据集中随机抽取一个样本 <span class="math inline">\(x_0\)</span>，并随机选择一个时间步 t。 2. 加噪：在选定的时间步 t，根据预先定义的噪声方差计划 <span class="math inline">\(\beta_t\)</span>，向样本 <span class="math inline">\(x_0\)</span> 添加噪声，生成带噪声的样本 <span class="math inline">\(x_t\)</span>。这一步模拟了前向扩散过程。 3. 模型预测：将带噪声的样本 <span class="math inline">\(x_t\)</span> 和时间步 t 输入到训练的神经网络（通常是U-Net结构）中，网络预测在时间步 t 添加的噪声$ _(x_t, t)$。 4. 计算损失：计算网络预测的噪声和实际添加的噪声之间的均方误差（MSE），作为损失函数。 5. 优化：通过反向传播和梯度下降等优化算法更新网络的参数，以最小化损失函数。 6. 重复：重复以上步骤，直到网络训练完成。 在训练过程中，DDPM使用了一个称为U-Net的神经网络结构，该结构包含多个残差块和注意力机制，以提高模型的学习能力和效率。U-Net的输入是带噪声的图像和时间步 t，输出是预测的噪声。通过训练，U-Net学习如何逐步去除噪声，最终恢复出清晰的图像。 此外，DDPM的训练还涉及到一个噪声调度器（DDPM_Scheduler），它根据时间步 t 来确定噪声的方差。这个调度器是固定的，不参与训练，它定义了从数据样本到纯噪声的整个扩散过程。 在实际的训练代码中，还会使用一些技术来提高模型的性能，例如指数移动平均（EMA）来平滑模型参数，以及在训练过程中保存检查点（checkpoints）以便恢复训练。 总的来说，DDPM的训练过程是通过模拟数据的扩散过程，并训练一个神经网络来逆转这个过程，从而学习生成与原始数据相似的新样本。这个过程涉及到复杂的数学推导和深度学习技术，但最终的目标是生成高质量、高逼真度的图像。</p>
</blockquote>
<p><strong>逆向阶段</strong>采用如下步骤进行采样:</p>
<ul>
<li>从高斯分布采样 <span class="math inline">\(x_T\)</span></li>
<li>按照 <span class="math inline">\(T,...,1\)</span> 的顺序进行迭代:
<ul>
<li>如果 <span class="math inline">\(t=1\)</span> , 令 <span class="math inline">\(\mathbf{z}=0\)</span> ; 如果 <span class="math inline">\(t&gt;1\)</span> , 从高斯分布中采样 $( 0, ) $ <span style="background:#FFCC99;">（最后一步不加噪音）</span></li>
<li>利用式 &lt;6&gt; 学习出均值 $_{}( _t,t ) =( <em>t-</em>{}( _t,t ) ) $ , 并计算均方差 <span class="math inline">\(\sigma _t=\sqrt{\tilde{\beta}_t}=\sqrt{\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\cdot \beta _t}\)</span></li>
<li>通过重参数技巧采样 <span class="math inline">\(x_{t-1}=\mu _{\theta}\left( x_t,t \right) +\sigma _t\mathbf{z}\)</span></li>
</ul></li>
<li>经过以上过程的迭代, 最终恢复 <span class="math inline">\(x_0\)</span> .</li>
</ul>
<blockquote>
<p>DDPM（Denoising Diffusion Probabilistic Models）的采样过程是一个逐步的去噪过程，它是训练过程的逆过程。以下是DDPM采样过程的详细步骤： 1. 初始化：从一个标准正态分布开始，采样一个纯噪声图像 <span class="math inline">\(x_T\)</span>。 2. 逆扩散过程：这个过程涉及多个时间步，通常从 T 到 1 递减。在每一步 t，执行以下操作： 使用预测模型（通常是训练好的U-Net）预测在时间步 t 的噪声 <span class="math inline">\(\epsilon_{\theta}(x_t, t)\)</span>​。 根据预测的噪声和当前噪声图像 x_t，计算下一个时间步 t-1 的图像 <span class="math inline">\(x_{t-1}\)</span>。这通常通过以下公式完成：_ <span class="math display">\[
    x_{t-1} = f_\theta(x_t) + \sqrt{\alpha_t} \cdot \epsilon
  \]</span></p>
<p>​ 其中 <span class="math inline">\(\alpha_t\)</span> 是预先定义的方差调度计划中的一个因子，<span class="math inline">\(\epsilon\)</span> 是从标准正态分布中采样的噪声。 3. 迭代：重复步骤2，直到时间步 t 减少到1。 4. 输出：最终的 <span class="math inline">\(x_0\)</span> 是生成的图像，它经过了从纯噪声到数据分布的逆扩散过程。 在采样过程中，DDPM模型的网络部分 <span class="math inline">\(f_\theta\)</span> 被用来预测在每一步添加到图像中的噪声。这个预测是基于当前的噪声图像和时间步的信息。通过逐步减少噪声，模型最终能够生成一幅与训练数据相似的图像。 这个过程可以被看作是一个确定性的、逐步的逆向过程，它能够生成高质量的图像样本。然而，由于每一步都需要模型的一次前向传播，因此整个采样过程可能需要大量的计算步骤，特别是当时间步 T 很大时。为了提高效率，一些改进的模型如 DDIM（Denoising Diffusion Implicit Models）被提出，它们通过减少所需的采样步骤来加速这个过程 。</p>
</blockquote>
<h1 id="参考文献">参考文献</h1>
<p>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.11239.pdf">Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.6114">Kingma D P, Welling M. Auto-encoding variational bayes[J]. arxiv preprint arxiv:1312.6114, 2013.</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/684456268">Van Den Oord A, Vinyals O. Neural discrete representation learning[J]. Advances in neural information processing systems, 2017, 30.</a></p>
<p>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.12092.pdf">Ramesh A, Pavlov M, Goh G, et al. Zero-shot text-to-image generation[C]International conference on machine learning. Pmlr, 2021: 8821-8831.</a></p>
<p>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1406.2661.pdf">Creswell A, White T, Dumoulin V, et al. Generative adversarial networks: An overview[J]. IEEE signal processing magazine, 2018, 35(1): 53-65.</a></p>
<p>[6] <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html">Esser P, Rombach R, Ommer B. Taming transformers for high-resolution image synthesis[C]Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 12873-12883.</a></p>
<p>[7] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2204.08583">Crowson K, Biderman S, Kornis D, et al. Vqgan-clip: Open domain image generation and editing with natural language guidance[C]European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022: 88-105.</a></p>
<p>[8] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.10741.pdf">Nichol A, Dhariwal P, Ramesh A, et al. Glide: Towards photorealistic image generation and editing with text-guided diffusion models[J]. arixv preprint arixv:2112.10741, 2021.</a></p>
<p>[9] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.06125.pdf">Ramesh A, Dhariwal P, Nichol A, et al. Hierarchical text-conditional image generation with clip latents[J]. arixv preprint arixv:2204.06125, 2022, 1(2): 3.</a></p>
<p>[10] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.11487.pdf">Saharia C, Chan W, Saxena S, et al. Photorealistic text-to-image diffusion models with deep language understanding[J]. Advances in neural information processing systems, 2022, 35: 36479-36494.</a></p>
<p>[11] <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf">Rombach R, Blattmann A, Lorenz D, et al. High-resolution image synthesis with latent diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 10684-10695.</a></p>
<p>[12] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.03585">Sohl-Dickstein J, Weiss E, Maheswaranathan N, et al. Deep unsupervised learning using nonequilibrium thermodynamics[C]//International conference on machine learning. PMLR, 2015: 2256-2265.</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">胖胖大藕片</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/10/05/Diffusion%20Model%20%EF%BC%88%E4%BA%8C%EF%BC%89/">http://example.com/2024/10/05/Diffusion Model （二）/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">藕片种植基地</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/post_img/20231125235054.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer=""></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/load.gif" data-original="/img/wechat.jpg" alt="微信"></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/load.gif" data-original="/img/alipay.jpg" alt="支付宝"></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/10/08/%E9%AB%98%E6%96%AF%E6%A0%B8%E5%87%BD%E6%95%B0RBF/" title="高斯核函数 RBF"><img class="cover" src="/img/load.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/post_img/20231125235032.jpg" onerror="onerror=null;src='/img/problem.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">高斯核函数 RBF</div></div></a></div><div class="next-post pull-right"><a href="/2024/09/21/%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D%E7%B4%A2%E5%BC%95/" title="专业名词索引"><img class="cover" src="/img/load.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/hexue.jpg" onerror="onerror=null;src='/img/problem.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">专业名词索引</div></div></a></div></nav><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/load.gif" data-original="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info__name">胖胖大藕片</div><div class="author-info__description">人生无完美，曲折亦风景</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">80</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">47</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/hahahaha5606"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/hahahaha5606" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:212188767@qq.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content"></div><timing></timing></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%95%E5%85%A5"><span class="toc-text">引入</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%91%E5%B1%95%E7%AE%80%E4%BB%8B"><span class="toc-text">发展简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8Evq-vae"><span class="toc-text">基于VQ-VAE</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ae"><span class="toc-text">AE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#vae"><span class="toc-text">VAE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#vq-vae"><span class="toc-text">VQ-VAE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#delle"><span class="toc-text">DELL·E</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E-gan"><span class="toc-text">基于 GAN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#gan"><span class="toc-text">GAN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#vqgan"><span class="toc-text">VQGAN</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E-diffusion"><span class="toc-text">基于 Diffusion</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#diffusion"><span class="toc-text">Diffusion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#glide"><span class="toc-text">GLIDE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dalle2"><span class="toc-text">DALL·E2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#imagen"><span class="toc-text">Imagen</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#stable-diffusion"><span class="toc-text">Stable Diffusion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sora"><span class="toc-text">Sora</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%84%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94"><span class="toc-text">各生成模型对比</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ddpm"><span class="toc-text">DDPM</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#forward-%E4%B8%8E-backward-%E8%BF%87%E7%A8%8B"><span class="toc-text">forward 与 backward 过程</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-text">模型训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="toc-text">预备知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%8D%E8%AF%8D%E6%A6%82%E5%BF%B5"><span class="toc-text">名词概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%8D%E5%8F%82%E6%95%B0%E6%8A%80%E5%B7%A7"><span class="toc-text">重参数技巧</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%8E%E9%AA%8C%E5%88%86%E5%B8%83"><span class="toc-text">后验分布</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%8F%98%E5%88%86"><span class="toc-text">什么是“变分”</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD"><span class="toc-text">变分推断</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%98%E5%88%86%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="toc-text">变分贝叶斯</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E5%AF%BC%E5%8F%98%E5%88%86%E4%B8%8B%E7%95%8C-l"><span class="toc-text">推导变分下界 \(L\)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E5%AF%BC-%E7%9B%B8%E5%AF%B9%E7%86%B5-kl-%E6%95%A3%E5%BA%A6"><span class="toc-text">推导 相对熵 / KL 散度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E5%AF%BC%E4%BA%A4%E5%8F%89%E7%86%B5"><span class="toc-text">推导交叉熵</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">损失函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%80%E7%BB%88%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="toc-text">最终算法流程</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-text">参考文献</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">©2023 - 2025  <i id="heartbeat" class="fa fas fa-heartbeat"></i> 胖胖大藕片</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><p id="ghbdages"></p></div></footer></div><div id="rightside"><!--这样修改较麻烦，不如直接修改"F:\Blog\themes\butterfly\source\css\_layout\rightside.styl"中的逻辑，将其取反，即可默认显示全部按钮，且点击后隐藏--><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><a class="icon-V hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><svg width="25" height="25" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'ybVvPd5f8ymUrtyDgiPIcU3E-gzGzoHsz',
      appKey: 'P08uK63s8SFzZd9oN7vr8tmn',
      avatar: 'monsterid',
      serverURLs: 'https://ybvvpd5f.lc-cn-n1-shared.com',
      emojiMaps: "",
      master: 'cda7cc5c3b41c1ce1e80dade99393150',   //博主邮箱md5加密32位小写
      tagMeta: ["博主","小伙伴","访客"],     //标识字段名
      friends:  [],  //小伙伴邮箱Md5
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('/js/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script></div><script src="https://cdn.jsdelivr.net/npm/blueimp-md5/js/md5.min.js"></script><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="/img/load.gif" data-original="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getIcon = (icon, mail) => {
    if (icon) return icon
    let defaultIcon = '?d=monsterid'
    let iconUrl = `https://gravatar.loli.net/avatar/${md5(mail.toLowerCase()) + defaultIcon}`
    return iconUrl
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }

        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const getComment = () => {
    const serverURL = 'https://ybvvpd5f.lc-cn-n1-shared.com'

    var settings = {
      "method": "GET",
      "headers": {
        "X-LC-Id": 'ybVvPd5f8ymUrtyDgiPIcU3E-gzGzoHsz',
        "X-LC-Key": 'P08uK63s8SFzZd9oN7vr8tmn',
        "Content-Type": "application/json"
      },
    }

    fetch(`${serverURL}/1.1/classes/Comment?limit=6&order=-createdAt`,settings)
      .then(response => response.json())
      .then(data => {
        const valineArray = data.results.map(function (e) {
          return {
            'avatar': getIcon(e.QQAvatar, e.mail),
            'content': changeContent(e.comment),
            'nick': e.nick,
            'url': e.url + '#' + e.objectId,
            'date': e.updatedAt,
          }
        })
        saveToLocal.set('valine-newest-comments', JSON.stringify(valineArray), 10/(60*24))
        generateHtml(valineArray)
      }).catch(e => {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.textContent= "无法获取评论，请确认相关配置是否正确"
      }) 
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('valine-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script src="https://libs.baidu.com/jquery/2.1.4/jquery.min.js"></script><script src="/js/crash_cheat.js"></script><script src="/js/heartbeat.js"></script><script src="/js/showAppreciation.js"></script><script src="/js/timing.js"></script><script src="/js/sun_moon.js" async=""></script><script defer="" src="https://rmt.dogedoge.com/fetch/~/source/jsdelivr/npm/jquery@latest/dist/jquery.min.js"></script><script defer="" data-pjax="" src="https://cdn.jsdelivr.net/gh/sirxemic/jquery.ripples/dist/jquery.ripples.js"></script><script defer="" data-pjax="" src="/js/ripples.js"></script><script async="" data-pjax="" src="/js/anzhiyu.js"></script><script async="" data-pjax="" src="/js/anzhiyufunction.js"></script><script async="" src="/js/anzhiyuOnlyOne.js"></script><script type="text/javascript" src="/js/szgotop.js"></script><script src="/js/perfectscrollbar.min.js"></script><script src="/js/table_scrollbar.js"></script><script src="/js/hexo_githubcalendar.js"></script><script src="/js/load_book.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="true" data-text="富强,民主,文明,和谐,爱国,敬业,诚信,友善,自由,平等,公正,法治" data-fontsize="15px" data-random="true" async="async"></script><link rel="stylesheet" href="/css/Aplayer.min.css" media="print" onload="this.media='all'"><script src="/js/Aplayer.min.js"></script><script src="/js/Meting2.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  //document.querySelectorAll('script[data-pjax]').forEach(item => {
  document.querySelectorAll('script[data-pjax], .pjax-reload script').forEach(*item* *=>* {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async="" data-pjax="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div><div class="pjax-reload"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInLeft');
    arr[i].setAttribute('data-wow-duration', '600ms');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInRightBig');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer="defer" src="https://cdn.jsdelivr.net/gh/graingert/wow@1.3.0/dist/wow.min.js"></script><script defer="defer" src="/js/wow_init.js"></script></div><div id="nav-music"><div id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()">播放音乐</div><!--meting-js#8152976493(server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random") 安知鱼歌单--><meting-js id="12389320665" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random"> </meting-js><!-- 我的网易云歌单--></div><!-- hexo injector body_end start -->
  <script data-pjax="" src="/js/hexo_githubcalendar.js"></script>
  <script data-pjax="">
        function GithubCalendarConfig(){
            var git_githubapiurl ="https://githubcalendarapi.shiguang666.eu.org/api?user=hahahaha5606";
            var git_color =['#ebedf0', '#fdcdec', '#fc9bd9', '#fa6ac5', '#f838b2', '#f5089f', '#c4067e', '#92055e', '#540336', '#48022f', '#30021f'];
            var git_user ="hahahaha5606";
            var parent_div_git = document.getElementById('recent-posts');
            var git_div_html = '<div class="recent-post-item github_calendar" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>';
            if(parent_div_git && location.pathname =='/'){
                console.log("已挂载hexo-github-calendar https://github.com/Barry-Flynn/hexo-github-calendar");
                // parent_div_git.innerHTML=git_div_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",git_div_html) // 有报错，但不影响使用(支持pjax跳转)
            };
            GithubCalendar(git_githubapiurl,git_color,git_user)
        }
        if(document.getElementById('recent-posts')){
            GithubCalendarConfig()
        }
    </script>
    <style>#github_container{min-height:280px}@media screen and (max-width:650px) {#github_container{background-image:;min-height:0px}}</style>
    <style></style><script data-pjax="">
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo" title=""><img src="/img/load.gif" data-original="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly" title=""><img src="/img/load.gif" data-original="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://www.jsdelivr.com/" style="margin-inline:5px" data-title="本站使用JsDelivr为静态资源提供CDN加速" title=""><img src="/img/load.gif" data-original="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&amp;logo=jsDelivr" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" data-title="本站采用双线部署，默认线路托管于Vercel" title=""><img src="/img/load.gif" data-original="https://img.shields.io/badge/Hosted-Vercel-brightgreen?style=flat&amp;logo=Vercel" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" data-title="本站采用双线部署，联通线路托管于Coding" title=""><img src="/img/load.gif" data-original="https://img.shields.io/badge/Hosted-Coding-0cedbe?style=flat&amp;logo=Codio" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Gtihub托管" title=""><img src="/img/load.gif" data-original="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src="/img/load.gif" data-original="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async="" src="/js/runtime_huibiao.js"></script><!-- hexo injector body_end end -->
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a,i=c[o];e=function(){c=c.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(n=new Image,a=t.getAttribute("data-original"),n.onload=function(){t.src=a,t.removeAttribute("data-original"),e&&e()},t.src!==a&&(n.src=a))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script><script src="https://cdn.jsdelivr.net/npm/live2d-widget@^3.0.5/lib/L2Dwidget.min.js"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"position":"right","width":300,"height":600,"hOffset":45,"vOffset":-150},"mobile":{"show":false,"scale":1},"react":{"opacityDefault":0.3,"opacityOnHover":0.3,"opacity":0.95},"log":false});</script></body></html>