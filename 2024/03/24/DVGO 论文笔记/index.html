<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>DVGO | 藕片侠大战论文怪</title><meta name="author" content="胖胖大藕片"><meta name="copyright" content="胖胖大藕片"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="论文名称：Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction （DVGO： 超快速收敛的辐射场重建）
 代码地址： GitHub - sunset1995/DirectVoxGO: Direct voxel grid optimization for fast rad">
<meta property="og:type" content="article">
<meta property="og:title" content="DVGO">
<meta property="og:url" content="http://example.com/2024/03/24/DVGO%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="藕片侠大战论文怪">
<meta property="og:description" content="论文名称：Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction （DVGO： 超快速收敛的辐射场重建）
 代码地址： GitHub - sunset1995/DirectVoxGO: Direct voxel grid optimization for fast rad">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/sonnnwwww.png">
<meta property="article:published_time" content="2024-03-24T11:59:39.363Z">
<meta property="article:modified_time" content="2024-03-30T09:28:43.854Z">
<meta property="article:author" content="胖胖大藕片">
<meta property="article:tag" content="CVPR2022">
<meta property="article:tag" content="voxel grid">
<meta property="article:tag" content="训练加速">
<meta property="article:tag" content="新视图合成">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/sonnnwwww.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/03/24/DVGO%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'DVGO',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-03-30 17:28:43'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/transpancy.css"><link rel="stylesheet" href="/css/showAppreciation.css"><link rel="stylesheet" href="/css/ancientPoetry.css"><link rel="stylesheet" href="/css/mycss.css"><style>#article-container.post-content h1:before, h2:before, h3:before, h4:before, h5:before, h6:before { -webkit-animation: avatar_turn_around 1s linear infinite; -moz-animation: avatar_turn_around 1s linear infinite; -o-animation: avatar_turn_around 1s linear infinite; -ms-animation: avatar_turn_around 1s linear infinite; animation: avatar_turn_around 1s linear infinite; }</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/macblack.css"><link rel="stylesheet" href="/css/datu.css"><link rel="stylesheet" href="/css/aplayer.css"></head><body><svg aria-hidden="true" style="position:absolute; overflow:hidden; width:0; height:0"><symbol id="icon-sun" viewBox="0 0 1024 1024"><path d="M960 512l-128 128v192h-192l-128 128-128-128H192v-192l-128-128 128-128V192h192l128-128 128 128h192v192z" fill="#FFD878" p-id="8420"></path><path d="M736 512a224 224 0 1 0-448 0 224 224 0 1 0 448 0z" fill="#FFE4A9" p-id="8421"></path><path d="M512 109.248L626.752 224H800v173.248L914.752 512 800 626.752V800h-173.248L512 914.752 397.248 800H224v-173.248L109.248 512 224 397.248V224h173.248L512 109.248M512 64l-128 128H192v192l-128 128 128 128v192h192l128 128 128-128h192v-192l128-128-128-128V192h-192l-128-128z" fill="#4D5152" p-id="8422"></path><path d="M512 320c105.888 0 192 86.112 192 192s-86.112 192-192 192-192-86.112-192-192 86.112-192 192-192m0-32a224 224 0 1 0 0 448 224 224 0 0 0 0-448z" fill="#4D5152" p-id="8423"></path></symbol><symbol id="icon-moon" viewBox="0 0 1024 1024"><path d="M611.370667 167.082667a445.013333 445.013333 0 0 1-38.4 161.834666 477.824 477.824 0 0 1-244.736 244.394667 445.141333 445.141333 0 0 1-161.109334 38.058667 85.077333 85.077333 0 0 0-65.066666 135.722666A462.08 462.08 0 1 0 747.093333 102.058667a85.077333 85.077333 0 0 0-135.722666 65.024z" fill="#FFB531" p-id="11345"></path><path d="M329.728 274.133333l35.157333-35.157333a21.333333 21.333333 0 1 0-30.165333-30.165333l-35.157333 35.157333-35.114667-35.157333a21.333333 21.333333 0 0 0-30.165333 30.165333l35.114666 35.157333-35.114666 35.157334a21.333333 21.333333 0 1 0 30.165333 30.165333l35.114667-35.157333 35.157333 35.157333a21.333333 21.333333 0 1 0 30.165333-30.165333z" fill="#030835" p-id="11346"></path></symbol></svg><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css">
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"><!--#loading-box//.loading-left-bg
//.loading-right-bg
//.spinner-box
  //.configure-border-1
    //.configure-core
  //.configure-border-2
    //.configure-core
  //.loading-word= _p('loading')
--><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="wizard-scene"><div class="wizard-objects"><div class="wizard-square"></div><div class="wizard-circle"></div><div class="wizard-triangle"></div></div><div class="wizard"><div class="wizard-body"></div><div class="wizard-right-arm"><div class="wizard-right-hand"></div></div><div class="wizard-left-arm"><div class="wizard-left-hand"></div></div><div class="wizard-head"><div class="wizard-beard"></div><div class="wizard-face"><div class="wizard-adds"></div></div><div class="wizard-hat"><div class="wizard-hat-of-the-hat"></div><div class="wizard-four-point-star --first"></div><div class="wizard-four-point-star --second"></div><div class="wizard-four-point-star --third"></div></div></div></div></div></div><script async="async">(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = 'auto'
      //$loadingBox.classList.add('loaded')
      setTimeout(function(){
      document.getElementById('loading-box').classList.add("loaded")
    }, 1500);
      document.getElementById('loading-box').style.transition = 'opacity 1s ease 0.3s' //000
      document.getElementById('loading-box').style.opacity = '0'  // 000
    },
    initLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.remove('loaded')
      document.getElementById('loading-box').style.transition = '';  //000
      document.getElementById('loading-box').style.opacity = '1'  //000
    }
  }

  preloader.initLoading()
  // window.addEventListener('load',() => { preloader.endLoading() })
  setTimeout(function(){preloader.endLoading();}, 1500);
  document.getElementById('loading-box').addEventListener('click',()=> {preloader.endLoading()})

  if (true) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }

})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/yazi.gif" data-original="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">31</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/Message/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/sonnnwwww.png')"><nav id="nav"><span id="blog-info"><a href="/" title="藕片侠大战论文怪"><span class="site-name">藕片侠大战论文怪</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/Message/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">DVGO</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-03-24T11:59:39.363Z" title="发表于 2024-03-24 19:59:39">2024-03-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-30T09:28:43.854Z" title="更新于 2024-03-30 17:28:43">2024-03-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%8E%9F%E7%90%86%E7%AC%94%E8%AE%B0/">原理笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>28分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title="DVGO"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="twikoo_visitors"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><div class="top-img" style="background-image: url('https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/sonnnwwww.png');"></div><article class="post-content" id="article-container"><blockquote>
<p>论文名称：<strong>Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction</strong> <strong>（DVGO： 超快速收敛的辐射场重建）</strong></p>
<p>代码地址： <a target="_blank" rel="noopener" href="https://github.com/sunset1995/DirectVoxGO">GitHub - sunset1995/DirectVoxGO: Direct voxel grid optimization for fast radiance field reconstruction.</a></p>
<p>论文主页： <a target="_blank" rel="noopener" href="https://sunset1995.github.io/dvgo/">DVGO (sunset1995.github.io)</a></p>
<p>参考网址：</p>
<ul>
<li>[<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/584734270">NeRF-训练加速] DVGO - 知乎 (zhihu.com)</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/hy592070616/article/details/120623303">机器学习中的数学——激活函数（十）：Softplus函数-CSDN博客</a></li>
</ul>
</blockquote>
<h1 id="论文基本信息">论文基本信息</h1>
<p>Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction<sup>[1]</sup>是由来自清华大学的 Cheng Sun 等人发表在 <strong>2022 CVPR(Oral)</strong> 会议上的SCI论文。</p>
<p>DVGO 的主要目的是<u>加速 NeRF<sup>[2]</sup> 的训练</u>（从数个小时到十几分钟的逐个场景重建，且不需要预训练），采用的方式是将场景使用显式体素表达。DVGO 仍属于<strong>稠密重建</strong>。</p>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240305143817326.png" alt="image-20240305143817326"><figcaption aria-hidden="true">image-20240305143817326</figcaption>
</figure>
<h2 id="优点">优点</h2>
<ol type="1">
<li><p><strong>在体素密度直接优化中，采用了两个先验算法来避免几何陷入局部最优解：</strong>直接优化密度体素网格会导致超快收敛，但容易出现次优解，所提出方法在自由空间分配"云"，并试图将光度损失与云拟合，而不是搜索具有更好多视图一致性的几何。对这个问题的解决方案简单而有效。<u>首先</u>，初始化密度体素网格，以产生非常接近于零的不透明度，以避免几何解决方案偏向于相机的附近平面。<u>其次</u>，给较少视图可见的体素一个较低的学习率，可以避免仅为解释少量视图的观察而分配的冗余体素。所提出的解决方案可以成功地避免次优几何，并在五个数据集上表现良好。</p></li>
<li><p><strong>提出了先插值后激活的体素网格插值，它可以在较低的网格分辨率下实现清晰的边界建模：</strong>之前的工作要么对激活的不透明度进行体素网格插值，要么使用最近邻插值，从而在每个网格单元中产生光滑的表面。从数学和经验上证明，所提出的后激活可以在单个网格单元内建模(超越)尖锐的线性表面。因此，可以使用更少的体素来实现更好的质量——具有160^3个密集体素的方法在大多数情况下已经优于NeRF。</p></li>
</ol>
<blockquote>
<p>[!IMPORTANT]</p>
<h3 id="features">Features</h3>
<ul>
<li><p><span style="background:#daf5e9;">Speedup NeRF by replacing the MLP with the voxel grid.</span></p></li>
<li><p><span style="background:#daf5e9;">Simple scene representation:</span></p></li>
<li><p><em>Volume densities</em>: dense voxel grid (3D).</p></li>
<li><p><em>View-dependent colors</em>: dense feature grid (4D) + shallow MLP.</p></li>
<li><p><span style="background:#daf5e9;">Pytorch cuda extention built just-in-time for another 2--3x speedup.</span></p></li>
<li><p><span style="background:#daf5e9;">O(N) realization for the distortion loss proposed by mip-nerf 360</span></p></li>
<li><p>The loss improves our training time and quality.</p></li>
<li><p>We have released a self-contained pytorch package: <a target="_blank" rel="noopener" href="https://github.com/sunset1995/torch_efficient_distloss">torch_efficient_distloss</a>.</p></li>
<li><p>Consider a batch of 8192 rays X 256 points.</p>
<ul>
<li>GPU memory consumption: 6192MB =&gt; 96MB.</li>
<li>Run times for 100 iters: 20 sec =&gt; 0.2sec.</li>
</ul></li>
<li><p><span style="background:#daf5e9;"><strong>Supported datasets:</strong></span></p></li>
<li><p><strong><em>Bounded inward-facing</em>: <a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1">NeRF</a>, <a target="_blank" rel="noopener" href="https://dl.fbaipublicfiles.com/nsvf/dataset/Synthetic_NSVF.zip">NSVF</a>, <a target="_blank" rel="noopener" href="https://dl.fbaipublicfiles.com/nsvf/dataset/BlendedMVS.zip">BlendedMVS</a>, <a target="_blank" rel="noopener" href="https://dl.fbaipublicfiles.com/nsvf/dataset/TanksAndTemple.zip">T&amp;T (masked)</a>, <a target="_blank" rel="noopener" href="https://drive.google.com/open?id=1ScsRlnzy9Bd_n-xw83SP-0t548v63mPH">DeepVoxels</a>.</strong></p></li>
<li><p><strong><em>Unbounded inward-facing</em>: <a target="_blank" rel="noopener" href="https://drive.google.com/file/d/11KRfN91W1AxAW6lOFs4EeYDbeoQZCi87/view?usp=sharing">T&amp;T</a>, <a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1gsjDjkbTh4GAR9fFqlIDZ__qR9NYTURQ/view?usp=sharing">LF</a>, <a target="_blank" rel="noopener" href="https://jonbarron.info/mipnerf360/">mip-NeRF360</a>.</strong></p></li>
<li><p><strong><em>Foward-facing</em>: <a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/14boI-o5hGO9srnWaaogTU5_ji7wkX2S7">LLFF</a>.</strong></p></li>
</ul>
<details>
<summary>
Directory structure for the datasets
</summary>
<p>
<span style="font-size: 18pt;"><strong>data</strong></span><br><span style="background-color: #bfedd2;">├── nerf_synthetic &nbsp; &nbsp; # Link: https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1</span><br><span style="background-color: #bfedd2;">│ &nbsp; └── [chair|drums|ficus|hotdog|lego|materials|mic|ship]</span><br><span style="background-color: #bfedd2;">│ &nbsp; &nbsp; &nbsp; ├── [train|val|test]</span><br><span style="background-color: #bfedd2;">│ &nbsp; &nbsp; &nbsp; │ &nbsp; └── r_*.png</span><br><span style="background-color: #bfedd2;">│ &nbsp; &nbsp; &nbsp; └── transforms_[train|val|test].json</span><br>│<br><span style="background-color: #fbeeb8;">├── Synthetic_NSVF &nbsp; &nbsp; # Link: https://dl.fbaipublicfiles.com/nsvf/dataset/Synthetic_NSVF.zip</span><br><span style="background-color: #fbeeb8;">│ &nbsp; └── [Bike|Lifestyle|Palace|Robot|Spaceship|Steamtrain|Toad|Wineholder]</span><br><span style="background-color: #fbeeb8;">│ &nbsp; &nbsp; &nbsp; ├── intrinsics.txt</span><br><span style="background-color: #fbeeb8;">│ &nbsp; &nbsp; &nbsp; ├── rgb</span><br><span style="background-color: #fbeeb8;">│ &nbsp; &nbsp; &nbsp; │ &nbsp; └── [0_train|1_val|2_test]_*.png</span><br><span style="background-color: #fbeeb8;">│ &nbsp; &nbsp; &nbsp; └── pose</span><br><span style="background-color: #fbeeb8;">│ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; └── [0_train|1_val|2_test]_*.txt</span><br>│<br><span style="background-color: #f8cac6;">├── BlendedMVS &nbsp; &nbsp; &nbsp; &nbsp; # Link: https://dl.fbaipublicfiles.com/nsvf/dataset/BlendedMVS.zip</span><br><span style="background-color: #f8cac6;">│ &nbsp; └── [Character|Fountain|Jade|Statues]</span><br><span style="background-color: #f8cac6;">│ &nbsp; &nbsp; &nbsp; ├── intrinsics.txt</span><br><span style="background-color: #f8cac6;">│ &nbsp; &nbsp; &nbsp; ├── rgb</span><br><span style="background-color: #f8cac6;">│ &nbsp; &nbsp; &nbsp; │ &nbsp; └── [0|1|2]_*.png</span><br><span style="background-color: #f8cac6;">│ &nbsp; &nbsp; &nbsp; └── pose</span><br><span style="background-color: #f8cac6;">│ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; └── [0|1|2]_*.txt</span><br>│<br><span style="background-color: #eccafa;">├── TanksAndTemple &nbsp; &nbsp; # Link: https://dl.fbaipublicfiles.com/nsvf/dataset/TanksAndTemple.zip</span><br><span style="background-color: #eccafa;">│ &nbsp; └── [Barn|Caterpillar|Family|Ignatius|Truck]</span><br><span style="background-color: #eccafa;">│ &nbsp; &nbsp; &nbsp; ├── intrinsics.txt</span><br><span style="background-color: #eccafa;">│ &nbsp; &nbsp; &nbsp; ├── rgb</span><br><span style="background-color: #eccafa;">│ &nbsp; &nbsp; &nbsp; │ &nbsp; └── [0|1|2]_*.png</span><br><span style="background-color: #eccafa;">│ &nbsp; &nbsp; &nbsp; └── pose</span><br><span style="background-color: #eccafa;">│ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; └── [0|1|2]_<em>.txt</em></span><em><br>│<br><span style="background-color: #c2e0f4;">├── deepvoxels &nbsp; &nbsp; &nbsp; &nbsp; # Link: https://drive.google.com/drive/folders/1ScsRlnzy9Bd_n-xw83SP-0t548v63mPH</span><br><span style="background-color: #c2e0f4;">│ &nbsp; └── [train|validation|test]</span><br><span style="background-color: #c2e0f4;">│ &nbsp; &nbsp; &nbsp; └── [armchair|cube|greek|vase]</span><br><span style="background-color: #c2e0f4;">│ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ├── intrinsics.txt</span><br><span style="background-color: #c2e0f4;">│ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ├── rgb/</span></em>.png<br><span style="background-color: #c2e0f4;">│ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; └── pose/<em>.txt</em></span><em><br>│<br><span style="background-color: #bfedd2;">├── nerf_llff_data &nbsp; &nbsp; # Link: https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1</span><br><span style="background-color: #bfedd2;">│ &nbsp; └── [fern|flower|fortress|horns|leaves|orchids|room|trex]</span><br>│<br><span style="background-color: #fbeeb8;">├── tanks_and_temples &nbsp;# Link: https://drive.google.com/file/d/11KRfN91W1AxAW6lOFs4EeYDbeoQZCi87/view?usp=sharing</span><br><span style="background-color: #fbeeb8;">│ &nbsp; └── [tat_intermediate_M60|tat_intermediate_Playground|tat_intermediate_Train|tat_training_Truck]</span><br><span style="background-color: #fbeeb8;">│ &nbsp; &nbsp; &nbsp; └── [train|test]</span><br><span style="background-color: #fbeeb8;">│ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ├── intrinsics/</span></em>txt<br><span style="background-color: #fbeeb8;">│ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ├── pose/<em>txt</em></span><em><br><span style="background-color: #fbeeb8;">│ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; └── rgb/</span></em>jpg<br>│<br><span style="background-color: #f8cac6;">├── lf_data &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Link: https://drive.google.com/file/d/1gsjDjkbTh4GAR9fFqlIDZ__qR9NYTURQ/view?usp=sharing</span><br><span style="background-color: #f8cac6;">│ &nbsp; └── [africa|basket|ship|statue|torch]</span><br><span style="background-color: #f8cac6;">│ &nbsp; &nbsp; &nbsp; └── [train|test]</span><br><span style="background-color: #f8cac6;">│ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ├── intrinsics/*txt</span><br><span style="background-color: #f8cac6;">│ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ├── pose/<em>txt</em></span><em><br><span style="background-color: #f8cac6;">│ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; └── rgb/</span></em>jpg<br>│<br><span style="background-color: #eccafa;">├── 360_v2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Link: https://jonbarron.info/mipnerf360/</span><br><span style="background-color: #eccafa;">│ &nbsp; └── [bicycle|bonsai|counter|garden|kitchen|room|stump]</span><br><span style="background-color: #eccafa;">│ &nbsp; &nbsp; &nbsp; ├── poses_bounds.npy</span><br><span style="background-color: #eccafa;">│ &nbsp; &nbsp; &nbsp; └── [images_2|images_4]</span><br>│<br><span style="background-color: #c2e0f4;">├── nerf_llff_data &nbsp; &nbsp; # Link: https://drive.google.com/drive/folders/14boI-o5hGO9srnWaaogTU5_ji7wkX2S7</span><br><span style="background-color: #c2e0f4;">│ &nbsp; └── [fern|flower|fortress|horns|leaves|orchids|room|trex]</span><br><span style="background-color: #c2e0f4;">│ &nbsp; &nbsp; &nbsp; ├── poses_bounds.npy</span><br><span style="background-color: #c2e0f4;">│ &nbsp; &nbsp; &nbsp; └── [images_2|images_4]</span><br>│<br><span style="background-color: #bfedd2;">└── co3d &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Link: https://github.com/facebookresearch/co3d</span><br><span style="background-color: #bfedd2;">&nbsp; &nbsp; └── [donut|teddybear|umbrella|...]</span><br><span style="background-color: #bfedd2;">&nbsp; &nbsp; &nbsp; &nbsp; ├── frame_annotations.jgz</span><br><span style="background-color: #bfedd2;">&nbsp; &nbsp; &nbsp; &nbsp; ├── set_lists.json</span><br><span style="background-color: #bfedd2;">&nbsp; &nbsp; &nbsp; &nbsp; └── [129_14950_29917|189_20376_35616|...]</span><br><span style="background-color: #bfedd2;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ├── images</span><br><span style="background-color: #bfedd2;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; │ &nbsp; └── frame<em>.jpg</em></span><em><br><span style="background-color: #bfedd2;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; └── masks</span><br><span style="background-color: #bfedd2;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; └── frame</span></em>.png
</p>
<p>
&nbsp;
</p>
</details></blockquote>
<blockquote>
<p><strong>DVGO v2</strong></p>
<p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.05085.pdf">2206.05085.pdf (arxiv.org)</a></p>
<p>论文代码：同DVGO</p>
<p>摘要：In this technical report, we improve the DVGO framework (called DVGOv2), which is based on Pytorch and uses the simplest dense grid representation. First, we reimplement part of the Pytorch operations with cuda, achieving 2–3× speedup. The cuda extension is automatically compiled just in time. <span style="background:#FFCC99;">Second, we extend DVGO to support Forward-facing and Unbounded Inward-facing capturing.(该改进之处正是DVGO 1.0 版本在论文末尾提出的不足之处)</span> Third, we improve the space time complexity of the distortion loss proposed by mip-NeRF 360 from O(<span class="math inline">\(N^2\)</span>) to O(<span class="math inline">\(N\)</span>). The distortion loss improves our quality and training speed. Our efficient implementation could allow more future works to benefit from the loss.</p>
</blockquote>
<h2 id="缺陷">缺陷</h2>
<ul>
<li>稠密重建：需要输入百张图片，需要的样本量大</li>
<li>几何不太好（见 voxurf 的对比）</li>
</ul>
<h1 id="算法框架">算法框架</h1>
<h2 id="dvgo-全貌">DVGO 全貌</h2>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240227190531174.png" alt="image-20240227190531174"><figcaption aria-hidden="true">image-20240227190531174</figcaption>
</figure>
<p>从上左图可以看到，DVGO 将整个场景表示为两个 voxel grid——密度体素网格和特征体素网格，并且渲染图像使用的依然是 NeRF 中的体渲染，对于从像素上发射的射线上的每一个点，首先在这两个 voxel grid 上 进行三线性插值，分别得到这个点的密度特征和颜色特征，再通过各自的解码过程最终得到该点的颜色和密度。<u>这样就把整个场景表示在 voxel grid 和对应的解码器里。</u></p>
<p>在训练的时候，DVGO 采用了 coarse to fine 的训练方式。在 coarse 阶段，使用先验信息和多视角图像训练两个粗粒度的voxel，然后使用其中的密度场确定场景中的空白区域（free space）。在 fine 阶段，利用 coarse 阶段确定的密度场可以得到更紧密的 bbox，可以将 grid 定义在这个 bbox 内，减少无关变量的训练。并且在体渲染的时候还可以通过粗的密度场提前得知射线上哪些点应当被跳过（空白点和被遮住的那些点）。</p>
<h2 id="类似-nerf-的体渲染">类似 NeRF 的体渲染</h2>
<p><img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/nerf9.png"></p>
<p>NeRF 将场景用多层感知机 MLP 表示，输入相机位置坐标 <span class="math inline">\(\boldsymbol{x}\)</span> 并输出密度 <span class="math inline">\(\sigma\)</span> 和中间特征 <span class="math inline">\(e\)</span> 。 然后再次用这个中间特征 <span class="math inline">\(e\)</span> 和视角 <span class="math inline">\(\boldsymbol{d}\)</span> 作为输入，推测出这个点的颜色 <span class="math inline">\(\boldsymbol{c}\)</span>​ ，下面将这两个过程分开写，其实就是 NeRF 中的网络： <span class="math display">\[
\begin{align}
\left( \sigma ,e \right) &amp;=MLP^{\left( pos \right)}\left( x \right) ,\tag{1.a}\\
c&amp;=MLP^{\left( rgb \right)}\left( e,d \right) , \tag{1.b}
\end{align}
\]</span> 然后通过体渲染得到像素点的值： <span class="math display">\[
\begin{align}
\hat{C}(\boldsymbol{r}) &amp; =\left(\sum_{i=1}^{K} T_{i} \alpha_{i} \boldsymbol{c}_{i}\right)+T_{K+1} \boldsymbol{c}_{\mathrm{bg}}, \tag{2.a}\\
\alpha_{i} &amp; =\operatorname{alpha}\left(\sigma_{i}, \delta_{i}\right)=1-\exp \left(-\sigma_{i} \delta_{i}\right),  \tag{2.b}\\
T_{i} &amp; =\prod_{j=1}^{i-1}\left(1-\alpha_{j}\right),\tag{2.c}
\end{align}
\]</span> 其中的符号和 NeRF 中的一致，就不详细介绍。不同的是这里加上了背景的考虑（加上背景的原因可参考<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=Mzg4ODA3MDkyMA==&amp;mid=2247485148&amp;idx=1&amp;sn=4e6acc1b358e624ac2301729d35cc89d&amp;chksm=cf81f0bbf8f679ad3f990f91d5c84fcc67cc6948c3cbc0c34c59874f7795e9263f2382979414&amp;token=400584181&amp;lang=zh_CN#rd">NeRF入门之体渲染 (Volume Rendering) (qq.com)</a>）：这里 <span class="math inline">\(\boldsymbol{c}_{bg}\)</span> 是一个实现定义好的背景像素值，透射比 <span class="math inline">\(T_{K+1}\)</span> 表示的是背景点被击中的概率。</p>
<blockquote>
<p>[!NOTE]</p>
<p>其实一条射线上只会采样 K 个点，这里假设最后还有一个虚拟的背景点，挂在每条射线的最后。当这条射线上所有采样点被击中的概率都很低时，就会呈现出背景的颜色。</p>
</blockquote>
<h2 id="求-alpha-后激活密度体素网格">求 <span class="math inline">\(\alpha\)</span> : 后激活密度体素网格</h2>
<blockquote>
<p>[!TIP]</p>
<p><strong>体素网格如何表示颜色/密度/特征？</strong></p>
<p>文中用以下式子概括： <span class="math display">\[
interp\left( x,\boldsymbol{V} \right) :\left( \mathbb{R}^3,\mathbb{R}^{C\times N_x\times N_y\times N_z} \right) \rightarrow \mathbb{R}^C,
\]</span> 其中 <span class="math inline">\(x\)</span> 为查询的三维点， <span class="math inline">\(\boldsymbol{V}\)</span> 表示 <span class="math inline">\(x, y, z\)</span> 方向上体素数量分别为 <span class="math inline">\(N_x, N_y, N_z\)</span> 的体素网格， <span class="math inline">\(C\)</span> 表示颜色/密度/特征的维度数。在本文中，运用的插值方法是三线性插值（trilinear interpolation）。</p>
</blockquote>
<p>前面介绍 DVGO 将场景表示为两种 voxel grid 和解码器的组合，这里想要探讨的问题是<u>如何从 density voxel grid 中得到一个点的密度信息</u>。</p>
<p>首先，density 体素网格上面定义了场景的密度信息，但不需要直接是真实的密度值。具体来说每个 voxel 上的值是一个待训练的参数实数（该实数的正负性不能保证），可理解为密度特征。<span style="background:#daf5e9;">由于密度是非负的，我们需要先通过激活函数 relu 或 softplus 将这个实数映射为一个大于零的数才能将输出与真实密度值建立关系。然后用每个点的密度值计算每个点的 <span class="math inline">\(\alpha\)</span> （见公式2.b）</span>。</p>
<p>因此我们想要进行后续体渲染的话，需要对一个点做下面三件事：<strong>插值、激活和求 <span class="math inline">\(\alpha\)</span>​</strong> 。</p>
<p>关于<strong>插值</strong>比较好理解，因为网格是稀疏的，而我们做体渲染的时候需要的是空间中任意点的信息，所以最简单直接的方式就是用空间点附近的 voxel 进行插值。</p>
<p>关于<strong>激活</strong>，这里采用的是 Mip-NeRF<sup>[3]</sup> 中的 shifted softplus，其中 <span class="math inline">\(\ddot{\sigma}\)</span> 就是 density grid 上的原始值，取值范围是 R，经过激活后得到非负的密度值 <span class="math inline">\(\sigma\)</span>。 <span class="math inline">\(b\)</span> 是一个超参数（hyperparameter） : <span class="math display">\[
\sigma=\operatorname{softplus}(\ddot{\sigma})=\log (1+\exp (\ddot{\sigma}+b)),
\]</span></p>
<blockquote>
<p>这里不用 relu 激活函数的原因是：Using softplus instead of ReLU is crucial to optimize voxel density directly, as it is irreparable when a voxel is falsely set to a negative value with ReLU as the density activation. Conversely, softplus allows us to explore density very close to 0.</p>
<p>意思就是，如果一个密度为正的地方的 <span class="math inline">\(\ddot{ \sigma}\)</span>​ 如果是负值会导致其经过 relu 之后是0，没有梯度能更新这个值让它能移动到正的部分。</p>
<p><strong>softplus 函数可以视为 relu 函数的平滑，随着 <span class="math inline">\(\ddot{ \sigma}\)</span> 的变小，函数值会无限接近于0但不会真正变为0</strong>。其函数表达式与图像如下： <span class="math display">\[
softplus\left( x \right) =\log \left( 1+e^x \right)
\]</span> <img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240101125119035.png" alt="image-20240101125119035"></p>
</blockquote>
<p>关于<strong>求 <span class="math inline">\(\alpha\)</span></strong> ，则<span style="background:#f9eda6;">是 NeRF 中计算一个点的瞬时击中率的操作，即式（2.b）</span>。</p>
<p>除了激活 softplus 操作必须在求 <span class="math inline">\(\alpha\)</span> 前外，显然这三件事是可以交换的。在给定查询三维点 <span class="math inline">\(x\)</span> 和密度体素网格中的密度信息 <span class="math inline">\(\boldsymbol{V}^{(density)}\)</span> 后，我们考虑了三种不同的顺序来得到 <span class="math inline">\(\alpha \in [0,1]\)</span> 值：</p>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301192035676.png" alt="image-20240301192035676"><figcaption aria-hidden="true">image-20240301192035676</figcaption>
</figure>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301201349921.png" alt="image-20240301193624141"><figcaption aria-hidden="true">image-20240301193624141</figcaption>
</figure>
<p>一般情况下，我们想到的是第三种情况：先插值、再激活，最后求 <span class="math inline">\(\alpha\)</span> 。 论文中根据插入三线性插值 interp 的顺序将其扩展为三种情况并展开对比发现，不同的顺序得到的结果还有所差异。<span style="background:#daf5e9;"><strong>所谓的“后激活”（Post-activated）指的是激活为 <span class="math inline">\(\alpha\)</span> 在插值之后(i.e. to activate density into alpha after interpolation)</strong></span>。作者证明，后激活能够用更少的网格单元生成尖锐的表面(决策边界)。作者在二维图像上证明了后激活的优越性：</p>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301193624141.png" alt="image-20240301201349921"><figcaption aria-hidden="true">image-20240301201349921</figcaption>
</figure>
<p>可以看到 ，后激活能够表示出更 sharp 的边缘，对应在 NeRF 中就是更 sharp 的几何。在二值图像上的实验也证明了同样的观点：</p>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301211423996.png" alt="image-20240301193832857"><figcaption aria-hidden="true">image-20240301193832857</figcaption>
</figure>
<blockquote>
<p>论文的补充材料中给出了后激活策略可以在单个网格单元中产生尖锐的线性表面（决策边界）的证明。论文先证明了一维网格和二维网格中的情况，然后利用这两种情况推导出三维网格中的情况。</p>
</blockquote>
<h2 id="由粗到细的训练过程">由粗到细的训练过程</h2>
<p>和 NeRF 的重要性采样类似，DVGO 也采用了两阶段的采样过程。</p>
<h3 id="coarse-to-fine-的好处">coarse to fine 的好处</h3>
<p>作者之所以选择 coarse to fine 的训练方式是因为：通常情况下，<u>场景由空闲空间(即未占用空间)主导</u>，即场景中的空白区占大部分，有物体的区域就一小部分。基于此，我们的目标是高效地找到感兴趣的粗三维区域，然后重构需要更多计算资源的精细细节和视图依赖效果。这样可以大大减少后期精细阶段每条射线上查询的点的数量。</p>
<p>在 NeRF 中也有类似的操作，主要目的还是增强采样点的针对性，尽量将采样点放在更有用的位置，不在无用位置进行采样。本文进行 coarse to fine 后，就可以快速有效地筛选出有物体存在的网格，进而在 fine 粒度的训练中跳过大量无关“空白点”，只关注有物体占据的网格，进一步加快了训练速度。</p>
<h3 id="体素分配方法">体素分配方法</h3>
<p>不论是 coarse 还是 fine 阶段，我们都需要知道应该在场景的什么位置创建 grid，才能包含所有要采样的点。作者采用的方式是找一个 BBox (bounding box) 将所有的摄像机截锥紧紧地包起来，像下面这样：</p>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301201828110.png" alt="image-20240301201828110"><figcaption aria-hidden="true">image-20240301201828110</figcaption>
</figure>
<p>寻找 BBox 的代码如下(.ipynb_checkpoints/run-checkpoint.py)：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_compute_bbox_by_cam_frustrm_bounded</span>(<span class="params">cfg, HW, Ks, poses, i_train, near, far</span>):</span><br><span class="line">    xyz_min = torch.Tensor([np.inf, np.inf, np.inf])</span><br><span class="line">    xyz_max = -xyz_min</span><br><span class="line">    <span class="keyword">for</span> (H, W), K, c2w <span class="keyword">in</span> <span class="built_in">zip</span>(HW[i_train], Ks[i_train], poses[i_train]):</span><br><span class="line">        rays_o, rays_d, viewdirs = dvgo.get_rays_of_a_view(</span><br><span class="line">                H=H, W=W, K=K, c2w=c2w,</span><br><span class="line">                ndc=cfg.data.ndc, inverse_y=cfg.data.inverse_y,</span><br><span class="line">                flip_x=cfg.data.flip_x, flip_y=cfg.data.flip_y)</span><br><span class="line">        <span class="keyword">if</span> cfg.data.ndc:</span><br><span class="line">            pts_nf = torch.stack([rays_o+rays_d*near, rays_o+rays_d*far])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pts_nf = torch.stack([rays_o+viewdirs*near, rays_o+viewdirs*far])</span><br><span class="line">        xyz_min = torch.minimum(xyz_min, pts_nf.amin((<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>)))</span><br><span class="line">        xyz_max = torch.maximum(xyz_max, pts_nf.amax((<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>)))</span><br><span class="line">    <span class="keyword">return</span> xyz_min, xyz_max</span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p>[!TIP]</p>
<p>这里的代码逻辑是，首先拿到训练集中所有的相机位置和视角，然后在生成射线，根据 near 和 far 确定射线上的采样点，把所有这些采样点放一块，求所有这些点的 x,y,z 的最值。通过这样可以保证 grid 能够包含所有要采样的点，并且不会包含过多的无关位置。</p>
</blockquote>
<p>令BBox的长宽高分别为 <span class="math inline">\(L_{x}^{\left( c \right)},L_{y}^{\left( c \right)},L_{z}^{\left( c \right)}\)</span> 。设粗粒度训练阶段的期望体素总数为超参数 <span class="math inline">\(M^{(c)}\)</span> ，则可计算每个体素的大小 size 为 <span class="math inline">\(s^{\left( c \right)}=\sqrt[3]{L_{x}^{\left( c \right)}\cdot L_{y}^{\left( c \right)}\cdot L_{z}^{\left( c \right)}/M^{\left( c \right)}}\)</span> ，进而可算得 BBox 长宽高方向上分别有 $N_{x}^{( c )},N_{y}^{( c )},N_{z}^{( c )}=L_{x}^{( c )}/s^{( c )} ,L_{y}^{( c )}/s^{( c )} ,L_{z}^{( c )}/s^{( c )} $ 个体素。</p>
<h3 id="粗粒度训练">粗粒度训练</h3>
<h4 id="几何与颜色表示">几何与颜色表示</h4>
<ul>
<li><strong>场景几何表示</strong>：通过粗粒度的密度体素网格 <span class="math inline">\(\boldsymbol{ V}^{(density)(c)}\)</span> 和后激活策略建模。这里 <span class="math inline">\(\boldsymbol{V}^{\left( density \right) \left( c \right)}\in \mathbb{R}^{1\times N_{x}^{\left( c \right)}\times N_{y}^{\left( c \right)}\times N_{z}^{\left( c \right)}}\)</span> 。</li>
<li><strong>颜色表示</strong>：通过在粗粒度阶段的颜色体素网格 <span class="math inline">\(\boldsymbol{ V}^{(rgb)(c)}\)</span> ，来建模视觉无关(view-invariant)的颜色 emissions 。这里 <span class="math inline">\(\boldsymbol{V}^{\left( rgb\right) \left( c \right)}\in \mathbb{R}^{3\times N_{x}^{\left( c \right)}\times N_{y}^{\left( c \right)}\times N_{z}^{\left( c \right)}}\)</span> 。</li>
</ul>
<p>任意三维点的密度和颜色信息可通过插值公式查询： <span class="math display">\[
\begin{array}{l}
\ddot{\sigma}^{\left( c \right)}=interp\left( x,\boldsymbol{V}^{\left( density \right) \left( c \right)} \right) ,\\
c^{\left( c \right)}=interp\left( x,\boldsymbol{V}^{\left( rgb \right) \left( c \right)} \right) ,
\end{array}
\]</span> 这里 <span class="math inline">\(\ddot{\sigma}^{\left( c \right)}\)</span> 表示原始的可正可负的体密度信息。由于颜色和视角无关，所以不需要输入视角，颜色和密度一样直接插值就能得到。而且需要注意的是：<span style="background:#fbd4d0;"><strong>在 coarse 阶段没有使用任何 MLP</strong></span>。</p>
<p>粗粒度训练阶段的<strong>点采样公式</strong>如下： <span class="math display">\[
\begin{array}{l}
\boldsymbol{x}_{0}=\boldsymbol{o}+t^{(\text {near) }} \boldsymbol{d} ,\\
\boldsymbol{x}_{i}=\boldsymbol{x}_{0}+i \cdot \delta^{(\mathrm{c})} \cdot \frac{\boldsymbol{d}}{\|\boldsymbol{d}\|^{2}},
\end{array}
\]</span> 显然， <span class="math inline">\(\boldsymbol{o}\)</span> 是相机中心， <span class="math inline">\(\boldsymbol{d}\)</span> 是射线投射方向。 $ t ^{ (near) } $ 是相机近平面 camera near bound ，<span class="math inline">\(\delta^{(\mathrm{c})}\)</span> 则是可根据体素大小 <span class="math inline">\(s^{(c)}\)</span> 自适应选择的超参数步长。查询索引 <span class="math inline">\(i\)</span> 取值从 1 到 $t^{( far )} <sup>2/</sup>{( c )} $ 。</p>
<h4 id="低密度初始化先验">低密度初始化先验</h4>
<blockquote>
<p>[!IMPORTANT]</p>
<p>作者发现，在训练开始时，远离相机的点的重要性由于累积的透过率项而降低。因此，粗密度体素网格可能会意外地被困在一个次优的“cloudy”几何，它在相机附近的平面密度更高。</p>
<p>导致这个问题的原因如果初始化做的不好 ，那么后面的采样点都被前面的遮住了，后面的采样点的被击中的概率 <span class="math inline">\(T_i\)</span> 就很低，那么这些位置就不会被训练到，网络就一直在纠结是不是前面那些点的颜色或者密度调的不够好，殊不知是重心在射线的后面。</p>
<p>作者解决这个问题的方法简单而有效，初始化密度网格的时候小心点，尽量让一开始的时候，射线上每个点都是可见的，也就是他们的透射比 <span class="math inline">\(T_i\)</span> 都比较高。</p>
</blockquote>
<p>由式(2.c)，</p>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301214544618.png" alt="image-20240301211423996"><figcaption aria-hidden="true">image-20240301211423996</figcaption>
</figure>
<blockquote>
<p>中间的推导详见论文的补充材料。</p>
</blockquote>
<p>所以我们只需要让 <span class="math inline">\(b=log((1-\alpha^{(init)})^{-1/s}-1)\)</span> 即可。我们可以在代码中给超参数 <span class="math inline">\(\alpha^{(init)}\)</span> 一个很小的初值就可以了，代码中给的值一般是 <span class="math inline">\(10^{-6}\)</span> （coarse）和 <span class="math inline">\(10^{-6}\)</span> （fine）。这一段对应的代码块为：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">self.alpha_init = alpha_init</span><br><span class="line">self.register_buffer(<span class="string">'act_shift'</span>, torch.FloatTensor([np.log(<span class="number">1</span>/(<span class="number">1</span>-alpha_init) - <span class="number">1</span>)]))</span><br></pre></td></tr></tbody></table></figure>
<h4 id="基于视角的学习率先验">基于视角的学习率先验</h4>
<blockquote>
<p>[!IMPORTANT]</p>
<p>意思是 grid 中的不同 voxel 在训练集中的可见性是不同的，有一些 voxel 在大部分训练集视角下都可见，而有一些只在少部分视角下可见。比较有代表性的就是包含表面的那些 voxel 和表面内部的那些 voxel。</p>
<p>作者认为，可见度不同的 voxel 上的特征应该具有不同的学习率，可见性好的学习率更大，否则设置更小的学习率。</p>
<p>具体来说，用 <span class="math inline">\(n_j\)</span>表示第 <span class="math inline">\(j\)</span> 个 voxel 可以被看见的视角个数，用 <span class="math inline">\(n_{max}\)</span> 表示所有 <span class="math inline">\(n_j\)</span> 的最大值， <span class="math inline">\(n_j/n_{max}\)</span> 为每个 voxel 的学习率的缩放因子。</p>
</blockquote>
<p>对应的代码为：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">voxel_count_views</span>(<span class="params">self, rays_o_tr, rays_d_tr, imsz, near, far, stepsize, downrate=<span class="number">1</span>, irregular_shape=<span class="literal">False</span></span>):</span><br><span class="line">       <span class="built_in">print</span>(<span class="string">'dvgo: voxel_count_views start'</span>)</span><br><span class="line">       far = <span class="number">1e9</span>  <span class="comment"># the given far can be too small while rays stop when hitting scene bbox</span></span><br><span class="line">       eps_time = time.time()</span><br><span class="line">       N_samples = <span class="built_in">int</span>(np.linalg.norm(np.array(self.world_size.cpu())+<span class="number">1</span>) / stepsize) + <span class="number">1</span></span><br><span class="line">       rng = torch.arange(N_samples)[<span class="literal">None</span>].<span class="built_in">float</span>()</span><br><span class="line">       count = torch.zeros_like(self.density.get_dense_grid())</span><br><span class="line">       device = rng.device</span><br><span class="line">       <span class="keyword">for</span> rays_o_, rays_d_ <span class="keyword">in</span> <span class="built_in">zip</span>(rays_o_tr.split(imsz), rays_d_tr.split(imsz)):</span><br><span class="line">           ones = grid.DenseGrid(<span class="number">1</span>, self.world_size, self.xyz_min, self.xyz_max)</span><br><span class="line">           <span class="keyword">if</span> irregular_shape:</span><br><span class="line">               rays_o_ = rays_o_.split(<span class="number">10000</span>)</span><br><span class="line">               rays_d_ = rays_d_.split(<span class="number">10000</span>)</span><br><span class="line">           <span class="keyword">else</span>:</span><br><span class="line">               rays_o_ = rays_o_[::downrate, ::downrate].to(device).flatten(<span class="number">0</span>,-<span class="number">2</span>).split(<span class="number">10000</span>)</span><br><span class="line">               rays_d_ = rays_d_[::downrate, ::downrate].to(device).flatten(<span class="number">0</span>,-<span class="number">2</span>).split(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">           <span class="keyword">for</span> rays_o, rays_d <span class="keyword">in</span> <span class="built_in">zip</span>(rays_o_, rays_d_):</span><br><span class="line">               vec = torch.where(rays_d==<span class="number">0</span>, torch.full_like(rays_d, <span class="number">1e-6</span>), rays_d)</span><br><span class="line">               rate_a = (self.xyz_max - rays_o) / vec</span><br><span class="line">               rate_b = (self.xyz_min - rays_o) / vec</span><br><span class="line">               t_min = torch.minimum(rate_a, rate_b).amax(-<span class="number">1</span>).clamp(<span class="built_in">min</span>=near, <span class="built_in">max</span>=far)</span><br><span class="line">               t_max = torch.maximum(rate_a, rate_b).amin(-<span class="number">1</span>).clamp(<span class="built_in">min</span>=near, <span class="built_in">max</span>=far)</span><br><span class="line">               step = stepsize * self.voxel_size * rng</span><br><span class="line">               interpx = (t_min[...,<span class="literal">None</span>] + step/rays_d.norm(dim=-<span class="number">1</span>,keepdim=<span class="literal">True</span>))</span><br><span class="line">               rays_pts = rays_o[...,<span class="literal">None</span>,:] + rays_d[...,<span class="literal">None</span>,:] * interpx[...,<span class="literal">None</span>]</span><br><span class="line">               ones(rays_pts).<span class="built_in">sum</span>().backward()</span><br><span class="line">           <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">               count += (ones.grid.grad &gt; <span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>这段代码也十分巧妙。作者首先构造了一个 <em>grid</em> 对象 <em>ones</em>，它是作者创建的一个类，这个类的作用就是构造可以训练的 voxel grid，并且可以完成插值等操作。然后在训练集视角上获取射线并在射线上采样点 <em>rays_pts</em>。对 <em>rays_pts</em> 中的每个点，找到它在 <em>ones</em> 中的插值结果，对应的语句是 <em>ones(rays_pts)。</em>然后将这些点的插值结果加起来：<em>ones(rays_pts).sum()。</em>可想而知，到目前为止，只有 <em>rays_pts</em> 中的那些点所在的 voxel 参与了加法，这个时候我们对加法结果进行反向传播：<em>ones(rays_pts).sum().backward()</em>，参与插值的那些 voxel 的梯度会是正的，<strong>我们找到这些梯度为正的 voxel：<em>ones.grid.grad &gt; 1</em>，就可以确定哪些 voxel 是可见的</strong>。count 是和 grid 同纬度的 tensor，<em>ones.grid.grad &gt; 1</em> 得到的是一个 bool tensor，其中为 True 的那些就是可见 voxel，两个加起来就能实现统计的目的。</p>
<h3 id="细粒度训练">细粒度训练</h3>
<h4 id="几何与颜色表示-1">几何与颜色表示</h4>
<p>与 coarse 阶段不同的是，fine 阶段采用了更密集的网格，并且使用显隐混合的表达方式（explicit-implicit hybrid representation），其实就是用一个 MLP 当解码器接在颜色网格后面，增强网络对颜色的表达能力。</p>
<p>fine 阶段查询一个空间点的密度和颜色的方式为： <span class="math display">\[
\begin{array}{l}
    \ddot{\sigma}^{\left( f \right)}=interp\left( x,\boldsymbol{V}^{\left( density \right) \left( f \right)} \right) ,\\
    c^{\left( f \right)}=MLP_{\varTheta}^{\left( rgb \right)}\left( interp\left( x,\boldsymbol{V}^{\left( feat \right) \left( f \right)} \right) ,\boldsymbol{x,d} \right) ,\\
\end{array}
\]</span> 这对应于框架图的左半部分。</p>
<p>在 fine 阶段，作者采用了一系列的加速手段：</p>
<ol type="1">
<li><strong>Known free space and unknown space</strong> 这里说明了怎么定义 free space 和 unknown space，方式就是手动设置一个阈值，之下的就是 free space。</li>
<li><strong>Fine voxels allocation</strong> coarse 阶段确定 grid 范围的方式是找到一个包含所有采样点的 BBox。但是有些地方是空的，所以我们可以更具 coarse 阶段得到的 density grid 确定 fine 阶段 grid 所在的区域，将有限的 voxel 尽可能地安排在有密度的地方。</li>
<li><strong>Progressive scaling</strong> 受到 NSVF 的启发，作者也是逐渐减小 voxel 的大小。随着训练不断的倍增 grid 的密度，直到最后达到指定数量的 voxel 停止倍增。</li>
<li><strong>Fine-stage points sampling</strong> 在 NeRF 中，一个场景的 near-bound 和 far-bound 是定好的一个大致的范围，但是由于我们这里明确知道场景就在空间的一个 grid 内，所以更好的方式是将 near-bound 和 far-bound 设置为每条射线进入 grid 和离开 grid 的点。（在代码中没找到对应部分 ）</li>
<li><strong>Free space skipping</strong> 意思是通过 coarse 阶段已经知道场景中哪些位置是空的了，那就不需要在这些位置采样点浪费时间了。</li>
<li><strong>Training objective for fine representation</strong> 使用与粗阶段相同的训练损失，但使用更小的权值作为正则化损失，因为作者发现，根据经验，它会带来略好的质量。</li>
</ol>
<h4 id="损失函数">损失函数</h4>
<p>Loss Function包含三个部分：来自NeRF的 MSE loss 以及两个从 DVGO 中借鉴的 loss。 <span class="math display">\[
\begin{align}
    \mathcal{L}_{photo}&amp;=\frac{1}{|\mathcal{R}|}\sum_{r\in \mathcal{R}}{|}\hat{C}\left( \boldsymbol{r} \right) -C\left( \boldsymbol{r} \right) |_{2}^{2}\\
    \mathcal{L}_{all\_pts}&amp;=\sum_{i=1}^N{T}_i\left( 1-\exp \left( -\sigma _i\delta _i \right) \right) |\mathbf{c}_i-C\left( \mathbf{r} \right) |_{2}^{2}\mid\\
    \mathcal{L}_{bg}&amp;=-T_{N+1}\log \left( T_{N+1} \right) -\left( 1-T_{N+1} \right) \log \left( 1-T_{N+1} \right)\\\\
    L&amp;=L_{photo}+\alpha L_{all\_pts}+\beta L_{bg}\\
\end{align}
\]</span> MSE loss 的作用是使得建模出来的辐射场在训练集视角上渲染要和原图尽可能一致。权重为 1。</p>
<p><span class="math inline">\(L_{all\_pts}\)</span> 的意思是从一个像素中发射的射线上的所有像素点都应该和该像素尽可能接近，这个约束能够帮助网络一开始不要陷入 local minima，使得优化过程更加稳定。但是这个约束并不是符合实际的，因为一条射线上的点的颜色常常是不同的，所以作者给了一个很小的系数 <span class="math inline">\(\alpha=0.1\)</span>（coarse）和 <span class="math inline">\(\alpha=0.01\)</span>（coarse）。</p>
<p><span class="math inline">\(L_{bg}\)</span> 中 <span class="math inline">\(T_{N+1}\)</span> 表示射线击中背景的概率，也可以理解为射线一路上什么都没有击中，一路通到背景上。 <span class="math inline">\(L_{bg}\)</span> 的函数图像如下，它的作用是让一个像素点要么为背景颜色，要么没有背景颜色，尽量避免是二者的混合。作者设置其系数为 $  <span class="math inline">\(（coarse）和\)</span>$（fine）。</p>
<figure>
<img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/image-20240301193832857.png" alt="image-20240301214544618"><figcaption aria-hidden="true">image-20240301214544618</figcaption>
</figure>
<h1 id="实验">实验</h1>
<h2 id="实现细节">实现细节</h2>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">coarse</th>
<th style="text-align: center;">fine</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">voxel 数量</td>
<td style="text-align: center;"><span class="math inline">\(100^3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(160^3\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\alpha\)</span> 初始值</td>
<td style="text-align: center;"><span class="math inline">\(10^{-6}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(10^{-2}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">采样间距/步长</td>
<td style="text-align: center;">coarse_voxel_size(<span class="math inline">\(\delta^{(c)}\)</span>)/2</td>
<td style="text-align: center;">fine_voxel_size(<span class="math inline">\(\delta^{(f)}\)</span>)/2</td>
</tr>
<tr class="even">
<td style="text-align: center;">MLP</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">C -&gt; 128 -&gt; 128 -&gt; 3</td>
</tr>
<tr class="odd">
<td style="text-align: center;">optimizer</td>
<td style="text-align: center;">Adam</td>
<td style="text-align: center;">Adam</td>
</tr>
<tr class="even">
<td style="text-align: center;">lr for grid</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">lr for MLP</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">0.001</td>
</tr>
</tbody>
</table>
<h2 id="实验结果">实验结果</h2>
<h2 id="消融实验">消融实验</h2>
<h1 id="代码">代码</h1>
<details>
<summary>
问题箱
</summary>
1.DVGO是否可以与diffusion结合？ 2.DVGO是否可以与注意力机制结合？ 比如从多视角图像中提取特征时
</details>
<h2 id="代码结构">代码结构</h2>
<details>
<summary>
.ipynb_checkpoints /原始文件看不见这个，从云平台下载后显示的隐藏文件
</summary>
run-checkpoint.py /内容与主函数运行文件 run.py 相同
</details>
<details>
<summary>
configs /各种数据集的配置文件
</summary>
<pre><code>blendedmvs 文件夹 &lt;/br&gt;</code></pre>
co3d 文件夹<br> ... ...<br> default.py
</details>
<details>
<summary>
data /存放使用的数据
</summary>
<p>
<span style="font-family: arial, helvetica, sans-serif; font-size: 14pt;">.ipynb_checkpoints 文件夹&nbsp; &nbsp; /空的，也是从云平台下载后显示的隐藏文件夹</span>
</p>
<p>
<span style="font-family: arial, helvetica, sans-serif; font-size: 14pt;">nerf_synthetic</span>
</p>
<p style="padding-left: 30px;">
<span style="font-family: arial, helvetica, sans-serif; font-size: 12pt;">lego</span>
</p>
<p style="padding-left: 60px;">
<span style="font-family: arial, helvetica, sans-serif; color: #e67e23;">pre_train&nbsp; &nbsp; /预训练用 200 张图片</span>
</p>
<p style="padding-left: 60px;">
<span style="font-family: arial, helvetica, sans-serif; color: #e67e23;">test&nbsp; &nbsp; /测试用 200 张图片，每张图片有原图、depth 深度图和 normal 法线图</span>
</p>
<p style="padding-left: 60px;">
<span style="font-family: arial, helvetica, sans-serif; color: #e67e23;">train&nbsp; &nbsp; /训练用 100 张图片</span>
</p>
<p style="padding-left: 60px;">
<span style="font-family: arial, helvetica, sans-serif; color: #e67e23;">val&nbsp; &nbsp; /验证用 100 张图片，和训练用的 100 张不一样哟</span>
</p>
<p style="padding-left: 60px;">
<span style="font-family: arial, helvetica, sans-serif; color: #169179;">transforms_pre_train.json</span>
</p>
<p style="padding-left: 60px;">
<span style="font-family: arial, helvetica, sans-serif; color: #169179;">transforms_test.json</span>
</p>
<p style="padding-left: 60px;">
<span style="font-family: arial, helvetica, sans-serif; color: #169179;">transforms_train.json</span>
</p>
<p style="padding-left: 60px;">
<span style="font-family: arial, helvetica, sans-serif; color: #169179;">transforms_val.json</span>
</p>
</details>
<details>
<summary>
figs /存放两张示例图片
</summary>
<pre><code>debug_cam_and_bbox.png &lt;/br&gt;</code></pre>
debug_coarse_volume.png
</details>
<details>
<summary>
lib /C文件、dvgo模型的py文件和加载数据集的文件等
</summary>
<p>
<span style="font-size: 12pt;">cuda&nbsp; &nbsp; /存放用来加速的 cpp 文件、cu 文件</span>
</p>
<p style="padding-left: 40px;">
<span style="color: #169179;">adam_upd.cpp</span>
</p>
<p style="padding-left: 40px;">
<span style="color: #169179;">adam_upd_kernel.cu</span>
</p>
<p style="padding-left: 40px;">
<span style="color: #e67e23;">render_utils.cpp</span>
</p>
<p style="padding-left: 40px;">
<span style="color: #e67e23;">render_utils_kernel.cu</span>
</p>
<p style="padding-left: 40px;">
<span style="color: #169179;">total_variation.cpp</span>
</p>
<p style="padding-left: 40px;">
<span style="color: #169179;">total_variation_kernel.cu</span>
</p>
<p style="padding-left: 40px;">
<span style="color: #e67e23;">ub360_utils.cpp</span>
</p>
<p style="padding-left: 40px;">
<span style="color: #e67e23;">ub360_utils_kernels.cu</span>
</p>
<p>
<span style="color: #e03e2d; font-size: 12pt;">dbvgo.py&nbsp; &nbsp; /DirectBiVoxGO</span>
</p>
<p>
<span style="color: #e03e2d; font-size: 12pt;">dcvgo.py&nbsp; &nbsp; /DirectContractedVoxGO：elif cfg.data.unbounded_inward</span>
</p>
<p>
<span style="color: #e03e2d; font-size: 12pt;">dmpigo.py&nbsp; &nbsp; /DirectMPIGO：if cfg.data.ndc</span>
</p>
<p>
<span style="color: #e03e2d; font-size: 12pt;">dvgo.py&nbsp; &nbsp; /DirectVoxGO：else</span>
</p>
<p>
<span style="color: #e03e2d; font-size: 12pt;">grid.py&nbsp; &nbsp; /DenseGrid 和 TensoRFGrid 两种模式</span>
</p>
<p>
<span style="color: #169179; font-size: 12pt;">load_blendedmvs.py</span>
</p>
<p>
<span style="color: #169179; font-size: 12pt;">load_blender.py</span>
</p>
<p>
<span style="color: #169179; font-size: 12pt;">load_co3d.py</span>
</p>
<p>
<span style="color: #169179; font-size: 12pt;">... ...</span>
</p>
<p>
<span style="color: #169179; font-size: 12pt;">load_tankstemple.py</span>
</p>
<p>
<span style="color: #e67e23; font-size: 12pt;">masked_adam.py&nbsp; &nbsp; /扩展 Adam 优化器：1.使其支持 per-voxel 学习率；2.masked update (ignore zero grad) which speeduping training</span>
</p>
<p>
<span style="color: #3598db; font-size: 12pt;">utils.py&nbsp; &nbsp; /Misc&nbsp; &nbsp; |checkpoint utils&nbsp; &nbsp; |Evaluation metrics (ssim,lpips)&nbsp; &nbsp;&nbsp;</span>
</p>
<p style="padding-left: 40px;">
&nbsp;
</p>
</details>
<details>
<summary>
logs /存放输出的结果数据
</summary>
<p>
<strong>nerf_synthetic</strong>
</p>
<p style="padding-left: 40px;">
<strong>dvgo_lego</strong>
</p>
<p style="padding-left: 80px;">
<span style="background-color: #fbeeb8;">render_test_fine_last</span>
</p>
<p style="padding-left: 120px;">
video.depth.mp4
</p>
<p style="padding-left: 120px;">
video.rgb.mp4
</p>
<p style="padding-left: 80px;">
<span style="background-color: #fbeeb8;">args.txt</span>
</p>
<p style="padding-left: 80px;">
<span style="background-color: #fbeeb8;">coarse_last.tar</span>
</p>
<p style="padding-left: 80px;">
<span style="background-color: #fbeeb8;">fine_last.tar</span>
</p>
<p style="padding-left: 80px;">
<span style="background-color: #fbeeb8;">config.py</span>
</p>
</details>
<details>
<summary>
tools /神奇工具箱
</summary>
<p>
<span style="color: #e03e2d; font-size: 14pt;">colmap_utils</span>
</p>
<p style="padding-left: 40px;">
<span style="color: #3598db; font-size: 12pt;">colmap_read_model.py</span>
</p>
<p style="padding-left: 40px;">
<span style="color: #3598db; font-size: 12pt;">colmap_wrapper.py</span>
</p>
<p style="padding-left: 40px;">
<span style="color: #3598db; font-size: 12pt;">pose_utils.py</span>
</p>
<p>
<span style="color: #e03e2d; font-size: 14pt;">imgs2poses.py</span>
</p>
<p>
<span style="color: #e03e2d; font-size: 14pt;">vis_train.py</span>
</p>
<p>
<span style="color: #e03e2d; font-size: 14pt;">vis_volume.py&nbsp;&nbsp;</span>
</p>
<p style="padding-left: 40px;">
&nbsp;
</p>
</details>
<p>.gitignore /告诉Git忽略哪些文件<br> IMPROVING_LOG.md /数据集在不同显卡环境下的评价指标<br> LICNESE /许可类型<br> README.md /说明文件<br> requirements.txt /需要的python依赖包列表<br> run.py /主函数运行文件<br></p>
<h2 id="主程序-run.py">主程序 run.py</h2>
<ul>
<li><p><span style="background:#daf5e9;">def config_parser</span> /定义命令行参数</p></li>
<li><p><span style="background:#f9eda6;">def render_viewpoints(no grad)</span> /从给定视图渲染图像，如果真实图给了还会计算测试的评价指标</p></li>
<li><p><span style="background:#daf5e9;">def seed_everything</span> /固定随机种子以确保实验的可复现性（一些 pytorch 操作比如网格采样的反向传播是不确定的）</p></li>
<li><p><span style="background:#f9eda6;">def load_everything</span> /以字典形式加载图片、位姿、相机设置等信息，将其设置为张量</p></li>
<li><p><span style="background:#daf5e9;">def _compute_bbox_by_cam_frustrm_bounded</span> /根据有边界的相机视锥体计算 bbox</p></li>
<li><p><span style="background:#f9eda6;">def _compute_bbox_by_cam_frustrm_unbounded</span> /根据无边界的相机视锥体计算 bbox：找一个立方体紧紧地包住所有相机中心</p></li>
<li><p><span style="background:#daf5e9;">def compute_bbox_by_cam_frustrm</span> /根据cfg.data.unbounded_inward是否为真选择上面两种 bbox 计算方式</p></li>
<li><p><span style="background:#f9eda6;">def compute_bbox_by_coarse_geo(no grad)</span> /根据粗阶段几何（alpha值是否达到门槛确定）确定细阶段的计算区域</p></li>
<li><p><span style="background:#daf5e9;">def create_new_model</span> /创建并返回dvgo模型和优化器：model=dvgo.DirectVoxGO</p></li>
<li><p><span style="background:#f9eda6;">def load_existed_model</span> /和 def create_new_model 一样用于初始化模型和优化器（两者是if…else… 关系）</p></li>
<li><p><span style="background:#daf5e9;">def scene_rep_reconstruction</span> /细粒度阶段的训练重建函数</p>
<details>
<summary>
<p>点击查看该函数详情</p>
</summary>
<p>
</p><ol type="1">
<li>init 初始化
<p></p>
<p>
</p><ol start="2" type="1">
<li>寻找是否存在 checkpoint 路径
<p></p>
<p>
</p><ol start="3" type="1">
<li>init 模型和优化器
<p></p>
<p>
</p><ol start="4" type="1">
<li>init 渲染参数
<p></p>
<p>
</p><ol start="5" type="1">
<li>init 批射线采样器（batch rays sampler）：<span style="background:#FFCC99;">def gather_training_rays</span>
<p></p>
<p>
</p><ol start="6" type="1">
<li>基于视角的学习率
<p></p>
<p>
</p><ol start="7" type="1">
<li>运行 GO GO！
<p></p>
<p>
&nbsp; &nbsp;​ &nbsp; &nbsp;renew occupancy grid
</p>
<p>
&nbsp; &nbsp;​ &nbsp; &nbsp;progress scaling checkpoint
</p>
<p>
&nbsp; &nbsp;​ &nbsp; &nbsp;random sample rays
</p>
<p>
&nbsp; &nbsp;​ &nbsp; &nbsp;volume rendering
</p>
<p>
&nbsp; &nbsp;​ &nbsp; &nbsp;gradient descent step
</p>
<p>
&nbsp; &nbsp;​ &nbsp; &nbsp;update lr
</p>
<p>
&nbsp; &nbsp;​ &nbsp; &nbsp;check log &amp; save
</p>
</li></ol></li></ol></li></ol></li></ol></li></ol></li></ol></li></ol></details></li>







<li><p><span style="background:#f9eda6;">def train</span> /训练函数</p>
<details>
<summary>
<p>点击查看 train 函数详情</p>
</summary>
<p>
</p><p><span style="color: #e03e2d;"><strong>init </strong>： 初始化存放输出结果的文件夹</span></p>
<p></p>
<p>
</p><p><span style="color: #3598db;"><strong>coarse geometry searching （only works for inward bounded scenes）</strong>：通过粗粒度训练中的 compute_bbox_by_cam_frustrm 寻找物体所在位置</span></p>
<p></p>
<p>
</p><p><span style="color: #e03e2d;"><strong>fine detail reconstruction </strong>：先通过 compute_bbox_by_coarse_geo 确定细粒度阶段的训练区域，再由 scene_rep_reconstruction 进行细粒度重建</span></p>
<p></p>
</details></li>
<li><p><strong><span style="background:#daf5e9;">if __name__=='__main__' </span></strong> /主函数</p>
<details>
<summary>
<p>点击查看主函数详情</p>
</summary>
<p>
</p><p><span style="color: #e67e23;"><strong>load setup </strong>：加载 parser、args 和 cfg （mmcv.Config.fromfile）等</span></p>
<p></p>
<p>
</p><p><span style="color: #169179;"><strong>init environment </strong>：设置环境为 cuda ，并通过 seed_everything 确保实验的可复现性</span></p>
<p></p>
<p>
</p><p><span style="color: #e67e23;"><strong>load images / poses / camera settings / data split </strong>：借助 load_everything 函数完成</span></p>
<p></p>
<p>
</p><p><span style="color: #169179;"><strong>export scene bbox and camera poses in 3d for debugging and visualization</strong> ：导出可视化等结果为 tar 压缩文件（coarse_last.tar）</span></p>
<p></p>
<p>
</p><p><span style="color: #e67e23;"><strong>train </strong>：如果 args.render_only 函数为假，通过 train 函数进行训练</span></p>
<p></p>
<p>
</p><p><span style="color: #169179;"><strong>load model for rendering</strong> ：根据 cfg.data 类型选择导入模型类型来进行相应渲染，设置渲染结果放入 fine_last.tar</span></p>
<p></p>
<p>
</p><p><span style="color: #e67e23;"><strong>render trainset and eval </strong>：如果相应渲染为 args.render_train，对训练渲染并将结果放入 render_train_{ckpt_name}</span></p>
<p></p>
<p>
</p><p><span style="color: #169179;"><strong>render testset and eval </strong>：如果相应渲染为 args.render_test，对测试渲染并将结果放入 render_test_{ckpt_name}</span></p>
<p></p>
<p>
</p><p><span style="color: #e67e23;"><strong>render video</strong> ：如果相应渲染为 args.video，对视频渲染并将结果放入 render_video_{ckpt_name}</span></p>
<p></p>
<p>
</p><p><span style="color: #169179;"><strong>&nbsp;输出结束信息</strong></span></p>
<p></p>
</details></li>
</ul>
<h1 id="bib-citation">Bib Citation</h1>
<figure class="highlight latex"><table><tbody><tr><td class="code"><pre><span class="line">@inproceedings{SunSC22,</span><br><span class="line">  author    = {Cheng Sun and Min Sun and Hwann{-}Tzong Chen},</span><br><span class="line">  title     = {Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction},</span><br><span class="line">  booktitle = {CVPR},</span><br><span class="line">  year      = {2022},</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h1 id="参考文献">参考文献</h1>
<p>[1] Sun C, Sun M, Chen H T. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction[C], Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 5459-5469.</p>
<p>[2] Mildenhall B, Srinivasan P P, Tancik M, et al. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis[C], European Conference on Computer Vision. Cham: Springer International Publishing, 2020: 405-421.</p>
<p>[3] Barron J T, Mildenhall B, Tancik M, et al. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields[C], Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021: 5855-5864.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">胖胖大藕片</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/03/24/DVGO%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">http://example.com/2024/03/24/DVGO 论文笔记/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">藕片侠大战论文怪</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/CVPR2022/">CVPR2022</a><a class="post-meta__tags" href="/tags/voxel-grid/">voxel grid</a><a class="post-meta__tags" href="/tags/%E8%AE%AD%E7%BB%83%E5%8A%A0%E9%80%9F/">训练加速</a><a class="post-meta__tags" href="/tags/%E6%96%B0%E8%A7%86%E5%9B%BE%E5%90%88%E6%88%90/">新视图合成</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/sonnnwwww.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer=""></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/yazi.gif" data-original="/img/wechat.jpg" alt="微信"></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/yazi.gif" data-original="/img/alipay.jpg" alt="支付宝"></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/03/24/TV-%E6%AD%A3%E5%88%99%E5%8C%96/" title="TV 正则化"><img class="cover" src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/sonnnwwww.png" onerror="onerror=null;src='/img/problem.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">TV 正则化</div></div></a></div><div class="next-post pull-right"><a href="/2024/03/24/Diffusion%20Model%20%EF%BC%88%E4%BA%8C%EF%BC%89/" title="Diffusion Model（二）"><img class="cover" src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/post_img/20231125235032.jpg" onerror="onerror=null;src='/img/problem.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Diffusion Model（二）</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/05/NeRF/" title="长江后浪扑前浪，NeRF现在沙滩上"><img class="cover" src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/post_img/20231125235032.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="title">长江后浪扑前浪，NeRF现在沙滩上</div></div></a></div></div></div><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/yazi.gif" data-original="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info__name">胖胖大藕片</div><div class="author-info__description">人生无完美，曲折亦风景</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">31</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/hahahaha5606"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/hahahaha5606" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:212188767@qq.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content"></div><timing></timing></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF"><span class="toc-number">1.</span> <span class="toc-text">论文基本信息</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E7%82%B9"><span class="toc-number">1.1.</span> <span class="toc-text">优点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#features"><span class="toc-number">1.1.1.</span> <span class="toc-text">Features</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E9%99%B7"><span class="toc-number">1.2.</span> <span class="toc-text">缺陷</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6"><span class="toc-number">2.</span> <span class="toc-text">算法框架</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#dvgo-%E5%85%A8%E8%B2%8C"><span class="toc-number">2.1.</span> <span class="toc-text">DVGO 全貌</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B1%BB%E4%BC%BC-nerf-%E7%9A%84%E4%BD%93%E6%B8%B2%E6%9F%93"><span class="toc-number">2.2.</span> <span class="toc-text">类似 NeRF 的体渲染</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%82-alpha-%E5%90%8E%E6%BF%80%E6%B4%BB%E5%AF%86%E5%BA%A6%E4%BD%93%E7%B4%A0%E7%BD%91%E6%A0%BC"><span class="toc-number">2.3.</span> <span class="toc-text">求 \(\alpha\) : 后激活密度体素网格</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%B1%E7%B2%97%E5%88%B0%E7%BB%86%E7%9A%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-number">2.4.</span> <span class="toc-text">由粗到细的训练过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#coarse-to-fine-%E7%9A%84%E5%A5%BD%E5%A4%84"><span class="toc-number">2.4.1.</span> <span class="toc-text">coarse to fine 的好处</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%93%E7%B4%A0%E5%88%86%E9%85%8D%E6%96%B9%E6%B3%95"><span class="toc-number">2.4.2.</span> <span class="toc-text">体素分配方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B2%97%E7%B2%92%E5%BA%A6%E8%AE%AD%E7%BB%83"><span class="toc-number">2.4.3.</span> <span class="toc-text">粗粒度训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%A0%E4%BD%95%E4%B8%8E%E9%A2%9C%E8%89%B2%E8%A1%A8%E7%A4%BA"><span class="toc-number">2.4.3.1.</span> <span class="toc-text">几何与颜色表示</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%8E%E5%AF%86%E5%BA%A6%E5%88%9D%E5%A7%8B%E5%8C%96%E5%85%88%E9%AA%8C"><span class="toc-number">2.4.3.2.</span> <span class="toc-text">低密度初始化先验</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E8%A7%86%E8%A7%92%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%85%88%E9%AA%8C"><span class="toc-number">2.4.3.3.</span> <span class="toc-text">基于视角的学习率先验</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%86%E7%B2%92%E5%BA%A6%E8%AE%AD%E7%BB%83"><span class="toc-number">2.4.4.</span> <span class="toc-text">细粒度训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%A0%E4%BD%95%E4%B8%8E%E9%A2%9C%E8%89%B2%E8%A1%A8%E7%A4%BA-1"><span class="toc-number">2.4.4.1.</span> <span class="toc-text">几何与颜色表示</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.4.4.2.</span> <span class="toc-text">损失函数</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">3.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82"><span class="toc-number">3.1.</span> <span class="toc-text">实现细节</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">3.2.</span> <span class="toc-text">实验结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-number">3.3.</span> <span class="toc-text">消融实验</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-number">4.</span> <span class="toc-text">代码</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84"><span class="toc-number">4.1.</span> <span class="toc-text">代码结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BB%E7%A8%8B%E5%BA%8F-run.py"><span class="toc-number">4.2.</span> <span class="toc-text">主程序 run.py</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#bib-citation"><span class="toc-number">5.</span> <span class="toc-text">Bib Citation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">6.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/04/03/%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%953%20%E6%97%A0%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" title="无题"><img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/post_img/20231125235032.jpg" onerror="this.onerror=null;this.src='/img/problem.jpg'" alt="无题"></a><div class="content"><a class="title" href="/2024/04/03/%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%953%20%E6%97%A0%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" title="无题">无题</a><time datetime="2024-04-03T11:35:54.415Z" title="发表于 2024-04-03 19:35:54">2024-04-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/03/24/TensoRF%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" title="TensoRF"><img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/sonnnwwww.png" onerror="this.onerror=null;this.src='/img/problem.jpg'" alt="TensoRF"></a><div class="content"><a class="title" href="/2024/03/24/TensoRF%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" title="TensoRF">TensoRF</a><time datetime="2024-03-24T11:59:39.634Z" title="发表于 2024-03-24 19:59:39">2024-03-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/03/24/TV-%E6%AD%A3%E5%88%99%E5%8C%96/" title="TV 正则化"><img src="/img/yazi.gif" data-original="https://cdn.jsdelivr.net/gh/hahahaha5606/blogImage/images/sonnnwwww.png" onerror="this.onerror=null;this.src='/img/problem.jpg'" alt="TV 正则化"></a><div class="content"><a class="title" href="/2024/03/24/TV-%E6%AD%A3%E5%88%99%E5%8C%96/" title="TV 正则化">TV 正则化</a><time datetime="2024-03-24T11:59:39.483Z" title="发表于 2024-03-24 19:59:39">2024-03-24</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">©2023 - 2024  <i id="heartbeat" class="fa fas fa-heartbeat"></i> 胖胖大藕片</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><p id="ghbdages"></p></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><a class="icon-V hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><svg width="25" height="25" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://blogtwikoo-one.vercel.app',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(init,0)
    else getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(init)
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://blogtwikoo-one.vercel.app',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else {
      loadTwikoo()
      
    }
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><script src="https://libs.baidu.com/jquery/2.1.4/jquery.min.js"></script><script src="/js/crash_cheat.js"></script><script src="/js/heartbeat.js"></script><script src="/js/showAppreciation.js"></script><script src="/js/timing.js"></script><script src="/js/sun_moon.js" async=""></script><script defer="" src="https://rmt.dogedoge.com/fetch/~/source/jsdelivr/npm/jquery@latest/dist/jquery.min.js"></script><script defer="" data-pjax="" src="https://cdn.jsdelivr.net/gh/sirxemic/jquery.ripples/dist/jquery.ripples.js"></script><script defer="" data-pjax="" src="/js/ripples.js"></script><script async="" data-pjax="" src="/js/anzhiyu.js"></script><script async="" data-pjax="" src="/js/anzhiyufunction.js"></script><script async="" src="/js/anzhiyuOnlyOne.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><link rel="stylesheet" href="/css/Aplayer.min.css" media="print" onload="this.media='all'"><script src="/js/Aplayer.min.js"></script><script src="/js/Meting2.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  //document.querySelectorAll('script[data-pjax]').forEach(item => {
  document.querySelectorAll('script[data-pjax], .pjax-reload script').forEach(*item* *=>* {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async="" data-pjax="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div><div class="pjax-reload"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInLeft');
    arr[i].setAttribute('data-wow-duration', '600ms');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInRightBig');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer="defer" src="https://cdn.jsdelivr.net/gh/graingert/wow@1.3.0/dist/wow.min.js"></script><script defer="defer" src="/js/wow_init.js"></script></div><div id="nav-music"><div id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()">播放音乐</div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random"></meting-js></div><!-- hexo injector body_end start --><script data-pjax="">
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo" title=""><img src="/img/yazi.gif" data-original="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly" title=""><img src="/img/yazi.gif" data-original="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://www.jsdelivr.com/" style="margin-inline:5px" data-title="本站使用JsDelivr为静态资源提供CDN加速" title=""><img src="/img/yazi.gif" data-original="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&amp;logo=jsDelivr" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" data-title="本站采用双线部署，默认线路托管于Vercel" title=""><img src="/img/yazi.gif" data-original="https://img.shields.io/badge/Hosted-Vercel-brightgreen?style=flat&amp;logo=Vercel" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" data-title="本站采用双线部署，联通线路托管于Coding" title=""><img src="/img/yazi.gif" data-original="https://img.shields.io/badge/Hosted-Coding-0cedbe?style=flat&amp;logo=Codio" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Gtihub托管" title=""><img src="/img/yazi.gif" data-original="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src="/img/yazi.gif" data-original="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async="" src="/js/runtime_huibiao.js"></script><!-- hexo injector body_end end -->
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a,i=c[o];e=function(){c=c.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(n=new Image,a=t.getAttribute("data-original"),n.onload=function(){t.src=a,t.removeAttribute("data-original"),e&&e()},t.src!==a&&(n.src=a))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script></body></html>